means$size <- 5
# create a plot with the decision boundary and a large point for each class mean
ggplot(grid, aes(x = X1, y = X2, color = class)) +
geom_point(alpha = 0.1) +
geom_point(data = means, aes(x = X1, y = X2, color = class, size = size)) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2")
View(means)
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2")
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = class, size = size)) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2")
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = class, size = size, shape=2)) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2")
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = class, size = size), shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2")
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = class, size = size), shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
theme(legend.position = "none")
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = class, size = size), shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
theme(legend.position = "none")
library(MASS) # For lda()
set.seed(100) # Ensure reproducibility
# Simulate data for two classes
features <- mvrnorm(n = 100, mu = c(0, 0), Sigma = matrix(c(2, 1, 1, 2), ncol = 2))
labels <- c(rep(0, 50), rep(1, 50))
data <- data.frame(features, class = factor(labels))
# plot the data
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() +
labs(title = "Simulated Data for Two Classes",
subtitle = "Each class is normally distributed with a different mean",
x = "Feature 1", y = "Feature 2")
# Fit LDA model
lda_fit <- lda(class ~ ., data = data)
# plot the decision boundary
x <- seq(-5, 5, length.out = 1000)
y <- seq(-5, 5, length.out = 1000)
grid <- expand.grid(X1 = x, X2 = y)
grid$class <- predict(lda_fit, newdata = grid)$class
means <- as.data.frame(lda_fit$means)
means$class <- factor(0:1)
means$size <- 5
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = class, size = size), shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
theme(legend.position = "none")
# Simulate data for two classes
features <- mvrnorm(n = 100, mu = c(0, 3), Sigma = matrix(c(2, 1, 1, 2), ncol = 2))
labels <- c(rep(0, 50), rep(1, 50))
data <- data.frame(features, class = factor(labels))
# plot the data
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() +
labs(title = "Simulated Data for Two Classes",
subtitle = "Each class is normally distributed with a different mean",
x = "Feature 1", y = "Feature 2")
# Fit LDA model
lda_fit <- lda(class ~ ., data = data)
# plot the decision boundary
x <- seq(-5, 5, length.out = 1000)
y <- seq(-5, 5, length.out = 1000)
grid <- expand.grid(X1 = x, X2 = y)
grid$class <- predict(lda_fit, newdata = grid)$class
means <- as.data.frame(lda_fit$means)
means$class <- factor(0:1)
means$size <- 5
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = class, size = size), shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
theme(legend.position = "none")
library(MASS) # For lda()
set.seed(100) # Ensure reproducibility
mu1 <- c(-3, 0) # Mean for class 1
mu2 <- c(3, 0)  # Mean for class 2
Sigma <- matrix(c(2, 1, 1, 2), ncol = 2) # Same covariance matrix for both classes
# Generate data
class1 <- mvrnorm(n = 50, mu = mu1, Sigma = Sigma)
class2 <- mvrnorm(n = 50, mu = mu2, Sigma = Sigma)
data <- rbind(class1, class2)
labels <- factor(c(rep(0, 50), rep(1, 50)))
data <- data.frame(data, class = labels)
# plot the data
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() +
labs(title = "Simulated Data for Two Classes",
subtitle = "Each class is normally distributed with a different mean",
x = "Feature 1", y = "Feature 2")
library(MASS) # For lda()
set.seed(100) # Ensure reproducibility
mu1 <- c(-2, 0) # Mean for class 1
mu2 <- c(2, 0)  # Mean for class 2
Sigma <- matrix(c(2, 1, 1, 2), ncol = 2) # Same covariance matrix for both classes
# Generate data
class1 <- mvrnorm(n = 50, mu = mu1, Sigma = Sigma)
class2 <- mvrnorm(n = 50, mu = mu2, Sigma = Sigma)
data <- rbind(class1, class2)
labels <- factor(c(rep(0, 50), rep(1, 50)))
data <- data.frame(data, class = labels)
# plot the data
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() +
labs(title = "Simulated Data for Two Classes",
subtitle = "Each class is normally distributed with a different mean",
x = "Feature 1", y = "Feature 2")
library(MASS) # For lda()
set.seed(100) # Ensure reproducibility
mu1 <- c(-1, 0) # Mean for class 1
mu2 <- c(1, 0)  # Mean for class 2
Sigma <- matrix(c(2, 1, 1, 2), ncol = 2) # Same covariance matrix for both classes
# Generate data
class1 <- mvrnorm(n = 50, mu = mu1, Sigma = Sigma)
class2 <- mvrnorm(n = 50, mu = mu2, Sigma = Sigma)
data <- rbind(class1, class2)
labels <- factor(c(rep(0, 50), rep(1, 50)))
data <- data.frame(data, class = labels)
# plot the data
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() +
labs(title = "Simulated Data for Two Classes",
subtitle = "Each class is normally distributed with a different mean",
x = "Feature 1", y = "Feature 2")
# Fit LDA model
lda_fit <- lda(class ~ ., data = data)
# plot the decision boundary
x <- seq(-5, 5, length.out = 1000)
y <- seq(-5, 5, length.out = 1000)
grid <- expand.grid(X1 = x, X2 = y)
grid$class <- predict(lda_fit, newdata = grid)$class
means <- as.data.frame(lda_fit$means)
means$class <- factor(0:1)
means$size <- 5
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = class, size = size), shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
theme(legend.position = "none")
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = class, size = 10), shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
theme(legend.position = "none")
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = class, size = 30), shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
theme(legend.position = "none")
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 10,shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
theme(legend.position = "none")
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
theme(legend.position = "none")
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
# add a legend that states the class and that the means are shown
guides(color = guide_legend(title = "Class"), size = guide_legend(title = "Mean"))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
# add a legend that states the class and that the second geom poionts represent the means of the class
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
# add a legend that states the class and that the second geom poionts represent the means of the class
guides(color = guide_legend(title = "Class", override.aes = list(size = 3))) +
theme(legend.position = "bottom") +
theme(legend.title = element_text(size = 12, face = "bold"),
legend.text = element_text(size = 10)) +
theme(legend.key.size = unit(0.5, "cm"))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = black), size = 7, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
# add a legend that states the class and that the second geom poionts represent the means of the class
guides(color = guide_legend(title = "Class", override.aes = list(size = 3))) +
theme(legend.position = "bottom") +
theme(legend.title = element_text(size = 12, face = "bold"),
legend.text = element_text(size = 10)) +
theme(legend.key.size = unit(0.5, "cm"))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = black), size = 7, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
# add a legend that states the class and that the second geom poionts represent the means of the class
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = 'black'), size = 7, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
# add a legend that states the class and that the second geom poionts represent the means of the class
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2),color = 'black', size = 7, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
# add a legend that states the class and that the second geom poionts represent the means of the class
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2),color = 'black', size = 8, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
# add a legend that states the class and that the second geom poionts represent the means of the class
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2),color = 'black', size = 8, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "Diamonds represent Class Means",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
# add a legend that states the class and that the second geom poionts represent the means of the class
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2),color = 'black', size = 8, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "Diamonds represent Class Means",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c('#E69F00', '#56B4E9')) +
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
# plot the data
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
labs(title = "LDA Decision Boundary",
subtitle = "Diamonds represent Class Means",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c('#E69F00', '#56B4E9')) +
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
# plot the data
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
labs(title = "LDA Decision Boundary",
subtitle = 'Simulated Data',
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c('#E69F00', '#56B4E9')) +
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
# plot the data
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
labs(title = "Plot of the Data",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c('#E69F00', '#56B4E9')) +
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
# Fit LDA model
lda_fit <- lda(class ~ ., data = data)
# plot the decision boundary
x <- seq(-5, 5, length.out = 1000)
y <- seq(-5, 5, length.out = 1000)
grid <- expand.grid(X1 = x, X2 = y)
grid$class <- predict(lda_fit, newdata = grid)$class
means <- as.data.frame(lda_fit$means)
means$class <- factor(0:1)
means$size <- 5
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2),color = 'black', size = 8, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "Diamonds represent Class Means",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c('#E69F00', '#56B4E9')) +
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2), color = 'black', size = 8, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "Diamonds represent Class Means",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c('#E69F00', '#56B4E9')) +
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2), color = 'black', size = 8, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "Diamonds represent Class Means",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c('#0e4a6b', '#ed7b2e')) +
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
## Load the packages
library(ISLR)
library(FNN)
default_data <- Default
summary(default_data)
default_data$default <- as.numeric(default_data$default == "Yes")
default_data$student <- as.numeric(default_data$student == "Yes")
set.seed(222)
## First pick the test observations (20% of the data)
test_obs <- sample(seq(nrow(default_data)),
round(0.2 * nrow(default_data)))
## The training observations are the remaining observations
train_obs <- setdiff(seq(nrow(default_data)), test_obs)
## Use the indices now to extract the corresponding subsets of the data
test_data <- default_data[test_obs,]
train_data <- default_data[train_obs,]
class_predictions <- as.numeric(logistic_predict > 0.5)
logistic_predict <-predict(logistic_default,
test_data,
type = "response")
## Load the packages
library(ISLR)
library(FNN)
default_data <- Default
summary(default_data)
summary(default_data)
default_data$default <- as.numeric(default_data$default == "Yes")
default_data$student <- as.numeric(default_data$student == "Yes")
summary(default_data)
set.seed(222)
(seq(nrow(default_data))
)
## First pick the test observations (20% of the data)
test_obs <- sample(seq(nrow(default_data)),
round(0.2 * nrow(default_data)))
test_obs
## The training observations are the remaining observations
train_obs <- setdiff(seq(nrow(default_data)), test_obs)
## Use the indices now to extract the corresponding subsets of the data
test_data <- default_data[test_obs,]
train_data <- default_data[train_obs,]
logistic_default <- glm(default ~ student + balance + income,
family = binomial,
data = train_data)
summary(logistic_default)
logistic_predict <-predict(logistic_default,
test_data,
type = "response")
logistic_predict
head(logistic_predict)
class_predictions <- as.numeric(logistic_predict > 0.5)
class_predictions
logistic_accuracy <- mean(class_predictions == test_data$default)
print(logistic_accuracy)
predict(logistic_default,test_data)
logistic_predict
true_pos_accuracy <- mean(class_predictions[which(test_data$default == 1)] == 1)
print(true_pos_accuracy)
true_neg_accuracy <- mean(class_predictions[which(test_data$default == 0)] == 0)
print(true_neg_accuracy)
table(class_predictions, test_data$default)
true_pos_error <- mean(class_predictions
[which(test_data$default == 1)] != 1)
print(true_pos_error)
true_neg_error <- mean(class_predictions
[which(test_data$default == 0)] != 0)
print(true_neg_error)
## First, we need to specify the list of threshold values to assess
threshold_values <- seq(from = 0.00, to = 0.50, by = 0.01)
threshold_values
error_rates <- matrix(0, nrow = length(threshold_values), ncol = 2)
error_rates
error_rates
threshold_values
indx <- 0
for(threshold in threshold_values) {
## Update the tracker to reflect the row
indx <- indx + 1
## Then generate the predicted classes using the specific threshold
class_predictions <- as.numeric(logistic_predict > threshold)
## Then calculate the true positive accuracy
true_pos_accuracy <- mean(class_predictions[which(test_data$default == 1)] == 1)
## Then calculate the true negative accuracy
true_neg_accuracy <- mean(class_predictions[which(test_data$default == 0)] == 0)
## Now we can add the results to our matrix
error_rates[indx,] <- c(true_pos_accuracy, true_neg_accuracy)
}
error_rates
matplot(x = threshold_values,
y = error_rates,
type = "l",
col = 3:4,
xlab = "Threshold",
ylab = "Accuracy",
main = "Accuracy as a Function of Threshold")
legend("topright", legend = c("Defaulters", "Non-defaulters"),
col = 3:4, pch = 1)
library(MASS)
lda_model <- lda(default ~., data = train_data)
lda_pred <- predict(lda_model, test_data[,-1])
lda_pred
class_pred_lda <- lda_pred$class
error_lda <- mean(class_pred_lda !=
test_data$default)
print(error_lda)
1-0.9745
print(error_lda)
1-0.9745
print(lda_model))
print(lda_model)
qda_model <- qda(default ~., data = train_data)
qda_pred <- predict(qda_model, test_data[,-1])
class_pred_qda <- qda_pred$class
error_qda <- mean(class_pred_qda !=
test_data$default)
print(error_qda)
