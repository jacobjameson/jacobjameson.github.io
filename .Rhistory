lda_model <- lda(default ~., data = train_data)
lda_pred <- predict(lda_model, test_data[,-1])
class_pred_lda <- lda_pred$class
error_lda <- mean(class_pred_lda !=
test_data$default)
print(error_lda)
qda_model <- qda(default ~., data = train_data)
qda_pred <- predict(qda_model, test_data[,-1])
class_pred_qda <- qda_pred$class
error_qda <- mean(class_pred_qda !=
test_data$default)
print(error_qda)
library(ROCR)
pred <- prediction(logistic_predict, test_data$default)
perf <- performance(pred, "tpr", "fpr")
plot(perf)
install.packages('ROCR')
library(ggplot2)
library(dplyr)
knitr::opts_chunk$set(fig.path = "images/")
# Generate some sample data
set.seed(123)
data <- data.frame(
x = rep(1:4, each = 25),
y = rnorm(100, mean = 5, sd = 2)
)
# Create plots for each section
plot_intro <- ggplot(data, aes(x = factor(x), y = y)) +
geom_boxplot() +
theme_minimal() +
labs(title = "Introduction", x = "Group", y = "Value")
plot_method <- ggplot(data, aes(x = x, y = y)) +
geom_point() +
geom_smooth(method = "lm") +
theme_minimal() +
labs(title = "Methodology", x = "X", y = "Y")
plot_results <- ggplot(data, aes(x = y)) +
geom_histogram(bins = 20, fill = "skyblue", color = "black") +
theme_minimal() +
labs(title = "Results", x = "Value", y = "Count")
plot_conclusion <- ggplot(data, aes(x = factor(x), y = y)) +
geom_violin() +
theme_minimal() +
labs(title = "Conclusion", x = "Group", y = "Value")
# Save plots
ggsave("images/plot_intro.png", plot_intro, width = 6, height = 4)
# Generate some sample data
set.seed(123)
data <- data.frame(
x = rep(1:4, each = 25),
y = rnorm(100, mean = 5, sd = 2)
)
# Create plots for each section
plot_intro <- ggplot(data, aes(x = factor(x), y = y)) +
geom_boxplot() +
theme_minimal() +
labs(title = "Introduction", x = "Group", y = "Value")
plot_method <- ggplot(data, aes(x = x, y = y)) +
geom_point() +
geom_smooth(method = "lm") +
theme_minimal() +
labs(title = "Methodology", x = "X", y = "Y")
plot_results <- ggplot(data, aes(x = y)) +
geom_histogram(bins = 20, fill = "skyblue", color = "black") +
theme_minimal() +
labs(title = "Results", x = "Value", y = "Count")
plot_conclusion <- ggplot(data, aes(x = factor(x), y = y)) +
geom_violin() +
theme_minimal() +
labs(title = "Conclusion", x = "Group", y = "Value")
# Save plots
ggsave("images/plot_intro.png", plot_intro, width = 6, height = 4)
ggsave("images/plot_method.png", plot_method, width = 6, height = 4)
ggsave("images/plot_results.png", plot_results, width = 6, height = 4)
ggsave("images/plot_conclusion.png", plot_conclusion, width = 6, height = 4)
diabetes_data <- read.csv("diabetes.csv", header = TRUE)
dim(diabetes_data)
summary(diabetes_data)
library(class)
function_name <- function(arg_1, arg_2, ...) {
Function body
## Example of a simple function
add_fun <- function(a1, a2){
res = a1 + a2
return(res)
}
add_fun(3, 4)
rep(seq(5), ceiling(nrow(diabetes_data) / 5))
fold_ids <- rep(seq(5), ceiling(nrow(diabetes_data) / 5))
fold_ids <- fold_ids[1:nrow(diabetes_data)]
fold_ids <- rep(seq(5), ceiling(nrow(diabetes_data) / 5))
fold_ids <- fold_ids[1:nrow(diabetes_data)]
sample(fold_ids, length(fold_ids))
cross_validation_KNN  <- function(data_x, data_y, k_seq, kfolds) {
## We will start by assigning each observation to one and
## only one fold for cross-validation. To do this, we use
## an index vector. The vector's length equals the number
## of observations in the data. The vector's entries are
## equal numbers of 1s, 2s, etc. up to the number of folds
## being used for k-fold cross validation.
## Recall seq(5) is a vector (1, 2, 3, 4, 5)
## ceiling() rounds a number up to the nearest integer
## rep(a, b) repeats a b times (e.g. rep(1, 3) => (1, 1, 1))
## So this says repeat the sequence from 1:kfolds
## enough times to fill a vector that is as long or
## slightly longer than the number of observations in
## the data. Then, if the length of the vector exceeds
## the number of observations, truncate it to the right
## length
fold_ids <- rep(seq(kfolds), ceiling(nrow(data_x) / kfolds))
fold_ids <- fold_ids[1:nrow(data_x)]
## To make the IDs random, randomly rearrange the vector
fold_ids <- sample(fold_ids, length(fold_ids))
## In order to store the prediction performance for each fold
## for each k, we initialize a matrix.
CV_error_mtx  <- matrix(0, nrow = length(k_seq), ncol = kfolds)
## To run CV, we will loop over all values of k that we want to
## consider for KNN. For each value of k, we will loop over all
## folds. For each fold, we will estimate KNN for the given k on
## all but one fold. We will then measure the model's accuracy
## on the hold out fold and save it. After we finish looping
## through all k's and all folds, we will find the average CV
## error for each value of k and use that as a measure of the
## model's performance.
for (k in k_seq) {
for (fold in 1:kfolds) {
## Train the KNN model (Note: if it throws a weird error, make sure
## all features are numeric variables -- not factors)
## Note: usually the features are normalized/re-scaled
## Otherwise KNN will give more weight to variables at the larger scale
## when calculating the distance metric.
## See page 165 of the textbook for a useful example
knn_fold_model <- knn(train = scale(data_x[which(fold_ids != fold),]),
test = scale(data_x[which(fold_ids == fold),]),
cl = data_y[which(fold_ids != fold)],
k = k)
## Measure and save error rate (% wrong)
CV_error_mtx[k, fold] <- mean(knn_fold_model !=
data_y[which(fold_ids == fold)])
}
}
## We want our function to return the accuracy matrix
return(CV_error_mtx)
}
set.seed(222)
knn_cv_error5 <- cross_validation_KNN(data_x = diabetes_data[, -9],
data_y = diabetes_data$Outcome,
k_seq = seq(20),
kfolds = 5)
print(knn_cv_error5)
knn_cv_error10 <- cross_validation_KNN(data_x = diabetes_data[,-9],
data_y = diabetes_data$Outcome,
k_seq = seq(50),
kfolds = 10)
print(knn_cv_error10)
mean_cv_error5 <- rowMeans(knn_cv_error5)
mean_cv_error10 <- rowMeans(knn_cv_error10)
which.min(mean_cv_error5)
which.min(mean_cv_error10)
plot(seq(20), mean_cv_error5, type = "l",
main = "Mean CV Error Rate as a Function of k",
ylab = "CV Error Rate", xlab = "k")
diabetes_data <- read.csv("diabetes.csv", header = TRUE)
dim(diabetes_data)
summary(diabetes_data)
dim(diabetes_data)
summary(diabetes_data)
library(class)
## Example of a simple function
add_fun <- function(a1, a2){
res = a1 + a2
return(res)
}
add_fun(3, 4)
seq(5)
seq(10)
nrow(diabetes_data)
nrow(diabetes_data) / 5
ceiling(nrow(diabetes_data) / 5)
fold_ids <- rep(seq(5), ceiling(nrow(diabetes_data) / 5))
fold_ids
fold_ids <- fold_ids[1:nrow(diabetes_data)]
sample(fold_ids, length(fold_ids))
CV_error_mtx  <- matrix(0, nrow = length(1:40), ncol = 5)
CV_error_mtx
cross_validation_KNN  <- function(data_x, data_y, k_seq, kfolds) {
## We will start by assigning each observation to one and
## only one fold for cross-validation. To do this, we use
## an index vector. The vector's length equals the number
## of observations in the data. The vector's entries are
## equal numbers of 1s, 2s, etc. up to the number of folds
## being used for k-fold cross validation.
## Recall seq(5) is a vector (1, 2, 3, 4, 5)
## ceiling() rounds a number up to the nearest integer
## rep(a, b) repeats a b times (e.g. rep(1, 3) => (1, 1, 1))
## So this says repeat the sequence from 1:kfolds
## enough times to fill a vector that is as long or
## slightly longer than the number of observations in
## the data. Then, if the length of the vector exceeds
## the number of observations, truncate it to the right
## length
fold_ids <- rep(seq(kfolds), ceiling(nrow(data_x) / kfolds))
fold_ids <- fold_ids[1:nrow(data_x)]
## To make the IDs random, randomly rearrange the vector
fold_ids <- sample(fold_ids, length(fold_ids))
## In order to store the prediction performance for each fold
## for each k, we initialize a matrix.
CV_error_mtx  <- matrix(0, nrow = length(k_seq), ncol = kfolds)
## To run CV, we will loop over all values of k that we want to
## consider for KNN. For each value of k, we will loop over all
## folds. For each fold, we will estimate KNN for the given k on
## all but one fold. We will then measure the model's accuracy
## on the hold out fold and save it. After we finish looping
## through all k's and all folds, we will find the average CV
## error for each value of k and use that as a measure of the
## model's performance.
for (k in k_seq) {
for (fold in 1:kfolds) {
## Train the KNN model (Note: if it throws a weird error, make sure
## all features are numeric variables -- not factors)
## Note: usually the features are normalized/re-scaled
## Otherwise KNN will give more weight to variables at the larger scale
## when calculating the distance metric.
## See page 165 of the textbook for a useful example
knn_fold_model <- knn(train = scale(data_x[which(fold_ids != fold),]),
test = scale(data_x[which(fold_ids == fold),]),
cl = data_y[which(fold_ids != fold)],
k = k)
## Measure and save error rate (% wrong)
CV_error_mtx[k, fold] <- mean(knn_fold_model !=
data_y[which(fold_ids == fold)])
}
}
## We want our function to return the accuracy matrix
return(CV_error_mtx)
}
set.seed(222)
knn_cv_error5 <- cross_validation_KNN(data_x = diabetes_data[, -9],
data_y = diabetes_data$Outcome,
k_seq = seq(20),
kfolds = 5)
print(knn_cv_error5)
knn_cv_error10 <- cross_validation_KNN(data_x = diabetes_data[,-9],
data_y = diabetes_data$Outcome,
k_seq = seq(50),
kfolds = 10)
print(knn_cv_error10)
knn_cv_error10 <- cross_validation_KNN(data_x = diabetes_data[,-9],
data_y = diabetes_data$Outcome,
k_seq = seq(50),
kfolds = nrow(diabetes_data))
knn_cv_error10 <- cross_validation_KNN(data_x = diabetes_data[,-9],
data_y = diabetes_data$Outcome,
k_seq = seq(50),
kfolds = 10)
print(knn_cv_error10)
mean_cv_error5 <- rowMeans(knn_cv_error5)
mean_cv_error10 <- rowMeans(knn_cv_error10)
which.min(mean_cv_error5)
which.min(mean_cv_error10)
set.seed(222)
knn_cv_error5 <- cross_validation_KNN(data_x = diabetes_data[, -9],
data_y = diabetes_data$Outcome,
k_seq = seq(20),
kfolds = 5)
print(knn_cv_error5)
knn_cv_error10 <- cross_validation_KNN(data_x = diabetes_data[,-9],
data_y = diabetes_data$Outcome,
k_seq = seq(50),
kfolds = 10)
print(knn_cv_error10)
mean_cv_error5 <- rowMeans(knn_cv_error5)
mean_cv_error10 <- rowMeans(knn_cv_error10)
which.min(mean_cv_error5)
which.min(mean_cv_error10)
plot(seq(20), mean_cv_error5, type = "l",
main = "Mean CV Error Rate as a Function of k",
ylab = "CV Error Rate", xlab = "k")
plot(seq(50), mean_cv_error10, type = "l",
main = "Mean CV Error Rate as a Function of k",
ylab = "CV Error Rate", xlab = "k")
table(diabetes_data$Outcome)
cross_validation <- function(full_data, model_type, kfolds,
logistic_formula = NULL) {
## Define fold_ids in exactly the same way as before
fold_ids <- rep(seq(kfolds), ceiling(nrow(full_data) / kfolds))
fold_ids <- fold_ids[1:nrow(full_data)]
fold_ids <- sample(fold_ids, length(fold_ids))
## Initialize a vector to store CV error
CV_error_vec  <- vector(length = kfolds, mode = "numeric")
## Loop through the folds
for (k in 1:kfolds){
if (model_type == "logistic") {
logistic_model <- glm(logistic_formula,
data = full_data[which(fold_ids != k),],
family = binomial)
logistic_pred <- predict(logistic_model,
full_data[which(fold_ids == k),],
type = "response")
class_pred <- as.numeric(logistic_pred > 0.5)
} else if (model_type == "LDA") {
lda_model <- lda(full_data[which(fold_ids != k),-9],
full_data[which(fold_ids != k),9])
lda_pred <- predict(lda_model, full_data[which(fold_ids == k), -9])
class_pred <- lda_pred$class
} else if (model_type == "QDA") {
qda_model <- qda(full_data[which(fold_ids != k), -9],
full_data[which(fold_ids != k), 9])
qda_pred <- predict(qda_model,
full_data[which(fold_ids == k), -9])
class_pred <- qda_pred$class
}
CV_error_vec[k] <- mean(class_pred != full_data[which(fold_ids == k), 9])
}
return(CV_error_vec)
}
## Run CV for logistic regression:
logistic_formula <- paste("Outcome", paste(colnames(diabetes_data)[1:8],
collapse = " + "),
sep = " ~ ")
logistic_formula
logistic_CV_error <- cross_validation(diabetes_data, "logistic", 5,
as.formula(logistic_formula))
## Run CV for LDA:
lda_CV_error <- cross_validation(diabetes_data, "LDA", 5)
library(MASS)
cross_validation <- function(full_data, model_type, kfolds,
logistic_formula = NULL) {
## Define fold_ids in exactly the same way as before
fold_ids <- rep(seq(kfolds), ceiling(nrow(full_data) / kfolds))
fold_ids <- fold_ids[1:nrow(full_data)]
fold_ids <- sample(fold_ids, length(fold_ids))
## Initialize a vector to store CV error
CV_error_vec  <- vector(length = kfolds, mode = "numeric")
## Loop through the folds
for (k in 1:kfolds){
if (model_type == "logistic") {
logistic_model <- glm(logistic_formula,
data = full_data[which(fold_ids != k),],
family = binomial)
logistic_pred <- predict(logistic_model,
full_data[which(fold_ids == k),],
type = "response")
class_pred <- as.numeric(logistic_pred > 0.5)
} else if (model_type == "LDA") {
lda_model <- lda(full_data[which(fold_ids != k),-9],
full_data[which(fold_ids != k),9])
lda_pred <- predict(lda_model, full_data[which(fold_ids == k), -9])
class_pred <- lda_pred$class
} else if (model_type == "QDA") {
qda_model <- qda(full_data[which(fold_ids != k), -9],
full_data[which(fold_ids != k), 9])
qda_pred <- predict(qda_model,
full_data[which(fold_ids == k), -9])
class_pred <- qda_pred$class
}
CV_error_vec[k] <- mean(class_pred != full_data[which(fold_ids == k), 9])
}
return(CV_error_vec)
}
## Run CV for logistic regression:
logistic_formula <- paste("Outcome", paste(colnames(diabetes_data)[1:8],
collapse = " + "),
sep = " ~ ")
logistic_CV_error <- cross_validation(diabetes_data, "logistic", 5,
as.formula(logistic_formula))
## Run CV for LDA:
lda_CV_error <- cross_validation(diabetes_data, "LDA", 5)
## Run CV for QDA:
qda_CV_error <- cross_validation(diabetes_data, "QDA", 5)
## Determine the best model in terms of lowest average CV error
print(paste("Logistic Error Rate:", round(mean(logistic_CV_error), 3)))
print(paste("LDA Error Rate:", round(mean(lda_CV_error), 3)))
print(paste("QDA Error Rate:", round(mean(qda_CV_error), 3)))
library(pls)
library(ISLR)
insurance_data <- Caravan
?Caravan
library(glmnet)
?cv.glmnet
as.matrix(insurance_data[, 1:85])
as.numeric(insurance_data[, 86]
)
View(insurance_data)
set.seed(222) # Important for replicability
lasso_ins <- cv.glmnet(x = as.matrix(insurance_data[, 1:85]), # the features
y = as.numeric(insurance_data[, 86]), # the outcome
standardize = TRUE, # Why do we do this?
alpha = 1) # Corresponds to LASSO
print(lasso_ins$lambda)
print(lasso_ins$lambda.min)
plot(lasso_ins)
LassoCV <- lasso_ins$glmnet.fit
plot(LassoCV, label = TRUE, xvar = "lambda")
abline(
v = log(c(lasso_ins$lambda.min, lasso_ins$lambda.1se))
) # Adds lines to mark the two key lambda values
predict(lasso_ins, type = "coefficients",
s = c(lasso_ins$lambda.min, lasso_ins$lambda.1se))
lasso_ins$lambda.min
lasso_ins$cvm[lasso_ins$lambda == lasso_ins$lambda.min]
lasso_ins$cvsd[lasso_ins$lambda == lasso_ins$lambda.min]
lasso_ins$lambda.1se
wage_data <- Wage
summary(wage_data)
str(wage_data)
levels(wage_data$maritl)
levels(wage_data$region)
View(wage_data)
wage_data <- wage_data[, -11]
for(i in 10:1){
if(is.factor(wage_data[, i])){
for(j in unique(wage_data[, i])){
new_col <- paste(colnames(wage_data)[i], j, sep = "_")
wage_data[, new_col] <- as.numeric(wage_data[, i] == j)
}
wage_data <- wage_data[, -i]
} else if(typeof(wage_data[, i]) == "integer") {
wage_data[, i] <- as.numeric(as.character(wage_data[, i]))
}
}
#View(wage_data)
summary(wage_data)
str(wage_data)
set.seed(222)
train <- sample(seq(nrow(wage_data)),
floor(nrow(wage_data)*0.8))
train <- sort(train)
test <- which(!(seq(nrow(wage_data)) %in% train))
library(pls)
library(pls)
## Try running PCR
pcr_fit  <- pcr(logwage ~ ., data = wage_data[train,],
scale = TRUE, validation = "CV")
train <- sample(seq(nrow(wage_data)),
floor(nrow(wage_data)*0.8))
train <- sort(train)
test <- which(!(seq(nrow(wage_data)) %in% train))
library(pls)
## Try running PCR
pcr_fit  <- pcr(logwage ~ ., data = wage_data[train,],
scale = TRUE, validation = "CV")
wage_data[train,]
wage_data <- Wage
summary(wage_data)
wage_data <- wage_data[, -11]
for(i in 10:1){
if(is.factor(wage_data[, i])){
for(j in unique(wage_data[, i])){
new_col <- paste(colnames(wage_data)[i], j, sep = "_")
wage_data[, new_col] <- as.numeric(wage_data[, i] == j)
}
wage_data <- wage_data[, -i]
} else if(typeof(wage_data[, i]) == "integer") {
wage_data[, i] <- as.numeric(as.character(wage_data[, i]))
}
}
#View(wage_data)
summary(wage_data)
str(wage_data)
set.seed(222)
train <- sample(seq(nrow(wage_data)),
floor(nrow(wage_data)*0.8))
train <- sort(train)
test <- which(!(seq(nrow(wage_data)) %in% train))
## to drop them from the analysis or not to scale the data.
for(col_num in 1:ncol(wage_data)){
if(var(wage_data[, col_num]) < 0.05){
print(colnames(wage_data)[col_num])
print(var(wage_data[, col_num]))
}
}
## Let's drop these low variance columns
for(col_num in ncol(wage_data):1){
if(var(wage_data[, col_num]) < 0.05) {
wage_data <- wage_data[, -col_num]
}
}
set.seed(222)
pcr_fit <- pcr(logwage ~ ., data = wage_data[train,],
scale = TRUE, validation = "CV")
summary(pcr_fit)
pcr_msep <- MSEP(pcr_fit)
pcr_min_indx <- which.min(pcr_msep$val[1, 1,])
print(pcr_min_indx)
print(pcr_msep$val[1, 1, pcr_min_indx])
validationplot(pcr_fit)
pcr_pred <- predict(pcr_fit, wage_data[test, ], ncomp = 12)
pcr_test_MSE <- mean((pcr_pred - wage_data[test, "logwage"])^2)
print(pcr_test_MSE)
print(sqrt(pcr_test_MSE))
## Step 1: Fit the model
pls_fit <- plsr(logwage ~ ., data = wage_data[train, ],
scale = TRUE, validation = "CV")
summary(pls_fit)
## Step 2: Which ncomp value had the lowest CV MSE?
pls_msep <- MSEP(pls_fit)
pls_min_indx <- which.min(pls_msep$val[1, 1,])
print(pls_min_indx)
## Step 3: Plot validation error as a function of # of components
validationplot(pls_fit, val.type = c("RMSEP"))
## Step 4: Identify the CV RMSE for the number of components with
## the lowest CV RMSE
pls_rmsep <- RMSEP(pls_fit)
print(pls_rmsep$val[1, 1, as.numeric(pls_min_indx)])
## Step 5: Predict test set logwage values
pls_pred <- predict(pls_fit, wage_data[test,],
ncomp = (as.numeric(pls_min_indx) -1))
## Step 6: Measure test MSE and RMSE
pls_test_MSE <- mean((pls_pred - wage_data[test, "logwage"])^2)
print(pls_test_MSE)
print(sqrt(pls_test_MSE))
