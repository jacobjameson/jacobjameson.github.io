mutate(across(c(PD, Primary_type,
Sex, BRAF_status, NRAS_status), as.factor),
BRAF_mutant = ifelse(BRAF_status == "MUTANT", 1, 0),
NRAS_mutant = ifelse(NRAS_status == "MUTANT", 1, 0),
primary_skin = ifelse(Primary_type == "Skin", 1, 0),
sex_male = ifelse(Sex == "MALE", 1, 0)) %>%
dplyr::select(-Primary_type, -BRAF_status, -NRAS_status, -Sex,
-`Sample ID`, -BR, -RvsP, -cohort, -os_days,
-dead, -DT_pred, -DT_mod_pred, -ploidy_q, -het_q, -TMB_q,
-logreg_pred, -PD_cat, X.clonal = `#clonal`,
X.subclon = `#subclon`)
library(data.table)
library(caret)
library(Matrix)
library(tidyverse)
library(pROC)
library(MASS)
library(rpart)
library(rpart.plot)
library(e1071)
library(kernlab)  # For SVM with caret
# Load data ---------------------------------------------------------------------
sup.1 <- fread("data/supplementary_table1.tsv") %>%
as.data.frame() %>%
dplyr::select(where(~ !any(is.na(.)))) %>%
mutate(across(c(PD, Primary_type,
Sex, BRAF_status, NRAS_status), as.factor),
BRAF_mutant = ifelse(BRAF_status == "MUTANT", 1, 0),
NRAS_mutant = ifelse(NRAS_status == "MUTANT", 1, 0),
primary_skin = ifelse(Primary_type == "Skin", 1, 0),
sex_male = ifelse(Sex == "MALE", 1, 0)) %>%
dplyr::select(-Primary_type, -BRAF_status, -NRAS_status, -Sex,
-`Sample ID`, -BR, -RvsP, -cohort, -os_days,
-dead, -DT_pred, -DT_mod_pred, -ploidy_q, -het_q, -TMB_q,
-logreg_pred, -PD_cat, X.clonal = `#clonal`,
X.subclon = `#subclon`)
sup.2 <- fread("data/supplementary_table2.tsv") %>%
as.data.frame()
sup.3 <- fread("data/supplementary_table3.tsv") %>%
as.data.frame() %>%
dplyr::rename(X.clonal = `#clonal`, X.subclon = `#subclon`)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
library(ISLR)
?ISLR
??ISLR
data('Credit')
plot(Credit$Income, Credit$Balance,
xlab = "Income",
ylab = "Balance",
main = "Scatter plot of Balance vs. Income")
# make it prettier
plot(Credit$Income, Credit$Balance,
xlab = "Income",
ylab = "Balance",
main = "Scatter plot of Balance vs. Income",
col = "blue", pch = 19)
ggplot(Credit, aes(x = Income, y = Balance)) +
geom_point(color = "blue", alpha = 0.5) +
labs(title = "Scatter plot of Balance vs. Income",
x = "Income",
y = "Balance") +
theme_minimal()
# make it aesthetic and use ggplot
library(ggplot2)
# make it aesthetic and use ggplot
library(ggplot2)
ggplot(Credit, aes(x = Income, y = Balance)) +
geom_point(color = "blue", alpha = 0.5) +
labs(title = "Scatter plot of Balance vs. Income",
x = "Income",
y = "Balance") +
theme_minimal()
summary(Credit)
sd(Credit$Income)
mean(Credit$Income)
model <- lm(Balance ~ Income, data = Credit)
summary(model)
library(stargaazer)
library(stargazer)
stargazer(model)
library(stargazer)
model <- lm(Balance ~ Income, data = Credit)
stargazer(model, type = "text")
stargazer(model, type = "QJE")
stargazer(model, style = "QJE")
library(stargazer)
model <- lm(Balance ~ Income, data = Credit)
stargazer(model, style = "QJE")
# make a stargazer model without the extra message
?stargazer
library(tidyverse)
plot(Credit$Income, Credit$Balance, xlab = "Income", ylab = "Balance", main = "Balance vs Income")
abline(model, col = "red")
glm(Balance ~ Income, data = Credit)
## Load the packages
library(ISLR)
library(FNN)
default_data <- Default
summary(default_data)
default_data$default <- as.numeric(default_data$default == "Yes")
default_data$student <- as.numeric(default_data$student == "Yes")
summary(default_data)
set.seed(222)
## First pick the test observations (20% of the data)
test_obs <- sample(seq(nrow(default_data)),
round(0.2 * nrow(default_data)))
## The training observations are the remaining observations
train_obs <- setdiff(seq(nrow(default_data)), test_obs)
## Use the indices now to extract the corresponding subsets of the data
test_data <- default_data[test_obs,]
train_data <- default_data[train_obs,]
test_obs
logistic_default <- glm(default ~ student + balance + income,
family = binomial,
data = train_data)
summary(logistic_default)
logistic_predict <- predict(logistic_default,
test_data,
type = "response")
hist(logistic_predict)
# plot auc curves
library(ROCR)
pred <- prediction(logistic_predict, test_data$default)
perf <- performance(pred, "tpr", "fpr")
plot(perf)
perf <- performance(logistic_predict, "tpr", "fpr")
plot(perf)
perf <- performance(logistic_predict, "tpr", "fpr")
head(logistic_predict)
class_predictions <- as.numeric(logistic_predict > 0.5)
class_predictions <- as.numeric(logistic_predict > 0.5)
logistic_accuracy <- mean(class_predictions == test_data$default)
print(logistic_accuracy)
summary(test_data$default)
true_pos_accuracy <- mean(class_predictions[which(test_data$default == 1)] == 1)
print(true_pos_accuracy)
true_neg_accuracy <- mean(
class_predictions[which(test_data$default == 0)] == 0)
print(true_neg_accuracy)
table(class_predictions, test_data$default)
table(class_predictions, test_data$default - 1)
table(class_predictions, test_data$default)
true_pos_error <- mean(class_predictions
[which(test_data$default == 1)] != 1)
print(true_pos_error)
true_neg_error <- mean(class_predictions
[which(test_data$default == 0)] != 0)
print(true_neg_error)
threshold_values <- seq(from = 0.00, to = 0.50, by = 0.01)
threshold_values
error_rates <- matrix(0, nrow = length(threshold_values), ncol = 2)
error_rates
error_rates <- matrix(0, nrow = length(threshold_values), ncol = 2)
indx <- 0
for(threshold in threshold_values) {
## Update the tracker to reflect the row
indx <- indx + 1
## Then generate the predicted classes using the specific threshold
class_predictions <- as.numeric(logistic_predict > threshold)
## Then calculate the true positive accuracy
true_pos_accuracy <- mean(class_predictions[which(test_data$default == 1)] == 1)
## Then calculate the true negative accuracy
true_neg_accuracy <- mean(class_predictions[which(test_data$default == 0)] == 0)
## Now we can add the results to our matrix
error_rates[indx,] <- c(true_pos_accuracy, true_neg_accuracy)
}
error_rates
matplot(x = threshold_values,
y = error_rates,
type = "l",
col = 3:4,
xlab = "Threshold",
ylab = "Accuracy",
main = "Accuracy as a Function of Threshold")
legend("topright", legend = c("Defaulters", "Non-defaulters"),
col = 3:4, pch = 1)
library(MASS)
lda_model <- lda(default ~., data = train_data)
lda_model <- lda(default ~ balance + income, data = train_data)
lda_model <- lda(default ~ ., data = train_data)
lda_pred <- predict(lda_model, test_data[,-1])
class_pred_lda <- lda_pred$class
error_lda <- mean(class_pred_lda !=
test_data$default)
print(error_lda)
qda_model <- qda(default ~., data = train_data)
qda_pred <- predict(qda_model, test_data[,-1])
class_pred_qda <- qda_pred$class
error_qda <- mean(class_pred_qda !=
test_data$default)
print(error_qda)
default_data <- Default
summary(default_data)
as.numeric(default_data$default)
diabetes_data <- read.csv("diabetes.csv", header = TRUE)
View(diabetes_data)
dim(diabetes_data)
summary(diabetes_data)
library(class)
## Example of a simple function
add_fun <- function(a1, a2){
res = a1 + a2
return(res)
}
add_fun(3, 4)
fold_ids <- rep(seq(5), ceiling(nrow(diabetes) / 5))
fold_ids <- rep(seq(5), ceiling(nrow(diabetes_data) / 5))
fold_ids
sample(fold_ids, length(fold_ids))
hist(sample(fold_ids, length(fold_ids)))
cross_validation_KNN  <- function(data_x, data_y, k_seq, kfolds) {
## We will start by assigning each observation to one and
## only one fold for cross-validation. To do this, we use
## an index vector. The vector's length equals the number
## of observations in the data. The vector's entries are
## equal numbers of 1s, 2s, etc. up to the number of folds
## being used for k-fold cross validation.
## Recall seq(5) is a vector (1, 2, 3, 4, 5)
## ceiling() rounds a number up to the nearest integer
## rep(a, b) repeats a b times (e.g. rep(1, 3) => (1, 1, 1))
## So this says repeat the sequence from 1:kfolds
## enough times to fill a vector that is as long or
## slightly longer than the number of observations in
## the data. Then, if the length of the vector exceeds
## the number of observations, truncate it to the right
## length
fold_ids <- rep(seq(kfolds), ceiling(nrow(data_x) / kfolds))
fold_ids <- fold_ids[1:nrow(data_x)]
## To make the IDs random, randomly rearrange the vector
fold_ids <- sample(fold_ids, length(fold_ids))
## In order to store the prediction performance for each fold
## for each k, we initialize a matrix.
CV_error_mtx  <- matrix(0, nrow = length(k_seq), ncol = kfolds)
## To run CV, we will loop over all values of k that we want to
## consider for KNN. For each value of k, we will loop over all
## folds. For each fold, we will estimate KNN for the given k on
## all but one fold. We will then measure the model's accuracy
## on the hold out fold and save it. After we finish looping
## through all k's and all folds, we will find the average CV
## error for each value of k and use that as a measure of the
## model's performance.
for (k in k_seq) {
for (fold in 1:kfolds) {
## Train the KNN model (Note: if it throws a weird error, make sure
## all features are numeric variables -- not factors)
## Note: usually the features are normalized/re-scaled
## Otherwise KNN will give more weight to variables at the larger scale
## when calculating the distance metric.
## See page 165 of the textbook for a useful example
knn_fold_model <- knn(train = scale(data_x[which(fold_ids != fold),]),
test = scale(data_x[which(fold_ids == fold),]),
cl = data_y[which(fold_ids != fold)],
k = k)
## Measure and save error rate (% wrong)
CV_error_mtx[k, fold] <- mean(knn_fold_model !=
data_y[which(fold_ids == fold)])
}
}
## We want our function to return the accuracy matrix
return(CV_error_mtx)
}
cross_validation_KNN  <- function(data_x, data_y, k_seq, kfolds) {
## We will start by assigning each observation to one and
## only one fold for cross-validation. To do this, we use
## an index vector. The vector's length equals the number
## of observations in the data. The vector's entries are
## equal numbers of 1s, 2s, etc. up to the number of folds
## being used for k-fold cross validation.
## Recall seq(5) is a vector (1, 2, 3, 4, 5)
## ceiling() rounds a number up to the nearest integer
## rep(a, b) repeats a b times (e.g. rep(1, 3) => (1, 1, 1))
## So this says repeat the sequence from 1:kfolds
## enough times to fill a vector that is as long or
## slightly longer than the number of observations in
## the data. Then, if the length of the vector exceeds
## the number of observations, truncate it to the right
## length
fold_ids <- rep(seq(kfolds), ceiling(nrow(data_x) / kfolds))
fold_ids <- fold_ids[1:nrow(data_x)]
## To make the IDs random, randomly rearrange the vector
fold_ids <- sample(fold_ids, length(fold_ids))
## In order to store the prediction performance for each fold
## for each k, we initialize a matrix.
CV_error_mtx  <- matrix(0, nrow = length(k_seq), ncol = kfolds)
## To run CV, we will loop over all values of k that we want to
## consider for KNN. For each value of k, we will loop over all
## folds. For each fold, we will estimate KNN for the given k on
## all but one fold. We will then measure the model's accuracy
## on the hold out fold and save it. After we finish looping
## through all k's and all folds, we will find the average CV
## error for each value of k and use that as a measure of the
## model's performance.
for (k in k_seq) {
for (fold in 1:kfolds) {
## Train the KNN model (Note: if it throws a weird error, make sure
## all features are numeric variables -- not factors)
## Note: usually the features are normalized/re-scaled
## Otherwise KNN will give more weight to variables at the larger scale
## when calculating the distance metric.
## See page 165 of the textbook for a useful example
knn_fold_model <- knn(train = scale(data_x[which(fold_ids != fold),]),
test = scale(data_x[which(fold_ids == fold),]),
cl = data_y[which(fold_ids != fold)],
k = k)
## Measure and save error rate (% wrong)
CV_error_mtx[k, fold] <- mean(knn_fold_model !=
data_y[which(fold_ids == fold)])
}
}
## We want our function to return the accuracy matrix
return(CV_error_mtx)
}
set.seed(222)
knn_cv_error5 <- cross_validation_KNN(data_x = diabetes_data[, -9],
data_y = diabetes_data$Outcome,
k_seq = seq(20),
kfolds = 5)
print(knn_cv_error5)
set.seed(222)
knn_cv_error5 <- cross_validation_KNN(data_x = diabetes_data[, -9],
data_y = diabetes_data$Outcome,
k_seq = seq(20),
kfolds = 5)
print(knn_cv_error5)
knn_cv_error10 <- cross_validation_KNN(data_x = diabetes_data[,-9],
data_y = diabetes_data$Outcome,
k_seq = seq(50),
kfolds = 10)
print(knn_cv_error10)
mean_cv_error5 <- rowMeans(knn_cv_error5)
mean_cv_error10 <- rowMeans(knn_cv_error10)
which.min(mean_cv_error5)
which.min(mean_cv_error10)
set.seed(222)
knn_cv_error5 <- cross_validation_KNN(data_x = diabetes_data[, -9],
data_y = diabetes_data$Outcome,
k_seq = seq(50),
kfolds = 5)
print(knn_cv_error5)
knn_cv_error10 <- cross_validation_KNN(data_x = diabetes_data[,-9],
data_y = diabetes_data$Outcome,
k_seq = seq(50),
kfolds = 10)
print(knn_cv_error10)
mean_cv_error5 <- rowMeans(knn_cv_error5)
mean_cv_error10 <- rowMeans(knn_cv_error10)
which.min(mean_cv_error5)
which.min(mean_cv_error10)
plot(seq(50), mean_cv_error10, type = "l",
main = "Mean CV Error Rate as a Function of k",
ylab = "CV Error Rate", xlab = "k")
knn_cv_error10 <- cross_validation_KNN(data_x = diabetes_data[,-9],
data_y = diabetes_data$Outcome,
k_seq = seq(200),
kfolds = 10)
plot(seq(200), mean_cv_error10, type = "l",
main = "Mean CV Error Rate as a Function of k",
ylab = "CV Error Rate", xlab = "k")
mean_cv_error10 <- rowMeans(knn_cv_error10)
plot(seq(200), mean_cv_error10, type = "l",
main = "Mean CV Error Rate as a Function of k",
ylab = "CV Error Rate", xlab = "k")
knn_cv_error10 <- cross_validation_KNN(data_x = diabetes_data[,-9],
data_y = diabetes_data$Outcome,
k_seq = seq(200),
kfolds = 10)
knn_cv_error10 <- cross_validation_KNN(data_x = diabetes_data[,-9],
data_y = diabetes_data$Outcome,
k_seq = seq(200),
kfolds = 10)
print(knn_cv_error10)
mean_cv_error10 <- rowMeans(knn_cv_error10)
plot(seq(200), mean_cv_error10, type = "l",
main = "Mean CV Error Rate as a Function of k",
ylab = "CV Error Rate", xlab = "k")
plot(seq(50), mean_cv_error10, type = "l",
main = "Mean CV Error Rate as a Function of k",
ylab = "CV Error Rate", xlab = "k")
plot(seq(50), mean_cv_error10[:50], type = "l",
plot(seq(50), mean_cv_error10[1:50], type = "l",
main = "Mean CV Error Rate as a Function of k",
ylab = "CV Error Rate", xlab = "k")
library(pls)
library(ISLR)
insurance_data <- Caravan
?Caravan
library(glmnet)
?cv.glmnet
set.seed(222) # Important for replicability
set.seed(222) # Important for replicability
lasso_ins <- cv.glmnet(x = as.matrix(insurance_data[, 1:85]), # the features
y = as.numeric(insurance_data[, 86]), # the outcome
standardize = TRUE, # Why do we do this?
alpha = 1) # Corresponds to LASSO
print(lasso_ins$lambda)
print(lasso_ins$lambda.min)
print(lasso_ins$lambda.min)
plot(lasso_ins)
LassoCV <- lasso_ins$glmnet.fit
plot(LassoCV, label = TRUE, xvar = "lambda")
abline(
v = log(c(lasso_ins$lambda.min, lasso_ins$lambda.1se))
) # Adds lines to mark the two key lambda values
LassoCV <- lasso_ins$glmnet.fit
plot(LassoCV, label = TRUE, xvar = "lambda")
abline(
v = log(c(lasso_ins$lambda.min, lasso_ins$lambda.1se))
) # Adds lines to mark the two key lambda values
plot(LassoCV, label = TRUE, xvar = "lambda")
abline(
v = log(c(lasso_ins$lambda.min, lasso_ins$lambda.1se))
) # Adds lines to mark the two key lambda values
LassoCV <- lasso_ins$glmnet.fit
plot(LassoCV, label = TRUE, xvar = "lambda")
abline(
v = log(c(lasso_ins$lambda.min, lasso_ins$lambda.1se))
) # Adds lines to mark the two key lambda values
predict(lasso_ins, type = "coefficients",
s = c(lasso_ins$lambda.min, lasso_ins$lambda.1se))
lasso_ins$lambda.min
lasso_ins$cvm[lasso_ins$lambda == lasso_ins$lambda.min]
lasso_ins$cvsd[lasso_ins$lambda == lasso_ins$lambda.min]
lasso_ins$lambda.1se
wage_data <- Wage
summary(wage_data)
str(wage_data)
levels(wage_data$maritl)
levels(wage_data$region)
wage_data <- wage_data[, -11]
for(i in 10:1){
if(is.factor(wage_data[, i])){
for(j in unique(wage_data[, i])){
new_col <- paste(colnames(wage_data)[i], j, sep = "_")
wage_data[, new_col] <- as.numeric(wage_data[, i] == j)
}
wage_data <- wage_data[, -i]
} else if(typeof(wage_data[, i]) == "integer") {
wage_data[, i] <- as.numeric(as.character(wage_data[, i]))
}
}
#View(wage_data)
summary(wage_data)
str(wage_data)
set.seed(222)
train <- sample(seq(nrow(wage_data)),
floor(nrow(wage_data)*0.8))
train <- sort(train)
test <- which(!(seq(nrow(wage_data)) %in% train))
library(pls)
## Try running PCR
pcr_fit  <- pcr(logwage ~ ., data = wage_data[train,],
scale = TRUE, validation = "CV")
## to drop them from the analysis or not to scale the data.
for(col_num in 1:ncol(wage_data)){
if(var(wage_data[, col_num]) < 0.05){
print(colnames(wage_data)[col_num])
print(var(wage_data[, col_num]))
}
}
## Let's drop these low variance columns
for(col_num in ncol(wage_data):1){
if(var(wage_data[, col_num]) < 0.05) {
wage_data <- wage_data[, -col_num]
}
}
set.seed(222)
pcr_fit <- pcr(logwage ~ ., data = wage_data[train,],
scale = TRUE, validation = "CV")
summary(pcr_fit)
pcr_msep <- MSEP(pcr_fit)
pcr_min_indx <- which.min(pcr_msep$val[1, 1,])
print(pcr_min_indx)
# plot the CV MSE by number of components
plot(pcr_msep, legendpos = "topright")
# plot cumulative variance explained
explvar <- explvar(pcr_fit)
plot(explvar, type = "b",
xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained")
set.seed(222)
pcr_fit <- pcr(logwage ~ ., data = wage_data[train,],
scale = TRUE, validation = "CV")
summary(pcr_fit)
validationplot(pcr_fit)
pcr_pred <- predict(pcr_fit, wage_data[test, ], ncomp = 12)
pcr_test_MSE <- mean((pcr_pred - wage_data[test, "logwage"])^2)
print(pcr_test_MSE)
print(sqrt(pcr_test_MSE))
## Step 1: Fit the model
pls_fit <- plsr(logwage ~ ., data = wage_data[train, ],
scale = TRUE, validation = "CV")
summary(pls_fit)
## Step 2: Which ncomp value had the lowest CV MSE?
pls_msep <- MSEP(pls_fit)
pls_min_indx <- which.min(pls_msep$val[1, 1,])
print(pls_min_indx)
## Step 3: Plot validation error as a function of # of components
validationplot(pls_fit, val.type = c("RMSEP"))
## Step 4: Identify the CV RMSE for the number of components with
## the lowest CV RMSE
pls_rmsep <- RMSEP(pls_fit)
print(pls_rmsep$val[1, 1, as.numeric(pls_min_indx)])
## Step 5: Predict test set logwage values
pls_pred <- predict(pls_fit, wage_data[test,],
ncomp = (as.numeric(pls_min_indx) -1))
## Step 6: Measure test MSE and RMSE
pls_test_MSE <- mean((pls_pred - wage_data[test, "logwage"])^2)
print(pls_test_MSE)
print(sqrt(pls_test_MSE))
predict(lasso_ins, type = "coefficients",
s = c(lasso_ins$lambda.min, lasso_ins$lambda.1se))
