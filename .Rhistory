kernel = "linear",
cost = 1000,
scale = FALSE)
plot(college_svm1, small_data)
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "linear",
cost = 1000000,
scale = FALSE)
plot(college_svm1, small_data)
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "linear",
cost = 10,
scale = FALSE)
plot(college_svm1, small_data)
?svm
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "linear",
scale = FALSE)
plot(college_svm1, small_data)
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "linear",
cost = 10,
scale = FALSE)
plot(college_svm1, small_data)
summary(college_svm1)
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "radial",
cost = 10,
scale = FALSE)
plot(college_svm1, small_data)
beta <- drop(t(college_svm1$coefs) %*% x[college_svm1$index,])
beta0 <- -college_svm1$rho
print(beta)
print(beta0)
make.grid = function(x, n = 75) {
grange <-  apply(x, 2, range)
x1 <- seq(from = grange[1, 1], to = grange[2, 1], length = n)
x2 <- seq(from = grange[1, 2], to = grange[2, 2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid <- make.grid(x)
colnames(xgrid) <- colnames(x)
ygrid <- predict(college_svm1, xgrid)
plot(xgrid,
col = c("red","blue")[as.numeric(ygrid)],
pch = 20,
cex = .2)
points(x,
col = y + 3,
pch = 19)
points(x[college_svm1$index,],
pch = 5,
cex = 2)
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "linear",
cost = 10,
scale = FALSE)
beta <- drop(t(college_svm1$coefs) %*% x[college_svm1$index,])
beta0 <- -college_svm1$rho
print(beta)
print(beta0)
make.grid = function(x, n = 75) {
grange <-  apply(x, 2, range)
x1 <- seq(from = grange[1, 1], to = grange[2, 1], length = n)
x2 <- seq(from = grange[1, 2], to = grange[2, 2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid <- make.grid(x)
colnames(xgrid) <- colnames(x)
ygrid <- predict(college_svm1, xgrid)
plot(xgrid,
col = c("red","blue")[as.numeric(ygrid)],
pch = 20,
cex = .2)
points(x,
col = y + 3,
pch = 19)
points(x[college_svm1$index,],
pch = 5,
cex = 2)
plot(xgrid,
col = c("red","blue")[as.numeric(ygrid)],
pch = 20,
cex = .2)
points(x,
col = y + 3,
pch = 19)
points(x[college_svm1$index,],
pch = 5,
cex = 2)
abline(-beta0 / beta[2], -beta[1]/beta[2])
plot(xgrid,
col = c("red","blue")[as.numeric(ygrid)],
pch = 20,
cex = .2)
points(x,
col = y + 3,
pch = 19)
points(x[college_svm1$index,],
pch = 5,
cex = 2)
abline((-1 - beta0) / beta[2], -beta[1] / beta[2], lty = 2)
abline((+1 - beta0) / beta[2], -beta[1] / beta[2], lty = 2)
svm_cost <- function(df, cost_val, xgrid) {
svmfit <- svm(Private ~ Outstate + accept_rate,
data = df,
kernel = "linear",
cost = cost_val,
scale = FALSE)
print(paste("# of support vectors =", length(svmfit$index)))
beta <- drop(t(svmfit$coefs) %*% x[svmfit$index,])
beta0 <- -svmfit$rho
ygrid <- predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[df$index,], pch = 5, cex = 2)
abline(-beta0 / beta[2], -beta[1]/beta[2])
abline((-1 - beta0) / beta[2], -beta[1] / beta[2], lty = 2)
abline((+1 - beta0) / beta[2], -beta[1] / beta[2], lty = 2)
}
svm_cost(small_data, .01, xgrid)
set.seed(222)
set.seed(222)
tune_linear <- tune(svm,
Private ~ accept_rate + Outstate,
data = transformed_data,
kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_linear)
?tune
tune_linear <- tune(svm,
Private ~ accept_rate + Outstate,
data = transformed_data,
kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_linear)
best_linear_mod <- tune_linear$best.model
summary(best_linear_mod)
tune_linear_full <- tune(svm,
Private ~ .,
data = college_data,
kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1,
1, 5, 10, 100)))
summary(tune_linear_full)
svm_poly3 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "polynomial",
degree = 3,
cost = 1)
plot(svm_poly3, small_data)
ygrid <- predict(svm_poly3, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)],
pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
predicted_grid <- predict(svm_poly3, xgrid,
decision.values = TRUE)
predicted_grid <- attributes(predicted_grid)$decision
ygrid <- predict(svm_poly3, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)],
pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
contour(unique(xgrid[,1]), unique(xgrid[,2]),
matrix(predicted_grid, 75, 75),
level = 0,
add = TRUE)
set.seed(222)
tune_poly <- tune(svm,
Private ~ .,
data = transformed_data,
kernel = "polynomial",
ranges = list(cost = c(0.001, 0.1, 1, 5, 100),
degree = c(2, 3)))
summary(tune_poly)
set.seed(222)
tune_radial <- tune(svm, Private ~ .,
data = transformed_data,
kernel = "radial",
ranges = list(cost = c(0.001, 0.1, 1, 5, 100),
gamma = c(0.0001, 0.001, 0.01, 0.1, 1)))
summary(tune_radial)
svm_radial <- tune_radial$best.model
## Make predictions for the xgrid
ygrid <- predict(svm_radial, xgrid)
## And plot the grid predictions and decision boundary
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)],
pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
predicted_grid <- predict(svm_radial, xgrid, decision.values = TRUE)
predicted_grid <- attributes(predicted_grid)$decision
contour(unique(xgrid[,1]), unique(xgrid[,2]),
matrix(predicted_grid, 75, 75), level = 0, add = TRUE)
full_poly <- tune(svm,
Private ~ .,
data = college_data,
kernel = "polynomial",
ranges = list(cost = c(0.001, 0.1, 1, 5, 100),
degree = c(2, 3)))
summary(full_poly)
summary(tune_poly)
full_radial <- tune(svm,
Private ~ .,
data = college_data,
kernel = "radial",
ranges = list(cost = c(0.001, 0.1, 1, 5, 100),
gamma = c(0.0001, 0.001, 0.01, 0.1, 1)))
summary(full_radial)
summary(tune_radial)
summary(full_poly)
install.packages('ISLR')
knitr::opts_chunk$set(
echo = TRUE,
warning = FALSE,
message = FALSE,
fig.width = 10,
fig.height = 6
)
# Simulate data with two examiners
simulate_two_examiner_data <- function(n = 500,
tough_rate = 0.4,
soft_rate = 0.7,
true_effect = 0.25) {
data <- tibble(
startup_id = 1:n,
quality = rnorm(n, 0, 1),
# Random assignment to examiner
examiner = sample(c("Tough", "Soft"), n, replace = TRUE),
# Approval probability depends on examiner AND quality
approval_prob = ifelse(
examiner == "Soft",
pmin(soft_rate + 0.1 * quality, 0.95),
pmin(tough_rate + 0.1 * quality, 0.95)
),
approved = rbinom(n, 1, approval_prob),
# Outcome
innovation = 5 + true_effect * approved + quality + rnorm(n, 0, 0.5)
)
return(data)
}
data_two_exam <- simulate_two_examiner_data()
library(tidyverse)
library(knitr)
library(kableExtra)
library(plotly)
set.seed(42)
# Simulation parameters
n <- 500
true_effect <- 0.25
# Generate data with selection
simulate_selection_data <- function() {
data <- tibble(
startup_id = 1:n,
# Unobserved quality (part of epsilon)
quality = rnorm(n, mean = 0, sd = 1),
# Approval depends on quality (selection!)
approval_prob = pnorm(quality * 0.8),
approved = rbinom(n, 1, approval_prob),
# Outcome depends on quality AND treatment
innovation = 5 + true_effect * approved + quality + rnorm(n, 0, 0.5)
)
return(data)
}
data_selected <- simulate_selection_data()
# Compare OLS to truth
ols_estimate <- coef(lm(innovation ~ approved, data = data_selected))["approved"]
# Visualize the selection problem
p1 <- ggplot(data_selected, aes(x = quality, y = innovation, color = factor(approved))) +
geom_point(alpha = 0.6, size = 2) +
geom_smooth(method = "lm", se = TRUE, size = 1.2) +
scale_color_manual(
values = c("0" = "#e74c3c", "1" = "#27ae60"),
labels = c("Denied", "Approved")
) +
labs(
title = "The Selection Problem",
subtitle = sprintf("OLS estimates effect = %.3f, but true effect = %.3f",
ols_estimate, true_effect),
x = "Startup Quality (Unobserved)",
y = "Future Innovation",
color = "Patent Status"
) +
theme_minimal(base_size = 13) +
theme(
legend.position = "bottom",
plot.title = element_text(face = "bold", size = 16),
plot.subtitle = element_text(color = "#666666")
)
print(p1)
# Simulate data with two examiners
simulate_two_examiner_data <- function(n = 500,
tough_rate = 0.4,
soft_rate = 0.7,
true_effect = 0.25) {
data <- tibble(
startup_id = 1:n,
quality = rnorm(n, 0, 1),
# Random assignment to examiner
examiner = sample(c("Tough", "Soft"), n, replace = TRUE),
# Approval probability depends on examiner AND quality
approval_prob = ifelse(
examiner == "Soft",
pmin(soft_rate + 0.1 * quality, 0.95),
pmin(tough_rate + 0.1 * quality, 0.95)
),
approved = rbinom(n, 1, approval_prob),
# Outcome
innovation = 5 + true_effect * approved + quality + rnorm(n, 0, 0.5)
)
return(data)
}
data_two_exam <- simulate_two_examiner_data()
# Create instrument: indicator for soft examiner
data_two_exam <- data_two_exam %>%
mutate(soft_examiner = ifelse(examiner == "Soft", 1, 0))
# Run IV regression manually (Wald estimator)
mean_y_soft <- mean(data_two_exam$innovation[data_two_exam$soft_examiner == 1])
mean_y_tough <- mean(data_two_exam$innovation[data_two_exam$soft_examiner == 0])
mean_x_soft <- mean(data_two_exam$approved[data_two_exam$soft_examiner == 1])
mean_x_tough <- mean(data_two_exam$approved[data_two_exam$soft_examiner == 0])
iv_estimate <- (mean_y_soft - mean_y_tough) / (mean_x_soft - mean_x_tough)
# Visualize
exam_summary <- data_two_exam %>%
group_by(examiner) %>%
summarize(
mean_innovation = mean(innovation),
approval_rate = mean(approved),
n = n()
)
p2 <- ggplot(exam_summary, aes(x = examiner, y = mean_innovation, fill = examiner)) +
geom_col(width = 0.6) +
geom_errorbar(
aes(ymin = mean_innovation - 0.1, ymax = mean_innovation + 0.1),
width = 0.2
) +
scale_fill_manual(values = c("Soft" = "#27ae60", "Tough" = "#e67e22")) +
labs(
title = "Average Innovation by Assigned Examiner",
subtitle = sprintf("IV estimate: %.3f (True effect: %.3f)", iv_estimate, true_effect),
x = "Assigned Examiner",
y = "Mean Future Innovation"
) +
theme_minimal(base_size = 13) +
theme(
legend.position = "none",
plot.title = element_text(face = "bold", size = 16)
) +
geom_text(
aes(label = sprintf("Approval rate: %.1f%%", approval_rate * 100)),
vjust = -1.5,
size = 4
)
print(p2)
library(AER)
simulate_many_examiner_data <- function(n_startups = 2000,
n_examiners = 50,
true_effect = 0.25) {
# Generate examiner characteristics
examiners <- tibble(
examiner_id = 1:n_examiners,
leniency = rnorm(n_examiners, 0.5, 0.15)
) %>%
mutate(leniency = pmax(0.2, pmin(0.8, leniency)))  # Bound between 0.2 and 0.8
# Generate startup applications
data <- tibble(
startup_id = 1:n_startups,
quality = rnorm(n_startups, 0, 1),
# Random examiner assignment
examiner_id = sample(1:n_examiners, n_startups, replace = TRUE)
) %>%
left_join(examiners, by = "examiner_id") %>%
mutate(
# Approval depends on examiner leniency and quality
approval_prob = pmin(leniency + 0.08 * quality, 0.95),
approved = rbinom(n_startups, 1, approval_prob),
# Innovation outcome
innovation = 5 + true_effect * approved + quality + rnorm(n_startups, 0, 0.5)
)
return(data)
}
# Run simulation multiple times
n_sims <- 200
results <- tibble(
sim = 1:n_sims,
ols = NA_real_,
tsls = NA_real_,
first_stage_f = NA_real_
)
for (i in 1:n_sims) {
data <- simulate_many_examiner_data(n_startups = 1000, n_examiners = 50)
# OLS
ols_model <- lm(innovation ~ approved, data = data)
results$ols[i] <- coef(ols_model)["approved"]
# 2SLS with examiner dummies
data$examiner_factor <- factor(data$examiner_id)
# First stage
first_stage <- lm(approved ~ examiner_factor, data = data)
results$first_stage_f[i] <- summary(first_stage)$fstatistic[1]
# 2SLS
tsls_model <- ivreg(
innovation ~ approved | examiner_factor,
data = data
)
results$tsls[i] <- coef(tsls_model)["approved"]
}
# Calculate bias
results <- results %>%
mutate(
ols_bias = ols - true_effect,
tsls_bias = tsls - true_effect
)
# Summary statistics
summary_stats <- results %>%
summarize(
`OLS Mean` = mean(ols),
`OLS Bias` = mean(ols_bias),
`2SLS Mean` = mean(tsls),
`2SLS Bias` = mean(tsls_bias),
`Mean F-stat` = mean(first_stage_f)
)
kable(summary_stats, digits = 3, caption = "Simulation Results: Many Examiners Problem") %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Visualize the bias
results_long <- results %>%
select(sim, ols, tsls) %>%
pivot_longer(cols = c(ols, tsls), names_to = "estimator", values_to = "estimate")
p3 <- ggplot(results_long, aes(x = estimate, fill = estimator)) +
geom_histogram(alpha = 0.6, position = "identity", bins = 40) +
geom_vline(xintercept = true_effect, linetype = "dashed", size = 1.2, color = "black") +
scale_fill_manual(
values = c("ols" = "#e74c3c", "tsls" = "#3498db"),
labels = c("OLS", "2SLS"),
name = "Estimator"
) +
annotate(
"text", x = true_effect, y = Inf,
label = "True Effect", vjust = 1.5, hjust = -0.1, size = 4
) +
labs(
title = "Distribution of Estimates Across 200 Simulations",
subtitle = sprintf("50 examiners, 1000 applications | Mean F-stat: %.1f",
mean(results$first_stage_f)),
x = "Estimated Effect",
y = "Frequency"
) +
theme_minimal(base_size = 13) +
theme(
legend.position = "bottom",
plot.title = element_text(face = "bold", size = 16)
)
print(p3)
# install.packages("neuralnet")
library(neuralnet)
library(tidyverse)
iris <- iris %>% mutate_if(is.character, as.factor)
View(iris)
summary(iris)
set.seed(222)
data_rows <- floor(0.80 * nrow(iris))
train_indices <- sample(c(1:nrow(iris)), data_rows)
train_data <- iris[train_indices,]
test_data <- iris[-train_indices,]
?neuralnet
model = neuralnet(
Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,
data = train_data,
hidden = c(4, 1),
linear.output = FALSE)
plot(model, rep = "best")
pred <- predict(model, test_data, response = "Species")
labels <- levels(train_data$Species)
prediction_label <- data.frame(max.col(pred)) %>%
mutate(pred = labels[max.col.pred.]) %>%
select(2) %>%
unlist()
errors = as.numeric(test_data$Species) != max.col(pred)
error_rate = (sum(errors)/nrow(test_data))*100
print(paste("Error Rate: ", round(error_rate,2), "%"))
table(test_data$Species, prediction_label)
#install.packages("ReinforcementLearning")
library(ReinforcementLearning)
states <- c("s1", "s2", "s3", "s4")
actions <- c("up", "down", "left", "right")
env <- gridworldEnvironment
print(env)
?sampleExperience
data <- sampleExperience(N = 1000,
env = env,
states = states,
actions = actions)
head(data)
View(data)
# Define reinforcement learning parameters
control <- list(alpha = 0.1, gamma = 0.5, epsilon = 0.1)
# Perform reinforcement learning
?ReinforcementLearning
model <- ReinforcementLearning(data,
s = "State",
a = "Action",
r = "Reward",
s_new = "NextState",
control = control)
# Print policy
computePolicy(model)
print(model)
summary(model)
tree
