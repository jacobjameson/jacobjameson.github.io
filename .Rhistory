)
# Calculate Gini impurity for a binary split
calculate_gini_binary <- function(subset) {
if (nrow(subset) == 0) return(1) # Return max Gini impurity if subset is empty
p <- sum(subset$Condition) / nrow(subset)
gini <- 1 - (p^2 + (1 - p)^2)
return(gini)
}
# Find the best split for a continuous variable
find_best_split_for_continuous <- function(data, feature) {
data <- data[order(data[[feature]]), ]
unique_values <- sort(unique(data[[feature]]))
best_gini <- 1
best_split_val <- NA
for (i in 1:(length(unique_values) - 1)) {
split_val <- (unique_values[i] + unique_values[i + 1]) / 2
left_subset <- data[data[[feature]] <= split_val, ]
right_subset <- data[data[[feature]] > split_val, ]
gini_left <- calculate_gini_binary(left_subset)
gini_right <- calculate_gini_binary(right_subset)
weighted_gini <- (nrow(left_subset) * gini_left + nrow(right_subset) * gini_right) / nrow(data)
# Check for NA or NaN in weighted_gini before comparison
if (!is.na(weighted_gini) && !is.nan(weighted_gini) && weighted_gini < best_gini) {
best_gini <- weighted_gini
best_split_val <- split_val
}
}
return(list(split_value = best_split_val, gini = best_gini))
}
# Recursive function to build the tree
build_tree <- function(dataset, features, max.depth, current.depth=0) {
if (nrow(dataset) == 0 || length(features) == 0 || current.depth == max.depth) {
return(list("Leaf", table(dataset$Condition)))
}
gini_scores <- sapply(features, function(f) {
if(is.numeric(dataset[[f]])) {
find_best_split_for_continuous(dataset, f)$gini
} else {
calculate_gini_binary(dataset[dataset[[f]] == 1,]) * mean(dataset[[f]]) +
calculate_gini_binary(dataset[dataset[[f]] == 0,]) * (1 - mean(dataset[[f]]))
}
})
best_feature <- names(which.min(gini_scores))
best_split_value <- if(is.numeric(dataset[[best_feature]])) find_best_split_for_continuous(dataset, best_feature)$split_value else NA
if (is.na(best_split_value)) { # Binary feature
left <- dataset[dataset[[best_feature]] == 1,]
right <- dataset[dataset[[best_feature]] == 0,]
} else { # Continuous feature
left <- dataset[dataset[[best_feature]] <= best_split_value,]
right <- dataset[dataset[[best_feature]] > best_split_value,]
}
cat(paste("Depth:", current.depth, "- Split on:", best_feature))
if (!is.na(best_split_value)) cat(paste(" at ", best_split_value))
cat("\n")
left_tree <- build_tree(left, setdiff(features, best_feature), max.depth, current.depth + 1)
right_tree <- build_tree(right, setdiff(features, best_feature), max.depth, current.depth + 1)
return(list(feature=best_feature, split_value=best_split_value, left=left_tree, right=right_tree))
}
# Initial call to build the tree with max.depth defaulting to number of features
features <- names(data)[2:5] # Exclude 'ID' and 'Condition'
max_depth <- length(features) # Default max.depth to number of features
decision_tree <- build_tree(data, features, max_depth)
# Define the dataset
data <- data.frame(
ID = 1:10,
A = c(23, 22, 40, 55, 40, 30, 55, 30, 20, 50),
B = c(25, 20, 28, 23, 25, 27, 22, 22, 24, 32),
S = c(0, 1, 1, 0, 1, 0, 0, 0, 0, 1),
E = c(1, 0, 1, 1, 0, 0, 1, 1, 0, 0),
Condition = c(0, 1, 1, 0, 1, 1, 0, 0, 0, 1)
)
# Calculate Gini impurity for a binary split
calculate_gini_binary <- function(subset) {
if (nrow(subset) == 0) return(1) # Return max Gini impurity if subset is empty
p <- sum(subset$Condition) / nrow(subset)
gini <- 1 - (p^2 + (1 - p)^2)
return(gini)
}
# Find the best split for a continuous variable
find_best_split_for_continuous <- function(data, feature) {
data <- data[order(data[[feature]]), ]
unique_values <- sort(unique(data[[feature]]))
best_gini <- 1
best_split_val <- NA
for (i in 1:(length(unique_values) - 1)) {
split_val <- (unique_values[i] + unique_values[i + 1]) / 2
left_subset <- data[data[[feature]] <= split_val, ]
right_subset <- data[data[[feature]] > split_val, ]
gini_left <- calculate_gini_binary(left_subset)
gini_right <- calculate_gini_binary(right_subset)
weighted_gini <- (nrow(left_subset) * gini_left + nrow(right_subset) * gini_right) / nrow(data)
# Check for NA or NaN in weighted_gini before comparison
if (!is.na(weighted_gini) && !is.nan(weighted_gini) && weighted_gini < best_gini) {
best_gini <- weighted_gini
best_split_val <- split_val
}
}
return(list(split_value = best_split_val, gini = best_gini))
}
# Recursive function to build the tree
build_tree <- function(dataset, features, max.depth, current.depth=0) {
if (nrow(dataset) == 0 || length(features) == 0 || current.depth == max.depth) {
return(list("Leaf", table(dataset$Condition)))
}
gini_scores <- sapply(features, function(f) {
if(is.numeric(dataset[[f]])) {
find_best_split_for_continuous(dataset, f)$gini
} else {
calculate_gini_binary(dataset[dataset[[f]] == 1,]) * mean(dataset[[f]]) +
calculate_gini_binary(dataset[dataset[[f]] == 0,]) * (1 - mean(dataset[[f]]))
}
})
best_feature <- names(which.min(gini_scores))
best_split_value <- if(is.numeric(dataset[[best_feature]])) {
split_info <- find_best_split_for_continuous(dataset, best_feature)
if (!is.null(split_info$split_value)) {
split_info$split_value
} else {
NA  # Ensure there is always a value
}
} else {
NA  # This line ensures that best_split_value is always initialized
}
# Perform the split, adjusting the if statement to check for NA and existence
if (!is.na(best_split_value) && length(best_split_value) > 0) {
# Split for continuous feature
left <- dataset[dataset[[best_feature]] <= best_split_value, ]
right <- dataset[dataset[[best_feature]] > best_split_value, ]
} else {
# Split for binary feature, or no split was found
left <- dataset[dataset[[best_feature]] == 1, ]
right <- dataset[dataset[[best_feature]] == 0, ]
}
cat(paste("Depth:", current.depth, "- Split on:", best_feature))
if (!is.na(best_split_value)) cat(paste(" at ", best_split_value))
cat("\n")
left_tree <- build_tree(left, setdiff(features, best_feature), max.depth, current.depth + 1)
right_tree <- build_tree(right, setdiff(features, best_feature), max.depth, current.depth + 1)
return(list(feature=best_feature, split_value=best_split_value, left=left_tree, right=right_tree))
}
# Initial call to build the tree with max.depth defaulting to number of features
features <- names(data)[2:5] # Exclude 'ID' and 'Condition'
max_depth <- length(features) # Default max.depth to number of features
decision_tree <- build_tree(data, features, max_depth)
# Define the dataset
data <- data.frame(
ID = 1:10,
A = c(23, 22, 40, 55, 40, 30, 55, 30, 20, 50),
B = c(25, 20, 28, 23, 25, 27, 22, 22, 24, 32),
S = c(0, 1, 1, 0, 1, 0, 0, 0, 0, 1),
E = c(1, 0, 1, 1, 0, 0, 1, 1, 0, 0),
Condition = c(0, 1, 1, 0, 1, 1, 0, 0, 0, 1)
)
# Calculate Gini impurity for a binary split
calculate_gini_binary <- function(subset) {
if (nrow(subset) == 0) return(1) # Return max Gini impurity if subset is empty
p <- sum(subset$Condition) / nrow(subset)
gini <- 1 - (p^2 + (1 - p)^2)
return(gini)
}
# Find the best split for a continuous variable
find_best_split_for_continuous <- function(data, feature) {
data <- data[order(data[[feature]]), ]
unique_values <- sort(unique(data[[feature]]))
best_gini <- 1
best_split_val <- NA  # Ensure initialization
for (i in 1:(length(unique_values) - 1)) {
split_val <- (unique_values[i] + unique_values[i + 1]) / 2
left_subset <- data[data[[feature]] <= split_val, ]
right_subset <- data[data[[feature]] > split_val, ]
gini_left <- calculate_gini_binary(left_subset)
gini_right <- calculate_gini_binary(right_subset)
weighted_gini <- (nrow(left_subset) * gini_left + nrow(right_subset) * gini_right) / nrow(data)
if (!is.na(weighted_gini) && !is.nan(weighted_gini) && weighted_gini < best_gini) {
best_gini <- weighted_gini
best_split_val <- split_val
}
}
return(list(split_value = best_split_val, gini = best_gini))
}
# Recursive function to build the tree
build_tree <- function(dataset, features, max.depth, current.depth=0) {
# Ensure we do not proceed if dataset is empty or we've reached max depth
if (nrow(dataset) == 0 || length(features) == 0 || current.depth >= max.depth) {
return(list("Leaf", table(dataset$Condition)))
}
gini_scores <- sapply(features, function(f) {
# Assume binary or find the best split for continuous
split_info <- ifelse(is.numeric(dataset[[f]]), find_best_split_for_continuous(dataset, f), list(gini=1))
return(split_info$gini)
})
best_feature <- names(which.min(gini_scores))
split_info <- find_best_split_for_continuous(dataset, best_feature)
best_split_value <- split_info$split_value
# Adjust the if statement to check for the existence and length of best_split_value
if (!is.null(best_split_value) && length(best_split_value) > 0) {
cat(paste("Depth:", current.depth, "- Split on:", best_feature, "at", best_split_value, "\n"))
left <- dataset[dataset[[best_feature]] <= best_split_value, ]
right <- dataset[dataset[[best_feature]] > best_split_value, ]
} else {
cat(paste("Depth:", current.depth, "- Split on:", best_feature, "\n"))
left <- dataset[dataset[[best_feature]] == 1, ]
right <- dataset[dataset[[best_feature]] == 0, ]
}
# Recursive build for left and right subtrees
left_tree <- build_tree(left, setdiff(features, best_feature), max.depth, current.depth + 1)
right_tree <- build_tree(right, setdiff(features, best_feature), max.depth, current.depth + 1)
return(list(feature=best_feature, split_value=best_split_value, left=left_tree, right=right_tree))
}
# Initial call to build the tree with max.depth defaulting to number of features
features <- names(data)[2:5] # Exclude 'ID' and 'Condition'
max_depth <- length(features) # Default max.depth to number of features
decision_tree <- build_tree(data, features, max_depth)
# Define the dataset
data <- data.frame(
ID = 1:10,
A = c(23, 22, 40, 55, 40, 30, 55, 30, 20, 50),
B = c(25, 20, 28, 23, 25, 27, 22, 22, 24, 32),
S = c(0, 1, 1, 0, 1, 0, 0, 0, 0, 1),
E = c(1, 0, 1, 1, 0, 0, 1, 1, 0, 0),
Condition = c(0, 1, 1, 0, 1, 1, 0, 0, 0, 1)
)
calculate_gini_binary <- function(subset) {
if (nrow(subset) == 0) return(1)
p <- sum(subset$Condition) / nrow(subset)
gini <- 1 - (p^2 + (1 - p)^2)
return(gini)
}
find_best_split_for_continuous <- function(data, feature) {
data <- data[order(data[[feature]]), ]
unique_values <- sort(unique(data[[feature]]))
best_gini <- 1
best_split_val <- NA
for (i in 1:(length(unique_values) - 1)) {
split_val <- (unique_values[i] + unique_values[i + 1]) / 2
left_subset <- data[data[[feature]] <= split_val, ]
right_subset <- data[data[[feature]] > split_val, ]
gini_left <- calculate_gini_binary(left_subset)
gini_right <- calculate_gini_binary(right_subset)
weighted_gini <- (nrow(left_subset) * gini_left + nrow(right_subset) * gini_right) / nrow(data)
if (weighted_gini < best_gini) {
best_gini <- weighted_gini
best_split_val <- split_val
}
}
return(list(split_value = best_split_val, gini = best_gini))
}
build_tree <- function(dataset, features, max.depth, current.depth=0) {
if (nrow(dataset) == 0 || length(features) == 0 || current.depth >= max.depth) {
return(list("Leaf", table(dataset$Condition)))
}
gini_scores <- sapply(features, function(f) calculate_gini_binary(dataset[dataset[[f]] == 1,]) * mean(dataset[[f]]) + calculate_gini_binary(dataset[dataset[[f]] == 0,]) * (1 - mean(dataset[[f]])))
best_feature <- names(which.min(gini_scores))
split_info <- find_best_split_for_continuous(dataset, best_feature)
best_split_value <- split_info$split_value
if (!is.null(best_split_value) && length(best_split_value) > 0) {
left <- dataset[dataset[[best_feature]] <= best_split_value, ]
right <- dataset[dataset[[best_feature]] > best_split_value, ]
} else {
left <- dataset[dataset[[best_feature]] == 1, ]
right <- dataset[dataset[[best_feature]] == 0, ]
}
left_tree <- build_tree(left, setdiff(features, best_feature), max.depth, current.depth + 1)
right_tree <- build_tree(right, setdiff(features, best_feature), max.depth, current.depth + 1)
return(list(feature=best_feature, split_value=best_split_value, left=left_tree, right=right_tree))
}
features <- names(data)[2:5] # Features for splits
max_depth <- length(features) # Max depth
decision_tree <- build_tree(data, features, max_depth)
# Define the dataset
data <- data.frame(
ID = 1:10,
A = c(23, 22, 40, 55, 40, 30, 55, 30, 20, 50),
B = c(25, 20, 28, 23, 25, 27, 22, 22, 24, 32),
S = c(0, 1, 1, 0, 1, 0, 0, 0, 0, 1),
E = c(1, 0, 1, 1, 0, 0, 1, 1, 0, 0),
Condition = c(0, 1, 1, 0, 1, 1, 0, 0, 0, 1)
)
calculate_gini_binary <- function(subset) {
if (nrow(subset) == 0) return(1)
p <- sum(subset$Condition) / nrow(subset)
gini <- 1 - (p^2 + (1 - p)^2)
return(gini)
}
find_best_split_for_continuous <- function(data, feature) {
data <- data[order(data[[feature]]), ]
unique_values <- sort(unique(data[[feature]]))
best_gini <- 1
best_split_val <- NA
for (i in 1:(length(unique_values) - 1)) {
split_val <- (unique_values[i] + unique_values[i + 1]) / 2
left_subset <- data[data[[feature]] <= split_val, ]
right_subset <- data[data[[feature]] > split_val, ]
if (nrow(left_subset) == 0 || nrow(right_subset) == 0) {
next  # Skip this split if either subset is empty
}
gini_left <- calculate_gini_binary(left_subset)
gini_right <- calculate_gini_binary(right_subset)
weighted_gini <- (nrow(left_subset) * gini_left + nrow(right_subset) * gini_right) / nrow(data)
if (weighted_gini < best_gini) {
best_gini <- weighted_gini
best_split_val <- split_val
}
}
return(list(split_value = best_split_val, gini = best_gini))
}
build_tree <- function(dataset, features, max.depth, current.depth=0) {
if (nrow(dataset) == 0 || length(features) == 0 || current.depth >= max.depth) {
return(list("Leaf", table(dataset$Condition)))
}
gini_scores <- sapply(features, function(f) calculate_gini_binary(dataset[dataset[[f]] == 1,]) * mean(dataset[[f]]) + calculate_gini_binary(dataset[dataset[[f]] == 0,]) * (1 - mean(dataset[[f]])))
best_feature <- names(which.min(gini_scores))
split_info <- find_best_split_for_continuous(dataset, best_feature)
best_split_value <- split_info$split_value
if (!is.null(best_split_value) && length(best_split_value) > 0) {
left <- dataset[dataset[[best_feature]] <= best_split_value, ]
right <- dataset[dataset[[best_feature]] > best_split_value, ]
} else {
left <- dataset[dataset[[best_feature]] == 1, ]
right <- dataset[dataset[[best_feature]] == 0, ]
}
left_tree <- build_tree(left, setdiff(features, best_feature), max.depth, current.depth + 1)
right_tree <- build_tree(right, setdiff(features, best_feature), max.depth, current.depth + 1)
return(list(feature=best_feature, split_value=best_split_value, left=left_tree, right=right_tree))
}
features <- names(data)[2:5] # Features for splits
max_depth <- length(features) # Max depth
decision_tree <- build_tree(data, features, max_depth)
decision_tree <- build_tree(data, features, max_depth)
p <- seq(0.01, 0.99, 0.01)
plot(p, 1-p^2-(1-p)^2, type='l', ylim=c(0,1), ylab = "Gini Impurity", xlab="Probability of Heads", col="blue")
plot(p, -p*log2(p)-(1-p)*log2(1-p), type='l', ylim=c(0,1),
ylab = "Entropy", xlab="Probability of Heads", col="blue")
p <- seq(0.01, 0.99, 0.01)
plot(p, -p*log2(p)-(1-p)*log2(1-p), type='l', ylim=c(0,1),
ylab = "Entropy", xlab="Probability of Heads", col="blue")
p <- seq(0.01, 0.99, 0.01)
plot(p, 1-p^2-(1-p)^2, type='l', ylim=c(0,1),
ylab = "Gini Impurity", xlab="Probability of Heads",
col="blue")
p <- seq(0.01, 0.99, 0.01)
plot(p, -p*log2(p)-(1-p)*log2(1-p), type='l', ylim=c(0,1),
ylab = "Entropy", xlab="Probability of Heads", col="blue")
library(datasets)
data(iris)
head(iris, 6)
entropy <- function(y) {
if(length(y) == 0) return(0)
p <- table(y) / length(y)
sum(-p * log2(p + 1e-9))
}
gini_impurity <- function(y) {
if(length(y) == 0) return(0)
p <- table(y) / length(y)
1 - sum(p^2)
}
information_gain <- function(y, mask, metric_func = entropy) {
# We don't want partitions with no data points
if (sum(mask) == 0 || sum(!mask) == 0) return(0)
metric_func(y) - (sum(mask) / length(mask)) * metric_func(y[mask]) - (sum(!mask) / length(!mask)) * metric_func(y[!mask])
}
information_gain <- function(y, mask, metric_func = entropy) {
# We don't want partitions with no data points
if (sum(mask) == 0 || sum(!mask) == 0) return(0)
metric_func(y) - (sum(mask) / length(mask)) * metric_func(y[mask]) -
(sum(!mask) / length(!mask)) * metric_func(y[!mask])
}
max_information_gain_split <- function(y, x, metric_func = gini_impurity) {
# Initialize the best change and split value
best_change <- NA
split_value <- NA
# Check if the feature is numeric or categorical
is_numeric <- !(is.factor(x) || is.logical(x) || is.character(x))
for(val in sort(unique(x))) {
mask <- if (is_numeric) { x < val } else { x == val }
change <- information_gain(y, mask, metric_func)
if(is.na(best_change) || change > best_change) {
best_change <- change
split_value <- val
}
}
return(list("best_change" = best_change, "split_value" = split_value, "is_numeric" = is_numeric))
}
print(max_information_gain_split(iris$Species, iris$Petal.Width))
sapply(iris[, 1:4], function(x) max_information_gain_split(iris$Species, x))
calculate_entropy <- function(y) {
if (length(y) == 0) return(0)
p <- table(y) / length(y)
-sum(p * log2(p + 1e-9))
}
calculate_gini_impurity <- function(y) {
if (length(y) == 0) return(0)
p <- table(y) / length(y)
1 - sum(p^2)
}
calculate_information_gain <- function(y, mask, metric_func) {
s1 <- sum(mask)
s2 <- length(mask) - s1
if (s1 == 0 || s2 == 0) return(0)
metric_func(y) - s1 / (s1 + s2) * metric_func(y[mask]) - s2 / (s1 + s2) * metric_func(y[!mask])
}
find_best_split <- function(X, y, metric_func) {
features <- names(X)
best_gain <- 0
best_split <- NULL
best_feature <- NULL
for (feature in features) {
unique_values <- unique(X[[feature]])
for (value in unique_values) {
mask <- X[[feature]] < value
gain <- calculate_information_gain(y, mask, metric_func)
if (gain > best_gain) {
best_gain <- gain
best_split <- value
best_feature <- feature
}
}
}
list(gain = best_gain, feature = best_feature, split = best_split)
}
split_data <- function(X, y, best_split) {
mask <- X[[best_split$feature]] < best_split$split
list(
left_X = X[mask, ],
left_y = y[mask],
right_X = X[!mask, ],
right_y = y[!mask]
)
}
create_decision_tree <- function(X, y, max_depth = 3, depth = 0, metric_func = calculate_gini_impurity) {
if (depth == max_depth || length(unique(y)) == 1) {
return(list(prediction = ifelse(is.factor(y), names(sort(-table(y)))[1], mean(y))))
}
best_split <- find_best_split(X, y, metric_func)
if (best_split$gain == 0) {
return(list(prediction = ifelse(is.factor(y), names(sort(-table(y)))[1], mean(y))))
}
split_sets <- split_data(X, y, best_split)
return(list(
feature = best_split$feature,
split = best_split$split,
left = create_decision_tree(split_sets$left_X, split_sets$left_y, max_depth, depth + 1, metric_func),
right = create_decision_tree(split_sets$right_X, split_sets$right_y, max_depth, depth + 1, metric_func)
))
}
# Using the functional approach to create a decision tree with the iris dataset
iris_tree <- create_decision_tree(iris[, 1:4], iris[, 5], max_depth = 3)
print(iris_tree)
print_decision_tree <- function(node, prefix = "") {
if (!is.null(node$prediction)) {
cat(prefix, "Predict:", node$prediction, "\n")
} else {
cat(prefix, "If", node$feature, "<", node$split, ":\n")
print_decision_tree(node$left, paste0(prefix, "  "))
cat(prefix, "Else (", node$feature, ">=", node$split, "):\n")
print_decision_tree(node$right, paste0(prefix, "  "))
}
}
iris_tree <- create_decision_tree(iris[, 1:4], iris[, 5], max_depth = 3)
print_decision_tree(iris_tree)
library(rpart)
# Building the decision tree model with rpart
rpart_tree <- rpart(Species ~ ., data = iris, method = "class")
summary(rpart_tree)
library(rpart.plot)
rpart.plot(rpart_tree, type = 3, box.palette = "RdBu", extra = 104)
predicted_manual <- sapply(1:nrow(iris), function(i) predict_decision_tree(iris_tree, iris[i, ]))
predicted_rpart <- predict(rpart_tree, iris, type = "class")
rpart_accuracy <- mean(predicted_rpart == iris$Species)
cat("Manual Tree Accuracy:", manual_accuracy, "\n")
cat("Rpart Tree Accuracy:", rpart_accuracy, "\n")
predicted_rpart <- predict(rpart_tree, iris, type = "class")
rpart_accuracy <- mean(predicted_rpart == iris$Species)
cat("Rpart Tree Accuracy:", rpart_accuracy, "\n")
predict_manual_tree <- function(tree, newdata) {
if (!is.null(tree$prediction)) {
# If it's a leaf node, return the prediction
return(tree$prediction)
} else {
# Determine whether to follow the left or right branch
if (!is.null(tree$split) && newdata[[tree$feature]] < tree$split) {
return(predict_manual_tree(tree$left, newdata))
} else {
return(predict_manual_tree(tree$right, newdata))
}
}
}
# Use sapply to predict each row in the iris dataset
predicted_manual <- sapply(1:nrow(iris), function(i) predict_manual_tree(iris_tree, iris[i, ]))
# Calculating manual tree accuracy
manual_accuracy <- mean(predicted_manual == iris$Species)
# Comparing with the rpart tree
library(rpart)
rpart_tree <- rpart(Species ~ ., data = iris, method = "class")
predicted_rpart <- predict(rpart_tree, iris, type = "class")
rpart_accuracy <- mean(max.col(predicted_rpart) == as.integer(iris$Species))
cat("Manual Tree Accuracy:", manual_accuracy, "\n")
cat("Rpart Tree Accuracy:", rpart_accuracy, "\n")
cat("Manual Tree Accuracy:", manual_accuracy, "\n")
cat("Rpart Tree Accuracy:", rpart_accuracy, "\n")
predicted_manual <- sapply(1:nrow(iris), function(i) predict_decision_tree(iris_tree, iris[i, ]))
predicted_manual <- sapply(1:nrow(iris), function(i) predict_manual_tree(iris_tree, iris[i, ]))
manual_accuracy <- mean(predicted_manual == iris$Species)
# For the rpart tree
predicted_rpart <- predict(rpart_tree, iris, type = "class")
rpart_accuracy <- mean(predicted_rpart == iris$Species)
manual_accuracy
rpart_accuracy
cat("Manual Tree Accuracy:", manual_accuracy, "\n")
cat("Rpart Tree Accuracy:", rpart_accuracy, "\n")
cat("Manual Tree Accuracy:", manual_accuracy, "\n")
cat("Rpart Tree Accuracy:", rpart_accuracy, "\n")
