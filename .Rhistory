X2 <- X1 + rnorm(n, mean = 0, sd = 5) # Positively correlated with X1
X3 <- X1 + rnorm(n, mean = 0, sd = 5) # Also positively correlated with X1
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 5) # Dependent variable
data <- data.frame(X1, X2, X3, y)
# Quick look at the relationships
ggpairs(data)
# Quick look at the relationships
ggpairs(data)
# Interactive 3D plot to visualize the relationships
fig <- plot_ly(data, x = ~X1, y = ~X2, z = ~X3, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'X1'),
yaxis = list(title = 'X2'),
zaxis = list(title = 'X3')))
fig
# Interactive 3D plot to visualize the relationships
fig <- plot_ly(data, x = ~X1, y = ~X2, z = ~X3, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'X1'),
yaxis = list(title = 'X2'),
zaxis = list(title = 'X3')))
fig
fig
# Interactive 3D plot to visualize the relationships
fig <- plot_ly(data, x = ~X1, y = ~X2, z = ~X3, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'X1'),
yaxis = list(title = 'X2'),
zaxis = list(title = 'X3')))
fig
# Perform PCA
pca_result <- prcomp(data[,1:3], center = TRUE, scale. = TRUE)
# Print summary of PCA to see explained variance
summary(pca_result)
# Plot PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Plot PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
library(ggbio)
install.packages('ggbio')
# Split data into training and testing sets
set.seed(123)
trainingIndex <- createDataPartition(data$y, p = .8, list = FALSE)
trainingData <- data[trainingIndex, ]
testingData <- data[-trainingIndex, ]
# PCA on training data
pca_train <- prcomp(trainingData[,1:3], center = TRUE, scale. = TRUE)
# Predict using the first two principal components
predictors <- predict(pca_train, newdata = testingData[,1:3])[,1:2]
# Fit model
model <- lm(y ~ ., data = as.data.frame(predictors))
library(prcomp) # For PCA
install.packages("prcomp")
install.packages("ggfortify")
library(ggfortify)
# Plot PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Perform PCA
pca_result <- prcomp(data[,1:3], center = TRUE, scale. = TRUE)
# Print summary of PCA to see explained variance
summary(pca_result)
# Plot PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Split data into training and testing sets
set.seed(123)
trainingIndex <- createDataPartition(data$y, p = .8, list = FALSE)
trainingData <- data[trainingIndex, ]
testingData <- data[-trainingIndex, ]
# PCA on training data
pca_train <- prcomp(trainingData[,1:3], center = TRUE, scale. = TRUE)
# Predict using the first two principal components
predictors <- predict(pca_train, newdata = testingData[,1:3])[,1:2]
# Fit model
model <- lm(y ~ ., data = as.data.frame(predictors))
# Splitting data into training and testing sets
trainingIndex <- createDataPartition(data$y, p = .8, list = FALSE)
trainingData <- data[trainingIndex, ]
testingData <- data[-trainingIndex, ]
# Perform PCA on training data
pca_train <- prcomp(trainingData[,1:3], center = TRUE, scale. = TRUE)
# Use PCA to predict on testing data, keeping the first two principal components
predictors <- predict(pca_train, newdata = testingData[,1:3])[,1:2]
# Create a new dataframe for modeling that includes the response variable from the testing set
modelingData <- as.data.frame(predictors)
modelingData$y <- testingData$y
# Fit the linear model using the predictors and the response variable
model <- lm(y ~ ., data = modelingData)
# Output the summary of the model to see the results
summary(model)
# Predict on testing data
predictions <- predict(model, newdata = as.data.frame(predictors))
# Calculate RMSE
rmse <- sqrt(mean((predictions - testingData$y)^2))
print(paste("RMSE:", rmse))
# Fit a linear model using all original predictors
full_model <- lm(y ~ X1 + X2 + X3, data = trainingData)
# Summary of the full model to see the results
summary(full_model)
# Predict on testing data
full_predictions <- predict(full_model, newdata = testingData)
# Calculate RMSE for the full model
full_rmse <- sqrt(mean((full_predictions - testingData$y)^2))
print(paste("Full Model RMSE:", full_rmse))
# Load necessary libraries
library(ggplot2)
library(plotly) # For interactive 3D plotting
library(caret)  # For PCA and regression
library(GGally) # For ggpairs
library(ggfortify)
set.seed(123) # Ensuring our pretend data is the same each time
# Generate our dataset
n <- 100 # Observations
X1 <- rnorm(n, mean = 50, sd = 10)
X2 <- X1 + rnorm(n, mean = 0, sd = 5) # Correlated with X1
X3 <- X1 + rnorm(n, mean = 0, sd = 5) # Also correlated with X1
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 5) # Our dependent variable
data <- data.frame(X1, X2, X3, y)
# Let's peek at the relationships
ggpairs(data)
# Perform PCA
pca_result <- prcomp(data[,1:3], center = TRUE, scale. = TRUE)
# Let's see the summary of our PCA to understand the variance explained
summary(pca_result)
# And visualize the PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Perform PCA
pca_result <- prcomp(data[,1:3], center = TRUE, scale. = TRUE)
# Let's see the summary of our PCA to understand the variance explained
summary(pca_result)
# And visualize the PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Split data into training and testing sets
set.seed(123) # Ensuring reproducibility
# Splitting data
trainingIndex <- createDataPartition(data$y, p = .8, list = FALSE)
trainingData <- data[trainingIndex, ]
testingData <- data[-trainingIndex, ]
# Perform PCA on training data
pca_train <- prcomp(trainingData[,1:3], center = TRUE, scale. = TRUE)
# Use PCA for predictions, keeping first two components
predictors <- predict(pca_train, newdata = testingData[,1:3])[,1:2]
# Preparing data for modeling
modelingData <- as.data.frame(predictors)
modelingData$y <- testingData$y
# Fit the linear model
model <- lm(y ~ ., data = modelingData)
# Model summary
summary(model)
# Predict and calculate RMSE
predictions <- predict(model, newdata = modelingData)
rmse <- sqrt(mean((predictions - testingData$y)^2))
print(paste("RMSE:", rmse))
# Traditional linear regression model
full_model <- lm(y ~ X1 + X2 + X3, data = trainingData)
summary(full_model)
# Prediction and RMSE calculation
full_predictions <- predict(full_model, newdata = testingData)
full_rmse <- sqrt(mean((full_predictions - testingData$y)^2))
print(paste("Full Model RMSE:", full_rmse))
# Load necessary libraries
library(ggplot2)
library(plotly) # For interactive 3D plotting
library(caret)  # For PCA and regression
library(GGally) # For ggpairs
library(ggfortify)
set.seed(123) # Ensuring our pretend data is the same each time
# Generate our dataset
X1 = np.random.normal(50, 10, n)
# Load necessary libraries
library(ggplot2)
library(plotly) # For interactive 3D plotting
library(caret)  # For PCA and regression
library(GGally) # For ggpairs
library(ggfortify)
set.seed(123) # Ensuring our pretend data is the same each time
# Generate our dataset
X1 <- rnorm(n, mean = 50, sd = 10)
X2 <- X1 + rnorm(n, mean = 0, sd = 10)
X3 <- X1 + rnorm(n, mean = 0, sd = 10)
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 20)    # Our dependent variable
data <- data.frame(X1, X2, X3, y)
# Let's peek at the relationships
ggpairs(data)
# Interactive 3D plot to visualize relationships
fig <- plot_ly(data, x = ~X1, y = ~X2, z = ~X3, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'X1'),
yaxis = list(title = 'X2'),
zaxis = list(title = 'X3')))
fig
# Load necessary libraries
library(ggplot2)
library(plotly) # For interactive 3D plotting
library(caret)  # For PCA and regression
library(GGally) # For ggpairs
library(ggfortify)
set.seed(123) # Ensuring our pretend data is the same each time
# Generate our dataset
X1 <- rnorm(n, mean = 50, sd = 10)
X2 <- X1 + rnorm(n, mean = 0, sd = 10)
X3 <- X1 + rnorm(n, mean = 0, sd = 10)
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 20)    # Our dependent variable
data <- data.frame(X1, X2, X3, y)
# Let's peek at the relationships
ggpairs(data)
# Generate our dataset
X1 <- rnorm(n, mean = 50, sd = 10)
X2 <- X1 + rnorm(n, mean = 0, sd = 10)
X3 <- X1 + rnorm(n, mean = 0, sd = 10)
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 20)    # Our dependent variable
# Load necessary libraries
library(ggplot2)
library(plotly) # For interactive 3D plotting
library(caret)  # For PCA and regression
library(GGally) # For ggpairs
library(ggfortify)
set.seed(123) # Ensuring our pretend data is the same each time
# Generate our dataset
n <- 100 # Number of observations
X1 <- rnorm(n, mean = 50, sd = 10)
X2 <- X1 + rnorm(n, mean = 0, sd = 10) # More noise added
X3 <- X1 + rnorm(n, mean = 0, sd = 10) # More noise added
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 20) # Increase noise in the dependent variable
data <- data.frame(X1, X2, X3, y)
# Let's peek at the relationships
ggpairs(data)
# Interactive 3D plot to visualize relationships
fig <- plot_ly(data, x = ~X1, y = ~X2, z = ~X3, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'X1'),
yaxis = list(title = 'X2'),
zaxis = list(title = 'X3')))
fig
n <- 300 # Number of observations
X1 <- rnorm(n, mean = 50, sd = 10)
X2 <- X1 + rnorm(n, mean = 0, sd = 10) # More noise added
X3 <- X1 + rnorm(n, mean = 0, sd = 10) # More noise added
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 20) # Increase noise in the dependent variable
data <- data.frame(X1, X2, X3, y)
# Let's peek at the relationships
ggpairs(data)
# Interactive 3D plot to visualize relationships
fig <- plot_ly(data, x = ~X1, y = ~X2, z = ~X3, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'X1'),
yaxis = list(title = 'X2'),
zaxis = list(title = 'X3')))
fig
# Perform PCA
pca_result <- prcomp(data[,1:3], center = TRUE, scale. = TRUE)
# Let's see the summary of our PCA to understand the variance explained
summary(pca_result)
# And visualize the PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Split data into training and testing sets
set.seed(123) # Ensuring reproducibility
# Splitting data
trainingIndex <- createDataPartition(data$y, p = .8, list = FALSE)
trainingData <- data[trainingIndex, ]
testingData <- data[-trainingIndex, ]
# Perform PCA on training data
pca_train <- prcomp(trainingData[,1:3], center = TRUE, scale. = TRUE)
# Use PCA for predictions, keeping first two components
predictors <- predict(pca_train, newdata = testingData[,1:3])[,1:2]
# Preparing data for modeling
modelingData <- as.data.frame(predictors)
modelingData$y <- testingData$y
# Fit the linear model
model <- lm(y ~ ., data = modelingData)
# Model summary
summary(model)
# Predict and calculate RMSE
predictions <- predict(model, newdata = modelingData)
rmse <- sqrt(mean((predictions - testingData$y)^2))
print(paste("RMSE:", rmse))
# Traditional linear regression model
full_model <- lm(y ~ X1 + X2 + X3, data = trainingData)
summary(full_model)
# Prediction and RMSE calculation
full_predictions <- predict(full_model, newdata = testingData)
full_rmse <- sqrt(mean((full_predictions - testingData$y)^2))
print(paste("Full Model RMSE:", full_rmse))
wage <- Wage
mtcars
glimpse(mtcars)
data <- mtcars %>% select(mpg, wt, qsec, am)
# Let's peek at the relationships
ggpairs(data)
# Interactive 3D plot to visualize relationships
fig <- plot_ly(data, x = ~X1, y = ~X2, z = ~X3, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'X1'),
yaxis = list(title = 'X2'),
zaxis = list(title = 'X3')))
fig
# Interactive 3D plot to visualize relationships
fig <- plot_ly(data, x = ~wt, y = ~qsec, z = ~am, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'X1'),
yaxis = list(title = 'X2'),
zaxis = list(title = 'X3')))
fig
data <- mtcars %>% select(mpg, disp, hp, qsec)
# Let's peek at the relationships
ggpairs(data)
# Interactive 3D plot to visualize relationships
fig <- plot_ly(data, x = ~disp,y = ~hp, z = ~qsec, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'disp'),
yaxis = list(title = 'hp'),
zaxis = list(title = 'qsec')))
fig
# Perform PCA
pca_result <- prcomp(data[,1:3], center = TRUE, scale. = TRUE)
# Let's see the summary of our PCA to understand the variance explained
summary(pca_result)
# And visualize the PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Perform PCA
pca_result <- prcomp(data[,2:4], center = TRUE, scale. = TRUE)
# Let's see the summary of our PCA to understand the variance explained
summary(pca_result)
# And visualize the PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
rnorm()
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(3)
rnorm(3)
rnorm(1)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
# Load necessary libraries
library(ggplot2)
library(plotly) # For interactive 3D plotting
library(caret)  # For PCA and regression
library(GGally) # For ggpairs
library(ggfortify)
set.seed(123) # Ensuring our pretend data is the same each time
# Generate our dataset
n <- 300 # Number of observations
X1 <- rnorm(n, mean = 50, sd = 10)
X2 <- X1 + rnorm(n, mean = 0, sd = 25) # More noise added
X3 <- X1 + rnorm(n, mean = 0, sd = 10) # More noise added
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 30) # Increase noise in the dependent variable
data <- data.frame(X1, X2, X3, y)
# Let's peek at the relationships
ggpairs(data)
# Perform PCA
pca_result <- prcomp(data[,1:3], center = TRUE, scale. = TRUE)
# Let's see the summary of our PCA to understand the variance explained
summary(pca_result)
# And visualize the PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Split data into training and testing sets
set.seed(123) # Ensuring reproducibility
# Splitting data
trainingIndex <- createDataPartition(data$y, p = .8, list = FALSE)
trainingData <- data[trainingIndex, ]
testingData <- data[-trainingIndex, ]
# Perform PCA on training data
pca_train <- prcomp(trainingData[,1:3], center = TRUE, scale. = TRUE)
# Use PCA for predictions, keeping first two components
predictors <- predict(pca_train, newdata = testingData[,1:3])[,1:2]
# Preparing data for modeling
modelingData <- as.data.frame(predictors)
modelingData$y <- testingData$y
# Fit the linear model
model <- lm(y ~ ., data = modelingData)
# Model summary
summary(model)
# Predict and calculate RMSE
predictions <- predict(model, newdata = modelingData)
rmse <- sqrt(mean((predictions - testingData$y)^2))
print(paste("RMSE:", rmse))
library(ISLR)
insurance_data <- Caravan
View(insurance_data)
?Caravan
library(glmnet)
?cv.glmnet
set.seed(222) # Important for replicability
names(inurance_data)
names(insurance_data)
lasso_ins <- cv.glmnet(x = as.matrix(insurance_data[, 1:85]), # the features
y = as.numeric(insurance_data[, 86]), # the outcome
standardize = TRUE, # Why do we do this?
alpha = 1) # Corresponds to LASSO
print(lasso_ins$lambda)
plot(lasso_ins)
LassoCV <- lasso_ins$glmnet.fit
plot(LassoCV, label = TRUE, xvar = "lambda")
abline(
v = log(c(lasso_ins$lambda.min, lasso_ins$lambda.1se))
) # Adds lines to mark the two key lambda values
predict(lasso_ins, type = "coefficients",
s = c(lasso_ins$lambda.min, lasso_ins$lambda.1se))
lasso_ins
predict(lasso_ins, type = "coefficients",
s = c(lasso_ins$lambda.min, lasso_ins$lambda.1se))
lasso_ins$lambda.min
lasso_ins
View(lasso_ins)
lasso_ins$cvm
lasso_ins$cvm[lasso_ins$lambda == lasso_ins$lambda.min]
lasso_ins$cvsd[lasso_ins$lambda == lasso_ins$lambda.min]
lasso_ins$lambda.1se
lasso_ins$lambda
lasso_ins$lambda == lasso_ins$lambda.min
lasso_ins$cvsd[19]
lasso_ins$cvm[lasso_ins$lambda == lasso_ins$lambda.min]
lasso_ins$cvsd
lasso_ins$cvm[lasso_ins$lambda == lasso_ins$lambda.min]
lasso_ins$cvm[19]
wage_data <- Wage
View(wage_data)
summary(wage_data)
str(wage_data)
levels(wage_data$maritl)
levels(wage_data$region)
wage_data <- wage_data[, -11]
for(i in 10:1){
print(i)
}
for(i in 10:1){
if(is.factor(wage_data[, i])){
for(j in unique(wage_data[, i])){
new_col <- paste(colnames(wage_data)[i], j, sep = "_")
wage_data[, new_col] <- as.numeric(wage_data[, i] == j)
}
wage_data <- wage_data[, -i]
} else if(typeof(wage_data[, i]) == "integer") {
wage_data[, i] <- as.numeric(as.character(wage_data[, i]))
}
}
#View(wage_data)
summary(wage_data)
str(wage_data)
set.seed(222)
train <- sample(seq(nrow(wage_data)),
floor(nrow(wage_data)*0.8))
train <- sort(train)
test <- which(!(seq(nrow(wage_data)) %in% train))
library(pls)
## Try running PCR
pcr_fit  <- pcr(logwage ~ ., data = wage_data[train,],
scale = TRUE, validation = "CV")
for(col_num in 1:ncol(wage_data)){
if(var(wage_data[, col_num]) < 0.05){
print(colnames(wage_data)[col_num])
print(var(wage_data[, col_num]))
}
}
## Let's drop these low variance columns
for(col_num in ncol(wage_data):1){
if(var(wage_data[, col_num]) < 0.05) {
wage_data <- wage_data[, -col_num]
}
}
set.seed(222)
pcr_fit <- pcr(logwage ~ ., data = wage_data[train,],
scale = TRUE, validation = "CV")
summary(pcr_fit)
pcr_msep <- MSEP(pcr_fit)
pcr_min_indx <- which.min(pcr_msep$val[1, 1,])
print(pcr_min_indx)
print(pcr_min_indx)[[1]]
print(pcr_min_indx)[1]
print(pcr_min_indx)
summary(pcr_fit)
print(pcr_msep$val[1, 1, pcr_min_indx])
summary(pcr_fit)
print(pcr_msep$val[1, 1, pcr_min_indx])
validationplot(pcr_fit)
pcr_pred <- predict(pcr_fit, wage_data[test, ], ncomp = 12)
pcr_pred <- predict(pcr_fit, wage_data[test, ], ncomp = 12)
pcr_test_MSE <- mean((pcr_pred - wage_data[test, "logwage"])^2)
print(pcr_test_MSE)
pcr_pred
print(pcr_test_MSE)
print(sqrt(pcr_test_MSE))
## Step 1: Fit the model
pls_fit <- plsr(logwage ~ ., data = wage_data[train, ],
scale = TRUE, validation = "CV")
summary(pls_fit)
## Step 2: Which ncomp value had the lowest CV MSE?
pls_msep <- MSEP(pls_fit)
pls_min_indx <- which.min(pls_msep$val[1, 1,])
print(pls_min_indx)
## Step 3: Plot validation error as a function of # of components
validationplot(pls_fit, val.type = c("RMSEP"))
## Step 4: Identify the CV RMSE for the number of components with
## the lowest CV RMSE
pls_rmsep <- RMSEP(pls_fit)
print(pls_rmsep$val[1, 1, as.numeric(pls_min_indx)])
## Step 5: Predict test set logwage values
pls_pred <- predict(pls_fit, wage_data[test,],
ncomp = (as.numeric(pls_min_indx) -1))
## Step 6: Measure test MSE and RMSE
pls_test_MSE <- mean((pls_pred - wage_data[test, "logwage"])^2)
print(pls_test_MSE)
print(sqrt(pls_test_MSE))
print(sqrt(pls_test_MSE))
