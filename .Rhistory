library(ggplot2)
ggplot(wage_data[test,], aes(x = age, y = wage, color = education)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "loess", span = 0.2) +
geom_smooth(method = "loess", span = 0.5)
ggplot(wage_data[test,], aes(x = age, y = wage, color = education)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "loess", span = 0.2) +
geom_smooth(method = "loess", span = 0.5) +
facet_wrap(~education)
ggplot(wage_data[test,], aes(x = age, y = wage, color = education)) +
geom_point(alpha = 0.5) +
#geom_smooth(method = "loess", span = 0.2) +
geom_smooth(method = "loess", span = 0.5) +
facet_wrap(~education)
ggplot(wage_data[test,], aes(x = age, y = wage, color = education)) +
geom_point(alpha = 0.5) +
#geom_smooth(method = "loess", span = 0.2) +
geom_smooth(method = "loess", span = 1) +
facet_wrap(~education)
?geom_smooth
ggplot(wage_data[test,], aes(x = age, y = wage, color = education)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", se = T, color = "black") +
#geom_smooth(method = "loess", span = 0.2) +
#geom_smooth(method = "loess", span = 1) +
facet_wrap(~education)
ggplot(wage_data[test,], aes(x = age, y = wage, color = education)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", se = T, color = "black") +
#geom_smooth(method = "loess", span = 0.2) +
geom_smooth(method = "loess", span = 0.5) +
facet_wrap(~education)
local2_age <- loess(wage ~ age, span = 0.2, data = wage_data)
local5_age <- loess(wage ~ age, span = 0.5, data = wage_data)
pred_local2_age <- predict(local2_age, newdata = data.frame(age = age_grid))
pred_local5_age <- predict(local5_age, newdata = data.frame(age = age_grid))
plot(wage_data[test, "age"], wage_data[test, "wage"],
cex =.5, col = "darkgrey",
xlab = "age", ylab = "wage")
lines(age_grid, pred_local2_age, col = "red", lwd = 2)
lines(age_grid, pred_local5_age, col = "blue", lwd = 2)
gam_yae <- lm(wage ~ ns(year, 4) + ns(age, 4) + education,
data = wage_data[train,])
library(gam)
gam_smooth <- gam(wage ~ s(year, 4, spar = 0.5) + s(age, 5) + education,
data = wage_data[train,])
par(mfrow = c(1, 3))
plot(gam_smooth, se = TRUE, col ="blue ")
plot.Gam(gam_yae, se = TRUE, col = "red") # Note the capitalization
gam_yae_pred <- predict(gam_yae, newdata = wage_data[test,])
gam_smooth_pred <- predict(gam_smooth, newdata = wage_data[test,])
print(msep_func(gam_yae_pred, wage_data[test, "wage"]))
print(msep_func(gam_smooth_pred, wage_data[test, "wage"]))
?s
gam_lo <- gam(wage ~ s(year, df = 4) + lo(age, span = 0.7) + education,
data = wage_data[train,])
plot.Gam(gam_lo, se = TRUE, col = "green")
gam_2lo <- gam(wage ~ lo(year, age, span =0.5) + education,
data = wage_data[train,])
?s
gam_2lo <- gam(wage ~ lo(year, age, span =0.5) + education,
data = wage_data[train,])
gam_2lo <- gam(wage ~ lo(year, age, span =0.7) + education,
data = wage_data[train,])
gam_2lo <- gam(wage ~ lo(year, age, span =0.9) + education,
data = wage_data[train,])
gam_2lo <- gam(wage ~ lo(year, age, span =0.2) + education,
data = wage_data[train,])
plot.Gam(gam_2lo, se = TRUE, col = "purple")
gam_2lo
#install.packages("tree")
library(ISLR)
library(tree)
library(ggplot2)
carseat_data <- Carseats
high_sales <- as.factor(ifelse(carseat_data$Sales > 8, "Yes", "No"))
carseat_data <- data.frame(carseat_data, high_sales)
carseat_data = carseat_data[, -1]
set.seed(222)
train <- sample(seq(nrow(carseat_data)),
round(nrow(carseat_data) * 0.5))
train <- sort(train)
test <- which(!(seq(nrow(carseat_data)) %in% train))
?tree
carseats_tree <- tree(high_sales ~ ., data = carseat_data[train,])
plot(carseats_tree)
text(carseats_tree, pretty = 0)
carseats_tree
carseats_tree
carseats_tree <- tree(high_sales ~ .,
split = "gini",
data = carseat_data[train,])
plot(carseats_tree)
text(carseats_tree, pretty = 0)
carseats_tree
carseats_tree <- tree(high_sales ~ .,
data = carseat_data[train,])
plot(carseats_tree)
text(carseats_tree, pretty = 0)
error_rate_func <- function(predictions, true_vals) {
error_rate <- mean(as.numeric(predictions != true_vals))
return(error_rate)
}
deep_tree_preds <- predict(carseats_tree, carseat_data[test,],
type = "class")
error_rate_func(predictions = deep_tree_preds,
true_vals = carseat_data[test, "high_sales"])
summary(carseats_tree)
error_rate_func(predictions = deep_tree_preds,
true_vals = carseat_data[test, "high_sales"])
summary(carseats_tree)
error_rate_func(predictions = deep_tree_preds,
true_vals = carseat_data[test, "high_sales"])
summary(carseats_tree)
set.seed(222)
cv_carseats_tree  <- cv.tree(carseats_tree, FUN = prune.misclass)
names(cv_carseats_tree)
cv_carseats_tree
par(mfrow = c(1, 2))
plot(cv_carseats_tree$size, cv_carseats_tree$dev, type = "b")
plot(cv_carseats_tree$k, cv_carseats_tree$dev, type = "b")
opt_indx <- which.min(cv_carseats_tree$dev)
opt_size <- cv_carseats_tree$size[opt_indx]
print(opt_size)
opt_size <- 7
pruned_carseats_tree <- prune.misclass(carseats_tree, best = opt_size)
plot(pruned_carseats_tree)
text(pruned_carseats_tree, pretty = 0)
pruned_tree_preds = predict(pruned_carseats_tree, carseat_data[test, ],
type = "class")
error_rate_func(predictions = pruned_tree_preds,
true_vals = carseat_data[test, "high_sales"])
pruned_tree_preds = predict(pruned_carseats_tree, carseat_data[test, ],
type = "class")
error_rate_func(predictions = pruned_tree_preds,
true_vals = carseat_data[test, "high_sales"])
library(MASS)
boston_data <- Boston
set.seed(222)
train <- sample(seq(nrow(boston_data)),
round(nrow(boston_data) * 0.8))
train <- sort(train)
test <- which(!(seq(nrow(boston_data)) %in% train))
boston_tree = tree(medv ~ ., Boston, subset = train)
summary(boston_tree)
plot(boston_tree)
text(boston_tree)
?Boston
boston_preds <- predict(boston_tree, newdata = boston_data[test,])
msep_func <- function(predictions, true_vals) {
MSEP <- mean((predictions - true_vals)^2)
return(MSEP)
}
print(msep_func(predictions = boston_preds,
true_vals = boston_data[test, "medv"]))
cv_boston_tree = cv.tree(boston_tree)
plot(cv_boston_tree$size, cv_boston_tree$dev, type = 'b')
cv_boston_tree
best_indx <- which.min(cv_boston_tree$dev)
best_size <- cv_boston_tree$size[best_indx]
best_size
prune_boston = prune.tree(boston_tree, best = best_size)
boston_prune_preds <- predict(prune_boston, newdata = boston_data[test,])
print(msep_func(boston_prune_preds, boston_data[test, "medv"]))
prune_boston = prune.tree(boston_tree, best = 4)
boston_prune_preds <- predict(prune_boston, newdata = boston_data[test,])
print(msep_func(boston_prune_preds, boston_data[test, "medv"]))
prune_boston = prune.tree(boston_tree, best = 7)
boston_prune_preds <- predict(prune_boston, newdata = boston_data[test,])
print(msep_func(boston_prune_preds, boston_data[test, "medv"]))
boston_data <- Boston
set.seed(222)
train <- sample(seq(nrow(boston_data)),
round(nrow(boston_data) * 0.8))
train <- sort(train)
test <- which(!(seq(nrow(boston_data)) %in% train))
## install.packages("randomForest")
library(randomForest)
## install.packages("randomForest")
library(randomForest)
rf.boston <- randomForest(medv ~ ., data = data.frame(boston_data[-test,]),
importance = TRUE, n.trees = 5000)
## Predictions
yhat.rf <- predict (rf.boston, newdata = Boston[-train ,])
boston.test = Boston[-train, "medv"]
mean((yhat.rf - boston.test)^2)
# get importance
rf.boston$importance
# get importance plot
varImpPlot(rf.boston)
bag.boston <- randomForest(medv ~ ., data = data.frame(boston_data[-test,]),
mtry = 13, importance = TRUE)
bag.boston <- randomForest(medv ~ ., data = data.frame(boston_data[-test,]),
mtry = 13, importance = TRUE)
bag.boston$importance
yhat.bag <- predict(bag.boston, newdata = Boston[-train,])
mean((yhat.bag - boston.test)^2)
## install.packages("gbm")
library(gbm)
set.seed(222)
?gbm
## Boosting model
boost.boston <- gbm(medv ~ ., data = data.frame(boston_data[-test,]),
distribution = "gaussian", n.trees = 5000,
interaction.depth = 4)
plot <- summary(boost.boston, plot = F)
## create a ggplot bar plot with labels of plot object
ggplot(plot, aes(x = reorder(var, -rel.inf), y = rel.inf)) +
geom_bar(stat = "identity") +
coord_flip() +
xlab("Variable") +
ylab("Relative Importance") +
ggtitle("Relative Importance of Variables in Boosting Model") +
theme_minimal()
# create a shap plot of the boosting model
library(iml)
install.packages('iml')
# create a shap plot of the boosting model
library(iml)
predictor <- Predictor$new(
model = boost.boston,
data = boston_data[-train, -which(names(boston_data) == "medv")],
y = boston_data[-train, "medv"]
)
shap <- Shapley$new(predictor, x.interest = boston_data[train[1], -which(names(boston_data) == "medv")])
plot(shap)
## create a ggplot bar plot with labels of plot object
ggplot(plot, aes(x = reorder(var, -rel.inf), y = rel.inf)) +
geom_bar(stat = "identity") +
coord_flip() +
xlab("Variable") +
ylab("Relative Importance") +
ggtitle("Relative Importance of Variables in Boosting Model") +
theme_minimal()
library(ISLR)
college_data <- College
library(dplyr)
transformed_data <- college_data %>%
mutate(accept_rate = Accept/Apps) %>%
select(accept_rate, Outstate, Private)
transformed_data <- transformed_data %>%
mutate(accept_rate = scale(accept_rate),
Outstate = scale(Outstate))
set.seed(222)
small_data <- transformed_data[sample(1:nrow(transformed_data), 100),]
x <- as.matrix(small_data[, 1:2])
y <- if_else(small_data[, 3] == "Yes", 1, -1)
summary(transformed_data)
plot(small_data[, c("accept_rate", "Outstate")],
col = small_data$Private)
# install.packages("e1071")
library(e1071)
# install.packages("e1071")
library(e1071)
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "linear",
cost = 10,
scale = FALSE)
plot(college_svm1, small_data)
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "radial",
cost = 10,
scale = FALSE)
plot(college_svm1, small_data)
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "polynomial",
cost = 10,
scale = FALSE)
plot(college_svm1, small_data)
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "linear",
cost = 10,
scale = FALSE)
plot(college_svm1, small_data)
summary(college_svm1)
college_svm1$index
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "linear",
cost = c(5,10,15),
scale = FALSE)
plot(college_svm1, small_data)
summary(college_svm1)
college_svm1$cost
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "linear",
cost = 0.001,
scale = FALSE)
plot(college_svm1, small_data)
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "linear",
cost = 1000,
scale = FALSE)
plot(college_svm1, small_data)
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "linear",
cost = 1000000,
scale = FALSE)
plot(college_svm1, small_data)
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "linear",
cost = 10,
scale = FALSE)
plot(college_svm1, small_data)
?svm
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "linear",
scale = FALSE)
plot(college_svm1, small_data)
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "linear",
cost = 10,
scale = FALSE)
plot(college_svm1, small_data)
summary(college_svm1)
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "radial",
cost = 10,
scale = FALSE)
plot(college_svm1, small_data)
beta <- drop(t(college_svm1$coefs) %*% x[college_svm1$index,])
beta0 <- -college_svm1$rho
print(beta)
print(beta0)
make.grid = function(x, n = 75) {
grange <-  apply(x, 2, range)
x1 <- seq(from = grange[1, 1], to = grange[2, 1], length = n)
x2 <- seq(from = grange[1, 2], to = grange[2, 2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid <- make.grid(x)
colnames(xgrid) <- colnames(x)
ygrid <- predict(college_svm1, xgrid)
plot(xgrid,
col = c("red","blue")[as.numeric(ygrid)],
pch = 20,
cex = .2)
points(x,
col = y + 3,
pch = 19)
points(x[college_svm1$index,],
pch = 5,
cex = 2)
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "linear",
cost = 10,
scale = FALSE)
beta <- drop(t(college_svm1$coefs) %*% x[college_svm1$index,])
beta0 <- -college_svm1$rho
print(beta)
print(beta0)
make.grid = function(x, n = 75) {
grange <-  apply(x, 2, range)
x1 <- seq(from = grange[1, 1], to = grange[2, 1], length = n)
x2 <- seq(from = grange[1, 2], to = grange[2, 2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid <- make.grid(x)
colnames(xgrid) <- colnames(x)
ygrid <- predict(college_svm1, xgrid)
plot(xgrid,
col = c("red","blue")[as.numeric(ygrid)],
pch = 20,
cex = .2)
points(x,
col = y + 3,
pch = 19)
points(x[college_svm1$index,],
pch = 5,
cex = 2)
plot(xgrid,
col = c("red","blue")[as.numeric(ygrid)],
pch = 20,
cex = .2)
points(x,
col = y + 3,
pch = 19)
points(x[college_svm1$index,],
pch = 5,
cex = 2)
abline(-beta0 / beta[2], -beta[1]/beta[2])
plot(xgrid,
col = c("red","blue")[as.numeric(ygrid)],
pch = 20,
cex = .2)
points(x,
col = y + 3,
pch = 19)
points(x[college_svm1$index,],
pch = 5,
cex = 2)
abline((-1 - beta0) / beta[2], -beta[1] / beta[2], lty = 2)
abline((+1 - beta0) / beta[2], -beta[1] / beta[2], lty = 2)
svm_cost <- function(df, cost_val, xgrid) {
svmfit <- svm(Private ~ Outstate + accept_rate,
data = df,
kernel = "linear",
cost = cost_val,
scale = FALSE)
print(paste("# of support vectors =", length(svmfit$index)))
beta <- drop(t(svmfit$coefs) %*% x[svmfit$index,])
beta0 <- -svmfit$rho
ygrid <- predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[df$index,], pch = 5, cex = 2)
abline(-beta0 / beta[2], -beta[1]/beta[2])
abline((-1 - beta0) / beta[2], -beta[1] / beta[2], lty = 2)
abline((+1 - beta0) / beta[2], -beta[1] / beta[2], lty = 2)
}
svm_cost(small_data, .01, xgrid)
set.seed(222)
set.seed(222)
tune_linear <- tune(svm,
Private ~ accept_rate + Outstate,
data = transformed_data,
kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_linear)
?tune
tune_linear <- tune(svm,
Private ~ accept_rate + Outstate,
data = transformed_data,
kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_linear)
best_linear_mod <- tune_linear$best.model
summary(best_linear_mod)
tune_linear_full <- tune(svm,
Private ~ .,
data = college_data,
kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1,
1, 5, 10, 100)))
summary(tune_linear_full)
svm_poly3 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "polynomial",
degree = 3,
cost = 1)
plot(svm_poly3, small_data)
ygrid <- predict(svm_poly3, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)],
pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
predicted_grid <- predict(svm_poly3, xgrid,
decision.values = TRUE)
predicted_grid <- attributes(predicted_grid)$decision
ygrid <- predict(svm_poly3, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)],
pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
contour(unique(xgrid[,1]), unique(xgrid[,2]),
matrix(predicted_grid, 75, 75),
level = 0,
add = TRUE)
set.seed(222)
tune_poly <- tune(svm,
Private ~ .,
data = transformed_data,
kernel = "polynomial",
ranges = list(cost = c(0.001, 0.1, 1, 5, 100),
degree = c(2, 3)))
summary(tune_poly)
set.seed(222)
tune_radial <- tune(svm, Private ~ .,
data = transformed_data,
kernel = "radial",
ranges = list(cost = c(0.001, 0.1, 1, 5, 100),
gamma = c(0.0001, 0.001, 0.01, 0.1, 1)))
summary(tune_radial)
svm_radial <- tune_radial$best.model
## Make predictions for the xgrid
ygrid <- predict(svm_radial, xgrid)
## And plot the grid predictions and decision boundary
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)],
pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
predicted_grid <- predict(svm_radial, xgrid, decision.values = TRUE)
predicted_grid <- attributes(predicted_grid)$decision
contour(unique(xgrid[,1]), unique(xgrid[,2]),
matrix(predicted_grid, 75, 75), level = 0, add = TRUE)
full_poly <- tune(svm,
Private ~ .,
data = college_data,
kernel = "polynomial",
ranges = list(cost = c(0.001, 0.1, 1, 5, 100),
degree = c(2, 3)))
summary(full_poly)
summary(tune_poly)
full_radial <- tune(svm,
Private ~ .,
data = college_data,
kernel = "radial",
ranges = list(cost = c(0.001, 0.1, 1, 5, 100),
gamma = c(0.0001, 0.001, 0.01, 0.1, 1)))
summary(full_radial)
summary(tune_radial)
summary(full_poly)
