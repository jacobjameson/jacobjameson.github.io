autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Perform PCA
pca_result <- prcomp(data[,2:4], center = TRUE, scale. = TRUE)
# Let's see the summary of our PCA to understand the variance explained
summary(pca_result)
# And visualize the PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
rnorm()
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(3)
rnorm(3)
rnorm(1)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
# Load necessary libraries
library(ggplot2)
library(plotly) # For interactive 3D plotting
library(caret)  # For PCA and regression
library(GGally) # For ggpairs
library(ggfortify)
set.seed(123) # Ensuring our pretend data is the same each time
# Generate our dataset
n <- 300 # Number of observations
X1 <- rnorm(n, mean = 50, sd = 10)
X2 <- X1 + rnorm(n, mean = 0, sd = 25) # More noise added
X3 <- X1 + rnorm(n, mean = 0, sd = 10) # More noise added
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 30) # Increase noise in the dependent variable
data <- data.frame(X1, X2, X3, y)
# Let's peek at the relationships
ggpairs(data)
# Perform PCA
pca_result <- prcomp(data[,1:3], center = TRUE, scale. = TRUE)
# Let's see the summary of our PCA to understand the variance explained
summary(pca_result)
# And visualize the PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Split data into training and testing sets
set.seed(123) # Ensuring reproducibility
# Splitting data
trainingIndex <- createDataPartition(data$y, p = .8, list = FALSE)
trainingData <- data[trainingIndex, ]
testingData <- data[-trainingIndex, ]
# Perform PCA on training data
pca_train <- prcomp(trainingData[,1:3], center = TRUE, scale. = TRUE)
# Use PCA for predictions, keeping first two components
predictors <- predict(pca_train, newdata = testingData[,1:3])[,1:2]
# Preparing data for modeling
modelingData <- as.data.frame(predictors)
modelingData$y <- testingData$y
# Fit the linear model
model <- lm(y ~ ., data = modelingData)
# Model summary
summary(model)
# Predict and calculate RMSE
predictions <- predict(model, newdata = modelingData)
rmse <- sqrt(mean((predictions - testingData$y)^2))
print(paste("RMSE:", rmse))
library(ISLR)
insurance_data <- Caravan
View(insurance_data)
?Caravan
library(glmnet)
?cv.glmnet
set.seed(222) # Important for replicability
names(inurance_data)
names(insurance_data)
lasso_ins <- cv.glmnet(x = as.matrix(insurance_data[, 1:85]), # the features
y = as.numeric(insurance_data[, 86]), # the outcome
standardize = TRUE, # Why do we do this?
alpha = 1) # Corresponds to LASSO
print(lasso_ins$lambda)
plot(lasso_ins)
LassoCV <- lasso_ins$glmnet.fit
plot(LassoCV, label = TRUE, xvar = "lambda")
abline(
v = log(c(lasso_ins$lambda.min, lasso_ins$lambda.1se))
) # Adds lines to mark the two key lambda values
predict(lasso_ins, type = "coefficients",
s = c(lasso_ins$lambda.min, lasso_ins$lambda.1se))
lasso_ins
predict(lasso_ins, type = "coefficients",
s = c(lasso_ins$lambda.min, lasso_ins$lambda.1se))
lasso_ins$lambda.min
lasso_ins
View(lasso_ins)
lasso_ins$cvm
lasso_ins$cvm[lasso_ins$lambda == lasso_ins$lambda.min]
lasso_ins$cvsd[lasso_ins$lambda == lasso_ins$lambda.min]
lasso_ins$lambda.1se
lasso_ins$lambda
lasso_ins$lambda == lasso_ins$lambda.min
lasso_ins$cvsd[19]
lasso_ins$cvm[lasso_ins$lambda == lasso_ins$lambda.min]
lasso_ins$cvsd
lasso_ins$cvm[lasso_ins$lambda == lasso_ins$lambda.min]
lasso_ins$cvm[19]
wage_data <- Wage
View(wage_data)
summary(wage_data)
str(wage_data)
levels(wage_data$maritl)
levels(wage_data$region)
wage_data <- wage_data[, -11]
for(i in 10:1){
print(i)
}
for(i in 10:1){
if(is.factor(wage_data[, i])){
for(j in unique(wage_data[, i])){
new_col <- paste(colnames(wage_data)[i], j, sep = "_")
wage_data[, new_col] <- as.numeric(wage_data[, i] == j)
}
wage_data <- wage_data[, -i]
} else if(typeof(wage_data[, i]) == "integer") {
wage_data[, i] <- as.numeric(as.character(wage_data[, i]))
}
}
#View(wage_data)
summary(wage_data)
str(wage_data)
set.seed(222)
train <- sample(seq(nrow(wage_data)),
floor(nrow(wage_data)*0.8))
train <- sort(train)
test <- which(!(seq(nrow(wage_data)) %in% train))
library(pls)
## Try running PCR
pcr_fit  <- pcr(logwage ~ ., data = wage_data[train,],
scale = TRUE, validation = "CV")
for(col_num in 1:ncol(wage_data)){
if(var(wage_data[, col_num]) < 0.05){
print(colnames(wage_data)[col_num])
print(var(wage_data[, col_num]))
}
}
## Let's drop these low variance columns
for(col_num in ncol(wage_data):1){
if(var(wage_data[, col_num]) < 0.05) {
wage_data <- wage_data[, -col_num]
}
}
set.seed(222)
pcr_fit <- pcr(logwage ~ ., data = wage_data[train,],
scale = TRUE, validation = "CV")
summary(pcr_fit)
pcr_msep <- MSEP(pcr_fit)
pcr_min_indx <- which.min(pcr_msep$val[1, 1,])
print(pcr_min_indx)
print(pcr_min_indx)[[1]]
print(pcr_min_indx)[1]
print(pcr_min_indx)
summary(pcr_fit)
print(pcr_msep$val[1, 1, pcr_min_indx])
summary(pcr_fit)
print(pcr_msep$val[1, 1, pcr_min_indx])
validationplot(pcr_fit)
pcr_pred <- predict(pcr_fit, wage_data[test, ], ncomp = 12)
pcr_pred <- predict(pcr_fit, wage_data[test, ], ncomp = 12)
pcr_test_MSE <- mean((pcr_pred - wage_data[test, "logwage"])^2)
print(pcr_test_MSE)
pcr_pred
print(pcr_test_MSE)
print(sqrt(pcr_test_MSE))
## Step 1: Fit the model
pls_fit <- plsr(logwage ~ ., data = wage_data[train, ],
scale = TRUE, validation = "CV")
summary(pls_fit)
## Step 2: Which ncomp value had the lowest CV MSE?
pls_msep <- MSEP(pls_fit)
pls_min_indx <- which.min(pls_msep$val[1, 1,])
print(pls_min_indx)
## Step 3: Plot validation error as a function of # of components
validationplot(pls_fit, val.type = c("RMSEP"))
## Step 4: Identify the CV RMSE for the number of components with
## the lowest CV RMSE
pls_rmsep <- RMSEP(pls_fit)
print(pls_rmsep$val[1, 1, as.numeric(pls_min_indx)])
## Step 5: Predict test set logwage values
pls_pred <- predict(pls_fit, wage_data[test,],
ncomp = (as.numeric(pls_min_indx) -1))
## Step 6: Measure test MSE and RMSE
pls_test_MSE <- mean((pls_pred - wage_data[test, "logwage"])^2)
print(pls_test_MSE)
print(sqrt(pls_test_MSE))
print(sqrt(pls_test_MSE))
library(tufte)
library(tidyverse)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
# Load necessary libraries
library(ggplot2)
library(plotly) # For interactive 3D plotting
library(caret)  # For PCA and regression
library(GGally) # For ggpairs
library(ggfortify)
set.seed(123) # Ensuring our pretend data is the same each time
# Generate our dataset
n <- 300 # Number of observations
X1 <- rnorm(n, mean = 50, sd = 10)
X2 <- X1 + rnorm(n, mean = 0, sd = 25) # More noise added
X3 <- X1 + rnorm(n, mean = 0, sd = 10) # More noise added
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 30) # Increase noise in the dependent variable
data <- data.frame(X1, X2, X3, y)
# Perform PCA
pca_result <- prcomp(data[,1:3], center = TRUE, scale. = TRUE)
# Let's see the summary of our PCA to understand the variance explained
summary(pca_result)
# And visualize the PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Splitting the dataset
trainingIndex <- createDataPartition(data$y, p = .8, list = FALSE)
trainingData <- data[trainingIndex, ]
testingData <- data[-trainingIndex, ]
# Performing PCA on training data
pca_train <- prcomp(trainingData[,1:3], center = TRUE, scale. = TRUE)
# Transforming both training and testing data using the PCA model
trainingData_transformed <- predict(pca_train, newdata = trainingData[,1:3])
testingData_transformed <- predict(pca_train, newdata = testingData[,1:3])
# Preparing data for modeling
trainingModelData <- as.data.frame(trainingData_transformed)
trainingModelData$y <- trainingData$y
# Fitting the linear model on transformed training data
model <- lm(y ~ ., data = trainingModelData)
# Summary of the model trained on PCA-transformed training data
summary(model)
# Predicting and calculating RMSE on transformed testing data
predictions <- predict(model, newdata = as.data.frame(testingData_transformed))
rmse <- sqrt(mean((predictions - testingData$y)^2))
print(paste("RMSE:", rmse))
# Transforming both training and testing data using the PCA model
trainingData_transformed <- predict(pca_train, newdata = trainingData[,1:3])[,1:2]
testingData_transformed <- predict(pca_train, newdata = testingData[,1:3])[,1:2]
# Preparing data for modeling
trainingModelData <- as.data.frame(trainingData_transformed)
trainingModelData$y <- trainingData$y
# Fitting the linear model on transformed training data
model <- lm(y ~ ., data = trainingModelData)
# Summary of the model trained on PCA-transformed training data
summary(model)
# Predicting and calculating RMSE on transformed testing data
predictions <- predict(model, newdata = as.data.frame(testingData_transformed))
rmse <- sqrt(mean((predictions - testingData$y)^2))
print(paste("RMSE:", rmse))
# Traditional linear regression model
full_model <- lm(y ~ X1 + X2 + X3, data = trainingData)
summary(full_model)
# Prediction and RMSE calculation
full_predictions <- predict(full_model, newdata = testingData)
full_rmse <- sqrt(mean((full_predictions - testingData$y)^2))
print(paste("Full Model RMSE:", full_rmse))
library(ISLR)
wage_data <- Wage
wage_data <- wage_data[, -10]
library(dplyr)
library(dplyr)
wage_data <- wage_data %>% arrange(age)
set.seed(222)
train <- sample(1:nrow(wage_data), round(nrow(wage_data) * 0.8))
train <- sort(train)
test <- which(!(seq(nrow(wage_data)) %in% train))
msep_func <- function(predictions, true_vals) {
MSEP <- mean((predictions - true_vals)^2)
return(MSEP)
}
age_poly <- lm(wage ~ poly(age, 4), data = wage_data[train,])
coef(summary(age_poly))
head(poly(wage_data$age, 4))
head(wage_data$age)
head(poly(wage_data$age, 4))
head(wage_data$age)
head(poly(wage_data$age, 4))
head(wage_data$age)
head(poly(wage_data$age, 4, raw = TRUE))
age_poly_TRUE <- lm(wage ~ poly(age, 4, raw = TRUE),
data = wage_data[train,])
coef(summary(age_poly_TRUE))
age_poly_pred <- predict(age_poly, newdata = wage_data[test,])
age_poly_TRUE_pred  <- predict(age_poly_TRUE, newdata = wage_data[test,])
head(age_poly_pred)
head(age_poly_TRUE_pred)
print(msep_func(age_poly_pred, wage_data[test, "wage"]))
pred_comparison <- cbind(age_poly_pred, age_poly_TRUE_pred)
colnames(pred_comparison) <- c("pred1", "pred2")
pred_comparison <- pred_comparison %>%
mutate(check_same = as.numeric(pred1 == pred2))
pred_comparison <- pred_comparison %>%
dplyr::mutate(check_same = as.numeric(pred1 == pred2))
pred_comparison <- as.dataframe(pred_comparison) %>%
mutate(check_same = as.numeric(pred1 == pred2))
pred_comparison <- as.data.frame(pred_comparison) %>%
mutate(check_same = as.numeric(pred1 == pred2))
pred_comparison <- data.frame(pred_comparison)
pred_comparison <- data.frame(pred_comparison) %>%
mutate(check_same = as.numeric(pred1 == pred2))
pred_comparison <- cbind(age_poly_pred, age_poly_TRUE_pred)
colnames(pred_comparison) <- c("pred1", "pred2")
pred_comparison <- pred_comparison %>%
mutate(check_same = as.numeric(pred1 == pred2))
pred_comparison <- data.frame(pred_comparison)
pred_comparison <- pred_comparison %>%
mutate(check_same = as.numeric(pred1 == pred2))
library(splines)
age_grid <- seq(from = min(wage_data$age, na.rm = TRUE),
to = max(wage_data$age, na.rm = TRUE))
spline_age <- lm(wage ~ bs(age, knots = c(25, 40, 60)),
data = wage_data[train,])
?bs # to check what the basis function does
spline_age_grid_pred  <- predict(spline_age,
newdata = list(age = age_grid),
se = TRUE)
plot(wage_data[test, "age"], wage_data[test, "wage"],
cex = 0.5, col = "darkgrey",
xlab = "age", ylab = "wage")
lines(age_grid, spline_age_grid_pred$fit, lwd = 2)
lines(age_grid, spline_age_grid_pred$fit +
2 * spline_age_grid_pred$se, lty ="dashed")
lines(age_grid, spline_age_grid_pred$fit -
2 * spline_age_grid_pred$se, lty ="dashed")
ns_age_poly <- lm(wage ~ ns(age, df = 4), data = wage_data[train,])
ns_age_grid_poly_pred <- predict(ns_age_poly,
newdata = list(age = age_grid),
se = TRUE)
lines(age_grid, ns_age_grid_poly_pred$fit, col ="red", lwd =2)
ns_age_poly <- lm(wage ~ ns(age, df = 4), data = wage_data[train,])
ns_age_grid_poly_pred <- predict(ns_age_poly,
newdata = list(age = age_grid),
se = TRUE)
plot(wage_data[test, "age"], wage_data[test, "wage"],
lines(age_grid, ns_age_grid_poly_pred$fit, col ="red", lwd =2)
lines(age_grid, ns_age_grid_poly_pred$fit +
ns_age_poly <- lm(wage ~ ns(age, df = 4), data = wage_data[train,])
ns_age_grid_poly_pred <- predict(ns_age_poly,
newdata = list(age = age_grid),
se = TRUE)
plot(wage_data[test, "age"], wage_data[test, "wage"],
lines(age_grid, ns_age_grid_poly_pred$fit, col ="red", lwd =2)
lines(age_grid, ns_age_grid_poly_pred$fit +
plot(wage_data[test, "age"], wage_data[test, "wage"],
cex = 0.5, col = "darkgrey",
xlab = "age", ylab = "wage")
lines(age_grid, ns_age_grid_poly_pred$fit +
2 * ns_age_grid_poly_pred$se, lty = "dashed")
ns_age_poly <- lm(wage ~ ns(age, df = 4), data = wage_data[train,])
ns_age_grid_poly_pred <- predict(ns_age_poly,
newdata = list(age = age_grid),
se = TRUE)
plot(wage_data[test, "age"], wage_data[test, "wage"],
cex = 0.5, col = "darkgrey",
xlab = "age", ylab = "wage")
lines(age_grid, ns_age_grid_poly_pred$fit, lwd = 2)
lines(age_grid, ns_age_grid_poly_pred$fit +
2 * ns_age_grid_poly_pred$se, lty = "dashed")
ns_age_poly <- lm(wage ~ ns(age, df = 4), data = wage_data[train,])
ns_age_grid_poly_pred <- predict(ns_age_poly,
newdata = list(age = age_grid),
se = TRUE)
plot(wage_data[test, "age"], wage_data[test, "wage"],
cex = 0.5, col = "darkgrey",
xlab = "age", ylab = "wage")
lines(age_grid, ns_age_grid_poly_pred$fit, lwd = 2)
lines(age_grid, ns_age_grid_poly_pred$fit +
2 * ns_age_grid_poly_pred$se, lty = "dashed")
lines(age_grid, ns_age_grid_poly_pred$fit -
2 * ns_age_grid_poly_pred$se, lty = "dashed")
smooth_age <- smooth.spline(wage_data[train,"age"],
wage_data[train, "wage"],
df = 16)
smoothCV_age <- smooth.spline(wage_data[train, "age"],
wage_data[train, "wage"],
cv = TRUE) # specify we want to use CV
smoothCV_age$df
smoothCV_age <- smooth.spline(wage_data[train, "age"],
wage_data[train, "wage"],
cv = TRUE) # specify we want to use CV
smoothCV_age$df
plot(wage_data[test, "age"], wage_data[test, "wage"],
cex =.5, col = "darkgrey",
xlab = "age", ylab = "wage")
title(" Smoothing Spline ")
lines(smooth_age, col ="red", lwd = 2)
lines(smoothCV_age, col ="blue", lwd =2)
local2_age <- loess(wage ~ age, span = 0.2, data = wage_data)
local5_age <- loess(wage ~ age, span = 0.5, data = wage_data)
pred_local2_age <- predict(local2_age, newdata = data.frame(age = age_grid))
pred_local5_age <- predict(local5_age, newdata = data.frame(age = age_grid))
plot(wage_data[test, "age"], wage_data[test, "wage"],
cex =.5, col = "darkgrey",
xlab = "age", ylab = "wage")
lines(age_grid, pred_local2_age, col = "red", lwd = 2)
lines(age_grid, pred_local5_age, col = "blue", lwd = 2)
gam_yae <- lm(wage ~ ns(year, 4) + ns(age, 4) + education,
data = wage_data[train,])
library(gam)
gam_smooth <- gam(wage ~ s(year, 4) + s(age, 5) + education,
data = wage_data[train,])
par(mfrow = c(1, 3))
plot(gam_smooth, se = TRUE, col ="blue ")
plot.Gam(gam_yae, se = TRUE, col = "red") # Note the capitalization
gam_yae_pred <- predict(gam_yae, newdata = wage_data[test,])
gam_smooth_pred <- predict(gam_smooth, newdata = wage_data[test,])
library(ISLR)
wage_data <- Wage
wage_data <- wage_data[, -10]
library(dplyr)
wage_data <- wage_data %>% arrange(age)
View(wage_data)
set.seed(222)
train <- sample(1:nrow(wage_data), round(nrow(wage_data) * 0.8))
train <- sort(train)
test <- which(!(seq(nrow(wage_data)) %in% train))
msep_func <- function(predictions, true_vals) {
MSEP <- mean((predictions - true_vals)^2)
return(MSEP)
}
age_poly <- lm(wage ~ poly(age, 4), data = wage_data[train,])
coef(summary(age_poly))
head(wage_data$age)
head(poly(wage_data$age, 4))
head(poly(wage_data$age, 4, raw = TRUE))
age_poly_TRUE <- lm(wage ~ poly(age, 4, raw = TRUE),
data = wage_data[train,])
coef(summary(age_poly_TRUE))
age_poly_pred <- predict(age_poly, newdata = wage_data[test,])
age_poly_TRUE_pred  <- predict(age_poly_TRUE, newdata = wage_data[test,])
head(age_poly_pred)
head(age_poly_TRUE_pred)
head(age_poly_pred)
head(age_poly_TRUE_pred)
print(msep_func(age_poly_pred, wage_data[test, "wage"]))
pred_comparison <- cbind(age_poly_pred, age_poly_TRUE_pred)
colnames(pred_comparison) <- c("pred1", "pred2")
library(splines)
age_grid <- seq(from = min(wage_data$age, na.rm = TRUE),
to = max(wage_data$age, na.rm = TRUE))
age_grid
?bs # to check what the basis function does
spline_age <- lm(wage ~ bs(age, knots = c(25, 40, 60)),
data = wage_data[train,])
spline_age_grid_pred  <- predict(spline_age,
newdata = list(age = age_grid),
se = TRUE)
plot(wage_data[test, "age"], wage_data[test, "wage"],
cex = 0.5, col = "darkgrey",
xlab = "age", ylab = "wage")
lines(age_grid, spline_age_grid_pred$fit, lwd = 2)
lines(age_grid, spline_age_grid_pred$fit +
2 * spline_age_grid_pred$se, lty ="dashed")
lines(age_grid, spline_age_grid_pred$fit -
2 * spline_age_grid_pred$se, lty ="dashed")
ns_age_poly <- lm(wage ~ ns(age, df = 4), data = wage_data[train,])
ns_age_grid_poly_pred <- predict(ns_age_poly,
newdata = list(age = age_grid),
se = TRUE)
plot(wage_data[test, "age"], wage_data[test, "wage"],
cex = 0.5, col = "darkgrey",
xlab = "age", ylab = "wage")
lines(age_grid, ns_age_grid_poly_pred$fit, lwd = 2)
lines(age_grid, ns_age_grid_poly_pred$fit +
2 * ns_age_grid_poly_pred$se, lty = "dashed")
lines(age_grid, ns_age_grid_poly_pred$fit -
2 * ns_age_grid_poly_pred$se, lty = "dashed")
smooth_age <- smooth.spline(wage_data[train,"age"],
wage_data[train, "wage"],
df = 16)
smoothCV_age <- smooth.spline(wage_data[train, "age"],
wage_data[train, "wage"],
cv = TRUE) # specify we want to use CV
smoothCV_age$df
plot(wage_data[test, "age"], wage_data[test, "wage"],
cex =.5, col = "darkgrey",
xlab = "age", ylab = "wage")
title(" Smoothing Spline ")
lines(smooth_age, col ="red", lwd = 2)
lines(smoothCV_age, col ="blue", lwd =2)
local2_age <- loess(wage ~ age, span = 0.2, data = wage_data)
local5_age <- loess(wage ~ age, span = 0.5, data = wage_data)
pred_local2_age <- predict(local2_age, newdata = data.frame(age = age_grid))
pred_local5_age <- predict(local5_age, newdata = data.frame(age = age_grid))
plot(wage_data[test, "age"], wage_data[test, "wage"],
cex =.5, col = "darkgrey",
xlab = "age", ylab = "wage")
lines(age_grid, pred_local2_age, col = "red", lwd = 2)
lines(age_grid, pred_local5_age, col = "blue", lwd = 2)
gam_yae <- lm(wage ~ ns(year, 4) + ns(age, 4) + education,
data = wage_data[train,])
library(gam)
gam_smooth <- gam(wage ~ s(year, 4) + s(age, 5) + education,
data = wage_data[train,])
par(mfrow = c(1, 3))
plot(gam_smooth, se = TRUE, col ="blue ")
plot.Gam(gam_yae, se = TRUE, col = "red") # Note the capitalization
gam_yae_pred <- predict(gam_yae, newdata = wage_data[test,])
gam_smooth_pred <- predict(gam_smooth, newdata = wage_data[test,])
print(msep_func(gam_yae_pred, wage_data[test, "wage"]))
print(msep_func(gam_smooth_pred, wage_data[test, "wage"]))
gam_lo <- gam(wage ~ s(year, df = 4) + lo(age, span = 0.7) + education,
data = wage_data[train,])
plot.Gam(gam_lo, se = TRUE, col = "green")
gam_2lo <- gam(wage ~ lo(year, age, span =0.5) + education,
data = wage_data[train,])
reticulate::repl_python()
