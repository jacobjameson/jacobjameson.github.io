library(gam)
gam_smooth <- gam(wage ~ s(year, 4) + s(age, 5) + education,
data = wage_data[train,])
par(mfrow = c(1, 3))
plot(gam_smooth, se = TRUE, col ="blue ")
plot.Gam(gam_yae, se = TRUE, col = "red") # Note the capitalization
gam_yae_pred <- predict(gam_yae, newdata = wage_data[test,])
gam_smooth_pred <- predict(gam_smooth, newdata = wage_data[test,])
print(msep_func(gam_yae_pred, wage_data[test, "wage"]))
print(msep_func(gam_smooth_pred, wage_data[test, "wage"]))
lm_pred <- predict(lm(wage ~ year + age + education, data = wage_data[train,]),
newdata = wage_data[test,])
print(msep_func(lm_pred, wage_data[test, "wage"]))
print(msep_func(gam_yae_pred, wage_data[test, "wage"]))
print(msep_func(gam_smooth_pred, wage_data[test, "wage"]))
# check the MSE of a linear regression model
lm_pred <- predict(lm(wage ~ year + age + education, data = wage_data[train,]),
newdata = wage_data[test,])
print(msep_func(lm_pred, wage_data[test, "wage"]))
gam_lo <- gam(wage ~ s(year, df = 4) + lo(age, span = 0.7) + education,
data = wage_data[train,])
plot.Gam(gam_lo, se = TRUE, col = "green")
gam_2lo <- gam(wage ~ lo(year, age, span =0.5) + education,
data = wage_data[train,])
gam_2lo <- gam(wage ~ lo(year, age, span =0.5) + education,
data = wage_data[train,])
plot.Gam(gam_2lo, se = TRUE, col = "purple")
pairs(wage_data)
cor(wage_data)
corr(wage_data)
# get correlation of data
cor(wage_data$year, wage_data$age)
# get correlation of data
cor(wage_data)
# get correlation of data
corr(wage_data)
# get correlation of data
cor(wage_data)
View(wage_data)
pairs(wage_data)
pairs(wage_data, color = 'blue')
# do pairs(wage_data) but make the dots blue
pairs(wage_data, col = "blue")
?s
#install.packages("tree")
library(ISLR)
library(tree)
library(ggplot2)
carseat_data <- Carseats
high_sales <- as.factor(ifelse(carseat_data$Sales > 8, "Yes", "No"))
carseat_data <- data.frame(carseat_data, high_sales)
carseat_data = carseat_data[, -1]
carseat_data <- Carseats
carseat_data$high_sales <- as.factor(ifelse(carseat_data$Sales > 8, "Yes", "No"))
carseat_data = carseat_data[, -1]
set.seed(222)
train <- sample(seq(nrow(carseat_data)),
round(nrow(carseat_data) * 0.5))
train <- sort(train)
test <- which(!(seq(nrow(carseat_data)) %in% train))
?tree
carseats_tree <- tree(high_sales ~ ., data = carseat_data[train,])
plot(carseats_tree)
text(carseats_tree, pretty = 0)
carseats_tree
error_rate_func <- function(predictions, true_vals) {
error_rate <- mean(as.numeric(predictions != true_vals))
return(error_rate)
}
deep_tree_preds <- predict(carseats_tree, carseat_data[test,],
type = "class")
error_rate_func(predictions = deep_tree_preds,
true_vals = carseat_data[test, "high_sales"])
summary(carseats_tree)
?cv.tree
set.seed(222)
cv_carseats_tree  <- cv.tree(carseats_tree, FUN = prune.misclass)
names(cv_carseats_tree)
cv_carseats_tree
par(mfrow = c(1, 2))
plot(cv_carseats_tree$size, cv_carseats_tree$dev, type = "b")
plot(cv_carseats_tree$k, cv_carseats_tree$dev, type = "b")
cv_carseats_tree$dev
opt_indx <- which.min(cv_carseats_tree$dev)
opt_size <- cv_carseats_tree$size[opt_indx]
print(opt_size)
opt_indx <- which.min(cv_carseats_tree$dev)
opt_size <- cv_carseats_tree$size[opt_indx]
print(opt_size)
cv_carseats_tree$dev
while (cv_carseats_tree$dev[opt_indx] == cv_carseats_tree$dev[opt_indx + 1]) {
opt_indx <- opt_indx + 1
}
opt_size <- cv_carseats_tree$size[opt_indx]
print(opt_size)
cv_carseats_tree$dev
cv_carseats_tree
summary(cv_carseats_tree)
cv_carseats_tree
opt_size <- 7
pruned_carseats_tree <- prune.misclass(carseats_tree, best = opt_size)
pruned_carseats_tree <- prune.misclass(carseats_tree, best = opt_size)
plot(pruned_carseats_tree)
text(pruned_carseats_tree, pretty = 0)
pruned_tree_preds = predict(pruned_carseats_tree, carseat_data[test, ],
type = "class")
error_rate_func(predictions = pruned_tree_preds,
true_vals = carseat_data[test, "high_sales"])
plot(pruned_carseats_tree)
text(pruned_carseats_tree, pretty = 0)
library(MASS)
boston_data <- Boston
set.seed(222)
train <- sample(seq(nrow(boston_data)),
round(nrow(boston_data) * 0.8))
train <- sort(train)
test <- which(!(seq(nrow(boston_data)) %in% train))
boston_tree = tree(medv ~ ., Boston, subset = train)
summary(boston_tree)
?Boston
plot(boston_tree)
text(boston_tree)
boston_preds <- predict(boston_tree, newdata = boston_data[test,])
msep_func <- function(predictions, true_vals) {
MSEP <- mean((predictions - true_vals)^2)
return(MSEP)
}
print(msep_func(predictions = boston_preds,
true_vals = boston_data[test, "medv"]))
cv_boston_tree = cv.tree(boston_tree)
plot(cv_boston_tree$size, cv_boston_tree$dev, type = 'b')
cv_boston_tree
best_indx <- which.min(cv_boston_tree$dev)
best_indx
best_size <- cv_boston_tree$size[best_indx]
best_size
prune_boston = prune.tree(boston_tree, best = 7)
boston_prune_preds <- predict(prune_boston, newdata = boston_data[test,])
print(msep_func(boston_prune_preds, boston_data[test, "medv"]))
prune_boston = prune.tree(boston_tree, best = best_size)
boston_prune_preds <- predict(prune_boston, newdata = boston_data[test,])
print(msep_func(boston_prune_preds, boston_data[test, "medv"]))
boston_data <- Boston
set.seed(222)
train <- sample(seq(nrow(boston_data)),
round(nrow(boston_data) * 0.8))
train <- sort(train)
test <- which(!(seq(nrow(boston_data)) %in% train))
## install.packages("randomForest")
library(randomForest)
rf.boston <- randomForest(medv ~ .,
data = data.frame(boston_data[-test,]),
importance = TRUE, n.trees = 5000)
## Predictions
yhat.rf <- predict (rf.boston, newdata = Boston[-train ,])
boston.test = Boston[-train, "medv"]
mean((yhat.rf - boston.test)^2)
# visualize the importance of each variable
rf.boston$importance
#visualize the importance
varImpPlot(rf.boston)
#visualize the importance
varImpPlot(rf.boston)
bag.boston <- randomForest(medv ~ ., data = data.frame(boston_data[-test,]),
mtry = 13, importance = TRUE)
bag.boston$importance
yhat.bag <- predict(bag.boston, newdata = Boston[-train,])
mean((yhat.bag - boston.test)^2)
## install.packages("gbm")
library(gbm)
set.seed(222)
?gbm
## Boosting model
boost.boston <- gbm(medv ~ ., data = data.frame(boston_data[-test,]),
distribution = "gaussian", n.trees = 5000,
interaction.depth = 4)
plot <- summary(boost.boston, plot = F)
## create a ggplot bar plot with labels of plot object
ggplot(plot, aes(x = reorder(var, -rel.inf), y = rel.inf)) +
geom_bar(stat = "identity") +
coord_flip() +
xlab("Variable") +
ylab("Relative Importance") +
ggtitle("Relative Importance of Variables in Boosting Model") +
theme_minimal()
yhat.boost <- predict(boost.boston, newdata = Boston[-train ,],
n.trees = 5000)
mean((yhat.boost - boston.test)^2)
## Boosting model
boost.boston <- gbm(medv ~ ., data = data.frame(boston_data[-test,]),
distribution = "gaussian", n.trees = 5000)
yhat.boost <- predict(boost.boston, newdata = Boston[-train ,],
n.trees = 5000)
mean((yhat.boost - boston.test)^2)
library(ISLR)
college_data <- College
library(dplyr)
transformed_data <- college_data %>%
mutate(accept_rate = Accept/Apps) %>%
select(accept_rate, Outstate, Private)
transformed_data <- transformed_data %>%
mutate(accept_rate = scale(accept_rate),
Outstate = scale(Outstate))
set.seed(222)
small_data <- transformed_data[sample(1:nrow(transformed_data), 100),]
x <- as.matrix(small_data[, 1:2])
y <- if_else(small_data[, 3] == "Yes", 1, -1)
summary(transformed_data)
plot(small_data[, c("accept_rate", "Outstate")],
col = small_data$Private)
# install.packages("e1071")
library(e1071)
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "linear",
cost = 10,
scale = FALSE)
plot(college_svm1, small_data)
plot(college_svm1, small_data)
college_svm1 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "linear",
cost = 100,
scale = FALSE)
plot(college_svm1, small_data)
plot(college_svm1, small_data)
plot(college_svm1, small_data)
summary(college_svm1)
college_svm1$index
beta <- drop(t(college_svm1$coefs) %*% x[college_svm1$index,])
beta0 <- -college_svm1$rho
print(beta)
print(beta0)
make.grid = function(x, n = 75) {
grange <-  apply(x, 2, range)
x1 <- seq(from = grange[1, 1], to = grange[2, 1], length = n)
x2 <- seq(from = grange[1, 2], to = grange[2, 2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid <- make.grid(x)
colnames(xgrid) <- colnames(x)
ygrid <- predict(college_svm1, xgrid)
plot(xgrid,
col = c("red","blue")[as.numeric(ygrid)],
pch = 20,
cex = .2)
points(x,
col = y + 3,
pch = 19)
points(x[college_svm1$index,],
pch = 5,
cex = 2)
plot(xgrid,
col = c("red","blue")[as.numeric(ygrid)],
pch = 20,
cex = .2)
points(x,
col = y + 3,
pch = 19)
points(x[college_svm1$index,],
pch = 5,
cex = 2)
abline(-beta0 / beta[2], -beta[1]/beta[2])
plot(xgrid,
col = c("red","blue")[as.numeric(ygrid)],
pch = 20,
cex = .2)
points(x,
col = y + 3,
pch = 19)
points(x[college_svm1$index,],
pch = 5,
cex = 2)
abline((-1 - beta0) / beta[2], -beta[1] / beta[2], lty = 2)
abline((+1 - beta0) / beta[2], -beta[1] / beta[2], lty = 2)
svm_cost <- function(df, cost_val, xgrid) {
svmfit <- svm(Private ~ Outstate + accept_rate,
data = df,
kernel = "linear",
cost = cost_val,
scale = FALSE)
print(paste("# of support vectors =", length(svmfit$index)))
beta <- drop(t(svmfit$coefs) %*% x[svmfit$index,])
beta0 <- -svmfit$rho
ygrid <- predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[df$index,], pch = 5, cex = 2)
abline(-beta0 / beta[2], -beta[1]/beta[2])
abline((-1 - beta0) / beta[2], -beta[1] / beta[2], lty = 2)
abline((+1 - beta0) / beta[2], -beta[1] / beta[2], lty = 2)
}
svm_cost(small_data, .01, xgrid)
set.seed(222)
tune_linear <- tune(svm,
Private ~ accept_rate + Outstate,
data = transformed_data,
kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
tune_linear <- tune(svm,
Private ~ accept_rate + Outstate,
data = transformed_data,
kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_linear)
best_linear_mod <- tune_linear$best.model
summary(best_linear_mod)
tune_linear_full <- tune(svm,
Private ~ .,
data = college_data,
kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1,
1, 5, 10, 100)))
summary(tune_linear_full)
svm_poly3 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "polynomial",
degree = 3,
cost = 1)
plot(svm_poly3, small_data)
svm_poly3 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "polynomial",
degree = 10,
cost = 1)
svm_poly3 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "polynomial",
degree = 10,
cost = 100000)
plot(svm_poly3, small_data)
svm_poly3 <- svm(Private ~ Outstate + accept_rate,
data = small_data,
kernel = "polynomial",
degree = 3,
cost = 1)
plot(svm_poly3, small_data)
ygrid <- predict(svm_poly3, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)],
pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
predicted_grid <- predict(svm_poly3, xgrid,
decision.values = TRUE)
predicted_grid <- attributes(predicted_grid)$decision
ygrid <- predict(svm_poly3, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)],
pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
contour(unique(xgrid[,1]), unique(xgrid[,2]),
matrix(predicted_grid, 75, 75),
level = 0,
add = TRUE)
set.seed(222)
tune_poly <- tune(svm,
Private ~ .,
data = transformed_data,
kernel = "polynomial",
ranges = list(cost = c(0.001, 0.1, 1, 5, 100),
degree = c(2, 3)))
summary(tune_poly)
set.seed(222)
tune_radial <- tune(svm, Private ~ .,
data = transformed_data,
kernel = "radial",
ranges = list(cost = c(0.001, 0.1, 1, 5, 100),
gamma = c(0.0001, 0.001, 0.01, 0.1, 1)))
summary(tune_radial)
svm_radial <- tune_radial$best.model
## Make predictions for the xgrid
ygrid <- predict(svm_radial, xgrid)
## And plot the grid predictions and decision boundary
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)],
pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
predicted_grid <- predict(svm_radial, xgrid, decision.values = TRUE)
predicted_grid <- attributes(predicted_grid)$decision
contour(unique(xgrid[,1]), unique(xgrid[,2]),
matrix(predicted_grid, 75, 75), level = 0, add = TRUE)
full_poly <- tune(svm,
Private ~ .,
data = college_data,
kernel = "polynomial",
ranges = list(cost = c(0.001, 0.1, 1, 5, 100),
degree = c(2, 3)))
summary(full_poly)
summary(tune_poly)
full_radial <- tune(svm,
Private ~ .,
data = college_data,
kernel = "radial",
ranges = list(cost = c(0.001, 0.1, 1, 5, 100),
gamma = c(0.0001, 0.001, 0.01, 0.1, 1)))
summary(full_radial)
summary(tune_radial)
summary(full_radial)
summary(tune_radial)
#install.packages("ReinforcementLearning")
library(ReinforcementLearning)
states <- c("s1", "s2", "s3", "s4")
actions <- c("up", "down", "left", "right")
env <- gridworldEnvironment
print(env)
?sampleExperience
data <- sampleExperience(N = 1000,
env = env,
states = states,
actions = actions)
head(data)
View(data)
# Define reinforcement learning parameters
control <- list(alpha = 0.1, gamma = 0.5, epsilon = 0.1)
# Perform reinforcement learning
?ReinforcementLearning
model <- ReinforcementLearning(data,
s = "State",
a = "Action",
r = "Reward",
s_new = "NextState",
control = control)
# Print policy
computePolicy(model)
?sampleExperience
data <- sampleExperience(N = 1000,
env = env,
states = states,
actions = actions)
# install.packages("neuralnet")
library(neuralnet)
library(tidyverse)
glimpse(iris)
iris <- iris %>% mutate_if(is.character, as.factor)
summary(iris)
set.seed(222)
data_rows <- floor(0.80 * nrow(iris))
train_indices <- sample(c(1:nrow(iris)), data_rows)
train_data <- iris[train_indices,]
test_data <- iris[-train_indices,]
neuralnet?
```
?neuralnet
model = neuralnet(
Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,
data = train_data,
hidden = c(4, 1),
linear.output = FALSE)
plot(model, rep = "best")
pred <- predict(model, test_data, response = "Species")
labels <- levels(train_data$Species)
prediction_label <- data.frame(max.col(pred)) %>%
mutate(pred = labels[max.col.pred.]) %>%
select(2) %>%
unlist()
errors = as.numeric(test_data$Species) != max.col(pred)
error_rate = (sum(errors)/nrow(test_data))*100
print(paste("Error Rate: ", round(error_rate,2), "%"))
table(test_data$Species, prediction_label)
labels <- levels(train_data$Species)
prediction_label <- data.frame(max.col(pred)) %>%
mutate(pred = labels[max.col.pred.]) %>%
select(2) %>%
unlist()
errors = as.numeric(test_data$Species) != max.col(pred)
error_rate = (sum(errors)/nrow(test_data))*100
print(paste("Error Rate: ", round(error_rate,2), "%"))
table(test_data$Species, prediction_label)
data_rows <- floor(0.80 * nrow(iris))
train_indices <- sample(c(1:nrow(iris)), data_rows)
train_data <- iris[train_indices,]
test_data <- iris[-train_indices,]
model = neuralnet(
Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,
data = train_data,
hidden = c(4, 1),
linear.output = FALSE)
pred <- predict(model, test_data, response = "Species")
labels <- levels(train_data$Species)
prediction_label <- data.frame(max.col(pred)) %>%
mutate(pred = labels[max.col.pred.]) %>%
select(2) %>%
unlist()
errors = as.numeric(test_data$Species) != max.col(pred)
error_rate = (sum(errors)/nrow(test_data))*100
print(paste("Error Rate: ", round(error_rate,2), "%"))
table(test_data$Species, prediction_label)
#install.packages("ReinforcementLearning")
library(ReinforcementLearning)
states <- c("s1", "s2", "s3", "s4")
actions <- c("up", "down", "left", "right")
env <- gridworldEnvironment
print(env)
?sampleExperience
data <- sampleExperience(N = 1000,
env = env,
states = states,
actions = actions)
View(data)
head(data)
# Define reinforcement learning parameters
control <- list(alpha = 0.1, gamma = 0.5, epsilon = 0.1)
# Perform reinforcement learning
?ReinforcementLearning
model <- ReinforcementLearning(data,
s = "State",
a = "Action",
r = "Reward",
s_new = "NextState",
control = control)
# Print policy
computePolicy(model)
print(model)
summary(model)
install.packages('gam')
