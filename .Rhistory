subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
# add a legend that states the class and that the second geom poionts represent the means of the class
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2, color = 'black'), size = 7, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
# add a legend that states the class and that the second geom poionts represent the means of the class
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2),color = 'black', size = 7, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
# add a legend that states the class and that the second geom poionts represent the means of the class
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2),color = 'black', size = 8, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "The decision boundary is linear",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
# add a legend that states the class and that the second geom poionts represent the means of the class
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2),color = 'black', size = 8, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "Diamonds represent Class Means",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c("red", "blue")) +
# add a legend that states the class and that the second geom poionts represent the means of the class
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2),color = 'black', size = 8, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "Diamonds represent Class Means",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c('#E69F00', '#56B4E9')) +
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
# plot the data
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
labs(title = "LDA Decision Boundary",
subtitle = "Diamonds represent Class Means",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c('#E69F00', '#56B4E9')) +
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
# plot the data
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
labs(title = "LDA Decision Boundary",
subtitle = 'Simulated Data',
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c('#E69F00', '#56B4E9')) +
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
# plot the data
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
labs(title = "Plot of the Data",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c('#E69F00', '#56B4E9')) +
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
# Fit LDA model
lda_fit <- lda(class ~ ., data = data)
# plot the decision boundary
x <- seq(-5, 5, length.out = 1000)
y <- seq(-5, 5, length.out = 1000)
grid <- expand.grid(X1 = x, X2 = y)
grid$class <- predict(lda_fit, newdata = grid)$class
means <- as.data.frame(lda_fit$means)
means$class <- factor(0:1)
means$size <- 5
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2),color = 'black', size = 8, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "Diamonds represent Class Means",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c('#E69F00', '#56B4E9')) +
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2), color = 'black', size = 8, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "Diamonds represent Class Means",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c('#E69F00', '#56B4E9')) +
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
ggplot(data, aes(x = X1, y = X2, color = class)) +
geom_point() + theme_minimal() +
geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = "black") +
geom_point(data = means, aes(x = X1, y = X2), color = 'black', size = 8, shape=18) +
geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +
labs(title = "LDA Decision Boundary",
subtitle = "Diamonds represent Class Means",
x = "Feature 1", y = "Feature 2") +
scale_color_manual(values = c('#0e4a6b', '#ed7b2e')) +
guides(color = guide_legend(title = "Class", override.aes = list(size = 3)))
## Load the packages
library(ISLR)
library(FNN)
default_data <- Default
summary(default_data)
default_data$default <- as.numeric(default_data$default == "Yes")
default_data$student <- as.numeric(default_data$student == "Yes")
set.seed(222)
## First pick the test observations (20% of the data)
test_obs <- sample(seq(nrow(default_data)),
round(0.2 * nrow(default_data)))
## The training observations are the remaining observations
train_obs <- setdiff(seq(nrow(default_data)), test_obs)
## Use the indices now to extract the corresponding subsets of the data
test_data <- default_data[test_obs,]
train_data <- default_data[train_obs,]
class_predictions <- as.numeric(logistic_predict > 0.5)
logistic_predict <-predict(logistic_default,
test_data,
type = "response")
## Load the packages
library(ISLR)
library(FNN)
default_data <- Default
summary(default_data)
summary(default_data)
default_data$default <- as.numeric(default_data$default == "Yes")
default_data$student <- as.numeric(default_data$student == "Yes")
summary(default_data)
set.seed(222)
(seq(nrow(default_data))
)
## First pick the test observations (20% of the data)
test_obs <- sample(seq(nrow(default_data)),
round(0.2 * nrow(default_data)))
test_obs
## The training observations are the remaining observations
train_obs <- setdiff(seq(nrow(default_data)), test_obs)
## Use the indices now to extract the corresponding subsets of the data
test_data <- default_data[test_obs,]
train_data <- default_data[train_obs,]
logistic_default <- glm(default ~ student + balance + income,
family = binomial,
data = train_data)
summary(logistic_default)
logistic_predict <-predict(logistic_default,
test_data,
type = "response")
logistic_predict
head(logistic_predict)
class_predictions <- as.numeric(logistic_predict > 0.5)
class_predictions
logistic_accuracy <- mean(class_predictions == test_data$default)
print(logistic_accuracy)
predict(logistic_default,test_data)
logistic_predict
true_pos_accuracy <- mean(class_predictions[which(test_data$default == 1)] == 1)
print(true_pos_accuracy)
true_neg_accuracy <- mean(class_predictions[which(test_data$default == 0)] == 0)
print(true_neg_accuracy)
table(class_predictions, test_data$default)
true_pos_error <- mean(class_predictions
[which(test_data$default == 1)] != 1)
print(true_pos_error)
true_neg_error <- mean(class_predictions
[which(test_data$default == 0)] != 0)
print(true_neg_error)
## First, we need to specify the list of threshold values to assess
threshold_values <- seq(from = 0.00, to = 0.50, by = 0.01)
threshold_values
error_rates <- matrix(0, nrow = length(threshold_values), ncol = 2)
error_rates
error_rates
threshold_values
indx <- 0
for(threshold in threshold_values) {
## Update the tracker to reflect the row
indx <- indx + 1
## Then generate the predicted classes using the specific threshold
class_predictions <- as.numeric(logistic_predict > threshold)
## Then calculate the true positive accuracy
true_pos_accuracy <- mean(class_predictions[which(test_data$default == 1)] == 1)
## Then calculate the true negative accuracy
true_neg_accuracy <- mean(class_predictions[which(test_data$default == 0)] == 0)
## Now we can add the results to our matrix
error_rates[indx,] <- c(true_pos_accuracy, true_neg_accuracy)
}
error_rates
matplot(x = threshold_values,
y = error_rates,
type = "l",
col = 3:4,
xlab = "Threshold",
ylab = "Accuracy",
main = "Accuracy as a Function of Threshold")
legend("topright", legend = c("Defaulters", "Non-defaulters"),
col = 3:4, pch = 1)
library(MASS)
lda_model <- lda(default ~., data = train_data)
lda_pred <- predict(lda_model, test_data[,-1])
lda_pred
class_pred_lda <- lda_pred$class
error_lda <- mean(class_pred_lda !=
test_data$default)
print(error_lda)
1-0.9745
print(error_lda)
1-0.9745
print(lda_model))
print(lda_model)
qda_model <- qda(default ~., data = train_data)
qda_pred <- predict(qda_model, test_data[,-1])
class_pred_qda <- qda_pred$class
error_qda <- mean(class_pred_qda !=
test_data$default)
print(error_qda)
diabetes_data <- read.csv("diabetes.csv", header = TRUE)
## Example of a simple function
add_fun <- function(a1, a2){
res = a1 + a2
return(res)
}
add_fun(3, 4)
set.seed(222)
knn_cv_error5 <- cross_validation_KNN(data_x = diabetes_data[, -9],
data_y = diabetes_data$Outcome,
k_seq = seq(20),
kfolds = 5)
cross_validation_KNN  <- function(data_x, data_y, k_seq, kfolds) {
## We will start by assigning each observation to one and
## only one fold for cross-validation. To do this, we use
## an index vector. The vector's length equals the number
## of observations in the data. The vector's entries are
## equal numbers of 1s, 2s, etc. up to the number of folds
## being used for k-fold cross validation.
## Recall seq(5) is a vector (1, 2, 3, 4, 5)
## ceiling() rounds a number up to the nearest integer
## rep(a, b) repeats a b times (e.g. rep(1, 3) => (1, 1, 1))
## So this says repeat the sequence from 1:kfolds
## enough times to fill a vector that is as long or
## slightly longer than the number of observations in
## the data. Then, if the length of the vector exceeds
## the number of observations, truncate it to the right
## length
fold_ids <- rep(seq(kfolds), ceiling(nrow(data_x) / kfolds))
fold_ids <- fold_ids[1:nrow(data_x)]
## To make the IDs random, randomly rearrange the vector
fold_ids <- sample(fold_ids, length(fold_ids))
## In order to store the prediction performance for each fold
## for each k, we initialize a matrix.
CV_error_mtx  <- matrix(0, nrow = length(k_seq), ncol = kfolds)
## To run CV, we will loop over all values of k that we want to
## consider for KNN. For each value of k, we will loop over all
## folds. For each fold, we will estimate KNN for the given k on
## all but one fold. We will then measure the model's accuracy
## on the hold out fold and save it. After we finish looping
## through all k's and all folds, we will find the average CV
## error for each value of k and use that as a measure of the
## model's performance.
for (k in k_seq) {
for (fold in 1:kfolds) {
## Train the KNN model (Note: if it throws a weird error, make sure
## all features are numeric variables -- not factors)
## Note: usually the features are normalized/re-scaled
## Otherwise KNN will give more weight to variables at the larger scale
## when calculating the distance metric.
## See page 165 of the textbook for a useful example
knn_fold_model <- knn(train = scale(data_x[which(fold_ids != fold),]),
test = scale(data_x[which(fold_ids == fold),]),
cl = data_y[which(fold_ids != fold)],
k = k)
## Measure and save error rate (% wrong)
CV_error_mtx[k, fold] <- mean(knn_fold_model !=
data_y[which(fold_ids == fold)])
}
}
## We want our function to return the accuracy matrix
return(CV_error_mtx)
}
knn_cv_error5 <- cross_validation_KNN(data_x = diabetes_data[, -9],
data_y = diabetes_data$Outcome,
k_seq = seq(20),
kfolds = 5)
library(class)
knn_cv_error5 <- cross_validation_KNN(data_x = diabetes_data[, -9],
data_y = diabetes_data$Outcome,
k_seq = seq(20),
kfolds = 5)
View(knn_cv_error5)
print(knn_cv_error5)
knn_cv_error10 <- cross_validation_KNN(data_x = diabetes_data[,-9],
data_y = diabetes_data$Outcome,
k_seq = seq(50),
kfolds = 10)
diabetes_data <- read.csv("diabetes.csv", header = TRUE)
dim(diabetes_data)
summary(diabetes_data)
library(class)
## Example of a simple function
add_fun <- function(a1, a2){
res = a1 + a2
return(res)
}
add_fun(3, 4)
seq(5)
rep(seq(5), ceiling(768 / 5))
fold_ids[1:768]
fold_ids <- rep(seq(5), ceiling(768 / 5))
fold_ids[1:768]
sample(fold_ids, length(fold_ids))
matrix(0, nrow = length(k_seq), ncol = kfolds)
matrix(0, nrow = length(c(1,2,3,4,5)), ncol = kfolds)
matrix(0, nrow = length(c(1,2,3,4,5)), ncol = 5)
cross_validation_KNN  <- function(data_x, data_y, k_seq, kfolds) {
## We will start by assigning each observation to one and
## only one fold for cross-validation. To do this, we use
## an index vector. The vector's length equals the number
## of observations in the data. The vector's entries are
## equal numbers of 1s, 2s, etc. up to the number of folds
## being used for k-fold cross validation.
## Recall seq(5) is a vector (1, 2, 3, 4, 5)
## ceiling() rounds a number up to the nearest integer
## rep(a, b) repeats a b times (e.g. rep(1, 3) => (1, 1, 1))
## So this says repeat the sequence from 1:kfolds
## enough times to fill a vector that is as long or
## slightly longer than the number of observations in
## the data. Then, if the length of the vector exceeds
## the number of observations, truncate it to the right
## length
fold_ids <- rep(seq(kfolds), ceiling(nrow(data_x) / kfolds))
fold_ids <- fold_ids[1:nrow(data_x)]
## To make the IDs random, randomly rearrange the vector
fold_ids <- sample(fold_ids, length(fold_ids))
## In order to store the prediction performance for each fold
## for each k, we initialize a matrix.
CV_error_mtx  <- matrix(0, nrow = length(k_seq), ncol = kfolds)
## To run CV, we will loop over all values of k that we want to
## consider for KNN. For each value of k, we will loop over all
## folds. For each fold, we will estimate KNN for the given k on
## all but one fold. We will then measure the model's accuracy
## on the hold out fold and save it. After we finish looping
## through all k's and all folds, we will find the average CV
## error for each value of k and use that as a measure of the
## model's performance.
for (k in k_seq) {
for (fold in 1:kfolds) {
## Train the KNN model (Note: if it throws a weird error, make sure
## all features are numeric variables -- not factors)
## Note: usually the features are normalized/re-scaled
## Otherwise KNN will give more weight to variables at the larger scale
## when calculating the distance metric.
## See page 165 of the textbook for a useful example
knn_fold_model <- knn(train = scale(data_x[which(fold_ids != fold),]),
test = scale(data_x[which(fold_ids == fold),]),
cl = data_y[which(fold_ids != fold)],
k = k)
## Measure and save error rate (% wrong)
CV_error_mtx[k, fold] <- mean(knn_fold_model !=
data_y[which(fold_ids == fold)])
}
}
## We want our function to return the accuracy matrix
return(CV_error_mtx)
}
View(cross_validation_KNN)
set.seed(222)
knn_cv_error5 <- cross_validation_KNN(data_x = diabetes_data[, -9],
data_y = diabetes_data$Outcome,
k_seq = seq(20),
kfolds = 5)
print(knn_cv_error5)
set.seed(222)
knn_cv_error5 <- cross_validation_KNN(data_x = diabetes_data[, -9],
data_y = diabetes_data$Outcome,
k_seq = seq(20),
kfolds = 5)
print(knn_cv_error5)
knn_cv_error10 <- cross_validation_KNN(data_x = diabetes_data[,-9],
data_y = diabetes_data$Outcome,
k_seq = seq(50),
kfolds = 10)
print(knn_cv_error10)
mean_cv_error5 <- rowMeans(knn_cv_error5)
mean_cv_error10 <- rowMeans(knn_cv_error10)
mean_cv_error5
which.min(mean_cv_error5)
which.min(mean_cv_error10)
plot(seq(20), mean_cv_error5, type = "l",
main = "Mean CV Error Rate as a Function of k",
ylab = "CV Error Rate", xlab = "k")
library(MASS)
cross_validation <- function(full_data, model_type, kfolds,
logistic_formula = NULL) {
## Define fold_ids in exactly the same way as before
fold_ids <- rep(seq(kfolds),
ceiling(nrow(full_data) / kfolds))
fold_ids <- fold_ids[1:nrow(full_data)]
fold_ids <- sample(fold_ids, length(fold_ids))
## Initialize a vector to store CV error
CV_error_vec  <- vector(length = kfolds, mode = "numeric")
## Loop through the folds
for (k in 1:kfolds){
if (model_type == "logistic") {
logistic_model <- glm(logistic_formula,
data = full_data[which(fold_ids != k),],
family = binomial)
logistic_pred <- predict(logistic_model,
full_data[which(fold_ids == k),],
type = "response")
class_pred <- as.numeric(logistic_pred > 0.5)
} else if (model_type == "LDA") {
lda_model <- lda(full_data[which(fold_ids != k),-9],
full_data[which(fold_ids != k),9])
lda_pred <- predict(lda_model, full_data[which(fold_ids == k), -9])
class_pred <- lda_pred$class
} else if (model_type == "QDA") {
qda_model <- qda(full_data[which(fold_ids != k), -9],
full_data[which(fold_ids != k), 9])
qda_pred <- predict(qda_model,
full_data[which(fold_ids == k), -9])
class_pred <- qda_pred$class
}
CV_error_vec[k] <- mean(class_pred != full_data[which(fold_ids == k), 9])
}
return(CV_error_vec)
}
## Run CV for logistic regression:
logistic_formula <- paste("Outcome", paste(colnames(diabetes_data)[1:8],
collapse = " + "),
sep = " ~ ")
logistic_CV_error <- cross_validation(diabetes_data, "logistic", 5,
as.formula(logistic_formula))
## Run CV for LDA:
lda_CV_error <- cross_validation(diabetes_data, "LDA", 5)
## Run CV for QDA:
qda_CV_error <- cross_validation(diabetes_data, "QDA", 5)
## Determine the best model in terms of lowest average CV error
print(paste("Logistic Error Rate:", round(mean(logistic_CV_error), 3)))
print(paste("LDA Error Rate:", round(mean(lda_CV_error), 3)))
print(paste("QDA Error Rate:", round(mean(qda_CV_error), 3)))
library(MASS)
cross_validation <- function(full_data, model_type, kfolds,
logistic_formula = NULL) {
## Define fold_ids in exactly the same way as before
fold_ids <- rep(seq(kfolds),
ceiling(nrow(full_data) / kfolds))
fold_ids <- fold_ids[1:nrow(full_data)]
fold_ids <- sample(fold_ids, length(fold_ids))
## Initialize a vector to store CV error
CV_error_vec  <- vector(length = kfolds, mode = "numeric")
## Loop through the folds
for (k in 1:kfolds){
if (model_type == "logistic") {
logistic_model <- glm(logistic_formula,
data = full_data[which(fold_ids != k),],
family = binomial)
logistic_pred <- predict(logistic_model,
full_data[which(fold_ids == k),],
type = "response")
class_pred <- as.numeric(logistic_pred > 0.5)
} else if (model_type == "LDA") {
lda_model <- lda(full_data[which(fold_ids != k),-9],
full_data[which(fold_ids != k),9])
lda_pred <- predict(lda_model, full_data[which(fold_ids == k), -9])
class_pred <- lda_pred$class
} else if (model_type == "QDA") {
qda_model <- qda(full_data[which(fold_ids != k), -9],
full_data[which(fold_ids != k), 9])
qda_pred <- predict(qda_model,
full_data[which(fold_ids == k), -9])
class_pred <- qda_pred$class
}
CV_error_vec[k] <- mean(class_pred != full_data[which(fold_ids == k), 9])
}
return(CV_error_vec)
}
## Run CV for logistic regression:
logistic_formula <- paste("Outcome", paste(colnames(diabetes_data)[1:8],
collapse = " + "),
sep = " ~ ")
logistic_CV_error <- cross_validation(diabetes_data, "logistic", 5,
as.formula(logistic_formula))
## Run CV for LDA:
lda_CV_error <- cross_validation(diabetes_data, "LDA", 5)
## Run CV for QDA:
qda_CV_error <- cross_validation(diabetes_data, "QDA", 5)
## Determine the best model in terms of lowest average CV error
print(paste("Logistic Error Rate:", round(mean(logistic_CV_error), 3)))
print(paste("LDA Error Rate:", round(mean(lda_CV_error), 3)))
print(paste("QDA Error Rate:", round(mean(qda_CV_error), 3)))
