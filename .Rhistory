print(paste("Logistic Error Rate:", round(mean(logistic_CV_error), 3)))
print(paste("LDA Error Rate:", round(mean(lda_CV_error), 3)))
print(paste("QDA Error Rate:", round(mean(qda_CV_error), 3)))
library(MASS)
cross_validation <- function(full_data, model_type, kfolds,
logistic_formula = NULL) {
## Define fold_ids in exactly the same way as before
fold_ids <- rep(seq(kfolds),
ceiling(nrow(full_data) / kfolds))
fold_ids <- fold_ids[1:nrow(full_data)]
fold_ids <- sample(fold_ids, length(fold_ids))
## Initialize a vector to store CV error
CV_error_vec  <- vector(length = kfolds, mode = "numeric")
## Loop through the folds
for (k in 1:kfolds){
if (model_type == "logistic") {
logistic_model <- glm(logistic_formula,
data = full_data[which(fold_ids != k),],
family = binomial)
logistic_pred <- predict(logistic_model,
full_data[which(fold_ids == k),],
type = "response")
class_pred <- as.numeric(logistic_pred > 0.5)
} else if (model_type == "LDA") {
lda_model <- lda(full_data[which(fold_ids != k),-9],
full_data[which(fold_ids != k),9])
lda_pred <- predict(lda_model, full_data[which(fold_ids == k), -9])
class_pred <- lda_pred$class
} else if (model_type == "QDA") {
qda_model <- qda(full_data[which(fold_ids != k), -9],
full_data[which(fold_ids != k), 9])
qda_pred <- predict(qda_model,
full_data[which(fold_ids == k), -9])
class_pred <- qda_pred$class
}
CV_error_vec[k] <- mean(class_pred != full_data[which(fold_ids == k), 9])
}
return(CV_error_vec)
}
## Run CV for logistic regression:
logistic_formula <- paste("Outcome", paste(colnames(diabetes_data)[1:8],
collapse = " + "),
sep = " ~ ")
logistic_CV_error <- cross_validation(diabetes_data, "logistic", 5,
as.formula(logistic_formula))
## Run CV for LDA:
lda_CV_error <- cross_validation(diabetes_data, "LDA", 5)
## Run CV for QDA:
qda_CV_error <- cross_validation(diabetes_data, "QDA", 5)
## Determine the best model in terms of lowest average CV error
print(paste("Logistic Error Rate:", round(mean(logistic_CV_error), 3)))
print(paste("LDA Error Rate:", round(mean(lda_CV_error), 3)))
print(paste("QDA Error Rate:", round(mean(qda_CV_error), 3)))
library(tufte)
library(tidyverse)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
library(plotly) # For interactive 3D plotting
library(plotly) # For interactive 3D plotting
# Load necessary libraries
library(ggplot2)
library(plotly) # For interactive 3D plotting
set.seed(123) # For reproducibility
# Generate a hypothetical dataset
n <- 100 # Number of observations
age <- rnorm(n, mean=50, sd=10)
income <- rnorm(n, mean=50000, sd=10000) + age * 500 # Add some correlation
education_level <- rnorm(n, mean=16, sd=2) + age * 0.2 # Slight correlation with age
data_3d <- data.frame(age, income, education_level)
# Visualize the 3D dataset
plot_ly(data_3d, x = ~age, y = ~income, z = ~education_level, type = 'scatter3d', mode = 'markers') %>%
layout(title = "3D Scatter Plot of Age, Income, and Education Level",
scene = list(xaxis = list(title = 'Age'),
yaxis = list(title = 'Income'),
zaxis = list(title = 'Education Level')))
# Standardize the data
data_3d_scaled <- scale(data_3d)
# Perform PCA
pca_result <- prcomp(data_3d_scaled)
# Print summary of PCA results
summary(pca_result)
# Plotting the variance explained by each principal component
var_explained <- pca_result$sdev^2 / sum(pca_result$sdev^2)
barplot(var_explained, main="Variance Explained by Each Principal Component",
xlab="Principal Component", ylab="Proportion of Variance Explained",
names.arg=c("PC1", "PC2", "PC3"))
# Project the original data into the principal component space
data_2d <- pca_result$x[, 1:2]
# Plot the 2D projection
ggplot(data.frame(data_2d), aes(x=PC1, y=PC2)) +
geom_point() +
theme_minimal() +
ggtitle("Data Projected onto the First Two Principal Components") +
xlab("First Principal Component") +
ylab("Second Principal Component")
library(ISLR)
wage_data <- Wage
original_data <- matrix(c(1, 2, 3, 2, 3, 4, 3, 4, 5), nrow=3, byrow=TRUE)
original_data
standardized_data <- scale(original_data)
standardized_data
X <- matrix(c(-1.37, -0.98, 0, 0.98, -0.39, 1.37, 0, -1.22, 1.22), nrow=3, byrow=TRUE)
X
standardized_data <- scale(original_data)
standardized_data
X <- matrix(c(-1.37, -0.98, 0, 0.98, -0.39, 1.37, 0, -1.22, 1.22),
nrow=3, byrow=TRUE)
standardized_data <- scale(original_data)
cov_matrix <- cov(standardized_data)
eigen_decomp <- eigen(cov_matrix)
eigenvalues <- eigen_decomp$values
eigenvectors <- eigen_decomp$vectors
eigenvectors
# Load necessary libraries
library(ggplot2)
library(plotly) # For interactive 3D plotting
library(caret)  # For PCA and regression
library(caret)  # For PCA and regression
library(GGally) # For ggpairs
set.seed(123) # For reproducibility
# Generate a pretend dataset
n <- 100 # Number of observations
X2 <- X1 + rnorm(n, mean = 0, sd = 5) # Positively correlated with X1
X3 <- X1 + rnorm(n, mean = 0, sd = 5) # Also positively correlated with X1
# Generate a pretend dataset
n <- 100 # Number of observations
X1 <- rnorm(n, mean = 50, sd = 10)
X2 <- X1 + rnorm(n, mean = 0, sd = 5) # Positively correlated with X1
X3 <- X1 + rnorm(n, mean = 0, sd = 5) # Also positively correlated with X1
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 5) # Dependent variable
data <- data.frame(X1, X2, X3, y)
# Quick look at the relationships
ggpairs(data)
# Quick look at the relationships
ggpairs(data)
# Interactive 3D plot to visualize the relationships
fig <- plot_ly(data, x = ~X1, y = ~X2, z = ~X3, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'X1'),
yaxis = list(title = 'X2'),
zaxis = list(title = 'X3')))
fig
# Interactive 3D plot to visualize the relationships
fig <- plot_ly(data, x = ~X1, y = ~X2, z = ~X3, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'X1'),
yaxis = list(title = 'X2'),
zaxis = list(title = 'X3')))
fig
fig
# Interactive 3D plot to visualize the relationships
fig <- plot_ly(data, x = ~X1, y = ~X2, z = ~X3, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'X1'),
yaxis = list(title = 'X2'),
zaxis = list(title = 'X3')))
fig
# Perform PCA
pca_result <- prcomp(data[,1:3], center = TRUE, scale. = TRUE)
# Print summary of PCA to see explained variance
summary(pca_result)
# Plot PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Plot PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
library(ggbio)
install.packages('ggbio')
# Split data into training and testing sets
set.seed(123)
trainingIndex <- createDataPartition(data$y, p = .8, list = FALSE)
trainingData <- data[trainingIndex, ]
testingData <- data[-trainingIndex, ]
# PCA on training data
pca_train <- prcomp(trainingData[,1:3], center = TRUE, scale. = TRUE)
# Predict using the first two principal components
predictors <- predict(pca_train, newdata = testingData[,1:3])[,1:2]
# Fit model
model <- lm(y ~ ., data = as.data.frame(predictors))
library(prcomp) # For PCA
install.packages("prcomp")
install.packages("ggfortify")
library(ggfortify)
# Plot PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Perform PCA
pca_result <- prcomp(data[,1:3], center = TRUE, scale. = TRUE)
# Print summary of PCA to see explained variance
summary(pca_result)
# Plot PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Split data into training and testing sets
set.seed(123)
trainingIndex <- createDataPartition(data$y, p = .8, list = FALSE)
trainingData <- data[trainingIndex, ]
testingData <- data[-trainingIndex, ]
# PCA on training data
pca_train <- prcomp(trainingData[,1:3], center = TRUE, scale. = TRUE)
# Predict using the first two principal components
predictors <- predict(pca_train, newdata = testingData[,1:3])[,1:2]
# Fit model
model <- lm(y ~ ., data = as.data.frame(predictors))
# Splitting data into training and testing sets
trainingIndex <- createDataPartition(data$y, p = .8, list = FALSE)
trainingData <- data[trainingIndex, ]
testingData <- data[-trainingIndex, ]
# Perform PCA on training data
pca_train <- prcomp(trainingData[,1:3], center = TRUE, scale. = TRUE)
# Use PCA to predict on testing data, keeping the first two principal components
predictors <- predict(pca_train, newdata = testingData[,1:3])[,1:2]
# Create a new dataframe for modeling that includes the response variable from the testing set
modelingData <- as.data.frame(predictors)
modelingData$y <- testingData$y
# Fit the linear model using the predictors and the response variable
model <- lm(y ~ ., data = modelingData)
# Output the summary of the model to see the results
summary(model)
# Predict on testing data
predictions <- predict(model, newdata = as.data.frame(predictors))
# Calculate RMSE
rmse <- sqrt(mean((predictions - testingData$y)^2))
print(paste("RMSE:", rmse))
# Fit a linear model using all original predictors
full_model <- lm(y ~ X1 + X2 + X3, data = trainingData)
# Summary of the full model to see the results
summary(full_model)
# Predict on testing data
full_predictions <- predict(full_model, newdata = testingData)
# Calculate RMSE for the full model
full_rmse <- sqrt(mean((full_predictions - testingData$y)^2))
print(paste("Full Model RMSE:", full_rmse))
# Load necessary libraries
library(ggplot2)
library(plotly) # For interactive 3D plotting
library(caret)  # For PCA and regression
library(GGally) # For ggpairs
library(ggfortify)
set.seed(123) # Ensuring our pretend data is the same each time
# Generate our dataset
n <- 100 # Observations
X1 <- rnorm(n, mean = 50, sd = 10)
X2 <- X1 + rnorm(n, mean = 0, sd = 5) # Correlated with X1
X3 <- X1 + rnorm(n, mean = 0, sd = 5) # Also correlated with X1
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 5) # Our dependent variable
data <- data.frame(X1, X2, X3, y)
# Let's peek at the relationships
ggpairs(data)
# Perform PCA
pca_result <- prcomp(data[,1:3], center = TRUE, scale. = TRUE)
# Let's see the summary of our PCA to understand the variance explained
summary(pca_result)
# And visualize the PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Perform PCA
pca_result <- prcomp(data[,1:3], center = TRUE, scale. = TRUE)
# Let's see the summary of our PCA to understand the variance explained
summary(pca_result)
# And visualize the PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Split data into training and testing sets
set.seed(123) # Ensuring reproducibility
# Splitting data
trainingIndex <- createDataPartition(data$y, p = .8, list = FALSE)
trainingData <- data[trainingIndex, ]
testingData <- data[-trainingIndex, ]
# Perform PCA on training data
pca_train <- prcomp(trainingData[,1:3], center = TRUE, scale. = TRUE)
# Use PCA for predictions, keeping first two components
predictors <- predict(pca_train, newdata = testingData[,1:3])[,1:2]
# Preparing data for modeling
modelingData <- as.data.frame(predictors)
modelingData$y <- testingData$y
# Fit the linear model
model <- lm(y ~ ., data = modelingData)
# Model summary
summary(model)
# Predict and calculate RMSE
predictions <- predict(model, newdata = modelingData)
rmse <- sqrt(mean((predictions - testingData$y)^2))
print(paste("RMSE:", rmse))
# Traditional linear regression model
full_model <- lm(y ~ X1 + X2 + X3, data = trainingData)
summary(full_model)
# Prediction and RMSE calculation
full_predictions <- predict(full_model, newdata = testingData)
full_rmse <- sqrt(mean((full_predictions - testingData$y)^2))
print(paste("Full Model RMSE:", full_rmse))
# Load necessary libraries
library(ggplot2)
library(plotly) # For interactive 3D plotting
library(caret)  # For PCA and regression
library(GGally) # For ggpairs
library(ggfortify)
set.seed(123) # Ensuring our pretend data is the same each time
# Generate our dataset
X1 = np.random.normal(50, 10, n)
# Load necessary libraries
library(ggplot2)
library(plotly) # For interactive 3D plotting
library(caret)  # For PCA and regression
library(GGally) # For ggpairs
library(ggfortify)
set.seed(123) # Ensuring our pretend data is the same each time
# Generate our dataset
X1 <- rnorm(n, mean = 50, sd = 10)
X2 <- X1 + rnorm(n, mean = 0, sd = 10)
X3 <- X1 + rnorm(n, mean = 0, sd = 10)
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 20)    # Our dependent variable
data <- data.frame(X1, X2, X3, y)
# Let's peek at the relationships
ggpairs(data)
# Interactive 3D plot to visualize relationships
fig <- plot_ly(data, x = ~X1, y = ~X2, z = ~X3, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'X1'),
yaxis = list(title = 'X2'),
zaxis = list(title = 'X3')))
fig
# Load necessary libraries
library(ggplot2)
library(plotly) # For interactive 3D plotting
library(caret)  # For PCA and regression
library(GGally) # For ggpairs
library(ggfortify)
set.seed(123) # Ensuring our pretend data is the same each time
# Generate our dataset
X1 <- rnorm(n, mean = 50, sd = 10)
X2 <- X1 + rnorm(n, mean = 0, sd = 10)
X3 <- X1 + rnorm(n, mean = 0, sd = 10)
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 20)    # Our dependent variable
data <- data.frame(X1, X2, X3, y)
# Let's peek at the relationships
ggpairs(data)
# Generate our dataset
X1 <- rnorm(n, mean = 50, sd = 10)
X2 <- X1 + rnorm(n, mean = 0, sd = 10)
X3 <- X1 + rnorm(n, mean = 0, sd = 10)
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 20)    # Our dependent variable
# Load necessary libraries
library(ggplot2)
library(plotly) # For interactive 3D plotting
library(caret)  # For PCA and regression
library(GGally) # For ggpairs
library(ggfortify)
set.seed(123) # Ensuring our pretend data is the same each time
# Generate our dataset
n <- 100 # Number of observations
X1 <- rnorm(n, mean = 50, sd = 10)
X2 <- X1 + rnorm(n, mean = 0, sd = 10) # More noise added
X3 <- X1 + rnorm(n, mean = 0, sd = 10) # More noise added
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 20) # Increase noise in the dependent variable
data <- data.frame(X1, X2, X3, y)
# Let's peek at the relationships
ggpairs(data)
# Interactive 3D plot to visualize relationships
fig <- plot_ly(data, x = ~X1, y = ~X2, z = ~X3, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'X1'),
yaxis = list(title = 'X2'),
zaxis = list(title = 'X3')))
fig
n <- 300 # Number of observations
X1 <- rnorm(n, mean = 50, sd = 10)
X2 <- X1 + rnorm(n, mean = 0, sd = 10) # More noise added
X3 <- X1 + rnorm(n, mean = 0, sd = 10) # More noise added
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 20) # Increase noise in the dependent variable
data <- data.frame(X1, X2, X3, y)
# Let's peek at the relationships
ggpairs(data)
# Interactive 3D plot to visualize relationships
fig <- plot_ly(data, x = ~X1, y = ~X2, z = ~X3, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'X1'),
yaxis = list(title = 'X2'),
zaxis = list(title = 'X3')))
fig
# Perform PCA
pca_result <- prcomp(data[,1:3], center = TRUE, scale. = TRUE)
# Let's see the summary of our PCA to understand the variance explained
summary(pca_result)
# And visualize the PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Split data into training and testing sets
set.seed(123) # Ensuring reproducibility
# Splitting data
trainingIndex <- createDataPartition(data$y, p = .8, list = FALSE)
trainingData <- data[trainingIndex, ]
testingData <- data[-trainingIndex, ]
# Perform PCA on training data
pca_train <- prcomp(trainingData[,1:3], center = TRUE, scale. = TRUE)
# Use PCA for predictions, keeping first two components
predictors <- predict(pca_train, newdata = testingData[,1:3])[,1:2]
# Preparing data for modeling
modelingData <- as.data.frame(predictors)
modelingData$y <- testingData$y
# Fit the linear model
model <- lm(y ~ ., data = modelingData)
# Model summary
summary(model)
# Predict and calculate RMSE
predictions <- predict(model, newdata = modelingData)
rmse <- sqrt(mean((predictions - testingData$y)^2))
print(paste("RMSE:", rmse))
# Traditional linear regression model
full_model <- lm(y ~ X1 + X2 + X3, data = trainingData)
summary(full_model)
# Prediction and RMSE calculation
full_predictions <- predict(full_model, newdata = testingData)
full_rmse <- sqrt(mean((full_predictions - testingData$y)^2))
print(paste("Full Model RMSE:", full_rmse))
wage <- Wage
mtcars
glimpse(mtcars)
data <- mtcars %>% select(mpg, wt, qsec, am)
# Let's peek at the relationships
ggpairs(data)
# Interactive 3D plot to visualize relationships
fig <- plot_ly(data, x = ~X1, y = ~X2, z = ~X3, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'X1'),
yaxis = list(title = 'X2'),
zaxis = list(title = 'X3')))
fig
# Interactive 3D plot to visualize relationships
fig <- plot_ly(data, x = ~wt, y = ~qsec, z = ~am, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'X1'),
yaxis = list(title = 'X2'),
zaxis = list(title = 'X3')))
fig
data <- mtcars %>% select(mpg, disp, hp, qsec)
# Let's peek at the relationships
ggpairs(data)
# Interactive 3D plot to visualize relationships
fig <- plot_ly(data, x = ~disp,y = ~hp, z = ~qsec, type = 'scatter3d', mode = 'markers',
marker = list(size = 5, opacity = 0.5))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'disp'),
yaxis = list(title = 'hp'),
zaxis = list(title = 'qsec')))
fig
# Perform PCA
pca_result <- prcomp(data[,1:3], center = TRUE, scale. = TRUE)
# Let's see the summary of our PCA to understand the variance explained
summary(pca_result)
# And visualize the PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Perform PCA
pca_result <- prcomp(data[,2:4], center = TRUE, scale. = TRUE)
# Let's see the summary of our PCA to understand the variance explained
summary(pca_result)
# And visualize the PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
rnorm()
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(3)
rnorm(3)
rnorm(1)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
rnorm(1, 10)
# Load necessary libraries
library(ggplot2)
library(plotly) # For interactive 3D plotting
library(caret)  # For PCA and regression
library(GGally) # For ggpairs
library(ggfortify)
set.seed(123) # Ensuring our pretend data is the same each time
# Generate our dataset
n <- 300 # Number of observations
X1 <- rnorm(n, mean = 50, sd = 10)
X2 <- X1 + rnorm(n, mean = 0, sd = 25) # More noise added
X3 <- X1 + rnorm(n, mean = 0, sd = 10) # More noise added
y <- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 30) # Increase noise in the dependent variable
data <- data.frame(X1, X2, X3, y)
# Let's peek at the relationships
ggpairs(data)
# Perform PCA
pca_result <- prcomp(data[,1:3], center = TRUE, scale. = TRUE)
# Let's see the summary of our PCA to understand the variance explained
summary(pca_result)
# And visualize the PCA
autoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')
# Split data into training and testing sets
set.seed(123) # Ensuring reproducibility
# Splitting data
trainingIndex <- createDataPartition(data$y, p = .8, list = FALSE)
trainingData <- data[trainingIndex, ]
testingData <- data[-trainingIndex, ]
# Perform PCA on training data
pca_train <- prcomp(trainingData[,1:3], center = TRUE, scale. = TRUE)
# Use PCA for predictions, keeping first two components
predictors <- predict(pca_train, newdata = testingData[,1:3])[,1:2]
# Preparing data for modeling
modelingData <- as.data.frame(predictors)
modelingData$y <- testingData$y
# Fit the linear model
model <- lm(y ~ ., data = modelingData)
# Model summary
summary(model)
# Predict and calculate RMSE
predictions <- predict(model, newdata = modelingData)
rmse <- sqrt(mean((predictions - testingData$y)^2))
print(paste("RMSE:", rmse))
