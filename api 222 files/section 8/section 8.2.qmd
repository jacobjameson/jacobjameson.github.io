---
title: "Section 8.2 - Decision Trees"
from: markdown+emoji
format:
  html:
    toc: true
    toc-location: left
page-layout: full
description: |
author: Jacob Jameson

---

## Decision Trees

```{r}
#install.packages("tree")
library(ISLR)
library(tree)
library(ggplot2)
```


We will use the Carseats data. Sales in this data set is a continuous variable. We start by converting it to a binary one that equals "Yes" if Sales $> 8$ and "No" otherwise.

```{r}
carseat_data <- Carseats
high_sales <- as.factor(ifelse(carseat_data$Sales > 8, "Yes", "No"))
carseat_data <- data.frame(carseat_data, high_sales)
carseat_data = carseat_data[, -1]
```


Let's again split the data into training and test sets

```{r}
set.seed(222)  
train <- sample(seq(nrow(carseat_data)),
                round(nrow(carseat_data) * 0.5))

train <- sort(train)
test <- which(!(seq(nrow(carseat_data)) %in% train))
```



We can now train a decision tree using the function `tree()`

```{r}
?tree
carseats_tree <- tree(high_sales ~ ., data = carseat_data[train,])
```



Plot the results

```{r}
plot(carseats_tree)
text(carseats_tree, pretty = 0)
```

From this, we see that shelving location seems to be the most important determinant and price is the second most. Beyond that, this tree is very hard to read. If we just type the tree object name, we get:


- The split criterion (e.g. Price < 92.5) 
- The number of observations in that branch
- The deviance
- The overall prediction for the branch
- The fraction of observations in that branch that are Yes/No
- Branches with terminal nodes are indicated by *


```{r}
carseats_tree
```


Given how deep our tree is grown, we may be worried about overfitting. We can start by evaluating the error rate on the test set for the current tree. We can write a helper function to compute the error rate

```{r}
error_rate_func <- function(predictions, true_vals) {
  error_rate <- mean(as.numeric(predictions != true_vals))
  return(error_rate)
}
```


Now generate predictions from the model

```{r}
deep_tree_preds <- predict(carseats_tree, carseat_data[test,], 
                           type = "class")
```

Calculate and summarize the error rate

```{r}
error_rate_func(predictions = deep_tree_preds, 
                true_vals = carseat_data[test, "high_sales"])

summary(carseats_tree)
```

The difference in our error rate between the training and test sets indicates that we overfit. To address this, we want to prune the tree. `cv.tree()` uses cross-validation to determine how much to prune the tree. 

```{r}
set.seed(222)
cv_carseats_tree  <- cv.tree(carseats_tree, FUN = prune.misclass)
names(cv_carseats_tree)
cv_carseats_tree
```

Size tells us the number of terminal nodes on each of the trees considered; dev gives us the CV errors; k gives us the cost-complexity parameter. We can plot the error as a function of size and k

```{r}
par(mfrow = c(1, 2))
plot(cv_carseats_tree$size, cv_carseats_tree$dev, type = "b")
plot(cv_carseats_tree$k, cv_carseats_tree$dev, type = "b")
```

Find and print the optimal size

```{r}
opt_indx <- which.min(cv_carseats_tree$dev)
opt_size <- cv_carseats_tree$size[opt_indx]
print(opt_size)
```

Now we can prune the tree using `prune.misclass()`

```{r}
pruned_carseats_tree <- prune.misclass(carseats_tree, best = opt_size)
plot(pruned_carseats_tree)
text(pruned_carseats_tree, pretty = 0)
```

Now evaluate model performance

```{r}
pruned_tree_preds = predict(pruned_carseats_tree, carseat_data[test, ], 
                            type = "class")

error_rate_func(predictions = pruned_tree_preds, 
                true_vals = carseat_data[test, "high_sales"])
```


## Regression Trees


For this, we will use the Boston data

```{r}
library(MASS)
boston_data <- Boston
```

Split the data into training and test sets

```{r}
set.seed(222)  

train <- sample(seq(nrow(boston_data)),
                round(nrow(boston_data) * 0.8))
train <- sort(train)

test <- which(!(seq(nrow(boston_data)) %in% train))

boston_tree = tree(medv ~ ., Boston, subset = train)

summary(boston_tree)
```

Plot the tree

```{r}
plot(boston_tree)
text(boston_tree)
```


Calculate the MSE for the Predicted Values

```{r}
boston_preds <- predict(boston_tree, newdata = boston_data[test,])
```

Create a helper function to calculate MSEP

```{r}
msep_func <- function(predictions, true_vals) {
  MSEP <- mean((predictions - true_vals)^2)
  return(MSEP)
}
```

Evaluate model performance

```{r}
print(msep_func(predictions = boston_preds, 
                true_vals = boston_data[test, "medv"]))
```

(1) Create an object called cv_boston_tree that runs CV on `boston_tree` to find the best size according to CV error

```{r}
cv_boston_tree = cv.tree(boston_tree)
```

Plot it

```{r}
plot(cv_boston_tree$size, cv_boston_tree$dev, type = 'b')
```

Let's see what the best size is

```{r}
cv_boston_tree
```

(2) Find which size had the lowest CV error and save in a variable called best_size 

```{r}
best_indx <- which.min(cv_boston_tree$dev)
best_size <- cv_boston_tree$size[best_indx]
```


Prune the tree using the best size as found above

```{r}
prune_boston = prune.tree(boston_tree, best = best_size)
```

Evaluate model performance

```{r}
boston_prune_preds <- predict(prune_boston, newdata = boston_data[test,])
print(msep_func(boston_prune_preds, boston_data[test, "medv"]))
```


There is a another popular package in R for decision trees called "rpart". We don't have time to go into it in class, but you can find more information using the link below.

https://cran.r-project.org/web/packages/rpart/rpart.pdf

You can also find several helpful tutorials online.


## Random Forest, Bagging and Boosting

```{r}
boston_data <- Boston
```


Create a training and a test set

```{r}
set.seed(222)
train <- sample(seq(nrow(boston_data)),
                round(nrow(boston_data) * 0.8))
train <- sort(train)
test <- which(!(seq(nrow(boston_data)) %in% train))
```

(1) Fit a random forest model to the Boston data using the `randomForest` function. Set the number of trees to 5000. 

```{r}
## install.packages("randomForest")
library(randomForest)
rf.boston <- randomForest(medv ~ ., data = data.frame(boston_data[-test,]), 
                          importance = TRUE, n.trees = 5000)
```

(2) Make predictions on the test set

```{r}
## Predictions
yhat.rf <- predict (rf.boston, newdata = Boston[-train ,])
boston.test = Boston[-train, "medv"]
mean((yhat.rf - boston.test)^2)
```

The "mtry" parameter of the "randomForest" function controls the number of variables to include at each branch. By setting this value to equal 13, we are performing bagging. You may be interested in the relative importance of each variable. By setting `importance = TRUE`, R will store the importance matrix. You can call this by "name of random forest"$importance

```{r}
bag.boston <- randomForest(medv ~ ., data = data.frame(boston_data[-test,]), 
                           mtry = 13, importance = TRUE)
bag.boston$importance
```


Now let's make some predictions 

```{r}
yhat.bag <- predict(bag.boston, newdata = Boston[-train,])
mean((yhat.bag - boston.test)^2)
```

We are going to compare the outcome with boosting. Boosting has the same general form except instead of randomForest, you will use "gbm". We list the distribution as gaussian" since this is a regression problem; if it were a binary classification problem, we would use distribution="bernoulli". The argument n.trees=5000 indicates that we want 5000 t trees, and the option interaction.depth=4 limits the depth of each tree. Just as before, we can see the relative importance by looking at the summary. lstat and rm are the most important variables. 

```{r}
## install.packages("gbm")  
library(gbm)
set.seed(222)

## Boosting model
boost.boston <- gbm(medv ~ ., data = data.frame(boston_data[-test,]), 
                    distribution = "gaussian", n.trees = 5000, 
                    interaction.depth = 4)

plot <- summary(boost.boston, plot = F) 

## create a ggplot bar plot with labels of plot object
ggplot(plot, aes(x = reorder(var, -rel.inf), y = rel.inf)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("Variable") +
  ylab("Relative Importance") +
  ggtitle("Relative Importance of Variables in Boosting Model") +
  theme_minimal()
```

Now let's make some predictions 

```{r}
yhat.boost <- predict(boost.boston, newdata = Boston[-train ,], 
                      n.trees = 5000)

mean((yhat.boost - boston.test)^2)
```




