---
title: "Section 4.2 - Classification and Helpful Commands in R"
from: markdown+emoji
format:
  html:
    toc: true
    toc-location: left
page-layout: full
description: |
author: Jacob Jameson

---

The goal of this session is to learn how to implement three classification models:
1. A logistic model
2. LDA
3. QDA

## Reminders
To run one line of code in RStudio, you can highlight the code you want to run and hit "Run" at the top of the script. Alternatively, on a mac, you can highlight the code to run and hit Command + Enter. Or on a PC, you can highlight the code to run and hit Ctrl + Enter. If you ever forget how a function works, you can type ? followed immediately (e.g. with no space) by the function name to get the help file

Let's start by loading the necessary packages and data. We will use the "Default" dataset available from the ISLR package. This dataset contains information on credit card defaults, including a binary response variable "default" and three predictors: "student", "balance", and "income". We will use this dataset to build and evaluate classification models.

```{r}
## Load the packages
library(ISLR)
library(FNN)
```


**Data preparation**

Now extract the data and name it "default_data"
```{r}
default_data <- Default
```

Let's get to know our data
```{r}
summary(default_data)
```

It looks like we have two categorical variables. We can convert them both to numeric
```{r}
default_data$default <- as.numeric(default_data$default == "Yes")
default_data$student <- as.numeric(default_data$student == "Yes")
```

Let's again split our data into test and training data sets with a 20/80 split. We use `set.seed()` to ensure replicability.
```{r}
set.seed(222)
```

Then we can use the sample function to split the data (as before)
```{r}
## First pick the test observations (20% of the data)
test_obs <- sample(seq(nrow(default_data)), 
                   round(0.2 * nrow(default_data)))

## The training observations are the remaining observations
train_obs <- setdiff(seq(nrow(default_data)), test_obs)

## Use the indices now to extract the corresponding subsets of the data
test_data <- default_data[test_obs,]
train_data <- default_data[train_obs,]
```


## Logistic regression

Now, let's say we are interested in using a logistic regression. For a base case, let's try to predict default from the other available variables using logistic regression. To run logistic regression in R, use `glm()`, which requires three arguments:
- 1st: Your formula (y ~ x1 + x2) 
- 2nd: `Family = binomial` tells it to use logistic regression
- 3rd: You data, including both x and y columns. 

We will train the model on the training data, make predictions for the test data using `predict()`, and measure performance with Accuracy.

```{r}
logistic_default <- glm(default ~ student + balance + income,
                        family = binomial,
                        data = train_data)
```

To view information about the logistic regression, including coefficients, use `summary()`
```{r}
summary(logistic_default)
```


To predict outcomes on the test_data using logistic regression, use:
```{r}
logistic_predict <-predict(logistic_default,
                           test_data,
                           type = "response")
```

Let's look at what we got from this prediction model. We will use the `head()` function, which prints the first few values of the object inside the parentheses. If you want to change the number of observations that are printed to 100, you can use `head(object, n = 100)`

```{r}
head(logistic_predict)
```

We see that the prediction outputs are probabilities, so in order to make predictions we have to decide on a decision rule. A common one is if the predicted probability is > 0.5, predict 1 otherwise 0. Let's see how we would do using this rule. Because it's a classification problem, accuracy is a good measure (% correct)

```{r}
class_predictions <- as.numeric(logistic_predict > 0.5)
logistic_accuracy <- mean(class_predictions == test_data$default)
print(logistic_accuracy)
```

The accuracy looks great! But, we might care more about different types of errors than overall error rate. For example, we may not want to give loans to people who will default even if this means denying loans to some people who wouldn't default. We can measure error rate by true default status. Note that for defaulters, default = 1. Here, I pull out all the predictions for the true defaulters and see what fraction of those equal 1.

```{r}
true_pos_accuracy <- mean(class_predictions[which(test_data$default == 1)] == 1)
print(true_pos_accuracy)
```

Like-wise for the non-defaulters, I see what fraction of those equal 0. This gives class-specific accuracy rates.

```{r}
true_neg_accuracy <- mean(class_predictions[which(test_data$default == 0)] == 0)
print(true_neg_accuracy)
```

These values summarise what can also be seen in the following table. Where the columns correspond to the true values and the rows correspond to the predicted values. 

```{r}
table(class_predictions, test_data$default)
```

Suppose instead of the accuracy, you wanted to directly calculate the error rate. How would you do it? Hint, errors are ones where the prediction does not equal the true value. In R, we use `!=` for "does not equal"

We can also calculate the error rate for the true defaulters and non-defaulters. 

```{r}
true_pos_error <- mean(class_predictions
                       [which(test_data$default == 1)] != 1)
print(true_pos_error)

true_neg_error <- mean(class_predictions
                       [which(test_data$default == 0)] != 0)
print(true_neg_error)
```

We see that we did a lot better on the true negatives than the true positives. Among all the people who will default, we only predicted about 1/3% of them would default. If we want to do a better job identifying these people, we can do this by lowering the default threshold from a predicted probability of 0.5 to something lower, say 0.2. Note, though, that lowering this threshold means increasing the number of default predictions for people who don't default as well. Since we aren't really sure how low we want to make this threshold, we can try for a bunch of threshold values and then see how the performance changes to pick the one that is best for our setting. This involves domain knowledge, such as the cost of default and the earnings on loans extended to people who repay, so there's not one right answer, but we can more clearly see the tradeoffs by trying many values and plotting the error rates in each group as a function of the threshold.

To do this, we can use a loop to try a bunch of threshold values and then calculate the error rates for each threshold. We can then plot the error rates as a function of the threshold to see how the error rates change as we change the threshold. 

```{r}
## First, we need to specify the list of threshold values to assess
threshold_values <- seq(from = 0.00, to = 0.50, by = 0.01)
```

Then we initialize a matrix of error rates. This matrix will have a number of rows corresponding to the length of the list of threshold values and 2 columns corresponding to the true positive and true negative accuracy for each value that we test

```{r}
error_rates <- matrix(0, nrow = length(threshold_values), ncol = 2)
```

Now we can start the loop. We initialize a tracker for the row index, then for each threshold value in our specified list of values, we update the tracker to reflect the row, generate the predicted classes using the specific threshold, calculate the true positive accuracy, calculate the true negative accuracy, and add the results to our matrix. 

```{r}
indx <- 0

for(threshold in threshold_values) {
  
  ## Update the tracker to reflect the row
  indx <- indx + 1
  
  ## Then generate the predicted classes using the specific threshold
  class_predictions <- as.numeric(logistic_predict > threshold)
  
  ## Then calculate the true positive accuracy
  true_pos_accuracy <- mean(class_predictions[which(test_data$default == 1)] == 1)
  
  ## Then calculate the true negative accuracy
  true_neg_accuracy <- mean(class_predictions[which(test_data$default == 0)] == 0)
  
  ## Now we can add the results to our matrix 
  error_rates[indx,] <- c(true_pos_accuracy, true_neg_accuracy)
}

```

Let's plot each of these as a function of the threshold

```{r}
matplot(x = threshold_values,
        y = error_rates, 
        type = "l",
        col = 3:4,
        xlab = "Threshold",
        ylab = "Accuracy",
        main = "Accuracy as a Function of Threshold")
        legend("topright", legend = c("Defaulters", "Non-defaulters"), 
               col = 3:4, pch = 1)
```

# LDAs and QDAs

We can also use other methods to predict default. For example, we can use linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA). These methods are similar to logistic regression, but they make different assumptions about the data. LDA assumes that the data are normally distributed and that the variance is the same in each group. QDA assumes that the data are normally distributed, but it does not assume that the variance is the same in each group.

Note that we need to load the package MASS, because it contains the functions lda() and qda()

```{r}
library(MASS)
```

Now we can use the "lda" function to run the LDA model. 

```{r}
lda_model <- lda(default ~., data = train_data)
```

We can then generate the predicted values

```{r}
lda_pred <- predict(lda_model, test_data[,-1])
```

And extract the predicted classes

```{r}
class_pred_lda <- lda_pred$class
```

Finally, we can calculate the error rate

```{r}
error_lda <- mean(class_pred_lda !=
                  test_data$default)
print(error_lda)
```

To run QDA, we use the "qda" function. Otherwise, the code is the same as with "lda"

```{r}
qda_model <- qda(default ~., data = train_data)
qda_pred <- predict(qda_model, test_data[,-1])
class_pred_qda <- qda_pred$class
error_qda <- mean(class_pred_qda !=
                  test_data$default)
print(error_qda)
```

# Summary

In this tutorial, we learned how to use logistic regression, LDA, and QDA to predict default. We also learned how to calculate the error rates for each method. We found that logistic regression had the lowest error rate, but we also saw that the error rates for LDA and QDA were not much higher. This suggests that all three methods are useful for predicting default.






    

    
    
    
    
    
    
    
    
    
    
    
    
