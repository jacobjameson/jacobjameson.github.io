---
title: "Section 6.2 - Ridge, Lasso, PCA/PCR, PLS Code"
from: markdown+emoji
format:
  html:
    toc: true
    toc-location: left
page-layout: full
description: |
author: Jacob Jameson

---

```{r, include=FALSE}
library(pls)
```

## LASSO and Ridge Regression

We will use the Caravan data set that is included in the package ISLR. This data set contains information on people offered Caravan insurance.

```{r}
library(ISLR)
insurance_data <- Caravan
```

Let's learn a little more about the data 

```{r,eval=FALSE}
?Caravan
```


Let's try to predict CARAVAN. Note: although this is a binary variable, ridge and lasso are regression algorithms -- regression can often give you a good sense of the ordinal distribution of likelihood that the outcome will be 1 even if the resulting value cannot be viewed as a probability). When you run lasso and ridge, you will need to provide a penalty parameter. Since we don't know which penalty parameter is best, we will use a built in cross-validation function to find the best penalty parameter (lambda) in the package glmnet

```{r}
library(glmnet)
?cv.glmnet
```

We will start by using the function's built-in sequence of lambdas and glmnet standardization

```{r}
set.seed(222) # Important for replicability
lasso_ins <- cv.glmnet(x = as.matrix(insurance_data[, 1:85]), # the features
                       y = as.numeric(insurance_data[, 86]), # the outcome
                       standardize = TRUE, # Why do we do this?
                       alpha = 1) # Corresponds to LASSO
```


We can see which lambda sequence was used

```{r}
print(lasso_ins$lambda)
```

Let's find the lambda that does the best as far as CV error goes

```{r}
print(lasso_ins$lambda.min)
```

You can plot the model results

```{r}
plot(lasso_ins)
```

You can also plot the CV-relevant outputs

```{r}
LassoCV <- lasso_ins$glmnet.fit

plot(LassoCV, label = TRUE, xvar = "lambda")

abline(
  v = log(c(lasso_ins$lambda.min, lasso_ins$lambda.1se))
  ) # Adds lines to mark the two key lambda values
```

You can see the coefficients corresponding to the two key lambda values using the predict function

```{r}
predict(lasso_ins, type = "coefficients",
        s = c(lasso_ins$lambda.min, lasso_ins$lambda.1se))
```

Questions you should be able to answer  :

- Which lambda in the sequence had the lowest CV error?
```{r}
lasso_ins$lambda.min


```

- What is the CV error of the "best" lambda?
```{r}
lasso_ins$cvm[lasso_ins$lambda == lasso_ins$lambda.min]
```

- What is the standard deviation of the CV error for the "best" lambda?
```{r}
lasso_ins$cvsd[lasso_ins$lambda == lasso_ins$lambda.min]
```

- What is the largest lambda whose CV error was within 1 standard deviation of the lowest CV error?
```{r}
lasso_ins$lambda.1se
```

When you do ridge regression, the code is almost exactly the same as for lasso in R. You just need to change the alpha parameter from 1 to 0. I'll leave this to you as an exercise.

## Principal Components Analysis (PCA) and Partial Least Squares (PLS)

We will use the Wage data set that is included in the package ISLR. This data set contains information on people's wages.

```{r}
wage_data <- Wage
```

Let's learn a little more about the data 

```{r,eval=FALSE}
summary(wage_data)
```

The data set contains 3000 observations on 11 variables. The variables are:

```{r}
str(wage_data)
```

To the view the levels of a particular factor variable, we can use:

```{r}
levels(wage_data$maritl)
levels(wage_data$region)
```

Notice there is a variable called "wage" and a variable called "logwage". We just need one of these two. Let's drop "wage".

```{r}
wage_data <- wage_data[, -11]
```

Looking at the data, we see there are integer, factor, and numeric variable types. Let's convert everything to numeric variables, which includes converting factor variables to a series of indicators.

```{r}
for(i in 10:1){
  if(is.factor(wage_data[, i])){
    for(j in unique(wage_data[, i])){
      new_col <- paste(colnames(wage_data)[i], j, sep = "_")
      wage_data[, new_col] <- as.numeric(wage_data[, i] == j) 
    }
    wage_data <- wage_data[, -i]     
  } else if(typeof(wage_data[, i]) == "integer") {
    wage_data[, i] <- as.numeric(as.character(wage_data[, i]))
  } 
}
```

Check your code worked

```{r}
#View(wage_data)
summary(wage_data)
str(wage_data)
```
Let's split our data into a training and a test set

```{r}
set.seed(222)
train <- sample(seq(nrow(wage_data)),
                floor(nrow(wage_data)*0.8))

train <- sort(train)

test <- which(!(seq(nrow(wage_data)) %in% train))
```


We are interested in predicting log wage. First, we will use principle components regression. Principle components regression does a linear regression but instead of using the X-variables as predictors, it uses principle components as predictors. The optimal number of principle components to use for PCR is usually found through cross-validation. To run PCR, we will use the package pls.

```{r, eval=FALSE}
library(pls)

## Try running PCR
pcr_fit  <- pcr(logwage ~ ., data = wage_data[train,],          
                scale = TRUE, validation = "CV")
```
`Error in La.svd(X) : infinite or missing values in 'x'`

Sometime you can get an error message. This error is because some of our variables have almost zero variance. Usually, variables with near-zero variance are indicator variables we generated for a rare event. Think about what happens to these predictors when the data are split into cross-validation/bootstrap sub-samples: if a few uncommon unique values are removed from one sample, the predictors could become zero-variance predictors which would cause many models to not run. We can figure out which variables have such low variance to determine how we want to handle them. The simplest approach to identify them is to set a manual threshold (which can be adjusted: 0.05 is a common choice). Our options are then to drop them from the analysis or not to scale the data. 

```{r}
## to drop them from the analysis or not to scale the data. 
for(col_num in 1:ncol(wage_data)){
  if(var(wage_data[, col_num]) < 0.05){
    print(colnames(wage_data)[col_num])
    print(var(wage_data[, col_num]))
  }
}

## Let's drop these low variance columns
for(col_num in ncol(wage_data):1){
  if(var(wage_data[, col_num]) < 0.05) {
    wage_data <- wage_data[, -col_num]
  }
}
```

We can now try again to run PCR

```{r}
set.seed(222)

pcr_fit <- pcr(logwage ~ ., data = wage_data[train,], 
               scale = TRUE, validation = "CV")

summary(pcr_fit)
```


We are interested in finding which number of principle components should be included in the regression to lead to the lowest cross-validation error. 

```{r}
pcr_msep <- MSEP(pcr_fit)
pcr_min_indx <- which.min(pcr_msep$val[1, 1,])
print(pcr_min_indx)
```

How could you get the RMSEP?

```{r}
print(pcr_msep$val[1, 1, pcr_min_indx])
```

It can also be helpful to plot the RMSEP as a function of the number of components. The black line is the CV, the red dashed line is the adjusted CV. 

```{r}
validationplot(pcr_fit)
```

Why does the plot look the way it does? Do you expect the PLS plot to look similar? Why or why not?


Let's predict `logwage` for our test observations

```{r}
pcr_pred <- predict(pcr_fit, wage_data[test, ], ncomp = 12)
```

We can measure test MSE

```{r}
pcr_test_MSE <- mean((pcr_pred - wage_data[test, "logwage"])^2)
print(pcr_test_MSE)
```

We can convert this to RMSE

```{r}
print(sqrt(pcr_test_MSE))
```

Let's repeat this exercise for PLS. Use `plsr()` instead of `pcr()`.

```{r}
## Step 1: Fit the model
pls_fit <- plsr(logwage ~ ., data = wage_data[train, ], 
                scale = TRUE, validation = "CV")
summary(pls_fit)

## Step 2: Which ncomp value had the lowest CV MSE?
pls_msep <- MSEP(pls_fit)
pls_min_indx <- which.min(pls_msep$val[1, 1,])
print(pls_min_indx)

## Step 3: Plot validation error as a function of # of components
validationplot(pls_fit, val.type = c("RMSEP"))

## Step 4: Identify the CV RMSE for the number of components with
## the lowest CV RMSE
pls_rmsep <- RMSEP(pls_fit)
print(pls_rmsep$val[1, 1, as.numeric(pls_min_indx)])

## Step 5: Predict test set logwage values
pls_pred <- predict(pls_fit, wage_data[test,],
                    ncomp = (as.numeric(pls_min_indx) -1))

## Step 6: Measure test MSE and RMSE
pls_test_MSE <- mean((pls_pred - wage_data[test, "logwage"])^2)
print(pls_test_MSE)
print(sqrt(pls_test_MSE))
```
  
 
