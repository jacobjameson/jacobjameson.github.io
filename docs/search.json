[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jacob Jameson",
    "section": "",
    "text": "|\n\n\nI am a PhD student at Harvard University studying Health Policy and Decision Sciences. I am a National Institute of Mental Health (NIMH) funded Predoctoral Research Fellow in Comparative Effectiveness Research for Suicide Prevention at Brigham and Women’s Hospital.\nMy research develops and applies methods in causal inference, Bayesian statistics, and reinforcement learning to improve decision-making in health and public policy. I work at the intersection of statistics, computer science, and health policy, with applications to problems in suicide prevention, emergency department operations, and precision medicine. Methodologically, I focus on causal estimation in complex settings and sequential decision-making under uncertainty, but I am broadly interested in statistical computing, machine learning, and reproducible research.\nAside from research, I am passionate about teaching. I have extensive experience teaching students in a variety of settings, including as a teaching fellow at Harvard, instructor at UChicago Medicine, and as a Teach For America corps member in New Haven Public Schools."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Jacob Jameson",
    "section": "",
    "text": "|\n\n\nI am a PhD student at Harvard University studying Health Policy and Decision Sciences. I am a National Institute of Mental Health (NIMH) funded Predoctoral Research Fellow in Comparative Effectiveness Research for Suicide Prevention at Brigham and Women’s Hospital.\nMy research develops and applies methods in causal inference, Bayesian statistics, and reinforcement learning to improve decision-making in health and public policy. I work at the intersection of statistics, computer science, and health policy, with applications to problems in suicide prevention, emergency department operations, and precision medicine. Methodologically, I focus on causal estimation in complex settings and sequential decision-making under uncertainty, but I am broadly interested in statistical computing, machine learning, and reproducible research.\nAside from research, I am passionate about teaching. I have extensive experience teaching students in a variety of settings, including as a teaching fellow at Harvard, instructor at UChicago Medicine, and as a Teach For America corps member in New Haven Public Schools."
  },
  {
    "objectID": "Data/viz/2023-01-31 Tidy Tuesday/TT_wk5.html",
    "href": "Data/viz/2023-01-31 Tidy Tuesday/TT_wk5.html",
    "title": "Tidy Tuesday, 2023 Week 5 🐱",
    "section": "",
    "text": "Week\nDate\nData\nSource\nArticle\n\n\n\n\n5\n2023-01-31\nPet Cats UK\nMovebank for Animal Tracking Data\nCats on the Move\nHere is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-01-31 Tidy Tuesday/TT_wk5.html#my-plot",
    "href": "Data/viz/2023-01-31 Tidy Tuesday/TT_wk5.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 5 🐱",
    "section": "My plot",
    "text": "My plot\n\n🐱 Pet Cats UK\n\nCode here"
  },
  {
    "objectID": "Data/viz/healthcare area chart/healthcare_areachart.html",
    "href": "Data/viz/healthcare area chart/healthcare_areachart.html",
    "title": "Healthcare Area Chart",
    "section": "",
    "text": "Health Insurance by Income: 2020\n\n\n\n Other\n\n\n Military\n\n\n CHIP\n\n\n Subsidized Exchange\n\n\n Medicare and Medicaid\n\n\n\n\n\n\nNote: The income measure used here is  disposable income  , which is defined as market income plus cash transfers from the government and noncash benefits like food stamps and public housing. It excludes taxes, out-of-pocket medical spending, insurance premiums and other expenses. There have been small changes in the census questionnaire between the two survey dates. Source: Current Population Survey, analysis by Matt Bruenig\n\n\n\n\n\nCode here"
  },
  {
    "objectID": "Data/viz/2023-02-07 Tidy Tuesday/TT_wk6.html",
    "href": "Data/viz/2023-02-07 Tidy Tuesday/TT_wk6.html",
    "title": "Tidy Tuesday, 2023 Week 6 📈",
    "section": "",
    "text": "Week\nDate\nData\nSource\nArticle\n\n\n\n\n6\n2023-02-07\nBig Tech Stock Prices\nBig Tech Stock Prices on Kaggle\n5 Charts on Big Tech Stocks’ Collapse\nHere is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-02-07 Tidy Tuesday/TT_wk6.html#my-plot",
    "href": "Data/viz/2023-02-07 Tidy Tuesday/TT_wk6.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 6 📈",
    "section": "My plot",
    "text": "My plot\n\n📈 Big Tech Stock Prices\n\nCode here"
  },
  {
    "objectID": "Data/viz/2023-01-10 Tidy Tuesday/TT_wk2.html",
    "href": "Data/viz/2023-01-10 Tidy Tuesday/TT_wk2.html",
    "title": "Tidy Tuesday, 2023 Week 2 🐦",
    "section": "",
    "text": "Data for this week’s TidyTuesday comes from FeederWatch!\nHere is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-01-10 Tidy Tuesday/TT_wk2.html#my-plot",
    "href": "Data/viz/2023-01-10 Tidy Tuesday/TT_wk2.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 2 🐦",
    "section": "My plot",
    "text": "My plot\n\n🐦 FeederWatch\n\nCode here"
  },
  {
    "objectID": "R files/0 Module/mod0.html",
    "href": "R files/0 Module/mod0.html",
    "title": "Module 0",
    "section": "",
    "text": "Mac Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Mac version, save the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder.\n\n\nWindows Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Windows version, and save the executable file. Run the .exe file and follow the installation instructions."
  },
  {
    "objectID": "R files/0 Module/mod0.html#r-and-rstudio-installation-guide",
    "href": "R files/0 Module/mod0.html#r-and-rstudio-installation-guide",
    "title": "Module 0",
    "section": "",
    "text": "Mac Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Mac version, save the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder.\n\n\nWindows Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Windows version, and save the executable file. Run the .exe file and follow the installation instructions."
  },
  {
    "objectID": "R files/6 Module/mod6.html",
    "href": "R files/6 Module/mod6.html",
    "title": "Module 6: Data Visualization as a Tool for Analysis",
    "section": "",
    "text": "Download a copy of Module 6 slides\nDownload data for Module 6 lab and tutorial"
  },
  {
    "objectID": "R files/6 Module/mod6.html#data-visualization-as-a-tool-for-analysis",
    "href": "R files/6 Module/mod6.html#data-visualization-as-a-tool-for-analysis",
    "title": "Module 6: Data Visualization as a Tool for Analysis",
    "section": "",
    "text": "Download a copy of Module 6 slides\nDownload data for Module 6 lab and tutorial"
  },
  {
    "objectID": "R files/6 Module/mod6.html#lab-6",
    "href": "R files/6 Module/mod6.html#lab-6",
    "title": "Module 6: Data Visualization as a Tool for Analysis",
    "section": "Lab 6",
    "text": "Lab 6\nIn this lab, you will work with midwest.dta.\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\n\n\n\nQuestions\nRecall ggplot works by mapping data to aesthetics and then telling ggplot how to visualize the aesthetic with geoms. Like so:\n\nmidwest %&gt;%\n  ggplot(aes(x = percollege, \n             y = percbelowpoverty,\n             color = state,\n             size = poptotal,\n             alpha = percpovertyknown)) + \n  geom_point() + facet_wrap(vars(state))\n\n\n\n\n\nWhich is more highly correlated with poverty at the county level, college completion rates or high school completion rates? Is it consistent across states? Change one line of code in the above graph.\n\n\n\ngeoms\nFor the following, write code to reproduce each plot using midwest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice here inmetro is numeric, but I want it to behave like a discrete variable so I use x = as.character(inmetro). Use labs(title = \"Asian population by metro status\") to create the title.\n\n\n\n\n\n\n\nUse geom_boxplot() instead of geom_point() for “Asian population by metro status”\nUse geom_jitter() instead of geom_point() for “Asian population by metro status”\nUse geom_jitter() and geom_boxplot() at the same time for “Asian population by metro status”. Does order matter?\nHistograms are used to visualize distributions. What happens when you change the bins argument? What happens if you leave the bins argument off?\n\n\nmidwest %&gt;%\n  ggplot(aes(x = perchsd)) +\n  geom_histogram(bins = 100) +\n  labs(title = \"distribution of county-level hs completion rate\")\n\n\n\n\n\nRemake “distribution of county-level hs completion rate” with geom_density() instead of geom_histogram().\nAdd a vertical line at the median perchsd using geom_vline. You can calculate the median directly in the ggplot code.\n\n\nAesthetics\nFor the following, write code to reproduce each plot using midwest\n\nUse x, y, color and size\n\n\n\n\n\n\n\nUse x, y, color and size\n\n\n\n\n\n\n\nWhen making bar graphs, color only changes the outline of the bar. Change the aestethic name to fill to get the desired result\n\n\nmidwest %&gt;% \n  count(state) %&gt;% \n  ggplot(aes(x = state,\n             y = n, \n             color = state)) + \n  geom_col()\n\n\n\n\n\nThere’s a geom called geom_bar that takes a dataset and calculates the count. Read the following code and compare it to the geom_col code above. Describe how geom_bar() is different than geom_col\n\n\nmidwest %&gt;% \n  ggplot(aes(x = state,\n             color = state)) +\n  geom_bar()\n\n\n\n\nWell done! You’ve learned how to work with R to create some awesome looking visuals!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/4 Module/mod4.html",
    "href": "R files/4 Module/mod4.html",
    "title": "Module 4: Data Manipulation",
    "section": "",
    "text": "Download a copy of Module 4 slides\nDownload data for Module 4 lab and tutorial"
  },
  {
    "objectID": "R files/4 Module/mod4.html#data-manipulation",
    "href": "R files/4 Module/mod4.html#data-manipulation",
    "title": "Module 4: Data Manipulation",
    "section": "",
    "text": "Download a copy of Module 4 slides\nDownload data for Module 4 lab and tutorial"
  },
  {
    "objectID": "R files/4 Module/mod4.html#lab-4",
    "href": "R files/4 Module/mod4.html#lab-4",
    "title": "Module 4: Data Manipulation",
    "section": "Lab 4",
    "text": "Lab 4\nIn this lab, you will work with 2 data sets (world_wealth_inequality.xlsx and midwest.dta).\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\n\n\n\nWarm-up\n\nWhich of these are commands from dplyr?\n\n\nmutate()\nfilter()\nmean()\n\n\nIn the videos, you learned about head(). What if you wanted to get the tail end of your data instead?\nImagine you have a data set, df with 4 variables, county, year, income, and employment. You only need the year and employment status of people whose income is below $5000. Which two dplyr commands do you need to do this? Can you write the code for this?\nRemember the mean() function? What dplyr commands would we need if we want the average income in counties for the year 2003? Can you write the code for this?\nLoad tidyverse, haven, and readxl in your Rmd. If you haven’t yet, download the data from this page and put the data in your data folder and set your working directory. The data source is the World Inequality Database where you can find data about the distribution of income and wealth in several contries over time. Outside of lab time, check out wid.world for more information.\nIf you followed the set-up from above, you should be able to run the following code with no error.\n\n\nwid_data &lt;- read_xlsx(\"world_wealth_inequality.xlsx\")\n\n\n\nExamining ’wid_data\n\nLook at the data. What is the main problem here?\nWe don’t have columns headers. The World Inequality Database says the “structure” of the download is as shown in the image below.\n\n\nSo we can create our own header in read_xlsx. Calling the read_xlsx function using readxl::read_xlsx() ensures that we use the read_xlsx() function from the readxl package.\n\nwid_data_raw &lt;- readxl::read_xlsx(\"world_wealth_inequality.xlsx\",\n                                   col_names = c(\"country\", \"indicator\",\n                                                 \"percentile\", \"year\", \"value\"))\n\nNow when we look at the second column. It’s a mess. We can separate it based on where the \\n are and then deal with the data later. Don’t worry about this code right now.\n\nwid_data_raw &lt;- readxl::read_xlsx(\"world_wealth_inequality.xlsx\",\n                                  col_names = c(\"country\", \"indicator\",\n                                                \"percentile\", \"year\", \n                                                \"value\")) %&gt;%\n  separate(indicator, sep = \"\\\\n\", into = c(\"row_tag\", \"type\", \"notes\"))\n\nNote: We want a clean reproducible script so you should just have one block of code reading the data: that last one. The other code were building blocks. If you want to keep “extra” code temporarily in your script you can use # to comment out the code.\n\n\nManipulating World Inequality Data with dplyr\nNow we have some data and are ready to use select(), filter(), mutate(), summarize() and arrange() to explore it.\n\nThe data comes with some redundant columns that add clutter when we examine the data. What dplyr verb let’s you choose what columns to see? Remove the unwanted column row_tag and move notes to the last column position and assign the output to the name wid_data1\nLet’s start to dig into the data. We have two types of data: “Net personal wealth” and “National income”. Start by filter()ing the data so we only have “Net personal wealth” for France, name the resulting data french_data and then run the code below to visualize the data.\n\n\n# replace each ... with relevant code\nfrench_data &lt;- wid_data %&gt;% filter( ... , ...)\n\nNote: When refering to words in the data, make sure they are in quotes “France”, “Net personal wealth”. When referring to columns, do not use quotes.\n\nfrench_data %&gt;% \n  ggplot(aes(y = value, x = year, color = percentile)) + \n  geom_line()\n\nNow we’re getting somewhere! The plot shows the proportion of national wealth owned by different segements of French society overtime. For example in 2000, the top 1 percent owned roughly 28 percent of the wealth, while the bottom 50 percent owned abouy 7 percent.\n\nExplain the gaps in the plot. Using filter(), look at french_data in the years between 1960 and 1970. Does what you see line up with what you guessed by looking at the graph?\nUsing mutate(), create a new column called perc_national_wealth that equals value multiplied by 100. Adjust the graph code so that the y axis shows perc_national_wealth instead of value.\nNow following the same steps, explore data from the “Russian Federation”.\nThe data for “Russian Federation” does not start in 1900, but our y-axis does. That’s because we have a bunch of NAs. Let’s filter out the NAs and remake the plot. You cannot test for NA using == (Try: NA == NA). Instead we have a function called is.na(). (Try: is.na(NA) and !is.na(NA)).\nUse two dplyr verbs to figure out what year the bottom 50 percent held the least wealth. First, choose the rows that cover the bottom 50 percent and then sort the data in descending order using arrange()2.\n\n\n# replace ... with relevant code\nrussian_data %&gt;% \n  filter(...) %&gt;% \n  arrange(...)\n\n\nFor both the Russian Federation and French data, calculate the average proportion of wealth owned by the top 10 percent over the period from 1995 to 2010. You’ll have to filter and then summarize with summarize().\n\n\n# replace ... with relevant code\nrussian_data %&gt;% \n  filter(...) %&gt;% \n  summarize(top10 = mean(...))\n\n\n\nManipulating Midwest Demographic Data with dplyr\n\nNow we’ll use midwestern demographic data which is at this link. The dataset includes county level data for a single year. We call data this type of data “cross-sectional” since it gives a point-in-time cross-section of the counties of the midwest. (The world inequality data is “timeseries” data).\nSave midwest.dta in your data folder and load it into R.\n\n\nmidwest &lt;- read_dta('midwest.dta')\n\n\nRun the following code to get a sense of what the data looks like:\n\n\nglimpse(midwest)\n\n\nI wanted a tibble called midwest_pop that only had county identifiers and the 9 columns from midwest concerned with population counts. Replicate my work to create midwest_pop on your own3.\n\n\nnames(midwest_pop)\n\n [1] \"county\"          \"state\"           \"poptotal\"        \"popdensity\"     \n [5] \"popwhite\"        \"popblack\"        \"popamerindian\"   \"popasian\"       \n [9] \"popother\"        \"popadults\"       \"poppovertyknown\"\n\n\nHint: I went to ?select and found a selection helper that allowed me to select those 9 columns without typing all their names\n\n# replace ... with relevant code\nmidwest_pop &lt;- midwest %&gt;% select(county, state, ...)\n\n\nFrom midwest_pop calculate the area of each county4. What’s the largest county in the midwest? How about in Illinois?\nFrom midwest_pop calculate percentage adults for each county. What county in the midwest has the highest proportion of adults? What’s county in the midwest has the lowest proportion of adults?\nHow many people live in Michigan?\nNote that together population density and population can give you information about the area (geographic size) of a location. What’s the total area of Illinois? You probably have no idea what the units are though. If you google, you’ll find that it doesn’t align perfectly with online sources. Given the units don’t align with other sources, can this data still be useful?\n\nWell done! You’ve learned how to work with R to perform simple data manipulation and analysis!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/4 Module/mod4.html#footnotes",
    "href": "R files/4 Module/mod4.html#footnotes",
    "title": "Module 4: Data Manipulation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHint: You can type all the column names or use the slicker select(-notes, everything())↩︎\nHint: Look at the examples in ?arrange↩︎\nHint: notice that all the columns start with the same few letters.↩︎\nNotice that \\(popdensity = \\frac{poptotal}{area}\\)↩︎"
  },
  {
    "objectID": "R files/Intro R.html",
    "href": "R files/Intro R.html",
    "title": "Introduction to Programming in R",
    "section": "",
    "text": "This page contains the content for the “Introduction to Data Analysis in R” mini-course. The goal is to introduce R programming concepts with a focus on preparing and analyzing data, and conducting statistical simulations. We will cover how to read data into R, manipulate data with a suite of tools from the tidyverse package dplyr. We will also discuss some basic programming concepts including data types and operators.\nThis mini-course is aimed at people with no programming background that are looking for a gentle and practical introduction to data analysis in R.\nEach Module involves:\n\na pre-recorded lecture with guided practice exercises\na corresponding lab to practice the skills taught in the video and stretch your thinking\n\nThe labs are an essential part of the learning process as they walkthrough the use of many key functions and topics that are not explicitly covered in the videos.\n\nAdditional Resources\n\n\n\ntidyverse cheetsheets start with dplyr and ggplot\n\n\nR for Data Science: free online book with clear explanations of many tidyverse functions, the book to read on data analysis with R"
  },
  {
    "objectID": "R files/Intro R.html#about-this-course",
    "href": "R files/Intro R.html#about-this-course",
    "title": "Introduction to Programming in R",
    "section": "",
    "text": "This page contains the content for the “Introduction to Data Analysis in R” mini-course. The goal is to introduce R programming concepts with a focus on preparing and analyzing data, and conducting statistical simulations. We will cover how to read data into R, manipulate data with a suite of tools from the tidyverse package dplyr. We will also discuss some basic programming concepts including data types and operators.\nThis mini-course is aimed at people with no programming background that are looking for a gentle and practical introduction to data analysis in R.\nEach Module involves:\n\na pre-recorded lecture with guided practice exercises\na corresponding lab to practice the skills taught in the video and stretch your thinking\n\nThe labs are an essential part of the learning process as they walkthrough the use of many key functions and topics that are not explicitly covered in the videos.\n\nAdditional Resources\n\n\n\ntidyverse cheetsheets start with dplyr and ggplot\n\n\nR for Data Science: free online book with clear explanations of many tidyverse functions, the book to read on data analysis with R"
  },
  {
    "objectID": "R files/Intro R.html#table-of-contents",
    "href": "R files/Intro R.html#table-of-contents",
    "title": "Introduction to Programming in R",
    "section": "Table of Contents",
    "text": "Table of Contents"
  },
  {
    "objectID": "R files/7 Module/mod7.html",
    "href": "R files/7 Module/mod7.html",
    "title": "Module 7: Grouped Analysis",
    "section": "",
    "text": "Download a copy of Module 7 slides\nDownload data for Module 7 lab and tutorial"
  },
  {
    "objectID": "R files/7 Module/mod7.html#grouped-analysis",
    "href": "R files/7 Module/mod7.html#grouped-analysis",
    "title": "Module 7: Grouped Analysis",
    "section": "",
    "text": "Download a copy of Module 7 slides\nDownload data for Module 7 lab and tutorial"
  },
  {
    "objectID": "R files/7 Module/mod7.html#lab-7",
    "href": "R files/7 Module/mod7.html#lab-7",
    "title": "Module 7: Grouped Analysis",
    "section": "Lab 7",
    "text": "Lab 7\nIn this lab, you will work with data_traffic.csv.\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\n\n\n\nBackground and Data\nRead the background and data section before lab.\n\nFollow the tweet thread and you’ll see how Professor Damon Jones, of Harris, prepares and analyzes his data. In this lab, you’re going to follow his lead and dig into traffic stop data from the University of Chicago Police Department, one of the largest private police forces in the world.\n\n\nWarm-up\n\nOpen a new Rmd and save it in your coding lab folder; if you downloaded the data, move your data file to your preferred data location.\nIn your Rmd, write code to load your packages. If you load packages in the console, you will get an error when you knit because knitting starts a fresh R session.\nLoad data_traffic.csv and assign it to the name traffic_data. This data was scrapped from the UCPD website and partially cleaned by Prof. Jones.\nRecall that group_by() operates silently. Below I create a new data frame called grouped_data.\n\n\ngrouped_data &lt;- traffic_data %&gt;%\n  group_by(Race, Gender)\n\n\n\nHow can you tell grouped_data is different from traffic_data?\n\n\nHow many groups (Race-Gender pairs) are in the data? (This information should be available without writing additional code!)\n\n\nWithout running the code, predict the dimensions (number of rows by number of columns) of the tibbles created by traffic_data %&gt;% summarize(n = n()) and grouped_data %&gt;% summarize(n = n()).\n\n\nNow check you intuition by running the code.\n\n\n\nUse group_by() and summarize() to recreate the following table.\n\n\n\n# A tibble: 6 × 2\n  Race                                       n\n  &lt;chr&gt;                                  &lt;int&gt;\n1 African American                        3278\n2 American Indian/Alaskan Native            12\n3 Asian                                    226\n4 Caucasian                                741\n5 Hispanic                                 217\n6 Native Hawaiian/Other Pacific Islander     4\n\n\n\nUse count() to produce the same table.\n\n\n\nMoving beyond counts\n\nRaw counts are okay, but frequencies (or proportions) are easier to compare across data sets. Add a column with frequencies and assign the new tibble to the name traffic_stop_freq. The result should be identical to Prof. Jones’s analysis on twitter.\n\nTry on your own first. If you’re not sure how to add a frequency though, you could google “add a proportion to count with tidyverse” and find this stackoverflow post. Follow the advice of the number one answer. The green checkmark and large number of upvotes indicate the answer is likely reliable.\n\nThe frequencies out of context are not super insightful. What additional information do we need to argue the police are disproportionately stopping members of a certain group? (Hint: Prof. Jones shares the information in his tweets.)\nFor the problem above, your groupmate tried the following code. Explain why the frequencies are all 1.\n\n\ntraffic_stop_freq_bad &lt;- traffic_data %&gt;%\n  group_by(Race) %&gt;% \n  summarize(n = n(),\n            freq = n / sum(n)) \n\ntraffic_stop_freq_bad\n\nNow we want to go a step further. Do outcomes differ by race? In the first code block below, I provide code so you can visualize disposition by race. “Disposition” is police jargon that means the current status or final outcome of a police interaction.\n\ncitation_strings &lt;- c(\"citation issued\", \"citations issued\",\n                      \"citation issued\" )\n\narrest_strings &lt;- c(\"citation issued, arrested on active warrant\",\n                    \"citation issued; arrested on warrant\",\n                    \"arrested by cpd\", \"arrested on warrant\",\n                    \"arrested\",\"arrest\")\n\ndisposition_by_race &lt;- traffic_data %&gt;%\n  mutate(Disposition = str_to_lower(Disposition),\n         Disposition = case_when(Disposition %in% citation_strings ~ \"citation\",\n                                 Disposition %in% arrest_strings ~ \"arrest\",\n                                 TRUE ~ Disposition)) %&gt;%\n  count(Race, Disposition) %&gt;% group_by(Race) %&gt;%\n  mutate(freq = round(n / sum(n), 3))\n\ndisposition_by_race %&gt;%\n  filter(n &gt; 5, Disposition == \"citation\") %&gt;%\n  ggplot(aes(y = freq, x = Race)) + geom_col() +\n  labs(y = \"Citation Rate Once Stopped\", x = \"\", title = \"Traffic Citation Rate\") +\n  theme_minimal()\n\n\n\n\nLet’s break down how we got to this code. First, I ran traffic_data %&gt;% count(Race, Disposition) and noticed that we have a lot of variety in how officers enter information into the system. I knew I could deal with some of the issue by standardizing capitalization.\n\n\nIn the console, try out str_to_lower(...) by replacing the … with different strings. The name may be clear enough, but what does str_to_lower() do?\n\n\nAfter using mutate with str_to_lower(), I piped into count() again and looked for strings that represent the same Disposition. I stored terms in character vectors (e.g. citation_strings). The purpose is to make the case_when() easier to code and read. Once I got that right, I added frequencies to finalize disposition_by_race.\n\nTo make the graph, I first tried to get all the disposition data on the same plot.\n\n\n disposition_by_race %&gt;%\n  ggplot(aes(y = freq, x = Race, fill = Disposition)) + \n  geom_col()\n\n\n\n\nBy default, the bar graph is stacked. Look at the resulting graph and discuss the pros and cons of this plot.\n\nI decided I would focus on citations only and added the filter(n &gt; 5, Disposition == \"citation\") to the code. What is the impact of filtering based on n &gt; 5? Would you make the same choice? This question doesn’t have a “right” answer. You should try different options and reflect.\nNow, you can create a similar plot based called “Search Rate” using the Search variable. Write code to re-produce this plot.\n\n\n\n\n\n\nWell done! You’ve learned how to conduct grouped analysis using real-world data!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/5 Module/mod5.html",
    "href": "R files/5 Module/mod5.html",
    "title": "Module 5: Data Manipulation and Analysis II",
    "section": "",
    "text": "Download a copy of Module 5 slides\nDownload data for Module 5 lab and tutorial"
  },
  {
    "objectID": "R files/5 Module/mod5.html#data-manipulation-and-analysis-ii",
    "href": "R files/5 Module/mod5.html#data-manipulation-and-analysis-ii",
    "title": "Module 5: Data Manipulation and Analysis II",
    "section": "",
    "text": "Download a copy of Module 5 slides\nDownload data for Module 5 lab and tutorial"
  },
  {
    "objectID": "R files/5 Module/mod5.html#lab-5",
    "href": "R files/5 Module/mod5.html#lab-5",
    "title": "Module 5: Data Manipulation and Analysis II",
    "section": "Lab 5",
    "text": "Lab 5\nIn this lab, you will work with data sets from recent_college_grads.dta.\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\n\n\n\nWarm up\n\nData wrangling and visualization with college data\n\nWe will explore data on college majors and earnings, specifically the data behind the FiveThirtyEight story “The Economic Guide To Picking A College Major”.\nWe read it in with the read_dta function, and save the result as a new data frame called college_recent_grads. Because read_dta is a function from haven, we will need to load that package.\n\nlibrary(tidyverse)\nlibrary(haven)\ncollege_recent_grads &lt;- read_dta('recent_college_grads.dta')\n\ncollege_recent_grads is a tidy data frame, with each row representing an observation and each column representing a variable.\nTo view the data, you can take a quick peek at your data frame and view its dimensions with the glimpse function.\n\nglimpse(college_recent_grads)\n\nThe description of the variables, i.e. the codebook, is given below.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nrank\nRank by median earnings\n\n\nmajor_code\nMajor code, FO1DP in ACS PUMS\n\n\nmajor\nMajor description\n\n\nmajor_category\nCategory of major from Carnevale et al\n\n\ntotal\nTotal number of people with major\n\n\nsample_size\nSample size (unweighted) of full-time, year-round ONLY (used for earnings)\n\n\nmen\nMale graduates\n\n\nwomen\nFemale graduates\n\n\nsharewomen\nWomen as share of total\n\n\nemployed\nNumber employed (ESR == 1 or 2)\n\n\nemployed_full_time\nEmployed 35 hours or more\n\n\nemployed_part_time\nEmployed less than 35 hours\n\n\nemployed_full_time_yearround\nEmployed at least 50 weeks (WKW == 1) and at least 35 hours (WKHP &gt;= 35)\n\n\nunemployed\nNumber unemployed (ESR == 3)\n\n\nunemployment_rate\nUnemployed / (Unemployed + Employed)\n\n\nmedian\nMedian earnings of full-time, year-round workers\n\n\np25th\n25th percentile of earnigns\n\n\np75th\n75th percentile of earnings\n\n\ncollege_jobs\nNumber with job requiring a college degree\n\n\nnon_college_jobs\nNumber with job not requiring a college degree\n\n\nlow_wage_jobs\nNumber in low-wage service jobs\n\n\n\n\n\nWhich major has the lowest unemployment rate?\nIn order to answer this question all we need to do is sort the data. We use the arrange function to do this, and sort it by the unemployment_rate variable. By default arrange sorts in ascending order, which is what we want here – we’re interested in the major with the lowest unemployment rate.\n\ncollege_recent_grads %&gt;%\n  arrange(unemployment_rate)\n\n# A tibble: 173 × 21\n    rank major…¹ major major…² total sampl…³ men   women share…⁴ emplo…⁵ emplo…⁶\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1    53    4005 Math… Comput… 609         7 500   109   0.1789…     559     584\n 2    74    3801 Mili… Indust… 124         4 124   0     0             0     111\n 3    84    3602 Bota… Biolog… 1329        9 626   703   0.5289…    1010     946\n 4   113    1106 Soil… Agricu… 685         4 476   209   0.3051…     613     488\n 5   121    2301 Educ… Educat… 804         5 280   524   0.6517…     703     733\n 6    15    2409 Engi… Engine… 4321       30 3526  795   0.1839…    3608    2999\n 7    20    3201 Cour… Law & … 1148       14 877   271   0.2360…     930     808\n 8   120    2305 Math… Educat… 14237     123 3872  10365 0.7280…   13115   11259\n 9     1    2419 Petr… Engine… 2339       36 2057  282   0.1205…    1976    1849\n10    65    1100 Gene… Agricu… 10399     158 6053  4346  0.4179…    8884    7589\n# … with 163 more rows, 10 more variables: employed_parttime &lt;dbl&gt;,\n#   employed_fulltime_yearround &lt;dbl&gt;, unemployed &lt;dbl&gt;,\n#   unemployment_rate &lt;dbl&gt;, p25th &lt;dbl&gt;, median &lt;dbl&gt;, p75th &lt;dbl&gt;,\n#   college_jobs &lt;dbl&gt;, non_college_jobs &lt;dbl&gt;, low_wage_jobs &lt;dbl&gt;, and\n#   abbreviated variable names ¹​major_code, ²​major_category, ³​sample_size,\n#   ⁴​sharewomen, ⁵​employed, ⁶​employed_fulltime\n\n\nThis gives us what we wanted, but not in an ideal form. First, the name of the major barely fits on the page. Second, some of the variables are not that useful (e.g. major_code, major_category) and some we might want front and center are not easily viewed (e.g. unemployment_rate).\nWe can use the select function to choose which variables to display, and in which order:\n\ncollege_recent_grads %&gt;%\n  arrange(unemployment_rate) %&gt;%\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   &lt;dbl&gt; &lt;chr&gt;                                                  &lt;dbl&gt;\n 1    53 Mathematics And Computer Science                     0      \n 2    74 Military Technologies                                0      \n 3    84 Botany                                               0      \n 4   113 Soil Science                                         0      \n 5   121 Educational Administration And Supervision           0      \n 6    15 Engineering Mechanics Physics And Science            0.00633\n 7    20 Court Reporting                                      0.0117 \n 8   120 Mathematics Teacher Education                        0.0162 \n 9     1 Petroleum Engineering                                0.0184 \n10    65 General Agriculture                                  0.0196 \n# … with 163 more rows\n\n\nOk, this is looking better, but do we really need all those decimal places in the unemployment variable? Not really!\n\n1a. Round unemployment_rate: We create a new variable with the mutate function. In this case, we’re overwriting the existing unemployment_rate variable, by rounding it to 1 decimal places. Incomplete code is given below to guide you in the right direction, however you will need to fill in the blanks.\n\n\ncollege_recent_grads&lt;- college_recent_grads %&gt;%\n  arrange(unemployment_rate) %&gt;%\n  select(rank, major, unemployment_rate) %&gt;%\n  mutate(unemployment_rate = ___(___, 1))\n\nWhile were making some changes, let’s change sharewomen to numeric (it appears to be a string). Make sure to save your changes by overwriting the existing data frame!\n\ncollege_recent_grads &lt;- college_recent_grads %&gt;%\n  mutate(sharewomen = as.numeric(___))\n\n\n\nWhich major has the highest percentage of women?\nTo answer such a question we need to arrange the data in descending order. For example, if earlier we were interested in the major with the highest unemployment rate, we would use the following:\nThe desc function specifies that we want unemployment_rate in descending order.\n\ncollege_recent_grads %&gt;%\n  arrange(desc(unemployment_rate)) %&gt;%\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   &lt;dbl&gt; &lt;chr&gt;                                                  &lt;dbl&gt;\n 1     6 Nuclear Engineering                                    0.177\n 2    90 Public Administration                                  0.159\n 3    85 Computer Networking And Telecommunications             0.152\n 4   171 Clinical Psychology                                    0.149\n 5    30 Public Policy                                          0.128\n 6   106 Communication Technologies                             0.120\n 7     2 Mining And Mineral Engineering                         0.117\n 8    54 Computer Programming And Data Processing               0.114\n 9    80 Geography                                              0.113\n10    59 Architecture                                           0.113\n# … with 163 more rows\n\n\n\n1b. Using what you’ve learned so far, arrange the data in descending order with respect to proportion of women in a major, and display only the major, the total number of people with major, and proportion of women. Show only the top 3 majors by adding head(3) at the end of the pipeline.\n\n\n#code here\n\n\n\nHow do the distributions of median income compare across major categories?\nA percentile is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value below which 20% of the observations may be found. (Source: Wikipedia\nThere are three types of incomes reported in this data frame: p25th, median, and p75th. These correspond to the 25th, 50th, and 75th percentiles of the income distribution of sampled individuals for a given major.\nThe question we want to answer “How do the distributions of median income compare across major categories?”. We need to do a few things to answer this question: First, we need to group the data by major_category. Then, we need a way to summarize the distributions of median income within these groups. This decision will depend on the shapes of these distributions. So first, we need to visualize the data.\n\n1c.Let’s start simple and take a look at the distribution of all median incomes using geom_histogram, without considering the major categories.\n\n\nggplot(data = ____,\n       mapping = aes(x = median)) +\n  geom_histogram()\n\n\n1d. Try binwidths of \\(1000\\) and \\(5000\\) and choose one. Explain your reasoning for your choice.\n\n\nggplot(data = ___,\n       mapping = aes(x = median)) +\n  geom_histogram(binwidth = ___)\n\nWe can also calculate summary statistics for this distribution using the summarise function:\n\ncollege_recent_grads %&gt;%\n  summarise(min = min(median), max = max(median),\n            mean = mean(median), med = median(median),\n            sd = sd(median), \n            q1 = quantile(median, probs = 0.25),\n            q3 = quantile(median, probs = 0.75))\n\n# A tibble: 1 × 7\n    min    max   mean   med     sd    q1    q3\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 22000 110000 40151. 36000 11470. 33000 45000\n\n\n\n1e. Based on the shape of the histogram you created in the previous 1e, determine which of these summary statistics above (min, max, mean, med, sd, q1, q3) is/are useful for describing the distribution. Write up your description and include the summary statistic output as well.You can pick single/multiple statistics and briefly explain why you pick it/them.\n1f. Next, we facet the plot by major category. Plot the distribution of median income using a histogram, faceted by major_category. Use the binwidth you chose in 1e.\n\n\nggplot(data = ___,\n       mapping = aes (x=median)) +\n  geom_histogram(bindwidth = 5000) +\n  facet_wrap(.~major_category)\n\n\n1g. Use filter to find out which major has the highest median income? lowest? Which major has the median() median income? Hint: refer to the statistics in 1d.\n\n\ncollege_recent_grads %&gt;%\n  ____(median == ____) \n\n\n1h. Which major category is the most popular in this sample? To answer this question we use a new function called count, which first groups the data , then counts the number of observations in each category and store the counts into a column named n. Add to the pipeline appropriately to arrange the results so that the major with the highest observations is on top.\n\n\ncollege_recent_grads %&gt;%\n  count(major_category) %&gt;% \n  ___(___(n))\n\n\n\nWhat types of majors do women tend to major in?\nFirst, let’s create a new vector called stem_categories that lists the major categories that are considered STEM fields.\n\nstem_categories &lt;- c(\"Biology & Life Science\",\n                     \"Computers & Mathematics\",\n                     \"Engineering\",\n                     \"Physical Sciences\")\n\nThen, we can use this to create a new variable in our data frame indicating whether a major is STEM or not.\n\ncollege_recent_grads &lt;- college_recent_grads %&gt;%\n  mutate(major_type = ifelse(major_category %in% \n                               stem_categories, \"stem\", \"not stem\"))\n\nLet’s unpack this: with mutate we create a new variable called major_type, which is defined as \"stem\" if the major_category is in the nector called stem_categories we created earlier, and as \"not stem\" otherwise.\n\n1i. Create a scatterplot of median income vs. proportion of women in that major, colored by whether the major is in a STEM field or not. Describe the association between these three variables.\n\n\nggplot(data = ___, \n       mapping = aes(x=median, \n           y= sharewomen, \n           color=major_type)) + \n  geom_point()\n\n\n1j.. We can use the logical operators to also filter our data for STEM majors whose median earnings is less than median for all majors’s median earnings, which we found to be $36,000 earlier. Your output should only show the major name and median, 25th percentile, and 75th percentile earning for that major and should be sorted such that the major with the lowest median earning is on top.\n\n\n#code here\n\nWell done! You’ve learned how to work with R to perform basic data analysis!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "api 222 files/section 5/section 5.1.html",
    "href": "api 222 files/section 5/section 5.1.html",
    "title": "Section 5.1 - Cross-Validation and Bootstrapping Notes",
    "section": "",
    "text": "Note that the material in these notes draws on past TF’s notes (Ibou Dieye, Laura Morris, Emily Mower, Amy Wickett), and the more thorough treatment of these topics in Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.\nIn the absence of a designated test data set, a number of methods can be used to estimate out-of-sample performance using the available training data. In this review session, we consider two popular types of resampling methods: Cross-validation and Bootstrap."
  },
  {
    "objectID": "api 222 files/section 5/section 5.1.html#cross-validation",
    "href": "api 222 files/section 5/section 5.1.html#cross-validation",
    "title": "Section 5.1 - Cross-Validation and Bootstrapping Notes",
    "section": "Cross-validation",
    "text": "Cross-validation\nCross-validation is a method that works by holding out a subset of the training observations from the model fitting process, and then applying the model to those held out observations.\n\nThe Validation Set Approach\nThis is the type of cross-validation method we have been using in review sessions so far. It involves randomly dividing the available set of observations into two parts, a training set and a validation set. The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The validation error rate provides an estimate of the test error rate.\nThe validation set approach is simple and is easy to implement, but it has a few drawbacks. First, it can yield estimates of the test error rate that are highly variable. In addition, since it trains the model using only a subset of the data, it may tend to overestimate the test error rate for the model fit on the entire data set.\n\n\nLeave-one-out Cross-validation\nLeave-one-out cross-validation (LOOCV) attempts to address the drawbacks of the validation set method. Instead of creating two subsets, a single observation is used for the validation set, and the remaining (\\(n - 1\\)) observations constitute the training set. The statistical learning method is fit on the \\(n - 1\\) training observations, and a prediction is made for the excluded observation to estimate a test error rate. The procedure is repeated \\(n\\) times, by selecting each individual observation for the validation data, training the procedure on the \\(n - 1\\) observations, and computing the test error. The LOOCV estimate for the test error is the average of these \\(n\\) test error estimates.\nCompared to the validation set approach, LOOCV is more expensive to implement. However, it has less bias since it uses almost all the training observations and yields estimates of the test error rate that are less variable through its averaging feature.\n\n\n\\(k\\)-fold Cross-validation\n\\(k\\)-fold Cross-validation is an alternative to LOOCV. For \\(j=1,...,k\\), the model is fit on all folds except fold \\(j\\). Then, the model’s predictive performance on the data is assessed in fold \\(j\\). Thus, for a given iteration, the “training” data is data from all the folds except for fold \\(j\\), and the ``test’’ data is data from fold \\(j\\). This process is repeated for \\(j=1,...,k\\) and a test error is estimated for each fold. The \\(k\\)-fold cross-validation estimate for the test error is the average of these \\(k\\) test error estimates.\nLOOCV can be thought of as a special type \\(k\\)-fold cross-validation, where \\(k\\) equals the number of observations in the data. In practice, one typically performs \\(k\\)-fold CV using \\(k = 5\\) or \\(k = 10\\), which provides computational advantage over \\(k = n\\). Beyond computational issues, LOOCV has lower bias but higher variance (why?), compared to \\(k\\)-fold CV with \\(k &lt; n\\). Overall, \\(k\\)-fold CV tends to win in terms of the bias-variance trade-off."
  },
  {
    "objectID": "api 222 files/section 5/section 5.1.html#bootstrapping",
    "href": "api 222 files/section 5/section 5.1.html#bootstrapping",
    "title": "Section 5.1 - Cross-Validation and Bootstrapping Notes",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nBootstrapping involves resampling a data set with replacement. It is a very useful statistical tool that allows one to quantify the uncertainty of an estimator, such as a coefficient. This is especially useful for models where there is not a convenient standard error formula. When the goal is to estimate the uncertainty of an estimate, one can bootstrap the data and generate the estimate many times (e.g. 1,000 times). Then, one can look at the distribution of the estimates and take the middle 95% as the 95% confidence interval.\nBootstrapping is also an important component of some very powerful machine learning models. We will learn about some of these models in the next few weeks."
  },
  {
    "objectID": "api 222 files/section 2/section2.html",
    "href": "api 222 files/section 2/section2.html",
    "title": "Section 2 - KNN and Linear Regression",
    "section": "",
    "text": "Note that the material in these notes draws on past TF’s notes (Ibou Dieye, Laura Morris, Emily Mower, Amy Wickett), and the more thorough treatment of these topics in Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.\n\n\n\n\nPrediction problems can be defined based on the characteristics of the outcome variable we want to predict.\n\nRegression problems are those where the outcome is quantitative\nClassification problems are those where the outcome is qualitative / categorical\n\nSometimes the same methods can be used for regression and classification problems, but many methods are useful for only one of the two problem types.\n\n\n\nThe variance of a statistical learning method is the amount by which the prediction function would change if it was estimated on a different training set. A model that overfits has high variance, whereas a model that underfits has low variance.\nTo remember the difference between low variance and high variance models, I find it helpful to think of examples. Suppose your model was ``use the mean of the training data as the predicted value for all new data points.’’ The mean shouldn’t change much across training sets, so this has low variance. On the other hand, a model that picked up super complex patterns is likely to be picking up noise in addition to signal. The noise will vary by training set, so such a method would have high variance.\nThe bias of a statistical learning method is the error produced by representing a real world problem by a statistical learning method. Very flexible models (which are prone to overfitting) can capture complex patterns and so tend to have low bias. Very simple models (which are prone to underfitting) are limited in their ability to pick up patterns and so may have high bias.\nThe book uses the example of representing a non-linear function by a linear one to show that no matter how much data you have, a linear model will not do a great prediction job when the process generating the data is non-linear. Bias also applies to methods that might not fit your traditional concept of a statistical function. In the K-Nearest Neighbors section, we will discuss bias in that setting.\nOften, we will talk about the bias-variance trade-off. In an ideal world, we would find a model that has low variance and low bias, because that would yield a good and consistent model. In practice, you usually have to allow bias to increase in order to decrease variance and vice versa. However, there are many models that will decrease one (bias or variance) significantly while only increasing the other a little.\n\n\n\nSupervised learning refers to problems where there is a known outcome. In these problems, you can train a model to take features and predict the known outcome.\nUnsupervised learning refers to problems where you are interested in uncovering patterns and do not have a target outcome in mind.\nAn example of supervised learning would be using students’ high school grades, class enrollments, and demographic variables to predict whether or not they attend college.\nAn example of unsupervised learning would be using the same grades, enrollment, and demographic features to identify ``types’’ of high school students. That is, students who look similar according to these features. Perhaps you are interested in this because you want to make classes that contain a mix of different types of students. Often, unsupervised learning is useful for creating features for supervised learning problems, but sometimes uncovering patterns is the final objective.\n\n\n\nThere are different functions you can use to measure model performance, and which function you choose depends on your data and your objective. These functions are called ``loss functions,’’ which is a somewhat intuitive name when you think about the fact that your machine learning algorithm is trying to minimize this function and thus minimize your loss.\nTo understand how and why loss functions depend on your data and objectives, examples can be helpful.\nConsider first that you are trying to predict the future college majors of this year’s incoming freshmen (a classification problem). In this case, your prediction will either be right (you predict the major they end up choosing) or it will be wrong. Therefore, you might use accuracy (% correct) to measure model performance.\nWhat if, though, you cared more about being wrong for some majors than others? For example, imagine that all biology majors are going to need personalized lab equipment in their junior year and that the lab equipment is really expensive if ordered last minute but a lot cheaper if ordered a year or more in advance? Then, you might want to give more weight to people who end up being biology majors so that your model does better for predicting biology majors than other majors.\nNow consider that you are trying to predict home prices (a regression problem). You might measure your performance using mean-squared error (MSE), which is found by taking the difference between the predicted sale price for each home and the true sale price (the error), squaring it for each home, and then taking the mean of these squared errors. However, home prices are skewed (e.g. some homes are extremely expensive compared to most homes on the market). This means that a 5% error on a $3 million home is a lot bigger than a 5% error on a $100,000 home. When you square the errors (as you do when calculating MSE), the difference becomes enormous.\nBut since both errors are 5%, maybe you want to penalize them the same. One option is to use Mean Percentage Error (MPE), but this has the weird effect that if you over-predict one home by 5% and under-predict the other by 5%, your MPE is zero. Therefore, a popular option is to use the Mean Absolute Percentage Error (MAPE), which is the mean of the absolute values of the percentage errors and thus would be 5% in this example.\nFor many prediction problems in the policy sphere, we may not only care about accuracy of prediction but also about fairness or other objectives. The loss function is a place where we can explicitly tell the model to optimize for these concerns in addition to predictive performance.\n\n\n\n\n\n\nThe idea underlying K-Nearest Neighbors (KNN) is that we expect observations with similar features to have similar outcomes. KNN makes no other assumptions about functional form, so it is quite flexible.\n\n\n\nKNN can be used for either regression or classification, though it works slightly differently depending on what setting we are in. In the classification setting, the prediction is a majority vote of the observation’s \\(K\\)-nearest neighbors. In the regression setting, the prediction is the average outcome of the observation’s \\(K\\)-nearest neighbors.\nFor KNN, bias will be lower when \\(K\\) is lower. Bias will increase quickly as k increases, with further away neighbors being included in the prediction.\nThe only choice we have to make when implementing KNN is the value of \\(K\\) (e.g. how many neighbors should we use in our prediction?). A good way to find \\(K\\) is through cross-validation, something we will cover a little later, but which broadly involves training the algorithm on one set of data and seeing how well it does on a different set.\n\n\n\nA concern with KNN is whether you have good coverage of your feature space. Imagine that all of your training points were in one region of the feature space, but some of your test points are far away from this region. You will still use the \\(K\\) nearest neighbors to predict the outcome for these far-away test points, but it might not work as well as if the points were close together. Therefore, when implementing KNN, it’s good to think about how similar the features in your test set will be to the features in your training set. If they differ systematically, that is a concern (as it would be for other ML methods as well).\nAnother important consideration is whether there is an imbalance in the frequency of one outcome compared to another. For example, suppose we are trying to classify points as true'' orfalse’’ and most points are true.'' Even if thefalse’’ outcomes are clustered together in the feature space, if we use a large enough value of \\(K\\), we will predict true'' for these observations simply because there are many moretrue’’ observations than ``false’’ observations. Therefore, we would do better to use a small value for \\(K\\) in this setting.\nAnother consideration is whether proximity in each variable is equally important or if proximity in one variable is more important than proximity in another variable. KNN will normalize variables so that they are all on the same scale (same mean and variance) and then treat distance in all normalized variables the same. If you want to up-weight proximity for some variables and down-weight it for others, you can change the way each variable is normalized to accomplish this. Alternatively, you can include only those variables you think are important. When you have this type of uncertainty, there are more principled ways of selecting variables that will be discussed later in the course.\n\n\n\nYou might think that neighbors that are really close should be weighted more than neighbors that are a bit further away. Many people agree, so there are methods to allow you to weight different observations differently. You might also think that you shouldn’t use just the \\(K\\) nearest neighbors, but all the neighbors within a certain distance. Or maybe you think there’s information available in all observations, but there’s more information in closer neighbors. All of these adjustments fall under the umbrella of kernel regression. In fact, KNN is a special case of kernel regression. Broadly defined, kernel regression methods are a class of methods that generate predictions by taking weighted averages of observations. Because these methods (KNN included) do not specify a functional form, they are called ``non-parametric regression’’ methods.\n\n\n\n\n\n\nLinear regression is a parametric model that is additive and linear in the provided features. It is a classic technique used in many fields, and its widespread popularity greatly pre-dates the popularity of machine learning. Its general form is\n\\[\\begin{equation}\n    \\hat{y} = \\hat{\\beta}X\n\\end{equation}\\]\nWhere \\(\\hat{y}\\) is a vector of predicted \\(y\\) values and \\(X\\) is a matrix whose rows correspond to observations and whose columns correspond to features.\nWhen there is only one feature on the right hand side, the model is called a simple linear regression.\" When there are multiple features on the right hand side, the model is calledmultiple linear regression.”\nWhen used for inference, we are interested in \\(\\hat{\\beta}\\). However, when used for prediction, we are only interested in \\(\\hat{y}\\), and we cannot say that the \\(\\hat{\\beta}\\)s reflect any sort of causal relationship between the features and the outcome. For more information on how to test the significance of regression coefficients, please see Chapter 3 of ISLR for a reference on \\(t\\)-tests (in the simple model) and \\(F\\)-tests (in the multivariate model).\n\n\n\nTo find the coefficients \\(\\hat{\\beta}\\) in a linear regression, we find the value of \\(\\hat{\\beta}\\) that minimizes the residual sum of squares (RSS) in the training data. The classic formula for \\(\\hat{\\beta}\\) uses matrix algebra and is \\[\\begin{equation}\n    \\hat{\\beta} = (X^\\prime X)^{-1}X^\\prime y\n\\end{equation}\\] We will estimate \\(\\hat{\\beta}\\) using statistical software.\nIt is worth noting that the traditional measure of fit for linear regression is \\(R^2\\), but \\(R^2\\) mechanically increases with the inclusion of additional features. Therefore, in the prediction setting, the \\(R^2\\) on the training data is less important than the mean squared error (MSE) on the test data.\n\n\n\nThere are a few things to watch out for as far as the features that you feed into a linear regression.\n\nThere must be fewer features than observations. Later in the semester, we will cover penalized regression methods that do variable selection to yield estimable linear models, even when the number of available features exceeds the number of observations. Common penalized regression methods are lasso and ridge regression.\nYou can use quantitative or qualitative features for the \\(X\\)s. When using qualitative features, generate indicator variables for all but one category. The omitted category will serve as the ``baseline,’’ meaning that the coefficients on the included categories can be thought of as the differential effect of being in that category compared to the baseline (omitted) one.\nThe reason you omit one category when making indicator variables is to avoid linear dependence. If all categories were represented, the indicator columns would all sum to 1, which would mean they were linearly dependent. More generally, you cannot have collinearity or multi-collinearity, which means you cannot have features that are (close to) perfectly correlated.\nYou can interact two features (e.g. create a feature that is the product of two other features), and such interactions are valid on categorical and continuous features. However, when you include an interaction, you should also include each of the features on their own as well. Interactions have intuitive appeal if you think there are synergies between two features in terms of their effect on \\(y\\).\nYou can exponentiate features and include the exponentiated features in your model. The resulting model is sometimes called polynomial regression and is appropriate when there appears to be a non-linear relationship between a feature and the outcome.\nCheck for influential points – those that are both outliers (they have an unusual or extreme \\(y\\) value) and high leverage (they have an unusual or extreme \\(x\\)), as these points can greatly influence the model fit. You may want to exclude them or at least check your model’s sensitivity to including them versus excluding them.\n\nIf you are interested in inference (e.g. looking at the \\(\\hat{\\beta}\\)s to understand a causal relationship), it is important to be aware of Omitted Variables Bias (OVB). OVB occurs when you have two correlated features that each have an effect on \\(y\\) but only one is included in the regression. In that case, the coefficient on the included feature is biased, because it is partially picking up the true effect of the feature on the outcome and is also partially picking up the effect of the omitted feature on the outcome (since the omitted feature is correlated with the included feature).\nAs an example of OVB, suppose \\(X_1\\) and \\(X_2\\) are positively (but not perfectly) correlated. If they are also both positively correlated with \\(y\\), then when \\(X_2\\) is omitted from the regression, the coefficient on \\(X_1\\) will be higher than when both \\(X_1\\) and \\(X_2\\) are included. This is because the coefficient on \\(X_1\\) will now pick up both the effect of \\(X_1\\) on \\(y\\) and part of the effect of \\(X_2\\) on \\(y\\) (since \\(X_1\\) is a proxy for \\(X_2\\) because the two features are positively correlated).\nWhen implementing linear regression, you should also take a look at your residuals and make sure there are no red flags:\n\nWhen you plot residuals, they should appear randomly scattered. Any non-linearity or patterns in the residuals suggest your model is not appropriate.\nLinear regression assumes residuals are uncorrelated. Evidence of correlated residuals indicates a problem with your model or your data that should be investigated.\nResiduals should have constant variance. If you plot your residuals and their variance seems to be a function of \\(x\\), then the errors are heteroskedastic (a fancy word for ``a function of \\(x\\)’’). In this case, traditional statistical measures of significance are invalid, but other valid methods are available.\n\n\n\n\nThe main difference of note between linear regression and KNN is that linear regression is a parametric model whereas KNN is a non-parametric model. There are a few general differences between parametric and non-parametric models that are worth noting\n\nNon-parametric models are more flexible whereas parametric models impose stronger assumptions\nWhen there is a small number of observations per feature, parametric models tend to outperform non-parametric models\n\nIn addition to the general differences between parametric and non-parametric models, a key difference between KNN and linear regression is that linear regression is quite simple to fit. In fact, it only requires estimation of a few \\(\\beta\\)s, whereas KNN is much more computationally intensive. Because of this simplicity, linear regression is also more interpretable than KNN."
  },
  {
    "objectID": "api 222 files/section 2/section2.html#important-machine-learning-concepts",
    "href": "api 222 files/section 2/section2.html#important-machine-learning-concepts",
    "title": "Section 2 - KNN and Linear Regression",
    "section": "",
    "text": "Prediction problems can be defined based on the characteristics of the outcome variable we want to predict.\n\nRegression problems are those where the outcome is quantitative\nClassification problems are those where the outcome is qualitative / categorical\n\nSometimes the same methods can be used for regression and classification problems, but many methods are useful for only one of the two problem types.\n\n\n\nThe variance of a statistical learning method is the amount by which the prediction function would change if it was estimated on a different training set. A model that overfits has high variance, whereas a model that underfits has low variance.\nTo remember the difference between low variance and high variance models, I find it helpful to think of examples. Suppose your model was ``use the mean of the training data as the predicted value for all new data points.’’ The mean shouldn’t change much across training sets, so this has low variance. On the other hand, a model that picked up super complex patterns is likely to be picking up noise in addition to signal. The noise will vary by training set, so such a method would have high variance.\nThe bias of a statistical learning method is the error produced by representing a real world problem by a statistical learning method. Very flexible models (which are prone to overfitting) can capture complex patterns and so tend to have low bias. Very simple models (which are prone to underfitting) are limited in their ability to pick up patterns and so may have high bias.\nThe book uses the example of representing a non-linear function by a linear one to show that no matter how much data you have, a linear model will not do a great prediction job when the process generating the data is non-linear. Bias also applies to methods that might not fit your traditional concept of a statistical function. In the K-Nearest Neighbors section, we will discuss bias in that setting.\nOften, we will talk about the bias-variance trade-off. In an ideal world, we would find a model that has low variance and low bias, because that would yield a good and consistent model. In practice, you usually have to allow bias to increase in order to decrease variance and vice versa. However, there are many models that will decrease one (bias or variance) significantly while only increasing the other a little.\n\n\n\nSupervised learning refers to problems where there is a known outcome. In these problems, you can train a model to take features and predict the known outcome.\nUnsupervised learning refers to problems where you are interested in uncovering patterns and do not have a target outcome in mind.\nAn example of supervised learning would be using students’ high school grades, class enrollments, and demographic variables to predict whether or not they attend college.\nAn example of unsupervised learning would be using the same grades, enrollment, and demographic features to identify ``types’’ of high school students. That is, students who look similar according to these features. Perhaps you are interested in this because you want to make classes that contain a mix of different types of students. Often, unsupervised learning is useful for creating features for supervised learning problems, but sometimes uncovering patterns is the final objective.\n\n\n\nThere are different functions you can use to measure model performance, and which function you choose depends on your data and your objective. These functions are called ``loss functions,’’ which is a somewhat intuitive name when you think about the fact that your machine learning algorithm is trying to minimize this function and thus minimize your loss.\nTo understand how and why loss functions depend on your data and objectives, examples can be helpful.\nConsider first that you are trying to predict the future college majors of this year’s incoming freshmen (a classification problem). In this case, your prediction will either be right (you predict the major they end up choosing) or it will be wrong. Therefore, you might use accuracy (% correct) to measure model performance.\nWhat if, though, you cared more about being wrong for some majors than others? For example, imagine that all biology majors are going to need personalized lab equipment in their junior year and that the lab equipment is really expensive if ordered last minute but a lot cheaper if ordered a year or more in advance? Then, you might want to give more weight to people who end up being biology majors so that your model does better for predicting biology majors than other majors.\nNow consider that you are trying to predict home prices (a regression problem). You might measure your performance using mean-squared error (MSE), which is found by taking the difference between the predicted sale price for each home and the true sale price (the error), squaring it for each home, and then taking the mean of these squared errors. However, home prices are skewed (e.g. some homes are extremely expensive compared to most homes on the market). This means that a 5% error on a $3 million home is a lot bigger than a 5% error on a $100,000 home. When you square the errors (as you do when calculating MSE), the difference becomes enormous.\nBut since both errors are 5%, maybe you want to penalize them the same. One option is to use Mean Percentage Error (MPE), but this has the weird effect that if you over-predict one home by 5% and under-predict the other by 5%, your MPE is zero. Therefore, a popular option is to use the Mean Absolute Percentage Error (MAPE), which is the mean of the absolute values of the percentage errors and thus would be 5% in this example.\nFor many prediction problems in the policy sphere, we may not only care about accuracy of prediction but also about fairness or other objectives. The loss function is a place where we can explicitly tell the model to optimize for these concerns in addition to predictive performance."
  },
  {
    "objectID": "api 222 files/section 2/section2.html#k-nearest-neighbors",
    "href": "api 222 files/section 2/section2.html#k-nearest-neighbors",
    "title": "Section 2 - KNN and Linear Regression",
    "section": "",
    "text": "The idea underlying K-Nearest Neighbors (KNN) is that we expect observations with similar features to have similar outcomes. KNN makes no other assumptions about functional form, so it is quite flexible.\n\n\n\nKNN can be used for either regression or classification, though it works slightly differently depending on what setting we are in. In the classification setting, the prediction is a majority vote of the observation’s \\(K\\)-nearest neighbors. In the regression setting, the prediction is the average outcome of the observation’s \\(K\\)-nearest neighbors.\nFor KNN, bias will be lower when \\(K\\) is lower. Bias will increase quickly as k increases, with further away neighbors being included in the prediction.\nThe only choice we have to make when implementing KNN is the value of \\(K\\) (e.g. how many neighbors should we use in our prediction?). A good way to find \\(K\\) is through cross-validation, something we will cover a little later, but which broadly involves training the algorithm on one set of data and seeing how well it does on a different set.\n\n\n\nA concern with KNN is whether you have good coverage of your feature space. Imagine that all of your training points were in one region of the feature space, but some of your test points are far away from this region. You will still use the \\(K\\) nearest neighbors to predict the outcome for these far-away test points, but it might not work as well as if the points were close together. Therefore, when implementing KNN, it’s good to think about how similar the features in your test set will be to the features in your training set. If they differ systematically, that is a concern (as it would be for other ML methods as well).\nAnother important consideration is whether there is an imbalance in the frequency of one outcome compared to another. For example, suppose we are trying to classify points as true'' orfalse’’ and most points are true.'' Even if thefalse’’ outcomes are clustered together in the feature space, if we use a large enough value of \\(K\\), we will predict true'' for these observations simply because there are many moretrue’’ observations than ``false’’ observations. Therefore, we would do better to use a small value for \\(K\\) in this setting.\nAnother consideration is whether proximity in each variable is equally important or if proximity in one variable is more important than proximity in another variable. KNN will normalize variables so that they are all on the same scale (same mean and variance) and then treat distance in all normalized variables the same. If you want to up-weight proximity for some variables and down-weight it for others, you can change the way each variable is normalized to accomplish this. Alternatively, you can include only those variables you think are important. When you have this type of uncertainty, there are more principled ways of selecting variables that will be discussed later in the course.\n\n\n\nYou might think that neighbors that are really close should be weighted more than neighbors that are a bit further away. Many people agree, so there are methods to allow you to weight different observations differently. You might also think that you shouldn’t use just the \\(K\\) nearest neighbors, but all the neighbors within a certain distance. Or maybe you think there’s information available in all observations, but there’s more information in closer neighbors. All of these adjustments fall under the umbrella of kernel regression. In fact, KNN is a special case of kernel regression. Broadly defined, kernel regression methods are a class of methods that generate predictions by taking weighted averages of observations. Because these methods (KNN included) do not specify a functional form, they are called ``non-parametric regression’’ methods."
  },
  {
    "objectID": "api 222 files/section 2/section2.html#linear-regression",
    "href": "api 222 files/section 2/section2.html#linear-regression",
    "title": "Section 2 - KNN and Linear Regression",
    "section": "",
    "text": "Linear regression is a parametric model that is additive and linear in the provided features. It is a classic technique used in many fields, and its widespread popularity greatly pre-dates the popularity of machine learning. Its general form is\n\\[\\begin{equation}\n    \\hat{y} = \\hat{\\beta}X\n\\end{equation}\\]\nWhere \\(\\hat{y}\\) is a vector of predicted \\(y\\) values and \\(X\\) is a matrix whose rows correspond to observations and whose columns correspond to features.\nWhen there is only one feature on the right hand side, the model is called a simple linear regression.\" When there are multiple features on the right hand side, the model is calledmultiple linear regression.”\nWhen used for inference, we are interested in \\(\\hat{\\beta}\\). However, when used for prediction, we are only interested in \\(\\hat{y}\\), and we cannot say that the \\(\\hat{\\beta}\\)s reflect any sort of causal relationship between the features and the outcome. For more information on how to test the significance of regression coefficients, please see Chapter 3 of ISLR for a reference on \\(t\\)-tests (in the simple model) and \\(F\\)-tests (in the multivariate model).\n\n\n\nTo find the coefficients \\(\\hat{\\beta}\\) in a linear regression, we find the value of \\(\\hat{\\beta}\\) that minimizes the residual sum of squares (RSS) in the training data. The classic formula for \\(\\hat{\\beta}\\) uses matrix algebra and is \\[\\begin{equation}\n    \\hat{\\beta} = (X^\\prime X)^{-1}X^\\prime y\n\\end{equation}\\] We will estimate \\(\\hat{\\beta}\\) using statistical software.\nIt is worth noting that the traditional measure of fit for linear regression is \\(R^2\\), but \\(R^2\\) mechanically increases with the inclusion of additional features. Therefore, in the prediction setting, the \\(R^2\\) on the training data is less important than the mean squared error (MSE) on the test data.\n\n\n\nThere are a few things to watch out for as far as the features that you feed into a linear regression.\n\nThere must be fewer features than observations. Later in the semester, we will cover penalized regression methods that do variable selection to yield estimable linear models, even when the number of available features exceeds the number of observations. Common penalized regression methods are lasso and ridge regression.\nYou can use quantitative or qualitative features for the \\(X\\)s. When using qualitative features, generate indicator variables for all but one category. The omitted category will serve as the ``baseline,’’ meaning that the coefficients on the included categories can be thought of as the differential effect of being in that category compared to the baseline (omitted) one.\nThe reason you omit one category when making indicator variables is to avoid linear dependence. If all categories were represented, the indicator columns would all sum to 1, which would mean they were linearly dependent. More generally, you cannot have collinearity or multi-collinearity, which means you cannot have features that are (close to) perfectly correlated.\nYou can interact two features (e.g. create a feature that is the product of two other features), and such interactions are valid on categorical and continuous features. However, when you include an interaction, you should also include each of the features on their own as well. Interactions have intuitive appeal if you think there are synergies between two features in terms of their effect on \\(y\\).\nYou can exponentiate features and include the exponentiated features in your model. The resulting model is sometimes called polynomial regression and is appropriate when there appears to be a non-linear relationship between a feature and the outcome.\nCheck for influential points – those that are both outliers (they have an unusual or extreme \\(y\\) value) and high leverage (they have an unusual or extreme \\(x\\)), as these points can greatly influence the model fit. You may want to exclude them or at least check your model’s sensitivity to including them versus excluding them.\n\nIf you are interested in inference (e.g. looking at the \\(\\hat{\\beta}\\)s to understand a causal relationship), it is important to be aware of Omitted Variables Bias (OVB). OVB occurs when you have two correlated features that each have an effect on \\(y\\) but only one is included in the regression. In that case, the coefficient on the included feature is biased, because it is partially picking up the true effect of the feature on the outcome and is also partially picking up the effect of the omitted feature on the outcome (since the omitted feature is correlated with the included feature).\nAs an example of OVB, suppose \\(X_1\\) and \\(X_2\\) are positively (but not perfectly) correlated. If they are also both positively correlated with \\(y\\), then when \\(X_2\\) is omitted from the regression, the coefficient on \\(X_1\\) will be higher than when both \\(X_1\\) and \\(X_2\\) are included. This is because the coefficient on \\(X_1\\) will now pick up both the effect of \\(X_1\\) on \\(y\\) and part of the effect of \\(X_2\\) on \\(y\\) (since \\(X_1\\) is a proxy for \\(X_2\\) because the two features are positively correlated).\nWhen implementing linear regression, you should also take a look at your residuals and make sure there are no red flags:\n\nWhen you plot residuals, they should appear randomly scattered. Any non-linearity or patterns in the residuals suggest your model is not appropriate.\nLinear regression assumes residuals are uncorrelated. Evidence of correlated residuals indicates a problem with your model or your data that should be investigated.\nResiduals should have constant variance. If you plot your residuals and their variance seems to be a function of \\(x\\), then the errors are heteroskedastic (a fancy word for ``a function of \\(x\\)’’). In this case, traditional statistical measures of significance are invalid, but other valid methods are available.\n\n\n\n\nThe main difference of note between linear regression and KNN is that linear regression is a parametric model whereas KNN is a non-parametric model. There are a few general differences between parametric and non-parametric models that are worth noting\n\nNon-parametric models are more flexible whereas parametric models impose stronger assumptions\nWhen there is a small number of observations per feature, parametric models tend to outperform non-parametric models\n\nIn addition to the general differences between parametric and non-parametric models, a key difference between KNN and linear regression is that linear regression is quite simple to fit. In fact, it only requires estimation of a few \\(\\beta\\)s, whereas KNN is much more computationally intensive. Because of this simplicity, linear regression is also more interpretable than KNN."
  },
  {
    "objectID": "api 222 files/section 3/section3.html",
    "href": "api 222 files/section 3/section3.html",
    "title": "Section 3 - More on Linear Regression: Exercises",
    "section": "",
    "text": "As Professor Saghafian noted on Slide 14 of lecture 6, there are certain skills you are expected to have about inference in general, particularly when it comes to linear regression models. The goal of today’s section is to practice (some of) these skills. The session involves executing coding exercises and answering conceptual questions along the way. We will work with the data set, which is a part of the package. I will give you time to work on each subsection and then I will share my proposed code and answers to the questions. You are encouraged to work in pairs.\nNote that in some cases there are several ways to write the code to yield the same result.\n\n1 Exploratory data analysis\n\nLoad the Credit data set from ISLR package. Check the codebook to understand the structure of the data set and the definition and unit of each variable.\n\n\n\nSample Solution\n# Store the data in a clean object and cast the data into a \"data.table\" object\n# As noted earlier this package simplifies some of the data cleaning...\ncredit_data  &lt;- as.data.table(Credit) \n\n\n\nHow many observations and variables does the data set include?\n\n\n\nSample Solution\ndim(credit_data)\n\n# [1] 400  12\n\n\n\nWhat are the categorical variables in the data set?\n\n\n\nSample Solution\nstr(credit_data)\n\n\n#Classes ‘data.table’ and 'data.frame': 400 obs. of  12 variables:\n# $ ID       : int  1 2 3 4 5 6 7 8 9 10 ...\n# $ Income   : num  14.9 106 104.6 148.9 55.9 ...\n# $ Limit    : int  3606 6645 7075 9504 4897 8047 3388 7114 3300 6819 ...\n# $ Rating   : int  283 483 514 681 357 569 259 512 266 491 ...\n# $ Cards    : int  2 3 4 3 2 4 2 2 5 3 ...\n# $ Age      : int  34 82 71 36 68 77 37 87 66 41 ...\n# $ Education: int  11 15 11 11 16 10 12 9 13 19 ...\n# $ Gender   : Factor w/ 2 levels \" Male\",\"Female\": 1 2 1 2 1 1 2 1 2 2 ...\n# $ Student  : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 1 1 1 1 2 ...\n# $ Married  : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 1 2 1 1 1 1 2 ...\n# $ Ethnicity: Factor w/ 3 levels \"African American\",..: 3 2 2 2 3 3 1 2 3 1 ...\n# $ Balance  : int  333 903 580 964 331 1151 203 872 279 1350 ...\n# - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\n# The categorical variables are: Gender, Student, Married, and Ethnicity.\n\n\n\nAre they rows with missing values? If so, how many? Hint: checkout the complete.cases function.\n\n\n\nSample Solution\nnrow(credit_data[!complete.cases(credit_data),])\n\n# [1] 0\n\n\n\nGenerate summary statistics of all the variables. What is the mean and standard deviation of income?\n\n\n\nSample Solution\nsummary(credit_data)\n\n\n#       ID            Income           Limit           Rating          Cards      \n# Min.   :  1.0   Min.   : 10.35   Min.   :  855   Min.   : 93.0   Min.   :1.000  \n# 1st Qu.:100.8   1st Qu.: 21.01   1st Qu.: 3088   1st Qu.:247.2   1st Qu.:2.000  \n# Median :200.5   Median : 33.12   Median : 4622   Median :344.0   Median :3.000  \n# Mean   :200.5   Mean   : 45.22   Mean   : 4736   Mean   :354.9   Mean   :2.958  \n# 3rd Qu.:300.2   3rd Qu.: 57.47   3rd Qu.: 5873   3rd Qu.:437.2   3rd Qu.:4.000  \n# Max.   :400.0   Max.   :186.63   Max.   :13913   Max.   :982.0   Max.   :9.000  \n#      Age          Education        Gender    Student   Married  \n# Min.   :23.00   Min.   : 5.00    Male :193   No :360   No :155  \n# 1st Qu.:41.75   1st Qu.:11.00   Female:207   Yes: 40   Yes:245  \n# Median :56.00   Median :14.00                                   \n# Mean   :55.67   Mean   :13.45                                   \n# 3rd Qu.:70.00   3rd Qu.:16.00                                   \n# Max.   :98.00   Max.   :20.00                                   \n#            Ethnicity      Balance       \n# African American: 99   Min.   :   0.00  \n# Asian           :102   1st Qu.:  68.75  \n# Caucasian       :199   Median : 459.50  \n#                        Mean   : 520.01  \n#                        3rd Qu.: 863.00  \n#                        Max.   :1999.00  \n\n\nThe mean income is:\n\n\nSample Solution\nround(mean(credit_data$Income, na.rm = TRUE), 2)\n\n# [1] 45.22\n\n\nThe standard deviation of income is:\n\n\nSample Solution\nround(sd(credit_data$Income, na.rm = TRUE), 2)\n\n# [1] 35.24\n\n\n\nPlot the relationship between balance (y-axis) and income (x-axis). What do you notice about the relationship?\n\n\n\nSample Solution\nplot(x = credit_data$Income, y = credit_data$Balance,\n     main = \"Average credit card balance vs. Income\",\n     xlab = \"Income\",\n     ylab = \"Average credit card balance\")\n\n\n# There appears to be a positive relationship between these two variables.\n\n\n\n\n2 Inference\n\nRegress balance (y-variable) on income (x-variable). Interpret the income coefficient.\n\n\n\nSample Solution\nmod1 &lt;- lm(Balance ~ Income, credit_data)\nsummary(mod1)\n\n\n#Call:\n#lm(formula = Balance ~ Income, data = credit_data)\n\n#Residuals:\n#    Min      1Q  Median      3Q     Max \n#-803.64 -348.99  -54.42  331.75 1100.25 \n\n#Coefficients:\n#            Estimate Std. Error t value Pr(&gt;|t|)    \n#(Intercept) 246.5148    33.1993   7.425  6.9e-13 ***\n#Income        6.0484     0.5794  10.440  &lt; 2e-16 ***\n#---\n#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n#Residual standard error: 407.9 on 398 degrees of freedom\n#Multiple R-squared:  0.215,    Adjusted R-squared:  0.213 \n#F-statistic:   109 on 1 and 398 DF,  p-value: &lt; 2.2e-16\n\n\n# A $1,000 increase in income is associated with an \n# average balance increase of $6.05. The coefficient is statistically significant. \n\n\n\nNow add gender as an explanatory variable.\n\n\n\nSample Solution\nmod2 &lt;- lm(Balance ~ Income + Gender, credit_data)\nsummary(mod2)\n\n\n#Call:\n#lm(formula = Balance ~ Income + Gender, data = credit_data)\n\n#Residuals:\n#    Min      1Q  Median      3Q     Max \n#-791.23 -351.34  -51.57  328.18 1112.87 \n\n#Coefficients:\n#             Estimate Std. Error t value Pr(&gt;|t|)    \n#(Intercept)  233.7663    39.5322   5.913 7.24e-09 ***\n#Income         6.0521     0.5799  10.437  &lt; 2e-16 ***\n#GenderFemale  24.3108    40.8470   0.595    0.552    \n#---\n#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n#Residual standard error: 408.2 on 397 degrees of freedom\n#Multiple R-squared:  0.2157,   Adjusted R-squared:  0.2117 \n#F-statistic: 54.58 on 2 and 397 DF,  p-value: &lt; 2.2e-16\n\n\n\nInterpret all three coefficients (intercept, income coefficient, gender coefficient).\n\n\n\nSample Solution\n# - The average balance for males is $233.77. \n\n# - The average balance for females is $24.31 higher than the average \n#   balance of males, when controlling for income. Note however, that this \n#   difference is not statistically significant. \n\n# - A $1,000 increase in income is associated with an average balance\n#   increase of \\$6.05. The coefficient is statistically significant.  \n\n\n\nTest the null hypothesis that there is no relationship between balance and gender (i.e. \\(\\beta_{gender} = 0\\)). What do you conclude about the test?\n\n\n\nSample Solution\n# - H0: the difference in balance between females and males \n#   (after controlling for income) is 0, that is $\\beta_{gender} = 0$. \n\n# - Ha: the difference in balance between females and males\n#   (after controlling for income) is different from 0, that is $\\beta_{gender} \\neq 0$.\n\n# P-value suggests we cannot reject the NULL at any reasonable level of \n# significance (1\\%, 5\\%, 10\\%)\n\n\n\nWhat is the confidence interval of the gender coefficient? Interpret this coefficient. Hint: checkout the function.\n\n\n\nSample Solution\nconfint(mod2)\n\n\n# We can be 95\\% confident that the true difference in balance is between \n# females and males is between -55.99 and 104.61. Notice this interval \n# includes 0, which is consistent with our conclusion on the hypothesis test above. \n\n\n\nFind and interpret the \\(R^2\\) of this regression.\n\n\n\nSample Solution\n# The $R^2$ is 0.2157. This means that income and gender together\n# explain ~22\\% of the variation in average card balance.\n\n\n\nNow add an interaction term between income and gender to the regression in part 2.\n\n\nInterpret the coefficient on the interaction term.\n\n\n\nSample Solution\nmod3 &lt;- lm(Balance ~ Income*Gender, credit_data)\nsummary(mod3)\n\n#Call:\n#lm(formula = Balance ~ Income * Gender, data = credit_data)\n\n#Residuals:\n#    Min      1Q  Median      3Q     Max \n#-797.35 -352.35  -53.42  328.98 1114.47 \n\n#Coefficients:\n#                    Estimate Std. Error t value Pr(&gt;|t|)    \n#(Intercept)         227.7682    47.8567   4.759 2.73e-06 ***\n#Income                6.1836     0.8276   7.472 5.12e-13 ***\n#GenderFemale         36.0236    66.5744   0.541    0.589    \n#Income:GenderFemale  -0.2589     1.1612  -0.223    0.824    \n#---\n#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n#Residual standard error: 408.7 on 396 degrees of freedom\n#Multiple R-squared:  0.2158,   Adjusted R-squared:  0.2098 \n#F-statistic: 36.32 on 3 and 396 DF,  p-value: &lt; 2.2e-16\n\n\n\nWhat is \\(R^2\\) and the adjusted \\(R^2\\) of this regression. What do these two values tell you about the usefulness of the interaction term?\n\n\n\nSample Solution\n# The $R^2$ is 0.2158 and the adjusted $R^2$ is 0.2098. In the model without the\n# interaction term the $R^2$ was 0.2157, and the adjusted $R^2$ was 0.2117. \n# The $R^2$ has increased as expected given we have a added a term. \n# However, the adjusted adjusted $R^2$ has decreased suggesting the \n# interaction term does not add value \n# (when considering the complexity it adds to the model).\n\n\n\nPlot the residuals. What does the plot tell you about your model fit?\n\n\n\nSample Solution\nplot(mod3$residuals)\n\n# There's no discernible pattern in the residuals, the model fit appears reasonable.\n\n\n\nRerun the model in part 3 using the log-transformed version of the balance and income variables. Interpret the coefficient on the income term.\n\n\n\nSample Solution\nmod4 &lt;- lm(log(Balance + 0.0001) ~ log(Income)*Gender, credit_data)\nsummary(mod4)\n\n\n#Call:\n#lm(formula = log(Balance + 1e-04) ~ log(Income) * Gender, data = credit_data)\n\n#Residuals:\n#     Min       1Q   Median       3Q      Max \n#-14.5434  -0.1351   2.4202   4.1069   7.2967 \n\n#Coefficients:\n#                         Estimate Std. Error t value Pr(&gt;|t|)    \n#(Intercept)               -6.2982     2.3019  -2.736 0.006498 ** \n#log(Income)                2.4470     0.6340   3.859 0.000133 ***\n#GenderFemale              -0.4506     3.2929  -0.137 0.891238    \n#log(Income):GenderFemale   0.3034     0.9073   0.334 0.738248    \n#---\n#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n#Residual standard error: 6.264 on 396 degrees of freedom\n#Multiple R-squared:  0.0789,   Adjusted R-squared:  0.07192 \n#F-statistic: 11.31 on 3 and 396 DF,  p-value: 3.939e-07\n\n\n\n# A 1\\% increase in income is associated with 2.45\\% decreases \n# average balance, when controlling for gender. \n\n\n\n3 BONUS: Prediction\nNow let’s revisit prediction models using linear regression and KNN.\n\nPrepare the input datasets\n\n\nDrop the ID column, the categorical columns, and any rows with missing values.\n\n\n\nSample Solution\n# Remove rows with non-missing values\ncredit_data_complete &lt;- credit_data[complete.cases(credit_data), ]\n\n# Drop the district and municipality variables\ncredit_data_complete[, c(\"ID\", \"Gender\", \"Student\", \n                         \"Married\", \"Ethnicity\") := NULL] # \"data.table\" syntax\n\n\n\nRandomly split the data into a training set (75% of the observations) and a test set (the remaining 25% of the observations).\n\n\n\nSample Solution\n# Set a seed\nset.seed(222)\n# Extract the random test and training IDs\ntest_ids &lt;- sample(seq(nrow(credit_data_complete)), \n                   round(0.25 * nrow(credit_data_complete)))\ntraining_ids &lt;- which(!(seq(nrow(credit_data_complete)) %in% test_ids))\n\n# Now use the IDs to get the two sets\ntest_data &lt;- credit_data_complete[test_ids,]\ntraining_data &lt;- credit_data_complete[training_ids,]\n\n\n\nWhen you use your training data to build a linear model that regresses account balance on all other features available in the data (plus an intercept), what is your test Mean Squared Error?\n\n\n\nSample Solution\n# The model\nmod5 &lt;- lm(Balance ~ ., training_data)\n\n# Generate test predictions\npredicted_bal &lt;- predict(mod5, test_data[, -7])\n\n## Let's see how well we did in terms of MSE\nMSE_lm_bal &lt;- mean((predicted_bal - test_data$Balance)^2)\nprint(MSE_lm_bal)\n\n# [1] 33096.38\n\n\n\nWhen you use your training data to build a KNN model that regresses account balance on all other features in the data, what is your test Mean Squared Error with \\(K = 1\\)?\n\n\n\nSample Solution\n# Library for KNN regression\nlibrary(FNN)\n\n# The model\nknn_reg1 &lt;- knn.reg(training_data[, -c(7)],\n                    test_data[, -c(7)],\n                    training_data$Balance,\n                    k = 1)\n\n# The MSE\nmse_knn1 &lt;- mean((knn_reg1$pred - test_data$Balance)^2)\nprint(mse_knn1)\n\n# [1] 72397.03\n\n\n\nIn last Friday’s review session, one of your classmates asked: “Instead of testing a few individual values of K, could we use a more systematic approach that computes the Mean Squared Error for many values of \\(K\\) and then plot model performance as a function of \\(K\\).”\n\n\nIn the first review session, we went through the basics of looping. Use a “for” loop to implement the approach your classmate suggested. Test \\(K\\) values going from 1 to 100.\n\n\n\nSample Solution\n# Define the range of K values to test\nk_guesses &lt;- 1:100\n\n# Initialize a tracker for the MSE values for each K\nmse_res &lt;- NULL\n\n# Now loop through all the values \nfor(i in 1:length(k_guesses)){\n  # For each value, run the model using the current K guess\n  knn_reg &lt;- knn.reg(training_data[, -c(7)],\n                     test_data[, -c(7)],\n                     training_data$Balance,\n                     k = k_guesses[i]) # key line here\n  \n  # The MSE\n  mse_knn &lt;- mean((knn_reg$pred - test_data$Balance)^2)\n  \n  # Now update the tracker\n  mse_res[i] &lt;- mse_knn\n}\n\n# Now plot the results\nplot(x = k_guesses, y = mse_res, main = \"MSE vs. K\", xlab = \"K\", ylab = \"MSE\")\n\n\n\nWhat can you conclude about the optimal \\(K\\) value for this model.\n\n\n\nSample Solution\n# Find the K that gives the minimum MSE\nwhich.min(mse_res)\n\n# It looks like $K = 8$ would give you the lowest MSE in this case. \n# Note: this result may be different from yours depending on how your sampling played out."
  },
  {
    "objectID": "api 222 files/section 4/section 4.2.html",
    "href": "api 222 files/section 4/section 4.2.html",
    "title": "Section 4.2 - Classification and Helpful Commands in R",
    "section": "",
    "text": "The goal of this session is to learn how to implement three classification models: 1. A logistic model 2. LDA 3. QDA"
  },
  {
    "objectID": "api 222 files/section 4/section 4.2.html#reminders",
    "href": "api 222 files/section 4/section 4.2.html#reminders",
    "title": "Section 4.2 - Classification and Helpful Commands in R",
    "section": "Reminders",
    "text": "Reminders\nTo run one line of code in RStudio, you can highlight the code you want to run and hit “Run” at the top of the script. Alternatively, on a mac, you can highlight the code to run and hit Command + Enter. Or on a PC, you can highlight the code to run and hit Ctrl + Enter. If you ever forget how a function works, you can type ? followed immediately (e.g. with no space) by the function name to get the help file\nLet’s start by loading the necessary packages and data. We will use the “Default” dataset available from the ISLR package. This dataset contains information on credit card defaults, including a binary response variable “default” and three predictors: “student”, “balance”, and “income”. We will use this dataset to build and evaluate classification models.\n\n## Load the packages\nlibrary(ISLR)\nlibrary(FNN)\n\nData preparation\nNow extract the data and name it “default_data”\n\ndefault_data &lt;- Default\n\nLet’s get to know our data\n\nsummary(default_data)\n\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n\n\nIt looks like we have two categorical variables. We can convert them both to numeric\n\ndefault_data$default &lt;- as.numeric(default_data$default == \"Yes\")\ndefault_data$student &lt;- as.numeric(default_data$student == \"Yes\")\n\nLet’s again split our data into test and training data sets with a 20/80 split. We use set.seed() to ensure replicability.\n\nset.seed(222)\n\nThen we can use the sample function to split the data (as before)\n\n## First pick the test observations (20% of the data)\ntest_obs &lt;- sample(seq(nrow(default_data)), \n                   round(0.2 * nrow(default_data)))\n\n## The training observations are the remaining observations\ntrain_obs &lt;- setdiff(seq(nrow(default_data)), test_obs)\n\n## Use the indices now to extract the corresponding subsets of the data\ntest_data &lt;- default_data[test_obs,]\ntrain_data &lt;- default_data[train_obs,]"
  },
  {
    "objectID": "api 222 files/section 4/section 4.2.html#logistic-regression",
    "href": "api 222 files/section 4/section 4.2.html#logistic-regression",
    "title": "Section 4.2 - Classification and Helpful Commands in R",
    "section": "Logistic regression",
    "text": "Logistic regression\nNow, let’s say we are interested in using a logistic regression. For a base case, let’s try to predict default from the other available variables using logistic regression. To run logistic regression in R, use glm(), which requires three arguments: - 1st: Your formula (y ~ x1 + x2) - 2nd: Family = binomial tells it to use logistic regression - 3rd: You data, including both x and y columns.\nWe will train the model on the training data, make predictions for the test data using predict(), and measure performance with Accuracy.\n\nlogistic_default &lt;- glm(default ~ student + balance + income,\n                        family = binomial,\n                        data = train_data)\n\nTo view information about the logistic regression, including coefficients, use summary()\n\nsummary(logistic_default)\n\n\nCall:\nglm(formula = default ~ student + balance + income, family = binomial, \n    data = train_data)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.108e+01  5.654e-01 -19.588   &lt;2e-16 ***\nstudent     -5.036e-01  2.638e-01  -1.909   0.0563 .  \nbalance      5.739e-03  2.619e-04  21.916   &lt;2e-16 ***\nincome       6.727e-06  9.312e-06   0.722   0.4701    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2320.3  on 7999  degrees of freedom\nResidual deviance: 1252.6  on 7996  degrees of freedom\nAIC: 1260.6\n\nNumber of Fisher Scoring iterations: 8\n\n\nTo predict outcomes on the test_data using logistic regression, use:\n\nlogistic_predict &lt;-predict(logistic_default,\n                           test_data,\n                           type = \"response\")\n\nLet’s look at what we got from this prediction model. We will use the head() function, which prints the first few values of the object inside the parentheses. If you want to change the number of observations that are printed to 100, you can use head(object, n = 100)\n\nhead(logistic_predict)\n\n        4350           18         6678         4788         9481         9994 \n5.133197e-04 2.176129e-04 2.515555e-03 3.317590e-02 2.508148e-05 4.935658e-03 \n\n\nWe see that the prediction outputs are probabilities, so in order to make predictions we have to decide on a decision rule. A common one is if the predicted probability is &gt; 0.5, predict 1 otherwise 0. Let’s see how we would do using this rule. Because it’s a classification problem, accuracy is a good measure (% correct)\n\nclass_predictions &lt;- as.numeric(logistic_predict &gt; 0.5)\nlogistic_accuracy &lt;- mean(class_predictions == test_data$default)\nprint(logistic_accuracy)\n\n[1] 0.9745\n\n\nThe accuracy looks great! But, we might care more about different types of errors than overall error rate. For example, we may not want to give loans to people who will default even if this means denying loans to some people who wouldn’t default. We can measure error rate by true default status. Note that for defaulters, default = 1. Here, I pull out all the predictions for the true defaulters and see what fraction of those equal 1.\n\ntrue_pos_accuracy &lt;- mean(class_predictions[which(test_data$default == 1)] == 1)\nprint(true_pos_accuracy)\n\n[1] 0.3333333\n\n\nLike-wise for the non-defaulters, I see what fraction of those equal 0. This gives class-specific accuracy rates.\n\ntrue_neg_accuracy &lt;- mean(class_predictions[which(test_data$default == 0)] == 0)\nprint(true_neg_accuracy)\n\n[1] 0.9974107\n\n\nThese values summarise what can also be seen in the following table. Where the columns correspond to the true values and the rows correspond to the predicted values.\n\ntable(class_predictions, test_data$default)\n\n                 \nclass_predictions    0    1\n                0 1926   46\n                1    5   23\n\n\nSuppose instead of the accuracy, you wanted to directly calculate the error rate. How would you do it? Hint, errors are ones where the prediction does not equal the true value. In R, we use != for “does not equal”\nWe can also calculate the error rate for the true defaulters and non-defaulters.\n\ntrue_pos_error &lt;- mean(class_predictions\n                       [which(test_data$default == 1)] != 1)\nprint(true_pos_error)\n\n[1] 0.6666667\n\ntrue_neg_error &lt;- mean(class_predictions\n                       [which(test_data$default == 0)] != 0)\nprint(true_neg_error)\n\n[1] 0.002589332\n\n\nWe see that we did a lot better on the true negatives than the true positives. Among all the people who will default, we only predicted about 1/3% of them would default. If we want to do a better job identifying these people, we can do this by lowering the default threshold from a predicted probability of 0.5 to something lower, say 0.2. Note, though, that lowering this threshold means increasing the number of default predictions for people who don’t default as well. Since we aren’t really sure how low we want to make this threshold, we can try for a bunch of threshold values and then see how the performance changes to pick the one that is best for our setting. This involves domain knowledge, such as the cost of default and the earnings on loans extended to people who repay, so there’s not one right answer, but we can more clearly see the tradeoffs by trying many values and plotting the error rates in each group as a function of the threshold.\nTo do this, we can use a loop to try a bunch of threshold values and then calculate the error rates for each threshold. We can then plot the error rates as a function of the threshold to see how the error rates change as we change the threshold.\n\n## First, we need to specify the list of threshold values to assess\nthreshold_values &lt;- seq(from = 0.00, to = 0.50, by = 0.01)\n\nThen we initialize a matrix of error rates. This matrix will have a number of rows corresponding to the length of the list of threshold values and 2 columns corresponding to the true positive and true negative accuracy for each value that we test\n\nerror_rates &lt;- matrix(0, nrow = length(threshold_values), ncol = 2)\n\nNow we can start the loop. We initialize a tracker for the row index, then for each threshold value in our specified list of values, we update the tracker to reflect the row, generate the predicted classes using the specific threshold, calculate the true positive accuracy, calculate the true negative accuracy, and add the results to our matrix.\n\nindx &lt;- 0\n\nfor(threshold in threshold_values) {\n  \n  ## Update the tracker to reflect the row\n  indx &lt;- indx + 1\n  \n  ## Then generate the predicted classes using the specific threshold\n  class_predictions &lt;- as.numeric(logistic_predict &gt; threshold)\n  \n  ## Then calculate the true positive accuracy\n  true_pos_accuracy &lt;- mean(class_predictions[which(test_data$default == 1)] == 1)\n  \n  ## Then calculate the true negative accuracy\n  true_neg_accuracy &lt;- mean(class_predictions[which(test_data$default == 0)] == 0)\n  \n  ## Now we can add the results to our matrix \n  error_rates[indx,] &lt;- c(true_pos_accuracy, true_neg_accuracy)\n}\n\nLet’s plot each of these as a function of the threshold\n\nmatplot(x = threshold_values,\n        y = error_rates, \n        type = \"l\",\n        col = 3:4,\n        xlab = \"Threshold\",\n        ylab = \"Accuracy\",\n        main = \"Accuracy as a Function of Threshold\")\n        legend(\"topright\", legend = c(\"Defaulters\", \"Non-defaulters\"), \n               col = 3:4, pch = 1)"
  },
  {
    "objectID": "api 222 files/section 6/section 6.2.html",
    "href": "api 222 files/section 6/section 6.2.html",
    "title": "Section 6.2 - Ridge, Lasso, PCA/PCR, PLS Code",
    "section": "",
    "text": "We will use the Caravan data set that is included in the package ISLR. This data set contains information on people offered Caravan insurance.\n\nlibrary(ISLR)\ninsurance_data &lt;- Caravan\n\nLet’s learn a little more about the data\n\n?Caravan\n\nLet’s try to predict CARAVAN. Note: although this is a binary variable, ridge and lasso are regression algorithms – regression can often give you a good sense of the ordinal distribution of likelihood that the outcome will be 1 even if the resulting value cannot be viewed as a probability). When you run lasso and ridge, you will need to provide a penalty parameter. Since we don’t know which penalty parameter is best, we will use a built in cross-validation function to find the best penalty parameter (lambda) in the package glmnet\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\n?cv.glmnet\n\nWe will start by using the function’s built-in sequence of lambdas and glmnet standardization\n\nset.seed(222) # Important for replicability\nlasso_ins &lt;- cv.glmnet(x = as.matrix(insurance_data[, 1:85]), # the features\n                       y = as.numeric(insurance_data[, 86]), # the outcome\n                       standardize = TRUE, # Why do we do this?\n                       alpha = 1) # Corresponds to LASSO\n\nWe can see which lambda sequence was used\n\nprint(lasso_ins$lambda)\n\n  [1] 3.577561e-02 3.259740e-02 2.970154e-02 2.706294e-02 2.465874e-02\n  [6] 2.246812e-02 2.047212e-02 1.865343e-02 1.699631e-02 1.548641e-02\n [11] 1.411064e-02 1.285709e-02 1.171490e-02 1.067418e-02 9.725915e-03\n [16] 8.861891e-03 8.074625e-03 7.357298e-03 6.703696e-03 6.108158e-03\n [21] 5.565526e-03 5.071100e-03 4.620597e-03 4.210116e-03 3.836101e-03\n [26] 3.495312e-03 3.184799e-03 2.901870e-03 2.644076e-03 2.409183e-03\n [31] 2.195158e-03 2.000146e-03 1.822459e-03 1.660557e-03 1.513037e-03\n [36] 1.378623e-03 1.256150e-03 1.144557e-03 1.042878e-03 9.502315e-04\n [41] 8.658156e-04 7.888989e-04 7.188153e-04 6.549577e-04 5.967731e-04\n [46] 5.437574e-04 4.954515e-04 4.514370e-04 4.113325e-04 3.747909e-04\n [51] 3.414955e-04 3.111580e-04 2.835156e-04 2.583288e-04 2.353796e-04\n [56] 2.144691e-04 1.954163e-04 1.780560e-04 1.622380e-04 1.478253e-04\n [61] 1.346929e-04 1.227271e-04 1.118244e-04 1.018902e-04 9.283857e-05\n [66] 8.459104e-05 7.707621e-05 7.022897e-05 6.399002e-05 5.830533e-05\n [71] 5.312564e-05 4.840611e-05 4.410584e-05 4.018760e-05 3.661744e-05\n [76] 3.336445e-05 3.040045e-05 2.769975e-05 2.523898e-05 2.299682e-05\n [81] 2.095385e-05 1.909237e-05 1.739625e-05 1.585082e-05 1.444267e-05\n [86] 1.315963e-05 1.199056e-05 1.092535e-05 9.954775e-06 9.070420e-06\n [91] 8.264629e-06 7.530422e-06 6.861440e-06 6.251889e-06 5.696488e-06\n [96] 5.190428e-06 4.729325e-06 4.309185e-06 3.926368e-06 3.577561e-06\n\n\nLet’s find the lambda that does the best as far as CV error goes\n\nprint(lasso_ins$lambda.min)\n\n[1] 0.006703696\n\n\nYou can plot the model results\n\nplot(lasso_ins)\n\n\n\n\nYou can also plot the CV-relevant outputs\n\nLassoCV &lt;- lasso_ins$glmnet.fit\n\nplot(LassoCV, label = TRUE, xvar = \"lambda\")\n\nabline(\n  v = log(c(lasso_ins$lambda.min, lasso_ins$lambda.1se))\n  ) # Adds lines to mark the two key lambda values\n\n\n\n\nYou can see the coefficients corresponding to the two key lambda values using the predict function\n\npredict(lasso_ins, type = \"coefficients\",\n        s = c(lasso_ins$lambda.min, lasso_ins$lambda.1se))\n\n86 x 2 sparse Matrix of class \"dgCMatrix\"\n                       s1          s2\n(Intercept)  0.9896239538 1.053595130\nMOSTYPE      .            .          \nMAANTHUI     .            .          \nMGEMOMV      .            .          \nMGEMLEEF     .            .          \nMOSHOOFD     .            .          \nMGODRK       .            .          \nMGODPR       .            .          \nMGODOV       .            .          \nMGODGE       .            .          \nMRELGE       0.0018180263 .          \nMRELSA       .            .          \nMRELOV       .            .          \nMFALLEEN     .            .          \nMFGEKIND     .            .          \nMFWEKIND     .            .          \nMOPLHOOG     0.0022611683 .          \nMOPLMIDD     .            .          \nMOPLLAAG    -0.0024753875 .          \nMBERHOOG     .            .          \nMBERZELF     .            .          \nMBERBOER    -0.0030373898 .          \nMBERMIDD     .            .          \nMBERARBG     .            .          \nMBERARBO     .            .          \nMSKA         .            .          \nMSKB1        .            .          \nMSKB2        .            .          \nMSKC         .            .          \nMSKD         .            .          \nMHHUUR      -0.0007565131 .          \nMHKOOP       .            .          \nMAUT1        0.0013643467 .          \nMAUT2        .            .          \nMAUT0        .            .          \nMZFONDS      .            .          \nMZPART       .            .          \nMINKM30      .            .          \nMINK3045     .            .          \nMINK4575     .            .          \nMINK7512     .            .          \nMINK123M     .            .          \nMINKGEM      0.0028225067 .          \nMKOOPKLA     0.0025557889 .          \nPWAPART      0.0076586029 .          \nPWABEDR      .            .          \nPWALAND     -0.0008184860 .          \nPPERSAUT     0.0087316235 0.002079863\nPBESAUT      .            .          \nPMOTSCO      .            .          \nPVRAAUT      .            .          \nPAANHANG     .            .          \nPTRACTOR     .            .          \nPWERKT       .            .          \nPBROM        .            .          \nPLEVEN       .            .          \nPPERSONG     .            .          \nPGEZONG      .            .          \nPWAOREG      0.0010472132 .          \nPBRAND       0.0042630439 .          \nPZEILPL      .            .          \nPPLEZIER     .            .          \nPFIETS       .            .          \nPINBOED      .            .          \nPBYSTAND     .            .          \nAWAPART      .            .          \nAWABEDR      .            .          \nAWALAND      .            .          \nAPERSAUT     0.0006293082 .          \nABESAUT      .            .          \nAMOTSCO      .            .          \nAVRAAUT      .            .          \nAAANHANG     .            .          \nATRACTOR     .            .          \nAWERKT       .            .          \nABROM        .            .          \nALEVEN       .            .          \nAPERSONG     .            .          \nAGEZONG      .            .          \nAWAOREG      .            .          \nABRAND       .            .          \nAZEILPL      .            .          \nAPLEZIER     0.2083116609 .          \nAFIETS       0.0076146528 .          \nAINBOED      .            .          \nABYSTAND     0.0361586998 .          \n\n\nQuestions you should be able to answer :\n\nWhich lambda in the sequence had the lowest CV error?\n\n\nlasso_ins$lambda.min\n\n[1] 0.006703696\n\n\n\nWhat is the CV error of the “best” lambda?\n\n\nlasso_ins$cvm[lasso_ins$lambda == lasso_ins$lambda.min]\n\n[1] 0.05384566\n\n\n\nWhat is the standard deviation of the CV error for the “best” lambda?\n\n\nlasso_ins$cvsd[lasso_ins$lambda == lasso_ins$lambda.min]\n\n[1] 0.002099205\n\n\n\nWhat is the largest lambda whose CV error was within 1 standard deviation of the lowest CV error?\n\n\nlasso_ins$lambda.1se\n\n[1] 0.02970154\n\n\nWhen you do ridge regression, the code is almost exactly the same as for lasso in R. You just need to change the alpha parameter from 1 to 0. I’ll leave this to you as an exercise."
  },
  {
    "objectID": "api 222 files/section 6/section 6.2.html#lasso-and-ridge-regression",
    "href": "api 222 files/section 6/section 6.2.html#lasso-and-ridge-regression",
    "title": "Section 6.2 - Ridge, Lasso, PCA/PCR, PLS Code",
    "section": "",
    "text": "We will use the Caravan data set that is included in the package ISLR. This data set contains information on people offered Caravan insurance.\n\nlibrary(ISLR)\ninsurance_data &lt;- Caravan\n\nLet’s learn a little more about the data\n\n?Caravan\n\nLet’s try to predict CARAVAN. Note: although this is a binary variable, ridge and lasso are regression algorithms – regression can often give you a good sense of the ordinal distribution of likelihood that the outcome will be 1 even if the resulting value cannot be viewed as a probability). When you run lasso and ridge, you will need to provide a penalty parameter. Since we don’t know which penalty parameter is best, we will use a built in cross-validation function to find the best penalty parameter (lambda) in the package glmnet\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\n?cv.glmnet\n\nWe will start by using the function’s built-in sequence of lambdas and glmnet standardization\n\nset.seed(222) # Important for replicability\nlasso_ins &lt;- cv.glmnet(x = as.matrix(insurance_data[, 1:85]), # the features\n                       y = as.numeric(insurance_data[, 86]), # the outcome\n                       standardize = TRUE, # Why do we do this?\n                       alpha = 1) # Corresponds to LASSO\n\nWe can see which lambda sequence was used\n\nprint(lasso_ins$lambda)\n\n  [1] 3.577561e-02 3.259740e-02 2.970154e-02 2.706294e-02 2.465874e-02\n  [6] 2.246812e-02 2.047212e-02 1.865343e-02 1.699631e-02 1.548641e-02\n [11] 1.411064e-02 1.285709e-02 1.171490e-02 1.067418e-02 9.725915e-03\n [16] 8.861891e-03 8.074625e-03 7.357298e-03 6.703696e-03 6.108158e-03\n [21] 5.565526e-03 5.071100e-03 4.620597e-03 4.210116e-03 3.836101e-03\n [26] 3.495312e-03 3.184799e-03 2.901870e-03 2.644076e-03 2.409183e-03\n [31] 2.195158e-03 2.000146e-03 1.822459e-03 1.660557e-03 1.513037e-03\n [36] 1.378623e-03 1.256150e-03 1.144557e-03 1.042878e-03 9.502315e-04\n [41] 8.658156e-04 7.888989e-04 7.188153e-04 6.549577e-04 5.967731e-04\n [46] 5.437574e-04 4.954515e-04 4.514370e-04 4.113325e-04 3.747909e-04\n [51] 3.414955e-04 3.111580e-04 2.835156e-04 2.583288e-04 2.353796e-04\n [56] 2.144691e-04 1.954163e-04 1.780560e-04 1.622380e-04 1.478253e-04\n [61] 1.346929e-04 1.227271e-04 1.118244e-04 1.018902e-04 9.283857e-05\n [66] 8.459104e-05 7.707621e-05 7.022897e-05 6.399002e-05 5.830533e-05\n [71] 5.312564e-05 4.840611e-05 4.410584e-05 4.018760e-05 3.661744e-05\n [76] 3.336445e-05 3.040045e-05 2.769975e-05 2.523898e-05 2.299682e-05\n [81] 2.095385e-05 1.909237e-05 1.739625e-05 1.585082e-05 1.444267e-05\n [86] 1.315963e-05 1.199056e-05 1.092535e-05 9.954775e-06 9.070420e-06\n [91] 8.264629e-06 7.530422e-06 6.861440e-06 6.251889e-06 5.696488e-06\n [96] 5.190428e-06 4.729325e-06 4.309185e-06 3.926368e-06 3.577561e-06\n\n\nLet’s find the lambda that does the best as far as CV error goes\n\nprint(lasso_ins$lambda.min)\n\n[1] 0.006703696\n\n\nYou can plot the model results\n\nplot(lasso_ins)\n\n\n\n\nYou can also plot the CV-relevant outputs\n\nLassoCV &lt;- lasso_ins$glmnet.fit\n\nplot(LassoCV, label = TRUE, xvar = \"lambda\")\n\nabline(\n  v = log(c(lasso_ins$lambda.min, lasso_ins$lambda.1se))\n  ) # Adds lines to mark the two key lambda values\n\n\n\n\nYou can see the coefficients corresponding to the two key lambda values using the predict function\n\npredict(lasso_ins, type = \"coefficients\",\n        s = c(lasso_ins$lambda.min, lasso_ins$lambda.1se))\n\n86 x 2 sparse Matrix of class \"dgCMatrix\"\n                       s1          s2\n(Intercept)  0.9896239538 1.053595130\nMOSTYPE      .            .          \nMAANTHUI     .            .          \nMGEMOMV      .            .          \nMGEMLEEF     .            .          \nMOSHOOFD     .            .          \nMGODRK       .            .          \nMGODPR       .            .          \nMGODOV       .            .          \nMGODGE       .            .          \nMRELGE       0.0018180263 .          \nMRELSA       .            .          \nMRELOV       .            .          \nMFALLEEN     .            .          \nMFGEKIND     .            .          \nMFWEKIND     .            .          \nMOPLHOOG     0.0022611683 .          \nMOPLMIDD     .            .          \nMOPLLAAG    -0.0024753875 .          \nMBERHOOG     .            .          \nMBERZELF     .            .          \nMBERBOER    -0.0030373898 .          \nMBERMIDD     .            .          \nMBERARBG     .            .          \nMBERARBO     .            .          \nMSKA         .            .          \nMSKB1        .            .          \nMSKB2        .            .          \nMSKC         .            .          \nMSKD         .            .          \nMHHUUR      -0.0007565131 .          \nMHKOOP       .            .          \nMAUT1        0.0013643467 .          \nMAUT2        .            .          \nMAUT0        .            .          \nMZFONDS      .            .          \nMZPART       .            .          \nMINKM30      .            .          \nMINK3045     .            .          \nMINK4575     .            .          \nMINK7512     .            .          \nMINK123M     .            .          \nMINKGEM      0.0028225067 .          \nMKOOPKLA     0.0025557889 .          \nPWAPART      0.0076586029 .          \nPWABEDR      .            .          \nPWALAND     -0.0008184860 .          \nPPERSAUT     0.0087316235 0.002079863\nPBESAUT      .            .          \nPMOTSCO      .            .          \nPVRAAUT      .            .          \nPAANHANG     .            .          \nPTRACTOR     .            .          \nPWERKT       .            .          \nPBROM        .            .          \nPLEVEN       .            .          \nPPERSONG     .            .          \nPGEZONG      .            .          \nPWAOREG      0.0010472132 .          \nPBRAND       0.0042630439 .          \nPZEILPL      .            .          \nPPLEZIER     .            .          \nPFIETS       .            .          \nPINBOED      .            .          \nPBYSTAND     .            .          \nAWAPART      .            .          \nAWABEDR      .            .          \nAWALAND      .            .          \nAPERSAUT     0.0006293082 .          \nABESAUT      .            .          \nAMOTSCO      .            .          \nAVRAAUT      .            .          \nAAANHANG     .            .          \nATRACTOR     .            .          \nAWERKT       .            .          \nABROM        .            .          \nALEVEN       .            .          \nAPERSONG     .            .          \nAGEZONG      .            .          \nAWAOREG      .            .          \nABRAND       .            .          \nAZEILPL      .            .          \nAPLEZIER     0.2083116609 .          \nAFIETS       0.0076146528 .          \nAINBOED      .            .          \nABYSTAND     0.0361586998 .          \n\n\nQuestions you should be able to answer :\n\nWhich lambda in the sequence had the lowest CV error?\n\n\nlasso_ins$lambda.min\n\n[1] 0.006703696\n\n\n\nWhat is the CV error of the “best” lambda?\n\n\nlasso_ins$cvm[lasso_ins$lambda == lasso_ins$lambda.min]\n\n[1] 0.05384566\n\n\n\nWhat is the standard deviation of the CV error for the “best” lambda?\n\n\nlasso_ins$cvsd[lasso_ins$lambda == lasso_ins$lambda.min]\n\n[1] 0.002099205\n\n\n\nWhat is the largest lambda whose CV error was within 1 standard deviation of the lowest CV error?\n\n\nlasso_ins$lambda.1se\n\n[1] 0.02970154\n\n\nWhen you do ridge regression, the code is almost exactly the same as for lasso in R. You just need to change the alpha parameter from 1 to 0. I’ll leave this to you as an exercise."
  },
  {
    "objectID": "api 222 files/section 6/section 6.2.html#principal-components-analysis-pca-and-partial-least-squares-pls",
    "href": "api 222 files/section 6/section 6.2.html#principal-components-analysis-pca-and-partial-least-squares-pls",
    "title": "Section 6.2 - Ridge, Lasso, PCA/PCR, PLS Code",
    "section": "Principal Components Analysis (PCA) and Partial Least Squares (PLS)",
    "text": "Principal Components Analysis (PCA) and Partial Least Squares (PLS)\nWe will use the Wage data set that is included in the package ISLR. This data set contains information on people’s wages.\n\nwage_data &lt;- Wage\n\nLet’s learn a little more about the data\n\nsummary(wage_data)\n\nThe data set contains 3000 observations on 11 variables. The variables are:\n\nstr(wage_data)\n\n'data.frame':   3000 obs. of  11 variables:\n $ year      : int  2006 2004 2003 2003 2005 2008 2009 2008 2006 2004 ...\n $ age       : int  18 24 45 43 50 54 44 30 41 52 ...\n $ maritl    : Factor w/ 5 levels \"1. Never Married\",..: 1 1 2 2 4 2 2 1 1 2 ...\n $ race      : Factor w/ 4 levels \"1. White\",\"2. Black\",..: 1 1 1 3 1 1 4 3 2 1 ...\n $ education : Factor w/ 5 levels \"1. &lt; HS Grad\",..: 1 4 3 4 2 4 3 3 3 2 ...\n $ region    : Factor w/ 9 levels \"1. New England\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ jobclass  : Factor w/ 2 levels \"1. Industrial\",..: 1 2 1 2 2 2 1 2 2 2 ...\n $ health    : Factor w/ 2 levels \"1. &lt;=Good\",\"2. &gt;=Very Good\": 1 2 1 2 1 2 2 1 2 2 ...\n $ health_ins: Factor w/ 2 levels \"1. Yes\",\"2. No\": 2 2 1 1 1 1 1 1 1 1 ...\n $ logwage   : num  4.32 4.26 4.88 5.04 4.32 ...\n $ wage      : num  75 70.5 131 154.7 75 ...\n\n\nTo the view the levels of a particular factor variable, we can use:\n\nlevels(wage_data$maritl)\n\n[1] \"1. Never Married\" \"2. Married\"       \"3. Widowed\"       \"4. Divorced\"     \n[5] \"5. Separated\"    \n\nlevels(wage_data$region)\n\n[1] \"1. New England\"        \"2. Middle Atlantic\"    \"3. East North Central\"\n[4] \"4. West North Central\" \"5. South Atlantic\"     \"6. East South Central\"\n[7] \"7. West South Central\" \"8. Mountain\"           \"9. Pacific\"           \n\n\nNotice there is a variable called “wage” and a variable called “logwage”. We just need one of these two. Let’s drop “wage”.\n\nwage_data &lt;- wage_data[, -11]\n\nLooking at the data, we see there are integer, factor, and numeric variable types. Let’s convert everything to numeric variables, which includes converting factor variables to a series of indicators.\n\nfor(i in 10:1){\n  if(is.factor(wage_data[, i])){\n    for(j in unique(wage_data[, i])){\n      new_col &lt;- paste(colnames(wage_data)[i], j, sep = \"_\")\n      wage_data[, new_col] &lt;- as.numeric(wage_data[, i] == j) \n    }\n    wage_data &lt;- wage_data[, -i]     \n  } else if(typeof(wage_data[, i]) == \"integer\") {\n    wage_data[, i] &lt;- as.numeric(as.character(wage_data[, i]))\n  } \n}\n\nCheck your code worked\n\n#View(wage_data)\nsummary(wage_data)\n\n      year           age           logwage      health_ins_2. No\n Min.   :2003   Min.   :18.00   Min.   :3.000   Min.   :0.0000  \n 1st Qu.:2004   1st Qu.:33.75   1st Qu.:4.447   1st Qu.:0.0000  \n Median :2006   Median :42.00   Median :4.653   Median :0.0000  \n Mean   :2006   Mean   :42.41   Mean   :4.654   Mean   :0.3057  \n 3rd Qu.:2008   3rd Qu.:51.00   3rd Qu.:4.857   3rd Qu.:1.0000  \n Max.   :2009   Max.   :80.00   Max.   :5.763   Max.   :1.0000  \n health_ins_1. Yes health_1. &lt;=Good health_2. &gt;=Very Good\n Min.   :0.0000    Min.   :0.000    Min.   :0.000        \n 1st Qu.:0.0000    1st Qu.:0.000    1st Qu.:0.000        \n Median :1.0000    Median :0.000    Median :1.000        \n Mean   :0.6943    Mean   :0.286    Mean   :0.714        \n 3rd Qu.:1.0000    3rd Qu.:1.000    3rd Qu.:1.000        \n Max.   :1.0000    Max.   :1.000    Max.   :1.000        \n jobclass_1. Industrial jobclass_2. Information region_2. Middle Atlantic\n Min.   :0.0000         Min.   :0.0000          Min.   :1                \n 1st Qu.:0.0000         1st Qu.:0.0000          1st Qu.:1                \n Median :1.0000         Median :0.0000          Median :1                \n Mean   :0.5147         Mean   :0.4853          Mean   :1                \n 3rd Qu.:1.0000         3rd Qu.:1.0000          3rd Qu.:1                \n Max.   :1.0000         Max.   :1.0000          Max.   :1                \n education_1. &lt; HS Grad education_4. College Grad education_3. Some College\n Min.   :0.00000        Min.   :0.0000            Min.   :0.0000           \n 1st Qu.:0.00000        1st Qu.:0.0000            1st Qu.:0.0000           \n Median :0.00000        Median :0.0000            Median :0.0000           \n Mean   :0.08933        Mean   :0.2283            Mean   :0.2167           \n 3rd Qu.:0.00000        3rd Qu.:0.0000            3rd Qu.:0.0000           \n Max.   :1.00000        Max.   :1.0000            Max.   :1.0000           \n education_2. HS Grad education_5. Advanced Degree race_1. White   \n Min.   :0.0000       Min.   :0.000                Min.   :0.0000  \n 1st Qu.:0.0000       1st Qu.:0.000                1st Qu.:1.0000  \n Median :0.0000       Median :0.000                Median :1.0000  \n Mean   :0.3237       Mean   :0.142                Mean   :0.8267  \n 3rd Qu.:1.0000       3rd Qu.:0.000                3rd Qu.:1.0000  \n Max.   :1.0000       Max.   :1.000                Max.   :1.0000  \n race_3. Asian     race_4. Other     race_2. Black     maritl_1. Never Married\n Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.000          \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.000          \n Median :0.00000   Median :0.00000   Median :0.00000   Median :0.000          \n Mean   :0.06333   Mean   :0.01233   Mean   :0.09767   Mean   :0.216          \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.000          \n Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :1.000          \n maritl_2. Married maritl_4. Divorced maritl_3. Widowed  maritl_5. Separated\n Min.   :0.0000    Min.   :0.000      Min.   :0.000000   Min.   :0.00000    \n 1st Qu.:0.0000    1st Qu.:0.000      1st Qu.:0.000000   1st Qu.:0.00000    \n Median :1.0000    Median :0.000      Median :0.000000   Median :0.00000    \n Mean   :0.6913    Mean   :0.068      Mean   :0.006333   Mean   :0.01833    \n 3rd Qu.:1.0000    3rd Qu.:0.000      3rd Qu.:0.000000   3rd Qu.:0.00000    \n Max.   :1.0000    Max.   :1.000      Max.   :1.000000   Max.   :1.00000    \n\nstr(wage_data)\n\n'data.frame':   3000 obs. of  24 variables:\n $ year                        : num  2006 2004 2003 2003 2005 ...\n $ age                         : num  18 24 45 43 50 54 44 30 41 52 ...\n $ logwage                     : num  4.32 4.26 4.88 5.04 4.32 ...\n $ health_ins_2. No            : num  1 1 0 0 0 0 0 0 0 0 ...\n $ health_ins_1. Yes           : num  0 0 1 1 1 1 1 1 1 1 ...\n $ health_1. &lt;=Good            : num  1 0 1 0 1 0 0 1 0 0 ...\n $ health_2. &gt;=Very Good       : num  0 1 0 1 0 1 1 0 1 1 ...\n $ jobclass_1. Industrial      : num  1 0 1 0 0 0 1 0 0 0 ...\n $ jobclass_2. Information     : num  0 1 0 1 1 1 0 1 1 1 ...\n $ region_2. Middle Atlantic   : num  1 1 1 1 1 1 1 1 1 1 ...\n $ education_1. &lt; HS Grad      : num  1 0 0 0 0 0 0 0 0 0 ...\n $ education_4. College Grad   : num  0 1 0 1 0 1 0 0 0 0 ...\n $ education_3. Some College   : num  0 0 1 0 0 0 1 1 1 0 ...\n $ education_2. HS Grad        : num  0 0 0 0 1 0 0 0 0 1 ...\n $ education_5. Advanced Degree: num  0 0 0 0 0 0 0 0 0 0 ...\n $ race_1. White               : num  1 1 1 0 1 1 0 0 0 1 ...\n $ race_3. Asian               : num  0 0 0 1 0 0 0 1 0 0 ...\n $ race_4. Other               : num  0 0 0 0 0 0 1 0 0 0 ...\n $ race_2. Black               : num  0 0 0 0 0 0 0 0 1 0 ...\n $ maritl_1. Never Married     : num  1 1 0 0 0 0 0 1 1 0 ...\n $ maritl_2. Married           : num  0 0 1 1 0 1 1 0 0 1 ...\n $ maritl_4. Divorced          : num  0 0 0 0 1 0 0 0 0 0 ...\n $ maritl_3. Widowed           : num  0 0 0 0 0 0 0 0 0 0 ...\n $ maritl_5. Separated         : num  0 0 0 0 0 0 0 0 0 0 ...\n\n\nLet’s split our data into a training and a test set\n\nset.seed(222)\ntrain &lt;- sample(seq(nrow(wage_data)),\n                floor(nrow(wage_data)*0.8))\n\ntrain &lt;- sort(train)\n\ntest &lt;- which(!(seq(nrow(wage_data)) %in% train))\n\nWe are interested in predicting log wage. First, we will use principle components regression. Principle components regression does a linear regression but instead of using the X-variables as predictors, it uses principle components as predictors. The optimal number of principle components to use for PCR is usually found through cross-validation. To run PCR, we will use the package pls.\n\nlibrary(pls)\n\n## Try running PCR\npcr_fit  &lt;- pcr(logwage ~ ., data = wage_data[train,],          \n                scale = TRUE, validation = \"CV\")\n\nError in La.svd(X) : infinite or missing values in 'x'\nSometime you can get an error message. This error is because some of our variables have almost zero variance. Usually, variables with near-zero variance are indicator variables we generated for a rare event. Think about what happens to these predictors when the data are split into cross-validation/bootstrap sub-samples: if a few uncommon unique values are removed from one sample, the predictors could become zero-variance predictors which would cause many models to not run. We can figure out which variables have such low variance to determine how we want to handle them. The simplest approach to identify them is to set a manual threshold (which can be adjusted: 0.05 is a common choice). Our options are then to drop them from the analysis or not to scale the data.\n\n## to drop them from the analysis or not to scale the data. \nfor(col_num in 1:ncol(wage_data)){\n  if(var(wage_data[, col_num]) &lt; 0.05){\n    print(colnames(wage_data)[col_num])\n    print(var(wage_data[, col_num]))\n  }\n}\n\n[1] \"region_2. Middle Atlantic\"\n[1] 0\n[1] \"race_4. Other\"\n[1] 0.01218528\n[1] \"maritl_3. Widowed\"\n[1] 0.006295321\n[1] \"maritl_5. Separated\"\n[1] 0.01800322\n\n## Let's drop these low variance columns\nfor(col_num in ncol(wage_data):1){\n  if(var(wage_data[, col_num]) &lt; 0.05) {\n    wage_data &lt;- wage_data[, -col_num]\n  }\n}\n\nWe can now try again to run PCR\n\nset.seed(222)\n\npcr_fit &lt;- pcr(logwage ~ ., data = wage_data[train,], \n               scale = TRUE, validation = \"CV\")\n\nsummary(pcr_fit)\n\nData:   X dimension: 2400 19 \n    Y dimension: 2400 1\nFit method: svdpc\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV          0.3484   0.2979   0.2936   0.2926   0.2926   0.2921   0.2911\nadjCV       0.3484   0.2978   0.2936   0.2925   0.2925   0.2920   0.2909\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV      0.2913   0.2904   0.2898    0.2874    0.2863    0.2794    0.2794\nadjCV   0.2912   0.2901   0.2898    0.2873    0.2862    0.2793    0.2793\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV       0.2794    0.2795    0.2795    0.2798    0.2798    0.2800\nadjCV    0.2793    0.2794    0.2794    0.2796    0.2796    0.2797\n\nTRAINING: % variance explained\n         1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX          14.50    25.94    36.75    46.22    54.83    61.89    68.87    75.11\nlogwage    27.05    29.19    29.67    29.73    30.11    30.62    30.62    31.24\n         9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX          81.28     86.91     92.14     96.43     99.52     99.79    100.00\nlogwage    31.35     32.46     33.11     36.40     36.45     36.45     36.47\n         16 comps  17 comps  18 comps  19 comps\nX          100.00    100.00    100.00    100.00\nlogwage     36.47     36.48     36.48     36.49\n\n\nWe are interested in finding which number of principle components should be included in the regression to lead to the lowest cross-validation error.\n\npcr_msep &lt;- MSEP(pcr_fit)\npcr_min_indx &lt;- which.min(pcr_msep$val[1, 1,])\nprint(pcr_min_indx)\n\n12 comps \n      13 \n\n\nHow could you get the RMSEP?\n\nprint(pcr_msep$val[1, 1, pcr_min_indx])\n\n[1] 0.07803666\n\n\nIt can also be helpful to plot the RMSEP as a function of the number of components. The black line is the CV, the red dashed line is the adjusted CV.\n\nvalidationplot(pcr_fit)\n\n\n\n\nWhy does the plot look the way it does? Do you expect the PLS plot to look similar? Why or why not?\nLet’s predict logwage for our test observations\n\npcr_pred &lt;- predict(pcr_fit, wage_data[test, ], ncomp = 12)\n\nWe can measure test MSE\n\npcr_test_MSE &lt;- mean((pcr_pred - wage_data[test, \"logwage\"])^2)\nprint(pcr_test_MSE)\n\n[1] 0.07887506\n\n\nWe can convert this to RMSE\n\nprint(sqrt(pcr_test_MSE))\n\n[1] 0.280847\n\n\nLet’s repeat this exercise for PLS. Use plsr() instead of pcr().\n\n## Step 1: Fit the model\npls_fit &lt;- plsr(logwage ~ ., data = wage_data[train, ], \n                scale = TRUE, validation = \"CV\")\nsummary(pls_fit)\n\nData:   X dimension: 2400 19 \n    Y dimension: 2400 1\nFit method: kernelpls\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV          0.3484   0.2843   0.2795   0.2789   0.2789   0.2789   0.2790\nadjCV       0.3484   0.2843   0.2794   0.2789   0.2788   0.2788   0.2789\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV      0.2791   0.2791   0.2791    0.2791    0.2791    0.2791    0.2791\nadjCV   0.2790   0.2790   0.2790    0.2790    0.2790    0.2790    0.2790\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV       0.2791    0.2791    0.2791    0.2791    0.2791    0.2791\nadjCV    0.2790    0.2790    0.2795    0.2795    0.2795    0.2795\n\nTRAINING: % variance explained\n         1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX          14.05    20.93    30.07    37.01    44.43    50.26    55.58    60.76\nlogwage    33.67    36.19    36.43    36.45    36.45    36.46    36.47    36.47\n         9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX          66.04     73.72     80.25     81.54     86.85     93.02    100.00\nlogwage    36.47     36.47     36.47     36.47     36.47     36.47     36.47\n         16 comps  17 comps  18 comps  19 comps\nX          101.29     102.6    103.92     105.3\nlogwage     36.21      36.2     36.21      36.2\n\n## Step 2: Which ncomp value had the lowest CV MSE?\npls_msep &lt;- MSEP(pls_fit)\npls_min_indx &lt;- which.min(pls_msep$val[1, 1,])\nprint(pls_min_indx)\n\n4 comps \n      5 \n\n## Step 3: Plot validation error as a function of # of components\nvalidationplot(pls_fit, val.type = c(\"RMSEP\"))\n\n\n\n## Step 4: Identify the CV RMSE for the number of components with\n## the lowest CV RMSE\npls_rmsep &lt;- RMSEP(pls_fit)\nprint(pls_rmsep$val[1, 1, as.numeric(pls_min_indx)])\n\n[1] 0.2788531\n\n## Step 5: Predict test set logwage values\npls_pred &lt;- predict(pls_fit, wage_data[test,],\n                    ncomp = (as.numeric(pls_min_indx) -1))\n\n## Step 6: Measure test MSE and RMSE\npls_test_MSE &lt;- mean((pls_pred - wage_data[test, \"logwage\"])^2)\nprint(pls_test_MSE)\n\n[1] 0.07881288\n\nprint(sqrt(pls_test_MSE))\n\n[1] 0.2807363"
  },
  {
    "objectID": "api 222 files/section 7/section 7.1.html",
    "href": "api 222 files/section 7/section 7.1.html",
    "title": "Section 7.1 -",
    "section": "",
    "text": "Note that the material in these notes draws on past TF’s notes (Ibou Dieye, Laura Morris, Emily Mower, Amy Wickett), and the more thorough treatment of these topics in Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.\nThe goal of this session is to review concepts related to regularization methods (LASSO and Ridge Regression) and dimension reduction techniques (PCA and PLS)."
  },
  {
    "objectID": "api 222 files/section 7/section 7.1.html#lasso-and-ridge-regression",
    "href": "api 222 files/section 7/section 7.1.html#lasso-and-ridge-regression",
    "title": "Section 7.1 -",
    "section": "LASSO and Ridge Regression",
    "text": "LASSO and Ridge Regression\n\nConcept\nLeast Absolute Shrinkage and Selection Operator (LASSO) and Ridge Regression both fall under a broader class of models called shrinkage models. Shrinkage models regularize or penalize coefficient estimates, which means that they shrink the coefficients toward zero. Shrinking the coefficient estimates can reduce their variance, so these methods are particularly useful for models where we are concerned about high variance (i.e. over-fitting), such as models with a large number of predictors relative to the number of observations.\nBoth LASSO and Ridge regression operate by adding a penalty to the normal OLS (Ordinary Least Squares) minimization problem. These penalties can be thought of as a budget, and sometimes the minimization problem is explicitly formulated as minimizing the Residual Sum of Squares (RSS) subject to a budget constraint. The idea of the budget is if you have a small budget, you can only afford a little “total” \\(\\beta\\), where the definition of “total” \\(\\beta\\) varies between LASSO and Ridge, but as your budget increases, you get closer and closer to the OLS \\(\\beta\\)s.\nLASSO and Ridge regression differ in exactly how they penalize the coefficients. In particular, LASSO penalizes the coefficients using an \\(L_1\\) penalty:\n\\[\n\\sum_{i=1}^n \\left(\n    y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}\n    \\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| = RSS + \\lambda \\sum_{j=1}^p |\\beta_j|\n\\]\nRidge regression penalizes the coefficients using an \\(L_2\\) penalty:\n\\[\n    \\sum_{i=1}^n \\left(\n    y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}\n    \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 = RSS + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\] You may notice that both the LASSO and Ridge Regression minimization problems feature a parameter \\(\\lambda\\). This is a tuning parameter that dictates how much we will penalize the “total” coefficients. Tuning parameters appear in many models and get their name from the fact that we tune (adjust / choose) them in order to improve our model’s performance.\nSince LASSO and Ridge Regression are used when we are concerned about variance (over-fitting), it should come as no surprise that increasing \\(\\lambda\\) decreases variance. It can be helpful to contextualize the \\(\\lambda\\) size by realizing that \\(\\lambda=0\\) is OLS and \\(\\lambda =\\infty\\) results in only \\(\\beta_0\\) being assigned a non-zero value.\nAs we increase \\(\\lambda\\) from zero, we decrease variance but increase bias (the classic bias-variance tradeoff). To determine the best \\(\\lambda\\) (e.g. the \\(\\lambda\\) that will lead to the best out of sample predictive performance), it is common to use cross validation. A good function in R that trains LASSO and Ridge Regression models and uses cross-validation to select a good \\(\\lambda\\) is cv.glmnet(), which is part of the glmnet package.\n\n\nKey difference between LASSO and Ridge\nAs noted above, both LASSO and Ridge regression shrink the coefficients toward zero. However, the penalty in Ridge (\\(\\lambda \\beta_j^2\\)) will not set any of coefficients exactly to zero (unless \\(\\lambda =\\infty\\)). Increasing the value of \\(\\lambda\\) will tend to reduce the magnitudes of the coefficients (which helps reduce variance), but will not result in exclusion of any of the variables. While this may not impact prediction accuracy, it can create a challenge in model interpretation in settings where the number of features is large. This is an obvious disadvantage.\nOn the other hand, the \\(L_1\\) penalty of LASSO forces some of the coefficient estimates to be exactly equal to zero when the tuning parameter \\(\\lambda\\) is sufficiently large. Therefore, LASSO performs variable selection, much like best subset selection.\n\n\nImplementation and Considerations\nLASSO and Ridge Regression are useful models to use when dealing with a large number of predictors \\(p\\) relative to the number of observations \\(n\\). It is good to standardize your features so that coefficients are not selected because of their scale rather than their relative importance.\nFor example, suppose you were predicting salary, and suppose you had a standardized test score that was highly predictive of salary and you also had parents’ income, which was only somewhat predictive of salary. Since standardized test scores are measured on a much smaller scale than the outcome and than parents’ income, we would expect the coefficient on test scores to be large relative to the coefficient on parents’ income. This means it would likely be shrunk by adding the penalty, even though it reflects a strong predictive relationship.\n\n\nInterpreting Coefficients\nCoefficients produced by LASSO and Ridge Regression should not be interpreted causally. These methods are used for prediction, and as such, our focus is on \\(\\hat{y}\\). This is in contrast to an inference problem, where we would be interested in \\(\\hat{\\beta}\\).\nThe intuition behind why we cannot interpret the coefficients causally is similar to the intuition underlying Omitted Variables Bias (OVB). In OVB, we said that if two variables \\(X_1\\) and \\(X_2\\) were correlated with each other and with the outcome \\(Y\\), then the coefficient on \\(X_1\\) in a regression of \\(Y\\) on \\(X_1\\) would differ from the coefficient on \\(X_1\\) in a regression where \\(X_2\\) was also included. This is because since \\(X_1\\) and \\(X_2\\) are correlated, when we omit \\(X_2\\), the coefficient on \\(X_1\\) picks up the effect of \\(X_1\\) on \\(Y\\) as well as some of the effect of \\(X_2\\) on \\(Y\\)."
  },
  {
    "objectID": "api 222 files/section 7/section 7.1.html#principal-components-analysis-and-regression",
    "href": "api 222 files/section 7/section 7.1.html#principal-components-analysis-and-regression",
    "title": "Section 7.1 -",
    "section": "Principal Components Analysis and Regression",
    "text": "Principal Components Analysis and Regression\n\nPrincipal Components Analysis\nPrincipal Components Analysis is a method of unsupervised learning. We have not yet covered unsupervised learning, though we will in the second half of the semester. The important thing to know at this point about unsupervised learning is that unsupervised learning methods do not use labels \\(Y\\).\nPrincipal Components Analysis (PCA), therefore, does not use \\(Y\\) in determining the principal components. Instead, the principal components are determined solely by looking at the predictors \\(X_1\\),…,\\(X_p\\). Each principal component is a weighted sum of the original predictors \\(X_1\\),…,\\(X_p\\). For clarity of notation, we will use \\(Z\\) to represent principal components and \\(X\\) to represent predictors.\nPrincipal components are created in an ordered way. The order can be described as follows: If you had to project all of the data onto only one line, the first principal component defines the line that would lead to points being as spread out as possible (e.g. having highest variance). If you had to project all of the data onto only one plane, the plane defined by the first two principal components (which are orthogonal by definition) would be the plane where the points were as spread out as possible (e.g. highest possible variance).\nSince variance is greatly affected by measurement scale, it is common practice and a good idea to scale your variables, so that results are not driven by the scale on which the variables were measured.\nWhen estimating principal components, you will estimate \\(p\\) components, where \\(p\\) is the number of predictors \\(X\\). However, you usually only pay attention to / use the first few components, since the last few components capture very little variance. Note that you can exactly recover your original data when using all \\(p\\) components.\n\n\nPrincipal Components Regression\nPrincipal Components Regression (PCR) regresses the outcome \\(Y\\) on the principal components \\(Z_1\\),…,\\(Z_M\\), where \\(M&lt;p\\) and \\(p\\) is the number of predictors \\(X_1\\),…,\\(X_p\\).\nNote that the principal components \\(Z_1\\),…,\\(Z_M\\) were defined by looking only at \\(X_1\\),…,\\(X_p\\) and not at \\(Y\\). Putting the principal components into a regression is the first time we are introducing any interaction between the principal components and the outcome \\(Y\\).\nThe main idea of Principal Components Regression is that hopefully only a few components explain most of the variance in the predictors overall and as is relevant to the relationship between the predictors and the response. In other words, when we use PCR, we assume that the directions in which \\(X\\) shows the most variance are also the directions most associated with \\(Y\\). When this assumption is true, we are able to use \\(M&lt;&lt;p\\) (e.g. \\(M\\) much smaller than \\(p\\)) parameters while still getting similar in-sample performance and hopefully better out-of-sample performance (due to not overfitting) to a regression of \\(Y\\) on all \\(p\\) predictors. Although the assumption is not guaranteed to be true, it is often a good enough approximation to give good results.\nPCA is one example of dimensionality reduction, because it reduces the dimension of the problem from \\(p\\) to \\(M\\). Note, though, that dimensionality reduction is different from feature selection. We are still using all features; we have just aggregated them into principal components.\nThe exact number \\(M\\) of principal components to use in PCR (the regression of \\(Y\\) on the principal components) is usually determined by cross-validation.\n\n\nImplementation and Considerations\nWhen using PCR, there are a few things to pay attention to. First, be sure to standardize variables or else the first few principal components will favor features measured on larger scales. Second, the number of principal components to use in PCR is determined using cross-validation. If the number is high and close to the number of features in your data, the assumption that the directions in which the predictors vary most are also the directions that explain the relationship between the predictors and response is false. In such a case, other methods, such as ridge and lasso, are likely to perform better."
  },
  {
    "objectID": "api 222 files/section 7/section 7.1.html#partial-least-squares",
    "href": "api 222 files/section 7/section 7.1.html#partial-least-squares",
    "title": "Section 7.1 -",
    "section": "Partial Least Squares",
    "text": "Partial Least Squares\nPartial Least Squares is a supervised alternative to PCR. Recall that for PCR, the principal components \\(Z_1,...,Z_M\\) were formed from the original features \\(X_1,...,X_p\\) without looking at \\(Y\\) (unsupervised).\nPartial Least Squares (PLS) also generates a new set of features \\(Z_1,...,Z_M\\) but it uses the correlation between the predictors \\(X_1,...,X_p\\) and the outcome \\(Y\\) to determine the weights on \\(X_1,...,X_p\\) for each \\(Z_1,...,Z_M\\). In this way, PLS attempts to find directions that explain both the response and the predictors.\nThe first feature \\(Z_1\\) is determined by weighting \\(X_1,...,X_p\\) proportional to each feature’s correlation with \\(Y\\). It then residualizes the predictors \\(X_1,...,X_p\\) by regressing them on \\(Z_1\\) and repeats the weighting procedure using the orthogonalized predictors. The process then repeats until we have \\(M\\) components, where \\(M\\) is determined by cross validation.\n\nImplementation and Considerations\nJust as with PCR, it’s best practice to scale the predictors before running PLS.\n\n\nComparison with PCR\nPLS directly uses \\(Y\\) in generating the features \\(Z_1,...,Z_M\\). It then regresses \\(Y\\) on these features. Therefore, PLS uses \\(Y\\) twice in the process of estimating the model.\nIn contrast, PCR only looks at the outcome \\(Y\\) when estimating the final regression. The components are estimated without ``peeking’’ at \\(Y\\).\nBecause \\(Y\\) is used twice in PLS and only once in PCR, in practice PLS exhibits lower bias (e.g. is better able to fit the training data) but higher variance (e.g. is more sensitive to the exact training data). Therefore, the two methods generally have similar out-of-sample predictive power in practice."
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html",
    "title": "Demystifying KNN",
    "section": "",
    "text": "In the realm of machine learning, few algorithms are as intuitively appealing as the K-Nearest Neighbors (KNN). It’s a method that echoes the human instinct to classify based on similarity and proximity, offering a gateway into the world of pattern recognition and predictive analytics."
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html#the-mathematical-compass",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html#the-mathematical-compass",
    "title": "Demystifying KNN",
    "section": "The Mathematical Compass",
    "text": "The Mathematical Compass\nBefore we dive into the practical applications, let’s lay the mathematical groundwork that underpins these techniques—a foundation as crucial to understanding their functionality as a compass is to navigation.\n\nK-Nearest Neighbors (KNN): A Non-Parametric Approach\nKNN’s beauty lies in its simplicity and the intuitive concept of “neighborliness.” It posits that data points with similar characteristics (features) are likely to share the same outcome. Whether we’re classifying flowers based on petal sizes or predicting housing prices from neighborhood characteristics, KNN asks, “Who are your nearest neighbors?”\n\nMathematical Intuition:\nFor a given data point, KNN looks at the ‘K’ closest points (neighbors) and makes a prediction based on their majority class (classification) or average value (regression). The distance between points—Euclidean, Manhattan, or any other metric—serves as the basis for determining “closeness.”\nTo elaborate on this with mathematical expressions and to visualize it within a coordinate plane, let’s dive deeper:\n\nDistance Metrics\nThe most common distance metric used in KNN is the Euclidean distance, which in a two-dimensional space can be expressed as:\n\\[d(p, q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}\\]\nwhere \\(p = (p_1, p_2)\\) and \\(q = (q_1, q_2)\\) are two points in the Euclidean plane.\nFor higher dimensions, the formula generalizes to:\n\\[d(p, q) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}\\]\nwhere \\(n\\) is the number of dimensions (features) and \\(p_i\\), \\(q_i\\) are the coordinates of \\(p\\) and \\(q\\) in each dimension.\n\n\nKNN Algorithm\n\nCompute Distance: Calculate the distance between the query instance and all the training samples.\nSort Distances: Order the training samples by their distance from the query instance.\nSelect K Nearest Neighbors: Identify the top \\(K\\) closest training samples.\nMajority Vote or Average: For classification, the predicted class is the most common class among the \\(K\\) nearest neighbors. For regression, it is the average of the values.\n\n\n\nDecision Boundary Visualization\nIn a 2D coordinate plane, imagine plotting various data points, each belonging to one of two classes. The decision boundary that KNN creates is not linear but forms curves that encircle clusters of points belonging to the same class. This can be visualized as follows:\n\nCreate a dense grid of points covering the entire plane.\nUse KNN to classify each point on the grid.\nColor the points differently based on the predicted class, revealing the decision boundary.\n\nThis boundary demarcates the regions of the plane where a query point would be classified as one class or the other. It’s worth noting that the shape of the boundary depends on \\(K\\) and the distance metric used.\n\n\nK Selection\nChoosing the right \\(K\\) is critical for the model’s performance. Too small a \\(K\\) leads to a highly complex model that may overfit, capturing noise in the training data. Conversely, too large a \\(K\\) simplifies the model excessively, potentially underfitting and missing key patterns.\n\\[K_{optimal} = \\text{argmin}_K (\\text{Error}(K))\\]\nThe optimal \\(K\\) minimizes the prediction error, which can be determined through cross-validation.\nBy understanding the mathematical foundations of KNN and visualizing its application in a coordinate plane, we can better appreciate its flexibility and the importance of distance metrics and \\(K\\) selection in shaping the decision boundaries."
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html#exploring-the-iris-dataset",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html#exploring-the-iris-dataset",
    "title": "Demystifying KNN",
    "section": "Exploring the Iris Dataset",
    "text": "Exploring the Iris Dataset\nThe Iris dataset is a classic in machine learning and statistics, known for its simplicity and utility in demonstrating basic principles of classification. It consists of 150 observations of iris flowers, divided into three species: Setosa, Versicolor, and Virginica. Each observation includes four features: sepal length, sepal width, petal length, and petal width, all measured in centimeters.\n\nSummary Statistics and Visualizations\nLet’s start by exploring the dataset with some summary statistics and visualizations:\n\nlibrary(ggplot2)\n\niris.data &lt;- iris \n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# Scatter plot visualizing petal width and length grouped by species\nscatter &lt;- ggplot(iris.data, aes(x = Petal.Width, y = Petal.Length, color = Species)) +\n  geom_point(size = 3, alpha = 0.6) +\n  theme_classic() +\n  theme(legend.position = \"right\") +\n  ggtitle(\"Scatter Plot of Petal Dimensions by Species\")\n\nprint(scatter)\n\n\n\n# Boxplot visualizing variation in petal width between species\nboxplot &lt;- ggplot(iris.data, aes(x = Species, y = Petal.Width, fill = Species)) +\n  geom_boxplot() +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  ggtitle(\"Boxplot of Petal Width by Species\")\n\nprint(boxplot)"
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html#a-quick-note-on-training-and-testing-data",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html#a-quick-note-on-training-and-testing-data",
    "title": "Demystifying KNN",
    "section": "A Quick Note on Training and Testing Data",
    "text": "A Quick Note on Training and Testing Data\nOne of the pivotal steps in the machine learning workflow is the division of your dataset into training and testing sets. This practice is not just routine but foundational, ensuring that we evaluate our models accurately and fairly. But why do we take this step, and what does it achieve?\n\nCrafting and Validating Predictive Models\nThe essence of machine learning lies in learning from data and making predictions. When we train a model, we are essentially ‘teaching’ it to recognize patterns and make decisions based on historical data. However, the true test of a model’s mettle is not how well it memorizes the training data, but how effectively it can apply its learned knowledge to new, unseen data. This is where the concept of generalization comes into play.\n\n\nWhy Not Learn from All the Data?\nA natural question arises: if our goal is to make the best possible predictions, why not train our model on the entire dataset? The answer lies in the risk of overfitting. An overfitted model is akin to a student who memorizes facts for an exam rather than understanding the underlying concepts. Just as the student might struggle to apply their knowledge in real-world situations, an overfitted model performs well on its training data but poorly on any new data.\n\n\nTraining Set: The Learning Phase\nThe training set serves as the educational cornerstone for our model. It’s the data on which the model trains, learns patterns, and adjusts its parameters. For KNN, this involves storing the features and labels of the training examples to later find the nearest neighbors of unseen instances.\n\n\nTesting Set: The Examination Phase\nAfter training, we introduce the model to the testing set, a separate portion of the data withheld from the training phase. This step is the model’s exam—it’s where we assess its ability to generalize the patterns it learned during training to new examples. The performance on the testing set gives us a realistic estimate of how the model is expected to perform in real-world scenarios.\n\n\nThe Significance of the Split\nSplitting data into training and testing sets is a critical step that balances the need for a model to learn effectively and the necessity of evaluating its predictive power honestly. By adhering to this practice, we ensure that our models are tested in a manner that mimics their eventual use on new, unseen data, providing a reliable measure of their performance and generalization capability.\nEnd of note :)"
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html#splitting-the-data-and-visualizing-decision-boundaries-for-k3",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html#splitting-the-data-and-visualizing-decision-boundaries-for-k3",
    "title": "Demystifying KNN",
    "section": "Splitting the Data and Visualizing Decision Boundaries for k=3",
    "text": "Splitting the Data and Visualizing Decision Boundaries for k=3\nNow, let’s split the dataset into training and testing sets, apply KNN classification with k=3, and visualize the decision boundary and compute the error rate. Why are we choosing k=3? It’s a common starting point, and we’ll explore the impact of different k values in the next section. We are going to work with a simplified version of the iris dataset using only three features: Sepal.Length, Sepal.Width, and Species.\n\n# Load necessary libraries\nlibrary(class)\nlibrary(dplyr)\n\nset.seed(513) # For reproducibility\n\n# Select features and species for simplicity\niris_simplified &lt;- iris %&gt;%\n  select(Sepal.Length, Sepal.Width, Species)\n\n# Split data into training and test sets\nsample_size &lt;- nrow(iris_simplified) * 0.7 # 70% for training\ntraining_indices &lt;- sample(1:nrow(iris_simplified), sample_size)\n\ntraining_data &lt;- iris_simplified[training_indices, ]\ntest_data &lt;- iris_simplified[-training_indices, ]\n\n# Perform KNN classification with k = 3\nknn_result &lt;- knn(train = training_data[, 1:2],\n                  test = test_data[, 1:2],\n                  cl = training_data[, 3],\n                  k = 3)\n\n# Add the predictions to the test_data dataframe for plotting\ntest_data$PredictedSpecies &lt;- as.factor(knn_result)\n\n# Visualize the results\nggplot(data = test_data, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(aes(color = Species, shape = PredictedSpecies), size = 3) +\n  scale_shape_manual(values = c(15, 17, 18)) + # Different shapes for actual vs. predicted\n  scale_color_manual(values = c('red', 'blue', 'green')) +\n  labs(title = \"KNN Classification of Iris Species\",\n       x = \"Sepal Length\",\n       y = \"Sepal Width\",\n       color = \"Actual Species\",\n       shape = \"Predicted Species\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html#fine-tuning-our-approach",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html#fine-tuning-our-approach",
    "title": "Demystifying KNN",
    "section": "Fine-tuning Our Approach",
    "text": "Fine-tuning Our Approach\nHaving visualized how K-Nearest Neighbors (KNN) operates with (k = 3), we’ve seen firsthand the impact of the choice of (k) on our model’s decision boundaries and, consequently, its predictions. But this naturally leads us to a pivotal question: Is (k = 3) truly the best choice for our Iris classification task? Or, more broadly, how do we pinpoint the most suitable number of neighbors for any given problem?"
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html#the-significance-of-k",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html#the-significance-of-k",
    "title": "Demystifying KNN",
    "section": "The Significance of (k)",
    "text": "The Significance of (k)\nThe parameter (k) in KNN serves as a tuning knob, adjusting the balance between the simplicity and complexity of the model. A smaller (k) makes the model more sensitive to noise in the data, potentially leading to overfitting. Conversely, a larger (k) smoothens the decision boundaries, which might simplify the model to the point of underfitting. Thus, finding the optimal (k) is crucial for achieving the best model performance."
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html#search-for-optimal-k",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html#search-for-optimal-k",
    "title": "Demystifying KNN",
    "section": "Search for Optimal (k)",
    "text": "Search for Optimal (k)\nTo embark on this quest, we employ a systematic approach: evaluating the model’s performance across a range of (k) values and selecting the one that minimizes error. Specifically, we compute the error rates for (k = 1) through (k = 10) on our Iris dataset. The error rate here is defined as the proportion of incorrect predictions out of all predictions made by the model on the test set.\n\nThe Process Unfolded\n\nIterate Over (k): We loop through each (k) value from 1 to 10, applying the KNN model to our training data and making predictions on our test data at each iteration.\nCompute Error Rates: For each (k), we calculate the error rate by comparing the predicted species against the actual species in our test set.\nTabulate Results: We record the (k) values and their corresponding error rates in a table, allowing us to clearly visualize how the error rate varies with (k).\nSelect the Optimal (k): The optimal (k) is the one with the lowest error rate, striking the perfect balance between overfitting and underfitting for our dataset.\n\n\n\nWhat This Means for Our Iris Classification Task\nBy undertaking this analysis, we ensure that our choice of (k) is not arbitrary but is instead data-driven and optimized for performance. This methodical approach not only enhances the accuracy of our KNN model on the Iris dataset but also exemplifies a best practice in machine learning that can be applied to various classification tasks.\n\n# Initialize an empty data frame to store k values and theirerror rates\nerror_rates &lt;- data.frame(k = integer(), error_rate = numeric())\n\n# Loop through k values from 1 to 10\nfor (k in 1:10) {\n  # Apply KNN model\n  predicted_species &lt;- knn(train = training_data[, 1:2],\n                           test = test_data[, 1:2],\n                           cl = training_data[, 3],\n                           k = k)\n\n  # Compute the error rate\n  error_rate &lt;- sum(predicted_species != test_data[, 3]) / nrow(test_data)\n  \n  # Add the results to the error_rates data frame\n  error_rates &lt;- rbind(error_rates, data.frame(k = k, error_rate = error_rate))\n}\n\n\nknitr::kable(error_rates, caption = \"Error Rates for K=1 to 10\")\n\n\nError Rates for K=1 to 10\n\n\nk\nerror_rate\n\n\n\n\n1\n0.2888889\n\n\n2\n0.3111111\n\n\n3\n0.2444444\n\n\n4\n0.2444444\n\n\n5\n0.1777778\n\n\n6\n0.2000000\n\n\n7\n0.1777778\n\n\n8\n0.2222222\n\n\n9\n0.2000000\n\n\n10\n0.1777778\n\n\n\n\n\nIt looks like the error rate is lowest for (k = 5), which means that the KNN model with (k = 5) yields the most accurate predictions on the Iris dataset.\nLet’s visualize our ned results and see how the decision boundaries look for (k = 5).\n\n# Plot the decision boundaries by coloring the grid\nx_range &lt;- seq(from = min(iris$Sepal.Length) - 0.5, \n               to = max(iris$Sepal.Length) + 0.5, by = 0.01)\n\ny_range &lt;- seq(from = min(iris$Sepal.Width) - 0.5, \n               to = max(iris$Sepal.Width) + 0.5, by = 0.01)\n\ngrid &lt;- expand.grid(Sepal.Length = x_range, Sepal.Width = y_range)\n\n# Predict species for each point in the grid\ngrid$Species &lt;- knn(train = training_data[, 1:2],\n                    test = grid,\n                    cl = training_data[, 3],\n                    k = 5)\n\n# Convert grid predictions into a factor for coloring\ngrid$Species &lt;- as.factor(grid$Species)\n\nggplot() +\n  geom_tile(data = grid, aes(x = Sepal.Length, y = Sepal.Width, fill = Species), alpha = 0.5) +\n  geom_point(data = test_data, aes(x = Sepal.Length, y = Sepal.Width, color = Species), size = 2) +\n  scale_fill_manual(values = c('setosa' = 'red', 'versicolor' = 'blue', 'virginica' = 'green')) +\n  scale_color_manual(values = c('setosa' = 'red', 'versicolor' = 'blue', 'virginica' = 'green')) +\n  labs(title = \"KNN Decision Boundaries with Test Data\",\n       x = \"Sepal Length\",\n       y = \"Sepal Width\",\n       fill = \"Predicted Species\",\n       color = \"Actual Species\") +\n  theme_bw()"
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html#conclusion",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html#conclusion",
    "title": "Demystifying KNN",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we have explored the K-Nearest Neighbors (KNN) algorithm and its application to the Iris dataset. We have seen how KNN operates by classifying new data points based on their similarity to existing data points. We have also visualized the decision boundaries of the KNN model and observed how the choice of (k) impacts the model’s predictions.\nFurthermore, we have demonstrated the significance of selecting the optimal (k) value for our KNN model. By systematically evaluating the model’s performance across a range of (k) values, we have identified the most suitable (k) for our Iris classification task. This approach ensures that our choice of (k) is not arbitrary but is instead data-driven and optimized for performance."
  },
  {
    "objectID": "posts/2024-02-15-LDA/2024-02-15-LDA.html",
    "href": "posts/2024-02-15-LDA/2024-02-15-LDA.html",
    "title": "What is Linear Discriminant Analysis (LDA)?",
    "section": "",
    "text": "Linear Discriminant Analysis (LDA) is a classic method in statistics and machine learning for classification and dimensionality reduction. LDA is particularly known for its simplicity, efficiency, and interpretability. This post aims to demystify LDA by exploring its mathematical foundations and demonstrating its application through a simulated example in R."
  },
  {
    "objectID": "posts/2024-02-15-LDA/2024-02-15-LDA.html#the-essence-of-lda",
    "href": "posts/2024-02-15-LDA/2024-02-15-LDA.html#the-essence-of-lda",
    "title": "What is Linear Discriminant Analysis (LDA)?",
    "section": "The Essence of LDA",
    "text": "The Essence of LDA\nImagine you’re at a party and there are two types of fruit on the table: apples and oranges. You’re blindfolded and asked to classify the fruits using only a scale (to weigh them) and a ruler (to measure their diameter). Intuitively, you might notice that, generally, oranges are slightly heavier and larger in diameter than apples. LDA does something similar with data; it tries to find the “scale” and “ruler” (metaphorically speaking) that best separate different classes, like apples and oranges, based on their features."
  },
  {
    "objectID": "posts/2024-02-15-LDA/2024-02-15-LDA.html#the-math-behind-the-magic",
    "href": "posts/2024-02-15-LDA/2024-02-15-LDA.html#the-math-behind-the-magic",
    "title": "What is Linear Discriminant Analysis (LDA)?",
    "section": "The Math Behind the Magic",
    "text": "The Math Behind the Magic\n\nBayes’ Theorem at a Glance\nLDA is based on Bayes’ theorem, a fundamental concept in probability theory. Bayes’ theorem tells us how to update our beliefs (probabilities) about an event happening (like identifying an orange) given some evidence (the fruit’s weight and diameter). It’s about revising our assumptions with new data.\n\n\nAssumptions of LDA\nLDA makes a couple of key assumptions:\n\nNormal Distribution: It assumes that the data points for each class (like our apples and oranges) are normally distributed. This means if you plot the features (weight and diameter), they’ll form a bell curve, with most apples (or oranges) near the average, and fewer as you move away from the center.\nEqual Variance: LDA assumes that these bell curves for each class have the same shape, though they might be centered at different points. This is like saying, while apples and oranges might differ in average size and weight, the variation around their averages is similar.\n\n\n\nThe LDA Decision Rule\nLDA looks for a line (or in more complex cases, a plane or hyperplane) that best separates our classes (apples from oranges) based on their features. It calculates the means (averages) and variances (spread) for each class and then finds the line where the distance between the means is maximized relative to the variance.\nMathematically, this involves calculating a score (the discriminant score) for each data point that measures how far it is from each class’s mean, adjusted for the overall variance. Data points are then classified based on which score is higher, indicating which class they’re closer to."
  },
  {
    "objectID": "posts/2024-02-15-LDA/2024-02-15-LDA.html#putting-lda-into-practice-simulating-data-in-r",
    "href": "posts/2024-02-15-LDA/2024-02-15-LDA.html#putting-lda-into-practice-simulating-data-in-r",
    "title": "What is Linear Discriminant Analysis (LDA)?",
    "section": "Putting LDA into Practice: Simulating Data in R",
    "text": "Putting LDA into Practice: Simulating Data in R\nNow, let’s see how this works in practice with our R example. We simulate two classes with distinct means but shared variances, plotting them to visualize the data. We will then fit an LDA model to the data and plot the decision boundary, which shows how LDA separates the two classes based on their features.\n\n# Simulate the data\n\nlibrary(MASS) # For lda() and mvrnorm()\nset.seed(100) # Ensure reproducibility\n\nmu1 &lt;- c(-1, 0) # Mean for class 1\nmu2 &lt;- c(1, 0)  # Mean for class 2\nSigma &lt;- matrix(c(2, 1, 1, 2), ncol = 2) # Same covariance matrix for both classes\n\n# Generate data\nclass1 &lt;- mvrnorm(n = 50, mu = mu1, Sigma = Sigma)\nclass2 &lt;- mvrnorm(n = 50, mu = mu2, Sigma = Sigma)\n\ndata &lt;- rbind(class1, class2)\nlabels &lt;- factor(c(rep(0, 50), rep(1, 50)))\ndata &lt;- data.frame(data, class = labels)\n\nWith the data simulated, we can plot it to visualize the distribution of the two classes:\n\nggplot(data, aes(x = X1, y = X2, color = class)) +\n  geom_point() + theme_minimal() +\n  labs(title = \"Plot of the Data\",\n       x = \"Feature 1\", y = \"Feature 2\") +\n  scale_color_manual(values = c('#0e4a6b', '#ed7b2e')) +\n  guides(color = guide_legend(title = \"Class\", override.aes = list(size = 3))) \n\n\n\n\nHaving visualized our data, we proceed to apply LDA to classify these points and find the decision boundary that best separates them:\n\n# Fit LDA model\nlda_fit &lt;- lda(class ~ ., data = data)\n\nWe can now visualize the decision boundary that LDA has learned from the data:\n\n# plot the decision boundary\nx &lt;- seq(-5, 5, length.out = 1000)\ny &lt;- seq(-5, 5, length.out = 1000)\ngrid &lt;- expand.grid(X1 = x, X2 = y)\ngrid$class &lt;- predict(lda_fit, newdata = grid)$class\n\nmeans &lt;- as.data.frame(lda_fit$means)\nmeans$class &lt;- factor(0:1)\nmeans$size &lt;- 5\n\nggplot(data, aes(x = X1, y = X2, color = class)) +\n  geom_point() + theme_minimal() +\n  geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = \"black\") +\n  geom_point(data = means, aes(x = X1, y = X2), color = 'black', size = 8, shape=18) +\n  geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +\n  labs(title = \"LDA Decision Boundary\",\n       subtitle = \"Diamonds represent Class Means\",\n       x = \"Feature 1\", y = \"Feature 2\") +\n  scale_color_manual(values = c('#0e4a6b', '#ed7b2e')) +\n  guides(color = guide_legend(title = \"Class\", override.aes = list(size = 3))) \n\n\n\n\n\nWhat Does This Mean?\nWhen we plot the decision boundary found by LDA, we’re seeing the line that best distinguishes between our classes based on the data. Points on one side of the line are more likely to be Class 0, and on the other, Class 1. The class means (marked as diamonds on our plot) help us visualize the centers around which our data clusters, and the decision boundary shows how LDA uses these centers to classify the data."
  },
  {
    "objectID": "posts/2024-02-15-LDA/2024-02-15-LDA.html#conclusion",
    "href": "posts/2024-02-15-LDA/2024-02-15-LDA.html#conclusion",
    "title": "What is Linear Discriminant Analysis (LDA)?",
    "section": "Conclusion",
    "text": "Conclusion\nLDA is a powerful and intuitive method for classification and dimensionality reduction. It’s a great starting point for understanding more complex methods. By demystifying the math behind LDA and demonstrating its application in R, I hope this post has made LDA more accessible and understandable.\nThanks for reading!"
  },
  {
    "objectID": "posts/2024-02-26-PCR/2024-02-26-PCR.html",
    "href": "posts/2024-02-26-PCR/2024-02-26-PCR.html",
    "title": "Does it Spark Joy? Using PCA for Dimensionality Reduction in R",
    "section": "",
    "text": "Principal Component Analysis (PCA) simplifies the complexity in high-dimensional data by transforming it into a new set of variables, called principal components, which capture the most significant information in the data.\nThe remainder of this post is organized as follows:\n\nHow it Works: We provide an overview of PCA mathematically, explaining its purpose and how it works.\nApplied Example: Navigating PCA and Predictive Modeling with R: We walk through a hands-on application of PCA using R, demonstrating the step-by-step process of PCA and its application in predictive modeling."
  },
  {
    "objectID": "posts/2024-02-26-PCR/2024-02-26-PCR.html#understanding-principal-component-analysis-pca",
    "href": "posts/2024-02-26-PCR/2024-02-26-PCR.html#understanding-principal-component-analysis-pca",
    "title": "Does it Spark Joy? Using PCA for Dimensionality Reduction in R",
    "section": "",
    "text": "Principal Component Analysis (PCA) simplifies the complexity in high-dimensional data by transforming it into a new set of variables, called principal components, which capture the most significant information in the data.\nThe remainder of this post is organized as follows:\n\nHow it Works: We provide an overview of PCA mathematically, explaining its purpose and how it works.\nApplied Example: Navigating PCA and Predictive Modeling with R: We walk through a hands-on application of PCA using R, demonstrating the step-by-step process of PCA and its application in predictive modeling."
  },
  {
    "objectID": "posts/2024-02-26-PCR/2024-02-26-PCR.html#how-it-works",
    "href": "posts/2024-02-26-PCR/2024-02-26-PCR.html#how-it-works",
    "title": "Does it Spark Joy? Using PCA for Dimensionality Reduction in R",
    "section": "How it Works",
    "text": "How it Works\n\nStep 1: Standardization\nSuppose we have a dataset with \\(n\\) observations and \\(p\\) variables.\n\\[\nX = \\begin{bmatrix} x_{11} & x_{12} & \\cdots & x_{1p} \\\\ x_{21} & x_{22} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & \\cdots & x_{np} \\end{bmatrix}\n\\]\nFirst, we standardize the data matrix \\(X\\) to have a mean of 0 and a standard deviation of 1 for each variable. This is crucial for ensuring that all variables contribute equally to the analysis.\nGiven a data matrix \\(X\\), the standardized value \\(z_{ij}\\) of each element is calculated as:\n\\[\nz_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}\n\\]\nwhere \\(\\mu_j\\) is the mean and \\(\\sigma_j\\) is the standard deviation of the \\(j^{th}\\) column of \\(X\\).\n\n\nStep 2: Covariance Matrix\nNext, we calculate the covariance matrix \\(C\\) from the standardized data matrix \\(Z\\). The covariance matrix reflects how variables in the dataset vary together.\nThe covariance matrix is calculated as:\n\\[\nC = \\frac{1}{n-1} Z^T Z\n\\]\nwhere \\(Z\\) is the standardized data matrix, and \\(n\\) is the number of observations.\n\n\nStep 3: Eigenvalue Decomposition\nThe eigenvalue decomposition of the covariance matrix \\(C\\) is performed next. This step identifies the principal components of the dataset.\nFor the covariance matrix \\(C\\), we find the eigenvalues \\(\\lambda\\) and the corresponding eigenvectors \\(v\\) by solving:\n\\[\nCv = \\lambda v\n\\]\n\n\nStep 4: Selection of Principal Components\nThe principal components are selected based on the eigenvalues. The eigenvectors, corresponding to the largest eigenvalues, represent the directions of maximum variance in the data.\nBy selecting the top \\(k\\) principal components, we reduce the dimensionality of our data while retaining most of its variability.\n\n\nSummary\nPCA transforms the original data into a set of linearly uncorrelated variables, the principal components, ordered by the amount of variance they capture. This process allows for the simplification of data, making it easier to explore and visualize."
  },
  {
    "objectID": "posts/2024-02-26-PCR/2024-02-26-PCR.html#applied-example-navigating-pca-and-predictive-modeling-with-r",
    "href": "posts/2024-02-26-PCR/2024-02-26-PCR.html#applied-example-navigating-pca-and-predictive-modeling-with-r",
    "title": "Does it Spark Joy? Using PCA for Dimensionality Reduction in R",
    "section": "Applied Example: Navigating PCA and Predictive Modeling with R",
    "text": "Applied Example: Navigating PCA and Predictive Modeling with R\nIn this segment, we dive into a hands-on application of Principal Component Analysis (PCA) using R, aimed at simplifying and demystifying the process through a simulated dataset. Our approach mirrors the methodical steps of a college textbook, focusing on clarity, precision, and practical insights.\n\nGenerating a Simulated Dataset\nFirst, let’s create a dataset that reflects the complexity often encountered in real data. Our simulated dataset includes:\n\nWe simulate 300 observations, ensuring a robust dataset for analysis.\nVariables \\(X1\\), \\(X2\\), and \\(X3\\) are created with varying degrees of noise, simulating real-world unpredictability. \\(X2\\) and \\(X3\\) are intentionally correlated with \\(X1\\) but with their unique twists of randomness.\nOur outcome variable y is a concoction of influences from \\(X1\\), \\(X2\\), and \\(X3\\), topped with its own layer of noise.\n\nHere’s the R code to generate this dataset:\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(plotly) # For interactive 3D plotting\nlibrary(caret)  # For PCA and regression\nlibrary(GGally) # For ggpairs\nlibrary(ggfortify) \n\nset.seed(123) # Ensuring our pretend data is the same each time\n\n# Generate our dataset\nn &lt;- 300 # Number of observations\nX1 &lt;- rnorm(n, mean = 50, sd = 10)\nX2 &lt;- X1 + rnorm(n, mean = 0, sd = 25) # More noise added\nX3 &lt;- X1 + rnorm(n, mean = 0, sd = 10) # More noise added\ny &lt;- X1 + X2 + X3 + rnorm(n, mean = 0, sd = 30) # Increase noise in the dependent variable\n\ndata &lt;- data.frame(X1, X2, X3, y)\n\n\n\nVisualizing the Data\nBefore applying PCA, we visually explore the dataset to understand the relationships between variables. ggpairs provides a comprehensive pairwise comparison, offering initial insights into correlations and variability among our predictors and with the outcome variable.\n\n# Let's peek at the relationships\nggpairs(data)\n\n\n\n\n\n\n3D Visualization\nA 3D scatter plot further elucidates the interactions among predictors, allowing us to appreciate the dataset’s complexity beyond 2D limitations..\n\n# Interactive 3D plot to visualize relationships\nfig &lt;- plot_ly(data, x = ~X1, y = ~X2, z = ~X3, type = 'scatter3d', mode = 'markers',\n               marker = list(size = 5, opacity = 0.5))\nfig &lt;- fig %&gt;% layout(scene = list(xaxis = list(title = 'X1'),\n                                   yaxis = list(title = 'X2'),\n                                   zaxis = list(title = 'X3')))\nfig\n\n\n\n\n\nIsn’t that cool? Visualizing data in 3D can offer insights that are not immediately obvious in 2D plots, providing a richer understanding of our dataset’s structure.\n\n\nSimplifying Complexity with PCA\nWith PCA, we aim to reduce the dataset’s dimensionality by identifying principal components that capture the most variance. This step simplifies the dataset, making subsequent analyses more manageable.\n\n# Perform PCA\npca_result &lt;- prcomp(data[,1:3], center = TRUE, scale. = TRUE)\n\n# Let's see the summary of our PCA to understand the variance explained\nsummary(pca_result)\n\nImportance of components:\n                          PC1    PC2    PC3\nStandard deviation     1.3418 0.9338 0.5723\nProportion of Variance 0.6002 0.2907 0.1092\nCumulative Proportion  0.6002 0.8908 1.0000\n\n# And visualize the PCA\nautoplot(pca_result, data = data, colour = 'y', frame = TRUE, frame.type = 'norm')\n\n\n\n\nThe summary and plot from PCA show us how much of the total variance is captured by each principal component. This step is crucial as it guides us in deciding how many components to retain for our predictive modeling."
  },
  {
    "objectID": "posts/2024-02-26-PCR/2024-02-26-PCR.html#predictive-modeling-principal-component-regression-pcr",
    "href": "posts/2024-02-26-PCR/2024-02-26-PCR.html#predictive-modeling-principal-component-regression-pcr",
    "title": "Does it Spark Joy? Using PCA for Dimensionality Reduction in R",
    "section": "Predictive Modeling: Principal Component Regression (PCR)",
    "text": "Predictive Modeling: Principal Component Regression (PCR)\nIn this section, we demonstrate the application of Principal Component Regression (PCR), a method that leverages dimensionality reduction through PCA for predictive modeling. The process involves training a regression model on a simplified version of the training data, where simplification is achieved by reducing the data’s dimensions using PCA.\nProcess Overview\n\nSplit the Data: We begin by dividing our dataset into training and testing sets to prepare for a robust evaluation of the model’s predictive performance.\nApply PCA to the Training Data: PCA is performed on the training data to identify the principal components, which are the directions in the data that account for the most variance.\nTransform Both Training and Testing Data: The principal components obtained from the training data are then used to transform both the training and testing sets into the PCA space. This ensures that the model is trained and evaluated on data represented in the same reduced-dimensionality space.\nTrain the Model on Transformed Training Data: A regression model is trained using the PCA-transformed training data. This model aims to predict the outcome variable based on the reduced set of features (principal components).\nEvaluate the Model on Transformed Testing Data: The model’s predictive performance is assessed using the PCA-transformed testing data, allowing us to understand how well the model generalizes to new, unseen data.\n\n\n# Splitting the dataset\ntrainingIndex &lt;- createDataPartition(data$y, p = .8, list = FALSE)\ntrainingData &lt;- data[trainingIndex, ]\ntestingData &lt;- data[-trainingIndex, ]\n\n# Performing PCA on training data\npca_train &lt;- prcomp(trainingData[,1:3], center = TRUE, scale. = TRUE)\n\n# Transforming both training and testing data using the PCA model first 2 comps\ntrainingData_transformed &lt;- predict(pca_train, newdata = trainingData[,1:3])[,1:2]\ntestingData_transformed &lt;- predict(pca_train, newdata = testingData[,1:3])[,1:2]\n\n# Preparing data for modeling\ntrainingModelData &lt;- as.data.frame(trainingData_transformed)\ntrainingModelData$y &lt;- trainingData$y\n\n# Fitting the linear model on transformed training data\nmodel &lt;- lm(y ~ ., data = trainingModelData)\n\n# Summary of the model trained on PCA-transformed training data\nsummary(model)\n\n\nCall:\nlm(formula = y ~ ., data = trainingModelData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-78.422 -24.126  -1.018  17.075  90.479 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  151.971      2.024  75.098   &lt;2e-16 ***\nPC1           27.096      1.510  17.940   &lt;2e-16 ***\nPC2           20.172      2.165   9.319   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.35 on 237 degrees of freedom\nMultiple R-squared:  0.633, Adjusted R-squared:  0.6299 \nF-statistic: 204.3 on 2 and 237 DF,  p-value: &lt; 2.2e-16\n\n# Predicting and calculating RMSE on transformed testing data\npredictions &lt;- predict(model, newdata = as.data.frame(testingData_transformed))\nrmse &lt;- sqrt(mean((predictions - testingData$y)^2))\nprint(paste(\"RMSE:\", rmse))\n\n[1] \"RMSE: 29.7476744937811\"\n\n\n\nComparing with Traditional Regression\nWe contrast PCR’s performance with a traditional regression model using all predictors. This comparison assesses whether PCA’s dimensionality reduction impacts predictive accuracy.\n\n# Traditional linear regression model\nfull_model &lt;- lm(y ~ X1 + X2 + X3, data = trainingData)\nsummary(full_model)\n\n\nCall:\nlm(formula = y ~ X1 + X2 + X3, data = trainingData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-77.406 -21.676  -0.272  17.163  92.270 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.59698   10.85595  -0.608   0.5440    \nX1           0.89290    0.29335   3.044   0.0026 ** \nX2           1.17467    0.08144  14.423  &lt; 2e-16 ***\nX3           1.09015    0.19330   5.640 4.85e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.95 on 236 degrees of freedom\nMultiple R-squared:  0.6437,    Adjusted R-squared:  0.6392 \nF-statistic: 142.1 on 3 and 236 DF,  p-value: &lt; 2.2e-16\n\n# Prediction and RMSE calculation\nfull_predictions &lt;- predict(full_model, newdata = testingData)\nfull_rmse &lt;- sqrt(mean((full_predictions - testingData$y)^2))\nprint(paste(\"Full Model RMSE:\", full_rmse))\n\n[1] \"Full Model RMSE: 28.7829853245021\"\n\n\n\n\nInsights and Reflections\n\nPCA Summary: Reveals PC1 as the major variance holder, suggesting much of the data’s information is concentrated here.\nPCR vs. Traditional Regression: Comparing models shows how PCA can reduce dimensionality of the data without significantly compromising on predictive power. While PCR focuses on the essence of data, traditional regression considers all details, including noise.\nPractical Implications: This exercise demonstrates PCA’s value in simplifying complex datasets, potentially enhancing model interpretability and efficiency."
  },
  {
    "objectID": "posts/2024-02-26-PCR/2024-02-26-PCR.html#conclusion",
    "href": "posts/2024-02-26-PCR/2024-02-26-PCR.html#conclusion",
    "title": "Does it Spark Joy? Using PCA for Dimensionality Reduction in R",
    "section": "Conclusion",
    "text": "Conclusion\nPrincipal Component Analysis (PCA) is a powerful tool for simplifying complex datasets, reducing dimensionality, and enhancing predictive modeling. By identifying principal components that capture the most variance, PCA transforms the original data into a set of linearly uncorrelated variables, making it easier to explore and visualize. In this tutorial, we applied PCA to a simulated dataset and used Principal Component Regression (PCR) to predict an outcome variable, comparing its performance with traditional regression. The results underscore PCA’s potential to streamline data and improve predictive accuracy, offering practical insights for real-world applications.\nThank you!"
  },
  {
    "objectID": "git files/section 5/section 5.html",
    "href": "git files/section 5/section 5.html",
    "title": "Part 5: Workflow",
    "section": "",
    "text": "When working with branches, here is the general workflow to adhere to.\n\nBefore starting work:\n# always start your branching from the master branch\ngit checkout master\n\n# pull the latest\ngit pull\n\n# create a new branch, branch-ed off of the master branch\ngit checkout -b my-awesome-feature\nCommit changes and push to your branch in GitHub regularly.\n\n# add files to the staging area\ngit add filename1\ngit add filename2\ngit add filename3\n\n# commit with a descritive message\ngit commit -m \"descriptive message of the change i just made\"\n\n# push to your branch on GitHub\ngit push\nAlso make sure to periodically pull from master:\ngit pull origin master\nPulling from master periodically is very important! This will keep your code relatively in-sync and prevent deferring massive merge conflicts down the line.\nWhen you’re done with your work\n# makes sure you've commited and pushed \n# all the changes to your branch in GitHub\n\ngit status\nthen open up GitHub and issue a pull request back to master.\n\n\n\n\n\n\n\nThe feature branch workflow is where every small or large feature gets its own branch. These branches are shortlived and are quickly merged back into master. Each feature branch corresponds to a pull request and the branch is deleted after the PR is merged.\n\n\n\n\nThe team member workflow is where each team member gets their own branch. These branches stay alive throughout the duration of the project and can be merged back in as frequently as you like. In this approach you can only have one PR open at a given time per team member."
  },
  {
    "objectID": "git files/section 5/section 5.html#overall-workflow",
    "href": "git files/section 5/section 5.html#overall-workflow",
    "title": "Part 5: Workflow",
    "section": "",
    "text": "When working with branches, here is the general workflow to adhere to.\n\nBefore starting work:\n# always start your branching from the master branch\ngit checkout master\n\n# pull the latest\ngit pull\n\n# create a new branch, branch-ed off of the master branch\ngit checkout -b my-awesome-feature\nCommit changes and push to your branch in GitHub regularly.\n\n# add files to the staging area\ngit add filename1\ngit add filename2\ngit add filename3\n\n# commit with a descritive message\ngit commit -m \"descriptive message of the change i just made\"\n\n# push to your branch on GitHub\ngit push\nAlso make sure to periodically pull from master:\ngit pull origin master\nPulling from master periodically is very important! This will keep your code relatively in-sync and prevent deferring massive merge conflicts down the line.\nWhen you’re done with your work\n# makes sure you've commited and pushed \n# all the changes to your branch in GitHub\n\ngit status\nthen open up GitHub and issue a pull request back to master."
  },
  {
    "objectID": "git files/section 5/section 5.html#workflow-types",
    "href": "git files/section 5/section 5.html#workflow-types",
    "title": "Part 5: Workflow",
    "section": "",
    "text": "The feature branch workflow is where every small or large feature gets its own branch. These branches are shortlived and are quickly merged back into master. Each feature branch corresponds to a pull request and the branch is deleted after the PR is merged.\n\n\n\n\nThe team member workflow is where each team member gets their own branch. These branches stay alive throughout the duration of the project and can be merged back in as frequently as you like. In this approach you can only have one PR open at a given time per team member."
  },
  {
    "objectID": "git files/section 3/section 3.html",
    "href": "git files/section 3/section 3.html",
    "title": "Part 3: Branching",
    "section": "",
    "text": "When using Git, branches are a part of your everyday development process. Git branches are effectively a pointer to a snapshot of your changes. When you want to add a new feature or fix a bug—no matter how big or how small—you spawn a new branch to encapsulate your changes. This makes it harder for unstable code to get merged into the main code base, and it gives you the chance to clean up your future’s history before merging it into the main branch.\nThere are many different use cases for branches ranging from the aforementioned “feature branches”, “release branches”, dev/test/qa branches, and more.\n\n\n\nWhen creating a new repository, git by default starts with a main branch which is what we have been using until now. When we create branches, we are branching off of the most recent commit on the main branch.\n\nThe diagram above visualizes a repository with two isolated lines of development, one for a little feature, and one for a longer-running feature. By developing them in branches, it’s not only possible to work on both of them in parallel, but it also keeps the main main branch free from questionable code.\nAfter developing a feature, it needs to be merged back into the main branch. We will do this using a Pull Request.\n\n\n\n\nWhen creating a branch, the head of the branch splits off of a common base as seen in the picture above. This is why we refer to Git as a non-linear workflow. There are three new commits on the feature branch and 2 new commits directly on the main branch.\n\nWhen come time to merge, the difference between the 2 new commits on the main branch and the new code in the feature branch is resolved in what is referred to as a merge commit. Git tries to be as smart as possible when merging code but occasionally there are changes that cannot automatically be merged. This is when we run into a merge conflict. We will talk more about this in detail.\nSources:\n\nhttps://www.atlassian.com/git/tutorials/using-branches\nhttps://www.atlassian.com/git/tutorials/using-branches/git-merge\n\n\n\n\nWhile there are other ways to merge branches, we will be using pull requests. When using the shared repository model (one repository, multiple collaborators),\n\nbase: almost always the main branch. this is where you are merging on to\ncompare: this is your feature branch. this is where you are merging from\n\n\n\n\n\n\ngit branch\n\n\n\nThe -b flag creates a new branch.\ngit checkout -b &lt;branchname&gt;\n\n\n\ngit checkout &lt;branchname&gt;\n\n\n\n\n\nGet into groups of 2 or 3.\nThe product owner should:\n\ncreate a repository under their account named demo-website (or use your project website!)\nadd the remaining team members (and me) as collaborators to the repo. This is done under the repo settings\nenable github pages on this repo with the main branch as the source\n\nThe product owner should create an index.html and a page called our-team/index.html. Commit and push this change directly to the main branch.\n\nHere is a sample index.html:\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Demo Website&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Our Demonstration Website&lt;/h1&gt;\n            &lt;a href='our-team/index.html'&gt;Learn more about our team&lt;/a&gt;\n        &lt;/body&gt;\n    &lt;/html&gt;\nHere is a sample `our-team/index.html`:\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Demo Website&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;About Us!&lt;/h1&gt;\n        &lt;/body&gt;\n    &lt;/html&gt;\n\nEveryone should clone the repository.\ngit clone git@github.com:XXXXX/demo-website.git\nAll team members, should create branches titled add-member-&lt;name&gt;. For example, I would do:\n\ngit checkout -b add-member-jacobjameson\n\nIn this branch, each team member should edit our-team/index.html and add some basic information about yourself to this page. Be sure to only create this one file - there should be no other changes to the repository. It’s important to keep your code changes isolated when working with git to avoid unecessary merge conflicts.\n\nEach member should:\n\ncommit this change to the branch\npush it\ncreate a pull request\n\nProduct owner should review and merge all of the PRs. There should be no conflicts.\n\nAll team members should checkout the main branch and pull the latest code.\n\n\n\ngit branch -d &lt;branchname&gt;"
  },
  {
    "objectID": "git files/section 3/section 3.html#why-branches",
    "href": "git files/section 3/section 3.html#why-branches",
    "title": "Part 3: Branching",
    "section": "",
    "text": "When using Git, branches are a part of your everyday development process. Git branches are effectively a pointer to a snapshot of your changes. When you want to add a new feature or fix a bug—no matter how big or how small—you spawn a new branch to encapsulate your changes. This makes it harder for unstable code to get merged into the main code base, and it gives you the chance to clean up your future’s history before merging it into the main branch.\nThere are many different use cases for branches ranging from the aforementioned “feature branches”, “release branches”, dev/test/qa branches, and more."
  },
  {
    "objectID": "git files/section 3/section 3.html#how-do-branches-work",
    "href": "git files/section 3/section 3.html#how-do-branches-work",
    "title": "Part 3: Branching",
    "section": "",
    "text": "When creating a new repository, git by default starts with a main branch which is what we have been using until now. When we create branches, we are branching off of the most recent commit on the main branch.\n\nThe diagram above visualizes a repository with two isolated lines of development, one for a little feature, and one for a longer-running feature. By developing them in branches, it’s not only possible to work on both of them in parallel, but it also keeps the main main branch free from questionable code.\nAfter developing a feature, it needs to be merged back into the main branch. We will do this using a Pull Request."
  },
  {
    "objectID": "git files/section 3/section 3.html#how-does-merging-work",
    "href": "git files/section 3/section 3.html#how-does-merging-work",
    "title": "Part 3: Branching",
    "section": "",
    "text": "When creating a branch, the head of the branch splits off of a common base as seen in the picture above. This is why we refer to Git as a non-linear workflow. There are three new commits on the feature branch and 2 new commits directly on the main branch.\n\nWhen come time to merge, the difference between the 2 new commits on the main branch and the new code in the feature branch is resolved in what is referred to as a merge commit. Git tries to be as smart as possible when merging code but occasionally there are changes that cannot automatically be merged. This is when we run into a merge conflict. We will talk more about this in detail.\nSources:\n\nhttps://www.atlassian.com/git/tutorials/using-branches\nhttps://www.atlassian.com/git/tutorials/using-branches/git-merge"
  },
  {
    "objectID": "git files/section 3/section 3.html#pull-requests",
    "href": "git files/section 3/section 3.html#pull-requests",
    "title": "Part 3: Branching",
    "section": "",
    "text": "While there are other ways to merge branches, we will be using pull requests. When using the shared repository model (one repository, multiple collaborators),\n\nbase: almost always the main branch. this is where you are merging on to\ncompare: this is your feature branch. this is where you are merging from"
  },
  {
    "objectID": "git files/section 3/section 3.html#commands",
    "href": "git files/section 3/section 3.html#commands",
    "title": "Part 3: Branching",
    "section": "",
    "text": "git branch\n\n\n\nThe -b flag creates a new branch.\ngit checkout -b &lt;branchname&gt;\n\n\n\ngit checkout &lt;branchname&gt;"
  },
  {
    "objectID": "git files/section 3/section 3.html#exercise",
    "href": "git files/section 3/section 3.html#exercise",
    "title": "Part 3: Branching",
    "section": "",
    "text": "Get into groups of 2 or 3.\nThe product owner should:\n\ncreate a repository under their account named demo-website (or use your project website!)\nadd the remaining team members (and me) as collaborators to the repo. This is done under the repo settings\nenable github pages on this repo with the main branch as the source\n\nThe product owner should create an index.html and a page called our-team/index.html. Commit and push this change directly to the main branch.\n\nHere is a sample index.html:\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Demo Website&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Our Demonstration Website&lt;/h1&gt;\n            &lt;a href='our-team/index.html'&gt;Learn more about our team&lt;/a&gt;\n        &lt;/body&gt;\n    &lt;/html&gt;\nHere is a sample `our-team/index.html`:\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Demo Website&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;About Us!&lt;/h1&gt;\n        &lt;/body&gt;\n    &lt;/html&gt;\n\nEveryone should clone the repository.\ngit clone git@github.com:XXXXX/demo-website.git\nAll team members, should create branches titled add-member-&lt;name&gt;. For example, I would do:\n\ngit checkout -b add-member-jacobjameson\n\nIn this branch, each team member should edit our-team/index.html and add some basic information about yourself to this page. Be sure to only create this one file - there should be no other changes to the repository. It’s important to keep your code changes isolated when working with git to avoid unecessary merge conflicts.\n\nEach member should:\n\ncommit this change to the branch\npush it\ncreate a pull request\n\nProduct owner should review and merge all of the PRs. There should be no conflicts.\n\nAll team members should checkout the main branch and pull the latest code.\n\n\n\ngit branch -d &lt;branchname&gt;"
  },
  {
    "objectID": "git files/section 1/section1.html",
    "href": "git files/section 1/section1.html",
    "title": "Part 1: Git Installation and SSH Instructions",
    "section": "",
    "text": "Install homebrew following the instructions at https://brew.sh/.\nOpen Terminal and check if git is already installed.\ngit --version\nIf not, install git using brew install git. Then verify, its installed by running git --version.\n\n\n\n\n\nGo to gitforwindows.org. Make sure you include Git Bash in your installation!"
  },
  {
    "objectID": "git files/section 1/section1.html#part-i-installing-git",
    "href": "git files/section 1/section1.html#part-i-installing-git",
    "title": "Part 1: Git Installation and SSH Instructions",
    "section": "",
    "text": "Install homebrew following the instructions at https://brew.sh/.\nOpen Terminal and check if git is already installed.\ngit --version\nIf not, install git using brew install git. Then verify, its installed by running git --version.\n\n\n\n\n\nGo to gitforwindows.org. Make sure you include Git Bash in your installation!"
  },
  {
    "objectID": "git files/section 1/section1.html#part-ii-creating-a-github-account",
    "href": "git files/section 1/section1.html#part-ii-creating-a-github-account",
    "title": "Part 1: Git Installation and SSH Instructions",
    "section": "Part II: Creating a GitHub Account",
    "text": "Part II: Creating a GitHub Account\n\nGo to GitHub.\nClick the “Sign Up” button.\nFollow the on-screen instructions to create your account."
  },
  {
    "objectID": "git files/section 1/section1.html#part-iii-git-setup",
    "href": "git files/section 1/section1.html#part-iii-git-setup",
    "title": "Part 1: Git Installation and SSH Instructions",
    "section": "Part III: Git Setup",
    "text": "Part III: Git Setup\n\nConfigure git with your name and email address. Be sure to use the same email associated with your Github account.\ngit config --global user.name \"YOUR NAME\"\ngit config --global user.email \"YOUR EMAIL ADDRESS\""
  },
  {
    "objectID": "git files/section 1/section1.html#part-iv-ssh",
    "href": "git files/section 1/section1.html#part-iv-ssh",
    "title": "Part 1: Git Installation and SSH Instructions",
    "section": "Part IV: SSH",
    "text": "Part IV: SSH\nIn order to write code locally on our computer and be able to push to GitHub (or pull from GitHub) daily without constantly having to enter a username and password each time, we’re going to set up SSH keys.\n\nSSH keys come in pairs, a public key that gets shared with services like GitHub, and a private key that is stored only on your computer. If the keys match, you’re granted access.\nThe cryptography behind SSH keys ensures that no one can reverse engineer your private key from the public one.\nsource: https://jdblischak.github.io/2014-09-18-chicago/novice/git/05-sshkeys.html\n\nThe following steps are a simplification of the steps found in GitHub’s documentation. If you prefer, feel free to follow the steps at that link. Otherwise, for a simplified experience continue on below!\nSimplified Setup Steps\n\nStep 1: Check to see if you already have keys.\nRun the following command.\nls -al ~/.ssh/\nIf you see any output, that probably means you already have a public and private SSH key. If you have keys, you will most likely you will have two files, one named id_rsa (that contains your private key) and id_rsa.pub (that contains your public key).\nsidenote: Those files may also be named something like: id_ecdsa.pub or id_ed25519.pub. That just means you’re using a different encryption algorithm to generate your keys. You can learn more about that here if you chose to. Or, don’t worry about it and power on!\nIf you already have keys, continue to step 3. Otherwise, read on!\nStep 2: Create new SSH keys.\nRun the following comamnd, but makes sure to replace your_email@example.com with your own email address. Use the same email address you used to sign up to GitHub with.\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\nYou may then see a prompt like the one below. Just hit enter to save the key in the default location.\nEnter file in which to save the key (/Users/jacob/.ssh/id_rsa):\nAfter that, the system will prompt you to enter a passphrase. We’re not going to use a passphrase here, so just go ahead and leave that blank and hit enter twice.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nFinally you should see some randomart that looks like this\nYour identification has been saved in /Users/jacob/.ssh/id_rsa.\nYour public key has been saved in /Users/jacob/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:2AazdvCBP8d1li9tF8cszM2KbtjPe7iwfCK8gUgzIGY your_email@example.com\nThe key's randomart image is:\n+---[RSA 4096]----+\n|                 |\n|       .     o * |\n|  E . = .   . B.*|\n| o . . X o . + =o|\n|      B S o . o =|\n|     o * + +   o.|\n|      . ..o =  . |\n|          o+.=o .|\n|          .ooo=+ |\n+----[SHA256]-----+\nStep 3: Add your key to GitHub\nRun the following command to view your public key\ncat ~/.ssh/id_rsa.pub\nNavigate to https://github.com/settings/keys and hit “New SSH key”. Paste the SSH key from the last command into the text box as shown below and then hit “Add SSH key”. Make sure you copy paste exactly. The key will likely start with ssh_rsa and end with your email address. You can give the key a title like “My Macbook Pro” so you know which computer this key comes from.\n\nStep 4: Verify that it worked!\nRun the following command to test your computer’s SSH connection to GitHub\nssh -T git@github.com\nIf the connection is successful, you will see a message like this\n&gt; Hi username! You've successfully authenticated, but GitHub does not\n&gt; provide shell access.\n\n\nRecap: What did we just do?\nWe just created a public/private SSH Key pair. There is now a folder on your computer called .ssh (it is a hidden folder, hidden folders have names that start with .). You can run this command to see the files in that folder.\nls -al ~/.ssh/\nid_rsa.pub contains your public key, you can see what that looks like by running:\ncat ~/.ssh/id_rsa.pub\nid_rsa contains your private key, you can see what that looks like by running:\ncat ~/.ssh/id_rsa\nThis public and private key pair are mathematically linked. As the name suggests, you can share your public key far and wide, but must keep your private key safe and secure. Since you have shared your public key with GitHub, your computer can encrypt files with your private key and send them to GitHub. Since GitHub has your public key, it can match that file and verify that it is coming from you. Your computer can now securely communicate with GitHub without needing a username and password every time.\nTo get started with Git, you need to install it on your computer. You can download the installer from the Git website. Once you have Git installed, you can use the git command in your terminal to interact with Git."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Sex Differences in Life-Course Suicide Rates by State Firearm Policy Environment\n\n\nAuthors: Glasser NJ, Jameson JC, Abou Baker N, Pollack HA, Tung EL\n\n\nJournal: American Journal of Preventive Medicine, 2025\n\n\nDOI: 10.1016/j.amepre.2025.107961\n\n\nView Article More Info\n\n\n\n\n\n\n×\n\nAbstract\n\n\nMale sex and permissive state firearm policy environments have been independently associated with suicide mortality. Few recent studies quantify how these factors interact, or how interactions vary across the life course. This study investigates how state firearm policy environments moderate sex differences in firearm, nonfirearm, and overall suicide rates across the life course.\n\n\nMethods: Using CDC data, linear regression was employed to model sex differences in firearm, nonfirearm, and total suicide mortality rates per 100,000 across the life course, with interactions by state firearm policy environments (permissive vs strict, based on Giffords Gun Law Scorecard). Data were collected in 2018–2022 and analyzed in 2024–2025.\n\n\nResults: Across all ages and policy environments, males had higher firearm (18.95; 95% CI=16.97–20.92), nonfirearm (7.07; 95% CI=6.63–7.14), and total (26.02; 95% CI=23.94–28.10) suicide rates. Permissive environments were associated with increased firearm (5.84; 95% CI=3.84–7.84) and total suicide rates (5.96; 95% CI=3.32–8.60). Interaction models showed permissive environments amplified male sex associations with firearm (9.02; 95% CI=5.75–12.29) and total (9.24; 95% CI=5.96–12.52) suicide rates. Sex also moderated age associations with suicide.\n\n\nConclusions: Males die by suicide at higher rates than females across the life course, particularly in permissive firearm policy states. At older ages, male suicide rates remain dramatically higher regardless of policy environment, underscoring unique challenges of aging requiring focused clinical attention.\n\n\n\n\n\n\n\nA National Study Among Diverse US Populations of Exposure to Prescription Medications with Evidence-Based Pharmacogenomic Information\n\n\nAuthors: Saulsberry L, Jameson JC, Gibbons RD, Dolan ME, Olopade OI, O’Donnell PH\n\n\nJournal: Clinical Pharmacology & Therapeutics, 2025\n\n\nDOI: 10.1002/cpt.3617\n\n\nView Article More Info\n\n\n\n\n\n\n×\n\nAbstract\n\n\nTailoring pharmacogenomic (PGx) implementation to diverse populations is vital to promoting health equity. We assessed prescriptions for medications with potentially actionable PGx information to identify patient characteristics associated with differential PGx medication exposure. We analyzed the nationally-representative MEPS dataset of adults who reported receiving prescriptions between 2014 and 2021. PGx medications include those the FDA and CPIC designate as having drug-gene associations supported by scientific evidence. With the primary outcome being PGx prescriptions, we performed Poisson regression adjusted for all other relevant covariates. In our final population (N = 119,722, 72% White/20% Black/4% Asian/8% Hispanic), 61% were prescribed a PGx medication, 56% were female, and 97% held health insurance coverage. Adults with private health insurance (65%) or public Medicaid/Medicare coverage (32%) were more likely to have PGx prescriptions than the uninsured (3%). Individuals with cardiovascular conditions [adjusted IRR (aIRR) = 1.45, 95% CI 1.41, 1.48], high cholesterol [aIRR = 1.37, 95% CI 1.35, 1.40], high blood pressure [aIRR = 1.14, 95% CI 1.12, 1.16], and cancer [aIRR = 1.02, 95% CI 1.00, 1.04] were more likely to receive PGx prescriptions. Self-reported Blacks were less likely than Whites to receive PGx medications [aIRR = 0.92, 95% CI 0.90, 0.94], and among those with health conditions, the likelihood of PGx medication exposure for underrepresented groups (Blacks, Hispanics, and Asians) was lower than for Whites. Our study using a comprehensive list of PGx medications in a nationally representative dataset indicates that certain populations are differentially exposed to genomically informed medications. This may suggest that if implementing a pharmacogenomics program based on reactive testing initiated by a prescription, a small underrepresentation of the Black population could be expected because of an underlying prescription disparity. Secondly, if implementing a pharmacogenomics program based on targeted preemptive testing, using clinical indication/comorbidity may be a reasonable way to enrich the population for prescriptions that would benefit from genotype tailoring.\n\n\n\n\n\n\n\nVariation in Batch Ordering of Imaging Tests in the Emergency Department and the Impact on Care Delivery\n\n\nAuthors: Jameson JC, Saghafian S, Huckman RS, Hodgson N\n\n\nJournal: Health Services Research, 2025, 60(1):e14406.\n\n\nDOI: 10.1111/1475-6773.14406\n\n\nView Article View Statistical Code More Info\n\n\n\n\n\n\n×\n\nAbstract\n\n\nObjectives: To examine heterogeneity in physician batch ordering practices and measure associations between a physician’s batch ordering tendency and patient outcomes and resource utilization.\n\n\nStudy Setting and Design: Retrospective study using EMR data from 50,836 ED visits to the Mayo Clinic of Arizona (2018–2019). Outcomes include patient length of stay (LOS), the number of imaging tests ordered, and 72-hour return with admission. Associations were measured with multivariable linear regression, controlling for covariates.\n\n\nPrincipal Findings: Physicians with a batch tendency 1SD higher than average had:\n\n\n4.5% longer ED LOS (p &lt; 0.001).\n\n\n14.8% lower probability of 72-hour return with admission (p &lt; 0.001).\n\n\n8 additional imaging tests per 100 patient encounters (p &lt; 0.001).\n\n\nThese results suggest batching can lead to more comprehensive evaluations and reduce short-term revisits but may increase unnecessary imaging.\n\n\nConclusions: Physician diagnostic test ordering strategies significantly impact ED efficiency and patient care. Guidelines are needed to optimize ED test ordering practices.\n\n\n\n\n\n\n\n\nGenomic Heterogeneity and Ploidy Identify Patients with Intrinsic Resistance to PD-1 Blockade in Metastatic Melanoma\n\n\nAuthors: Tarantino G, Ricker CA, Wang A, Ge W, et al., Jacob Jameson, et al.\n\n\nJournal: Science Advances, 2024, 10(48):eadp4670\n\n\nDOI: 10.1126/sciadv.adp4670\n\n\nView Article More Info\n\n\n\n\n\n\n×\n\nAbstract\n\n\nThe introduction of immune checkpoint blockade (ICB) has markedly improved outcomes for advanced melanoma. However, many patients develop resistance through unknown mechanisms. While combination ICB has improved response rate and progression-free survival, it substantially increases toxicity. Biomarkers to distinguish patients who would benefit from combination therapy versus aPD-1 remain elusive. We analyzed whole-exome sequencing of pretreatment tumors from four cohorts (n = 140) of ICB-naïve patients treated with aPD-1. High genomic heterogeneity and low ploidy robustly identified patients intrinsically resistant to aPD-1. To establish clinically actionable predictions, we optimized and validated a predictive model using ploidy and heterogeneity to confidently identify (90% PPV) patients with intrinsic resistance to and worse survival on aPD-1. We further observed that three of seven (43%) patients predicted to be intrinsically resistant to single-agent PD-1 ICB responded to combination ICB, suggesting that these patients may benefit disproportionately from combination ICB. These findings highlight the importance of heterogeneity and ploidy, nominating an approach toward clinical actionability.\n\n\n\n\n\n\n\n\nMale Gender Expressivity and Diagnosis and Treatment of Cardiovascular Disease Risks in Men\n\n\nAuthors: Glasser NJ, Jameson JC, Huang ES, et al.\n\n\nJournal: JAMA Network Open, 2024, 7(10):e2441281\n\n\nDOI: 10.1001/jamanetworkopen.2024.41281\n\n\nView Article View Statistical Code More Info\n\n\n\n\n\n\n×\n\nAbstract\n\n\nImportance: Male gender expressivity (MGE), reflecting sociocultural masculinity norms, may influence cardiovascular disease (CVD) risks.\n\n\nObjective: To examine associations between adolescent and young adult MGE and CVD risk diagnoses and treatment in men.\n\n\nDesign: A cohort study of 4,230 U.S. males (aged 12-18 years in adolescence, followed into adulthood) from the National Longitudinal Study of Adolescent to Adult Health (Add Health).\n\n\nResults: Higher MGE in adolescence and young adulthood was linked to lower rates of hypertension, diabetes, and hyperlipidemia diagnoses, as well as less hypertension and diabetes treatment in adulthood.\n\n\nConclusions: Elevated MGE in youth may reduce adult CVD risk and treatment, highlighting the need for tailored public health strategies to address distinctive risks in males with high MGE.\n\n\n\n\n\n\n\nAssociations of Adolescent School Social Networks, Gender Norms, and Adolescent-to-Young Adult Changes in Male Gender Expression With Young Adult Substance Use\n\n\nAuthors: Glasser NJ, Jameson JC, Tung EL, Lindau ST, Pollack HA\n\n\nJournal: Journal of Adolescent Health, 2024, 74(4):755-763\n\n\nDOI: 10.1016/j.jadohealth.2023.11.018\n\n\nView Article View Statistical Code More Info\n\n\n\n\n\n\n×\n\nAbstract\n\n\nPurpose: Male gender expression (MGE), shaped by sociocultural pressures, has been associated with health. This study examines links between adolescent school social networks, gender norms, changes in MGE, and young adult substance use.\n\n\nMethods: Using data from 4,776 males in a nationally representative longitudinal cohort, adolescent-to-young adult MGE changes were analyzed in relation to adolescent social networks, school norms, and young adult substance use behaviors (e.g., cigarette, marijuana, heavy alcohol, and recreational drug use).\n\n\nResults: MGE changes were associated with school gender norms, with greater shifts toward school averages (β = -0.83, p &lt; .01) observed among adolescents whose MGE initially diverged from school norms. Increases in MGE were linked to higher odds of substance use, except prescription drug misuse.\n\n\nDiscussion: Adolescent gender norms influence MGE shifts, which may predict substance use in young adulthood. School-based gender norms offer a potential target for interventions to reduce substance misuse.\n\n\n\n\n\n\n\nDoes the Doctor-Patient Relationship Affect Enrollment in Clinical Research?\n\n\nAuthors: Soo J, Jameson J, Flores A, Dubin L, Perish E, Afzal A, Berry G, DiMaggio V, Krishnamoorthi VR, Porter J, Tang J, Meltzer D\n\n\nJournal: Academic Medicine, 2023, 98(6S):S17-S24\n\n\nDOI: 10.1097/ACM.0000000000005195\n\n\nView Article More Info\n\n\n\n\n\n\n×\n\nAbstract\n\n\nPurpose: Recruiting patients for clinical research is challenging, especially for underrepresented populations, and may be influenced by patients’ relationships with their physicians, care experiences, and engagement with care. This study examines predictors of research enrollment in care models promoting continuity in the doctor-patient relationship.\n\n\nMethod: Conducted within 2 care model studies at the University of Chicago from 2020-2022, this study assessed predictors of enrollment in a vitamin D research study. Predictors included patient-reported care experiences, clinic engagement, and parent study follow-up survey completion, analyzed using univariate tests and multivariable logistic regression.\n\n\nResults: Of 773 eligible participants, 63% of intervention arm participants enrolled in the vitamin D study versus 17% of control arm participants. Enrollment was not associated with reported quality of communication or trust in the doctor but was linked to timely care, more completed clinic visits, and higher parent study follow-up survey completion.\n\n\nConclusions: Study enrollment may be influenced more by continuity of care, timely access, and patient engagement than by the quality of the doctor-patient relationship. These findings can inform strategies for improving research recruitment."
  },
  {
    "objectID": "research.html#published-papers",
    "href": "research.html#published-papers",
    "title": "Research",
    "section": "",
    "text": "Reflecting sociocultural pressures around masculinity, measures of male gender expression (MGE) have previously been associated with health outcomes. In this study, we examine associations between of adolescent school social networksnetwork variables and school, gender norms, and with adolescent-to-young-adult MGE changes, and of MGE changes in male gender expression (GE) with young adult substance use.\n\n\n\nPublisher’s Site Code Repo\n\n\n\n\n\n\nRecruiting patients for clinical research is challenging, especially for underrepresented populations, and may be influenced by patients’ relationships with their physicians, care experiences, and engagement with care. This study sought to understand predictors of enrollment in a research study among socioeconomically diverse participants in studies of care models that promote continuity in the doctor–patient relationship.\n\n\n\nPublisher’s Site PDF"
  },
  {
    "objectID": "research.html#under-review",
    "href": "research.html#under-review",
    "title": "Research",
    "section": "Under Review",
    "text": "Under Review\n\nTo Batch or Not to Batch: Test-Ordering Variability in the Emergency Department and the Impact on Care Delivery\nEmergency Department (ED) patients may receive varying diagnostic workups and dispositions based on physician factors instead of solely based on presenting conditions. This study delves into the contrasting practices of batch-ordering multiple tests simultaneously versus the sequential ordering of tests based on previous results. Our analysis revealed stark differences in physician diagnostic approaches, even when working in similar environments. Findings suggest that physicians who predominantly make use of batching (“batchers”) tend to order more tests, which is associated with longer lengths of stay and increased costs. In contrast, other physicians (“non-batchers”) order fewer tests, which is associated with lower lengths of stay and costs, without any impact on primary ED outcome measures, such as the 72-hour rate of return.\n\n\n\nPDF Code Repo"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "My teaching experiences are quite diverse. The summer after completing my undergraduate degree, I began teaching in a high school Algebra II credit recovery program in The School District of Philadelphia. I then transitioned to a full-time middle school mathematics position in New Haven, CT, where I spent the next two years falling in love with the profession. During my second year, I held the Grade-level Chair position for the 8th grade and consulted for the Achievement First Charter Network on middle school mathematics curriculum. I am also passionate about computer science education, and have experience running Girls Who Code clubs and participating in the Code.org Middle and High School Computer Science Professional Learning Program for educators in computer science. Since teaching at the middle school level, I have been fortunate to hold a variety of teaching positions at The University of Chicago and Harvard University. I hope to pursue a teaching career after completing my PhD.\n\n\n\n\nHarvard T.H. Chan School of Public Health\n\n\n\nDecision Science for Public Health — Teaching Fellow (2023-2025)\n\n\n\n\n\n\nHarvard Kennedy School\n\n\n\nEconomics for Public Policy — Instructor (2024,2025)\n\n\nMath Camp — Instructor (2023-2025)\n\n\nBig Data and Machine Learning — Teaching Fellow (2023-2025)\n\n\nGame Theory — Teaching Fellow (2023,2024)\n\n\nData and Programming for Policymakers — Course Assistant (2023)\n\n\nResources, Incentives, and Choices I: Markets and Market Failures — Teaching Fellow (2022-2025)\n\n\n\n\n\n\nThe University of Chicago, Center for Translational Science\n\n\n\nData, Quantitative Methods, and Applications in HSR — Teaching Assistant and Instructor (2021-2023)\n\n\nIntroduction to Health Services Research — Teaching Assistant (2021)\n\n\n\n\n\n\nThe University of Chicago, Department of Computer Science\n\n\n\nMachine Learning for Public Policy — Teaching Assistant (2022)\n\n\nMathematics for Data Analysis and Computer Science — Teaching Assistant (2022)\n\n\n\n\n\n\nThe University of Chicago Booth School of Business\n\n\n\nIntroductory Finance — Teaching Assistant (2020-2022)\n\n\nData Analysis in R and Python — Teaching Assistant (2021)\n\n\n\n\n\n\nThe University of Chicago Harris School of Public Policy\n\n\nCoding Lab for Public Policy\n\n\nInstructor and Curriculum Developer (2021)\n\n\n\n\n\nTeach For America, Achievement First Amistad Academy Middle School\n\n\n7th/8th Grade Mathematics\n\n\nMathematics Teacher (2018-2020)"
  },
  {
    "objectID": "git files/section 6/section 6.html",
    "href": "git files/section 6/section 6.html",
    "title": "Part 6: Additional Resources",
    "section": "",
    "text": "Understanding Git Conceptually\n\n​https://www.sbf5.com/~cduan/technical/git/\n\nReference Guides to Git Commands\n\nhttps://git-scm.com/docs\n\nGit Screwup Guide\n\nhttp://ohshitgit.com/\n\nGit/Github Cheat Sheets\n\nhttps://training.github.com/kit/downloads/github-git-cheat-sheet.pdf\nhttp://ndpsoftware.com/git-cheatsheet.html\n\nLicensing Open Source Code\n\n​http://choosealicense.com/\nhttps://help.github.com/articles/open-source-licensing/\n\nhttps://cs61.seas.harvard.edu/wiki/2012/Git\nhttp://www.eecs.harvard.edu/~cs161/resources/git.html\nhttps://git-scm.com/video/what-is-version-control\nhttps://git-scm.com/video/quick-wins\nhttp://slides.com/dhrumilmehta/how-to-tell-a-story-with-data-tools-of-the-trade-2#/4/21\n\n\n\n\n\nInteractive Tutorials\n\n​https://try.github.io\nhttps://github.com/jlord/git-it-electron\n\nBasic Git Commands\n\n​http://www.teaching-materials.org/git/slides.html\nhttp://rogerdudler.github.io/git-guide/\n\n\n\n\n\nInformation sourced from: https://github.com/AlJohri/DAT-DC-12/blob/master/notebooks/intro-git.ipynb and Dhrumil Mehta"
  },
  {
    "objectID": "git files/section 6/section 6.html#further-reading",
    "href": "git files/section 6/section 6.html#further-reading",
    "title": "Part 6: Additional Resources",
    "section": "",
    "text": "Understanding Git Conceptually\n\n​https://www.sbf5.com/~cduan/technical/git/\n\nReference Guides to Git Commands\n\nhttps://git-scm.com/docs\n\nGit Screwup Guide\n\nhttp://ohshitgit.com/\n\nGit/Github Cheat Sheets\n\nhttps://training.github.com/kit/downloads/github-git-cheat-sheet.pdf\nhttp://ndpsoftware.com/git-cheatsheet.html\n\nLicensing Open Source Code\n\n​http://choosealicense.com/\nhttps://help.github.com/articles/open-source-licensing/\n\nhttps://cs61.seas.harvard.edu/wiki/2012/Git\nhttp://www.eecs.harvard.edu/~cs161/resources/git.html\nhttps://git-scm.com/video/what-is-version-control\nhttps://git-scm.com/video/quick-wins\nhttp://slides.com/dhrumilmehta/how-to-tell-a-story-with-data-tools-of-the-trade-2#/4/21"
  },
  {
    "objectID": "git files/section 6/section 6.html#learn-to-use-git-from-the-command-line",
    "href": "git files/section 6/section 6.html#learn-to-use-git-from-the-command-line",
    "title": "Part 6: Additional Resources",
    "section": "",
    "text": "Interactive Tutorials\n\n​https://try.github.io\nhttps://github.com/jlord/git-it-electron\n\nBasic Git Commands\n\n​http://www.teaching-materials.org/git/slides.html\nhttp://rogerdudler.github.io/git-guide/"
  },
  {
    "objectID": "git files/section 6/section 6.html#credits",
    "href": "git files/section 6/section 6.html#credits",
    "title": "Part 6: Additional Resources",
    "section": "",
    "text": "Information sourced from: https://github.com/AlJohri/DAT-DC-12/blob/master/notebooks/intro-git.ipynb and Dhrumil Mehta"
  },
  {
    "objectID": "git files/section 4/section 4.html",
    "href": "git files/section 4/section 4.html",
    "title": "Part 4: Reference",
    "section": "",
    "text": "Git: Reference\n\n\nGetting Started\n\nCreate a new repository on GitHub\nClone that repository locally onto your computer\ngit clone git@github.com:jacobjameson/simples_script.git\n\n\n\nLinear Workflow\n\ncd into the folder containing your project.\ncd ~/path/to/project\nCheck the status of your local repository to make sure you didn’t forget to commit any work.\ngit status\nThen pull the latest changes from the remote repository on GitHub.\ngit pull\nDo a discrete chunk of work on your project (lets say you added a new analysis)\nCheck the status again, then add the files you’d like to commit to the staging area.\ngit status\ngit add analysis.R\ngit status\nCommit with a descriptive summary of exactly what you did\ngit commit -m \"add new analysis\"\nPush that change back to GitHub\ngit push\n\n\n\nNon-linear branching workflow\n\ncd into the folder containing your project.\ncd ~/path/to/project\nCheck the status of your local repository to make sure you didn’t forget to commit any work. Run git branch to see which branch you’re on. You should ideally be on the master branch.\ngit status\ngit branch\nThen pull the latest changes from the master branch of the remote repository on GitHub.\ngit pull\nCreate a new branch with a descriptive name (remember the -b option will create a new branch, you can check out an existing branch by not using that option)\ngit checkout -b analysis-new\nDo your work in discrete chunks. at the end of each chunk, add the file to the staging area, then commit it. Its usually a good idea to also push the latest to GitHub, although some people prefer to do that at the end.\nDo some work\ngit status\ngit add analysis.R\ngit commit -m \"add blank script\"\ngit push\ngit status\nDo more work\ngit status\ngit add analysis.R\ngit commit -m \"add code to analysis script\"\ngit push\ngit status\nDo more work\ngit status\ngit add analysis.R\ngit commit -m \"fixed bug\"\ngit push\ngit status\nDo more work - lets imagine this work took place across two files, an html file and a stylesheet file.\ngit status\ngit add analysis.R\ngit add graph.png\ngit commit -m \"Added the plot from the analysis\"\ngit push\ngit status\nOnce everything has been pushed to GitHub, issue a pull request from your branch back to the master branch.\n\nYou can have a discussion on this pull request using GitHub’s social features, and then merge it into the master branch when everyone agrees its a good idea to do so.\nFinally, once the pull request has been merged into the master branch in the remote repository on GitHub, you’ll want to get the latest version of the master branch on your local machine. Checkout the master branch locally and then pull.\ngit checkout master\ngit pull"
  },
  {
    "objectID": "git files/section 2/section 2.html",
    "href": "git files/section 2/section 2.html",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "",
    "text": "Git (/ɡɪt/) is a version control system (VCS) for tracking changes in computer files and coordinating work on those files among multiple people. It is primarily used for source code management in software development, but it can be used to keep track of changes in any set of files. As a distributed revision control system it is aimed at speed, data integrity, and support for distributed, non-linear workflows. -Wikipedia\n\n\nGit is an open source program for tracking changes in text files. -GitHub (https://help.github.com/articles/github-glossary/)\n\n\n\nKeeping track of file versions is hard.\n\n\n\nAbove all else, Git is a fast and distributed version control system, that allows you to efficiently handle projects large and small.\nHere are some problems we face as developers, and how git solves them:\n\n\nGit allows us to make save points at any time. These save points are called ‘commits’. Once a save point is made, it’s permanent, and allows us to go back to that save point at any time. From there, we can see what the code looked like at that point, or even start building off that version.\n\n\n\n\nEvery commit has a description (commit message), which allows us to describe what changes were made between the current and previous commit. This is usually a description of what features were added or what bugs were fixed.\nAdditionally, git supports tagging, which allows us to mark a specific commit as a specific version of our code (e.g. ‘2.4.5’).\n\n\n\n\nIt’s often important to see content of the actual changes that were made. This can be useful when:\n\ntracking down when and how a bug was introduced\nunderstanding the changes a team member made so you can stay up-to-date with progress\nreviewing code as a team for correctness or quality/style\n\nGit allows us to easily see these changes (called a diff) for any given commit.\n\n\n\nGit enables you to work using a non-linear workflow. This means that you can have multiple versions of a project or “branches” with different save points, or “commits”, simultaneously within the same folder and easily toggle bgttween them. You can split new branches off a project when you’re looking to experiment or implement a new feature, and you can merge those branches back into the main (formerly known as “master”) branch when you’re ready to incorporate them into a project.\n\n\n\n\nIn developing software, we often want to experiment in adding a feature or refactoring (rewriting) existing code. Because git makes it easy to go back to a known good state, we can experiment without worrying that we’ll be unable to undo the experimental work.\n\n\n\n\n\n\nGit is a distributed version control system. It is a technology.\n\nGitHub is a social coding platform where git repositories are stored and where people can collaborate on projects. GitHub is great both for collaboration within your organization, but also provides an excellent model for open source collaboration across organizations or with the public.\n\nOn GitHub you can find Git repositories.\n\n\nLearn More: https://jahya.net/blog/git-vs-github/\n\n\n\nThe basics:\n\nGit - version control software\nRepository - a folder containing your files and also containing a structure that helps keep track of changes in those files. When you intialize a repository, git creates a hidden folder (.git folder) that stores the changes to those files.\nCommit - the basic unit of a git repository is a commit. It is a set of changes to a file. A commit usually comes with an id as well as a commit message that describes the change.\n\nWorking with others:\n\nGitHub - a place to host git repositories and collaborate\nLocal Repository - the version of a git repository on your local computer\nRemote Repository - the version of a git repository stored somewhere else that your local repository is connected to (frequently on GitHub)"
  },
  {
    "objectID": "git files/section 2/section 2.html#what-is-git",
    "href": "git files/section 2/section 2.html#what-is-git",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "",
    "text": "Keeping track of file versions is hard.\n\n\n\nAbove all else, Git is a fast and distributed version control system, that allows you to efficiently handle projects large and small.\nHere are some problems we face as developers, and how git solves them:\n\n\nGit allows us to make save points at any time. These save points are called ‘commits’. Once a save point is made, it’s permanent, and allows us to go back to that save point at any time. From there, we can see what the code looked like at that point, or even start building off that version.\n\n\n\n\nEvery commit has a description (commit message), which allows us to describe what changes were made between the current and previous commit. This is usually a description of what features were added or what bugs were fixed.\nAdditionally, git supports tagging, which allows us to mark a specific commit as a specific version of our code (e.g. ‘2.4.5’).\n\n\n\n\nIt’s often important to see content of the actual changes that were made. This can be useful when:\n\ntracking down when and how a bug was introduced\nunderstanding the changes a team member made so you can stay up-to-date with progress\nreviewing code as a team for correctness or quality/style\n\nGit allows us to easily see these changes (called a diff) for any given commit.\n\n\n\nGit enables you to work using a non-linear workflow. This means that you can have multiple versions of a project or “branches” with different save points, or “commits”, simultaneously within the same folder and easily toggle bgttween them. You can split new branches off a project when you’re looking to experiment or implement a new feature, and you can merge those branches back into the main (formerly known as “master”) branch when you’re ready to incorporate them into a project.\n\n\n\n\nIn developing software, we often want to experiment in adding a feature or refactoring (rewriting) existing code. Because git makes it easy to go back to a known good state, we can experiment without worrying that we’ll be unable to undo the experimental work."
  },
  {
    "objectID": "git files/section 2/section 2.html#git-versus-github",
    "href": "git files/section 2/section 2.html#git-versus-github",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "",
    "text": "Git is a distributed version control system. It is a technology.\n\nGitHub is a social coding platform where git repositories are stored and where people can collaborate on projects. GitHub is great both for collaboration within your organization, but also provides an excellent model for open source collaboration across organizations or with the public.\n\nOn GitHub you can find Git repositories.\n\n\nLearn More: https://jahya.net/blog/git-vs-github/"
  },
  {
    "objectID": "git files/section 2/section 2.html#some-vocabulary",
    "href": "git files/section 2/section 2.html#some-vocabulary",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "",
    "text": "The basics:\n\nGit - version control software\nRepository - a folder containing your files and also containing a structure that helps keep track of changes in those files. When you intialize a repository, git creates a hidden folder (.git folder) that stores the changes to those files.\nCommit - the basic unit of a git repository is a commit. It is a set of changes to a file. A commit usually comes with an id as well as a commit message that describes the change.\n\nWorking with others:\n\nGitHub - a place to host git repositories and collaborate\nLocal Repository - the version of a git repository on your local computer\nRemote Repository - the version of a git repository stored somewhere else that your local repository is connected to (frequently on GitHub)"
  },
  {
    "objectID": "git files/section 2/section 2.html#lets-dive-in",
    "href": "git files/section 2/section 2.html#lets-dive-in",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "Lets Dive In!",
    "text": "Lets Dive In!\nVocab\n\nlocal repository a folder stored on your computer\nremote repository a folder stored on on GitHub\n\nLets take a look at a repository that is on GitHub.\n\nhttps://github.com/code4MDM/simple-r-script"
  },
  {
    "objectID": "git files/section 2/section 2.html#some-of-githubs-features",
    "href": "git files/section 2/section 2.html#some-of-githubs-features",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "(Some of) GitHub’s Features",
    "text": "(Some of) GitHub’s Features\n\nThe README.md file\nGithub looks for a “readme” file and renders it as you’re navigating through the file structure. This is a great way to guide people through your code.\n\nReadme files are often given the .md extension, meaning they’re written in a language called markdown that allows for nicer formatting. You can check out this markdown cheet sheet (https://www.markdownguide.org/cheat-sheet/) if you want to see how formatting works, but you can also save a readme files as plain text. Github will also detect .txt files, or you can just write plain text inside your .md file.\n\n\nCommit Log\n\nVocab\n\ncommit - the basic unit of a git repository is a commit. It is a set of changes to a file. A commit usually comes with an commit id as well as a commit message that describes the change.\ncommit log (aka Git History) - all of the commits (previous changes) to all of the files in your repository\n\n https://github.com/dmil/my-simple-website/commits/master\n\n\nHistory, Raw, and Blame for any file\n\n\nRaw - actual contents of the file without any formatting applied.\n\nexample: https://github.com/fivethirtyeight/chartbuilder-2/blob/master/src/styles/core.scss\n\nHistory - every change ever made to that file within this branch.\n\nexample: https://github.com/fivethirtyeight/chartbuilder-2/commits/master/src/styles/core.scss\n\nBlame - provenance of each line currently in the file you’re looking at in the branch you’re looking at\n\nexample: https://github.com/fivethirtyeight/chartbuilder-2/blame/master/src/styles/core.scss\n\n\n\n\nBranches\n\n\nPull Requests\n\n\nProposing Edits\n\nYou can edit a file in GitHub in a new branch, thus proposing a change without actually making the change in the master branch. Just make sure to leave a meaningful description of the change you made in the commit message.\n\n\n\nDrag and Drop\n\n\n\nCollaboration\nCollaborators can push to the repository without asking your permission, they have full read and write access.\n\nIf I wasn’t a collaborator, I could still work with you on an open source project through a process called forking where I can make a copy of your repository in my GitHub account, make changes, and request that you merge them back into your project. We will discuss forking more in depth later.\n\n\nServing up Websites!\nGitHub is also great for serving up static websites. GitHub is only storing the code. Luckily, if your code happens to be a website, GitHub can also host it for you through a feature called “GitHub Pages”.\nSimply go to the “settings” menu, scroll down to “GitHub Pages”, and select “master branch”\n\nWhatever is in your master branch on GitHub should now appear at\nhttp://your-username.github.io/repository-name\nin my case it is http://madisoncoots.github.io/"
  },
  {
    "objectID": "git files/section 2/section 2.html#github-for-things-other-than-code",
    "href": "git files/section 2/section 2.html#github-for-things-other-than-code",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "GitHub for things other than code",
    "text": "GitHub for things other than code\n\nAuditing system for changes on a file\nFor collaboratively editing a text document\nFor drafting government web design standards!\nOpen comment period for policy\nDrafting and collaborating on legal documents\nDesign (image diff)\n\nhttps://help.github.com/articles/rendering-and-diffing-images/\n\nOpen journalsim showcase\n\nhttps://github.com/showcases/open-journalism\n\nGithub for Government\n\nhttps://government.github.com/\nhttps://government.github.com/community/"
  },
  {
    "objectID": "git files/section 2/section 2.html#some-vocabulary-1",
    "href": "git files/section 2/section 2.html#some-vocabulary-1",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "Some Vocabulary",
    "text": "Some Vocabulary\n\nGit - version control software\nRepository - a folder containing your files and also containing a structure that helps keep track of changes in those files. When you intialize a repository, git creates a hidden folder (.git folder) that stores the changes to those files.\nGitHub - a place to host git repositories and collaborate\nLocal Repository - the version of a git repository on your local computer\nRemote Repository - the version of a git repository stored somewhere else that your local repository is connected to (frequently on GitHub)\nCommit - the basic unit of a git repository is a commit. It is a set of changes to a file. A commit usually comes with an id as well as a commit message that describes the change.\n\nWithin a Repository you have\n\nUntracked Changes - files that are in your folder but that git doesn’t pay attention to.\nStaging Area - a place where you can put files before you commit them. Once files are in the staging area, git is paying attention to them.\nCommit Log (aka Git History) - all of the commits (previous changes) to all of the files in your repository."
  },
  {
    "objectID": "git files/section 2/section 2.html#components-of-a-git-repository",
    "href": "git files/section 2/section 2.html#components-of-a-git-repository",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "Components of a Git Repository",
    "text": "Components of a Git Repository\n\n\nThe working directory\n\n\ngit init creates a git repo inside current working directory. This means that this command can turn a regular folder into a git repository by generating a hidden .git folder that starts to keep track of changes.\ngit clone takes a git repo from somewhere else and makes a copy of that repo into your current working directory. We will frequently be cloning repos from GitHub.\n\n\nThe staging area\n\n\ngit add . adds changes from the working directory to the staging area\ngit add &lt;filename&gt; adds changes to filenames specified from the working directory to the staging area\n\n\nThe commit\n\n\ngit commit -m \"commit message\" adds changes in staging area to the repository\ngit log shows\n\nProtip: Run git status after each command in the beginning because it allows you to visualize what just happaned."
  },
  {
    "objectID": "git files/section 2/section 2.html#pushing-to-github",
    "href": "git files/section 2/section 2.html#pushing-to-github",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "Pushing to GitHub",
    "text": "Pushing to GitHub\n\nKey Terms\n\ngithub - a service that hosts git remote repositories, and provides a web app to interact / collaborate on them\nremote - another repository that can be syncronized with a remote\nupstream - the name for a remote read-only repository\norigin - the name for a remote read-and-write repository\nclone - download an entire remote repository, to be used as a local repository\nfetch - downloading the set of changes (commits) from a remote repository\npull - fetching changes and merging them into the current branch\n\n\nIn order to show your remotes, you can run git remote -v show. The default remote is named “origin”\nIn order to push, you run git push. By default this will push from the branch you are on to a remote branch with the same name. (If you’d like to specify a branch, you can do that. The full formulation of this command is git push &lt;remote&gt; &lt;branch&gt;. So, for example you might say git push origin main to push to the “main” branch of the “origin” remote.)"
  },
  {
    "objectID": "git files/section 2/section 2.html#example-simple-rscript",
    "href": "git files/section 2/section 2.html#example-simple-rscript",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "❇️ Example: simple-rscript",
    "text": "❇️ Example: simple-rscript\nLet’s give it a try! We’re going to clone a repository for a simple R script from GitHub down to our computer where we can work with it locally. We will make some edits to the code, commit those changes and then push the changes back up to the remote repository in GitHub.\nhttps://github.com/code4MDM/simple-r-script"
  },
  {
    "objectID": "git files/Intro Git.html",
    "href": "git files/Intro Git.html",
    "title": "Introduction to Git and GitHub",
    "section": "",
    "text": "Welcome to the Introduction to Git and GitHub. This is a collection of notes and references for using Git and GitHub. The goal is to provide a quick introduction and reference for common commands and workflows.\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nGit Installation Instructions\n\n\n\n\n\n\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nPart 1: Git Installation and SSH Instructions\n\n\n\n\n\nHelp on installing git and setting up SSH keys for GitHub\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nPart 2: Introduction to Git and GitHub\n\n\n\n\n\nIn this guide, we’ll cover the basics of Git and GitHub\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nPart 3: Branching\n\n\n\n\n\n\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nPart 4: Reference\n\n\n\n\n\nA reference guide for git commands\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nPart 5: Workflow\n\n\n\n\n\nA reference guide for git commands\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nPart 6: Additional Resources\n\n\n\n\n\nLinks to additional git resource\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "",
    "text": "“There is certainly no unanimity on exactly what centrality is or on its conceptual foundations, and there is little agreement on the proper procedure for its measurement.” - Linton Freeman (1977)\nSocial network analysis can be used to measure the importance of a person as a function of the social structure of a community or organization. This post uses visualization as a tool to explain how different measures of centrality may be used to analyze different questions in a network analysis. In these examples we will be specifically looking at directed graphs to compare the following centrality measures and their use-cases:\n\nDegree Centrality\nBetweenness Centrality\nEigenvector Centrality\nKatz Centrality\nHITS Hubs and Authorities\n\nAn example of a directed graph would be one in which people nominate their top 2 friends. In this graph, nodes (people) would connect to others nodes through directed edges (nominations). It is possible for Jacob to nominate Jenna without Jenna nominating him back. You can imagine why centrality in a friendship network might take into account the direction of these nominations. If I list 100 people as my friends and none of them list me back, do we think I am a popular person?"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#social-network-analysis",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#social-network-analysis",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "",
    "text": "“There is certainly no unanimity on exactly what centrality is or on its conceptual foundations, and there is little agreement on the proper procedure for its measurement.” - Linton Freeman (1977)\nSocial network analysis can be used to measure the importance of a person as a function of the social structure of a community or organization. This post uses visualization as a tool to explain how different measures of centrality may be used to analyze different questions in a network analysis. In these examples we will be specifically looking at directed graphs to compare the following centrality measures and their use-cases:\n\nDegree Centrality\nBetweenness Centrality\nEigenvector Centrality\nKatz Centrality\nHITS Hubs and Authorities\n\nAn example of a directed graph would be one in which people nominate their top 2 friends. In this graph, nodes (people) would connect to others nodes through directed edges (nominations). It is possible for Jacob to nominate Jenna without Jenna nominating him back. You can imagine why centrality in a friendship network might take into account the direction of these nominations. If I list 100 people as my friends and none of them list me back, do we think I am a popular person?"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#simulate-our-data",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#simulate-our-data",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Simulate Our Data",
    "text": "Simulate Our Data\nFor our simulated data we are going to be looking at a classroom that contains 14 male and 14 female students. Suppose that each student was asked to name their top 2 male and top 2 female friends in the class. We are interested in analyzing a slew of different research questions where the centrality of student in the class may be of importance.\nLet’s create the data:\n\nlibrary(tidyverse)\n\nWe begin with a 4 vectors: all males in the classroom, all females in the classroom, a probability distribution for selecting friends of the same sex, a probability distribution for selecting friends of the opposite sex.\n\nmales &lt;- c('Jacob', 'Louis', 'Chris', 'Wyatt', 'Nolan', 'Robert', \n           'Zach', 'John','Bob', 'David', 'Avery', 'Ronald', \n           'Dallas', 'Dylan')\n\nfemales &lt;- c('Bohan', 'Jenna', 'Katarina', 'Hassina', 'Towo', \n             'Becca', 'Meredith', 'Gracie', 'Kayla', 'Marlene', \n             'Jade', 'Allyssa', 'Reigne', 'Wendy')\n\nprobs.diff.sex = c(0.15,0.15,0.05,0.05,0.1,0.1,0.05,0.1,0.01,0.09,0.05,0.05,0.025,0.025)\nprobs.same.sex = c(0.15,0.15,0.05,0.05,0.1,0.1,0.05,0.1,0.01,0.09,0.05,0.05,0.05)\n\nThe function below, simulate.top.friends, will produce a dataframe that will contain each student’s picks for their top 2 male and top 2 female friends in the classroom.\n\nset.seed(1997)\n\nsimulate.top.friends &lt;- function(males, females, probs.diff.sex, probs.same.sex) {\n\n  dat &lt;- setNames(data.frame(matrix(ncol = 6, nrow = 0)), \n                  c(\"Ego\", \"Ego Sex\", \"MF1\", \"MF2\", \"FF1\", \"FF2\"))\n\n  for (ego in males) {\n    temp.males &lt;- males[! males %in% ego]\n    \n    male.friends.i &lt;- sample.int(13, 2, replace = FALSE, prob = probs.same.sex)\n    female.friends.i &lt;- sample.int(14, 2, replace = FALSE, prob = probs.diff.sex)\n    \n    male.friend.1 &lt;- temp.males[male.friends.i[1]]\n    male.friend.2 &lt;- temp.males[male.friends.i[2]]\n    \n    female.friend.1 &lt;- females[female.friends.i[1]]\n    female.friend.2 &lt;- females[female.friends.i[2]]\n    \n    dat[nrow(dat) + 1,] = c(ego, 'Male', male.friend.1, male.friend.2, \n                           female.friend.1, female.friend.2)\n    \n  }\n    for (ego in females) {\n    temp.females &lt;- females[! females %in% ego]\n    \n    male.friends.i &lt;- sample.int(14, 2, replace = FALSE, prob = probs.diff.sex)\n    female.friends.i &lt;- sample.int(13, 2, replace = FALSE, prob = probs.same.sex)\n    \n    male.friend.1 &lt;- males[male.friends.i[1]]\n    male.friend.2 &lt;- males[male.friends.i[2]]\n    \n    female.friend.1 &lt;- temp.females[female.friends.i[1]]\n    female.friend.2 &lt;- temp.females[female.friends.i[2]]\n    \n    dat[nrow(dat) + 1,] = c(ego, 'Female', male.friend.1, male.friend.2, \n                           female.friend.1, female.friend.2)\n    \n  }\n  return(dat)\n}\n\nLet’s take a look at our friendship data that we will be working with!\n\nsimulate.top.friends(males,females, probs.diff.sex, probs.same.sex)\n\n\n\n\n\n\n\nEgo\nEgo Sex\nMF1\nMF2\nFF1\nFF2\n\n\n\n\nJacob\nMale\nChris\nAvery\nKatarina\nMeredith\n\n\nLouis\nMale\nJacob\nAvery\nTowo\nHassina\n\n\nChris\nMale\nJacob\nRobert\nGracie\nBecca\n\n\nWyatt\nMale\nNolan\nJacob\nMarlene\nJenna\n\n\nNolan\nMale\nJacob\nBob\nBohan\nTowo\n\n\nRobert\nMale\nNolan\nLouis\nJenna\nBohan\n\n\nZach\nMale\nJacob\nAvery\nMeredith\nTowo\n\n\nJohn\nMale\nBob\nLouis\nBohan\nWendy\n\n\nBob\nMale\nAvery\nJacob\nJenna\nWendy\n\n\nDavid\nMale\nWyatt\nLouis\nBecca\nJenna\n\n\nAvery\nMale\nLouis\nJacob\nJenna\nBecca\n\n\nRonald\nMale\nJacob\nRobert\nBecca\nWendy\n\n\nDallas\nMale\nRobert\nJacob\nBohan\nJenna\n\n\nDylan\nMale\nJacob\nDavid\nTowo\nMarlene\n\n\nBohan\nFemale\nRobert\nNolan\nAllyssa\nKayla\n\n\nJenna\nFemale\nDavid\nBob\nKatarina\nMeredith\n\n\nKatarina\nFemale\nDavid\nNolan\nGracie\nReigne\n\n\nHassina\nFemale\nRobert\nDavid\nBecca\nJade\n\n\nTowo\nFemale\nJohn\nNolan\nJade\nJenna\n\n\nBecca\nFemale\nNolan\nDallas\nKayla\nTowo\n\n\nMeredith\nFemale\nLouis\nJacob\nTowo\nBohan\n\n\nGracie\nFemale\nJacob\nRonald\nBohan\nHassina\n\n\nKayla\nFemale\nRonald\nLouis\nBohan\nGracie\n\n\nMarlene\nFemale\nJohn\nRobert\nHassina\nTowo\n\n\nJade\nFemale\nRonald\nLouis\nHassina\nBohan\n\n\nAllyssa\nFemale\nAvery\nZach\nGracie\nBohan\n\n\nReigne\nFemale\nJohn\nZach\nBohan\nHassina\n\n\nWendy\nFemale\nNolan\nJacob\nTowo\nBohan"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#creating-a-graphing-object",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#creating-a-graphing-object",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Creating a Graphing Object",
    "text": "Creating a Graphing Object\nThere are many available centrality measures that have been developed for network analysis. At this time, there are no packages that are so comprehensive that it includes all of the measures. I will, therefore, limit this discussion to a subset of the measures that are included in igraph.\nigraph has a really great function that allows us to turn a dataframe into an igraph object. However, the function requires our data to be in “long” form so we will need to do some reshaping. Let’s restructure our data such that each row represents one directed friend nomination. We are going to call this “Source-Target Form”.\n\nlibrary(reshape) \n\nfriendships &lt;- melt(simulate.top.friends(males,females,probs.diff.sex, probs.same.sex), \n                    id=c(\"Ego\", \"Ego Sex\")) %&gt;%\n  select(source=Ego, source_sex =`Ego Sex`, target=value) %&gt;% \n  arrange(source)\n\nLet’s look at the first 10 observations so we can understand the format needed to turn this data into an igraph object.\n\nhead(friendships, 10)\n\n\n\n\n\n\n\nsource\nsource_sex\ntarget\n\n\n\n\nAllyssa\nFemale\nDavid\n\n\nAllyssa\nFemale\nNolan\n\n\nAllyssa\nFemale\nJenna\n\n\nAllyssa\nFemale\nTowo\n\n\nAvery\nMale\nJohn\n\n\nAvery\nMale\nChris\n\n\nAvery\nMale\nAllyssa\n\n\nAvery\nMale\nMeredith\n\n\nBecca\nFemale\nChris\n\n\nBecca\nFemale\nWyatt\n\n\n\n\n\n\n\n\nWe will use the graph_from_data_frame function to create the igraph object.\n\nlibrary(igraph)\nnetwork &lt;- graph_from_data_frame(friendships[,c('source','target','source_sex')],\n                                 directed = TRUE)\n\nnetwork\n\nIGRAPH 006a09e DN-- 28 112 -- \n+ attr: name (v/c), source_sex (e/c)\n+ edges from 006a09e (vertex names):\n [1] Allyssa-&gt;David    Allyssa-&gt;Nolan    Allyssa-&gt;Jenna    Allyssa-&gt;Towo    \n [5] Avery  -&gt;John     Avery  -&gt;Chris    Avery  -&gt;Allyssa  Avery  -&gt;Meredith\n [9] Becca  -&gt;Chris    Becca  -&gt;Wyatt    Becca  -&gt;Jade     Becca  -&gt;Jenna   \n[13] Bob    -&gt;Dylan    Bob    -&gt;David    Bob    -&gt;Marlene  Bob    -&gt;Wendy   \n[17] Bohan  -&gt;Nolan    Bohan  -&gt;John     Bohan  -&gt;Katarina Bohan  -&gt;Reigne  \n[21] Chris  -&gt;Jacob    Chris  -&gt;Avery    Chris  -&gt;Bohan    Chris  -&gt;Allyssa \n[25] Dallas -&gt;Louis    Dallas -&gt;Robert   Dallas -&gt;Jenna    Dallas -&gt;Towo    \n[29] David  -&gt;Zach     David  -&gt;Dallas   David  -&gt;Bohan    David  -&gt;Jenna   \n+ ... omitted several edges\n\n\nLet’s better understand the information contained in an igraph object:\n\nIGRAPH simply annotates network as an igraph object\nWhatever random six digit alphanumeric string follows IGRAPH is simply how igraph identifies the graph for itself, it’s not important for our purposes.\nD would tell us that it is directed graph\nN indicates that network is a named graph, in that the vertices have a name attribute\n– refers to attributes not applicable to network, but we will see them in the future:\n28 refers to the number of vertices in network\n112 refers to the number of edges in network\nattr: is a list of attributes within the graph.\n(v/c), which will appear following name, tells us that it is a vertex attribute of a character data type.\n(e/c) or (e/n) referring to edge attributes that are of character or numeric data types\n\nedges from arbitrary igraph name (vertex names): lists a sample of network’s edges using the names of the vertices which they connect.\n\n\nNow let’s create a rough plot to look at our network!\n\nlay &lt;- layout_with_kk(network)\n\npar(bg=\"grey98\")\nplot(network, layout = lay, edge.color=\"grey80\",\n     vertex.color=\"lightblue\", vertex.label.color = \"black\")\n\n\n\n\n\n\n\n\nFor the rest of this post, we are going to talk about a few different measures of centrality, what they capture mathematically and intuitively, and we will look at plots where the size of the node corresponds to the relative centrality score."
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#measures-of-centrality",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#measures-of-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Measures of Centrality",
    "text": "Measures of Centrality\n\nDegree Centrality\nFor directed graphs, in-degree, or number of incoming points, is one way we can determine the importance factor for nodes. The Degree of a node is the number of edges that it has. The basic intuition is that, nodes with more connections are more influential and important in a network. In other words, the people with more friend nominations in our simulated social network are the ones with greater importance according to this metric.\n\nDegree.Directed &lt;- degree(network)\nIndegree &lt;- degree(network, mode=\"in\")\nOutdegree &lt;- degree(network, mode=\"out\")\n\nCompareDegree &lt;- cbind(Degree.Directed, Indegree, Outdegree)\n\n\nhead(CompareDegree, 10)\n\n\n\n\n\n\n\n\nDegree.Directed\nIndegree\nOutdegree\n\n\n\n\nAllyssa\n7\n3\n4\n\n\nAvery\n7\n3\n4\n\n\nBecca\n7\n3\n4\n\n\nBob\n7\n3\n4\n\n\nBohan\n12\n8\n4\n\n\nChris\n7\n3\n4\n\n\nDallas\n7\n3\n4\n\n\nDavid\n11\n7\n4\n\n\nDylan\n7\n3\n4\n\n\nGracie\n5\n1\n4\n\n\n\n\n\n\n\n\nThis is a very reasonable way to measure importance within a network. If we are trying to determine who in our classroom is the most popular, we might define that as the greatest number of friendship nominations.\n\nlay &lt;- layout_with_kk(network)\n\npar(bg=\"grey98\")\nplot(network, layout = lay, edge.color=\"grey80\",\n     vertex.size=degree(network, mode=\"in\")*2,  # Rescaled by multiplying by 2\n     main=\"In-Degree\", vertex.label.dist=1.5,\n     vertex.color=\"lightblue\", vertex.label.color = \"black\")"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#betweenness-centrality",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#betweenness-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Betweenness Centrality",
    "text": "Betweenness Centrality\nBetweenness Centrality is another centrality that is based on shortest path between nodes. It is determined as number of the shortest path passing by the given node. For starting node \\(s\\), destination node \\(t\\) and the input node \\(i\\) that holds \\(s \\ne t \\ne i\\), let \\(n_{st}^i\\) be 1 if node \\(i\\) lies on the shortest path between \\(s\\) and \\(t\\); and \\(0\\) if not. So the betweenness centrality is defined as:\n\\[x_i = \\sum_{st} n_{st}^i\\] However, there can be more than one shortest path between \\(s\\) and \\(t\\) and that will count for centrality measure more than once. Thus, we need to divide the contribution to \\(g\\_{st}\\), total number of shortest paths between \\(s\\) and \\(t\\).\n\\[x_i = \\sum_{st} \\frac{n_{st}^i}{g_{st}}\\]\nEssentially the size of the node here represents the frequency with which that node lies on the shortest path between other nodes.\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=betweenness(network)*0.25,  # Rescaled by multiplying by 0.25\n     main=\"Betweenness Centrality\", vertex.color=\"lightblue\")"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#eigenvector-centrality",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#eigenvector-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Eigenvector Centrality",
    "text": "Eigenvector Centrality\nEigenvector centrality is a basic extension of degree centrality, which defines centrality of a node as proportional to its neighbors’ importance. When we sum up all connections of a node, not all neighbors are equally important. This is a very interesting way to measure popularity in our classroom where we say the popularity of your friends matters more than the number of friends. Let’s consider two nodes in a friend network with same degree, the one who is connected to more central nodes should be more central.\nFirst, we define an initial guess for the centrality of nodes in a graph as \\(x_i=1\\). Now we are going to iterate for the new centrality value \\(x_i'\\) for node \\(i\\) as following:\n\\[x_i' = \\sum_{j} A_{ij}x_j\\]\nHere \\(A_{ij}\\) is an element of the adjacency matrix, where it gives 1 or 0 for whether an edge exists between nodes \\(i\\) and \\(j\\). it can also be written in matrix notation as \\(\\mathbf{x'} = \\mathbf{Ax}\\).\nWe iterate over t steps to find the vector \\(\\mathbf{x}(t)\\) as:\n\\[\\mathbf{x}(t) = \\mathbf{A^t x}(0)\\]\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=evcent(network)$vector*15,   # Rescaled by multiplying by 15\n     main=\"Eigenvector Centrality\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\nThe plot shows, the students which have the same number of friend nominations are not necessarily in the same size. The one that is connected to more central, or “popular” nodes are larger in this visualization.\nHowever, as we can see from the definition, this can be a problematic measure for directed graphs. Let’s say that a student who received no friend nominations themselves nominates another student as a friend. Because that person has 0 friend nominations themselves, they would not contribute any importance to the person they nominated. In other words, eigenvector centrality would not take zero in-degree nodes into account in directed graphs.\nHowever there is a solution to this!"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#katz-centrality",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#katz-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Katz Centrality",
    "text": "Katz Centrality\nKatz centrality introduces two positive constants \\(\\alpha\\) and \\(\\beta\\) to tackle the problem of eigenvector centrality with zero in-degree nodes:\n\\[x_i = \\alpha \\sum_{j} A_{ij} x_j + \\beta\\],\nagain \\(A_{ij}\\) is an element of the adjacency matrix, and it can also be written in matrix notation as \\(\\mathbf{x} = \\alpha \\mathbf{Ax} + \\beta \\mathbf{1}\\). This \\(\\beta\\) constant gives a free centrality contribution for all nodes even though they don’t get any contribution from other nodes. The existence of a node alone would provide it some importance. \\(\\alpha\\) constant determines the balances between the contribution from other nodes and the free constant.\nUnfortunately, igraph does not have a function to compute Katz centrality, so we will need to do it the old fashioned way.\n\nkatz.centrality = function(g, alpha, beta, t) {\n  n = vcount(g);\n  A = get.adjacency(g);\n  x0 = rep(0, n);\n  x1 = rep(1/n, n);\n  eps = 1/10^t;\n  iter = 0;\n  while (sum(abs(x0 - x1)) &gt; eps) {\n    x0 = x1;\n    x1 = as.vector(alpha * x1 %*% A) + beta;\n    iter = iter + 1;\n  } \n  return(list(aid = x0, vector = x1, iter = iter))\n}\n\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=katz.centrality(network, 0.1, 1, 0.01)$vector*5,   # Rescaled by multiplying by 15\n     main=\"Katz Centrality\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\nAlthough this method is introduced as a solution for directed graphs, it can be useful for some applications of undirected graphs as well."
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#hits-hubs-and-authorities",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#hits-hubs-and-authorities",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "HITS Hubs and Authorities",
    "text": "HITS Hubs and Authorities\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=hub.score(network)$vector*15,   # Rescaled by multiplying by 15\n     main=\"HITS Hubs\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5 , vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=authority.score(network)$vector*15,   # Rescaled by multiplying by 15\n     main=\"HITS Authorities\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\nUp until this point, we have discussed the measures that captures high node centrality, however, there can be nodes in the network which are important for the network, but they are not central. In order to find out such nodes, the HITS algorithm introduces two types of central nodes: Hubs and Authorities. For Hubs, we might consider a node important if it links to many highly nominated nodes (i.e. the person nominates many popular people as their friends). For Authorities, we might consider a node to be of importance if many highly nominated nodes link to it (i.e. nominated by many popular people).\nAuthority Centrality is defined as the sum of the hub centralities which point to the node (i):\n\\[x_i = \\alpha \\sum_{j} A_{ij} y_j,\\]\nwhere \\(\\alpha\\) is constant. Likewise, Hub Centrality is the sum of the authorities which are pointed by the node \\(i\\):\n\\[y_i = \\beta \\sum_{j} A_{ji} x_j,\\]\nwith constant \\(\\beta\\). Here notice that the element of the adjacency matrix are swapped for Hub Centrality because we are concerned with outgoing edges for hubs. So in matrix notation:\n\\[\\mathbf{x} = \\alpha \\mathbf{Ay}, \\quad\\]\n\\[\\mathbf{y} = \\beta \\mathbf{A^Tx}.\\] As it can be seen from the drawing, HITS Algorithm also tackles the problem with zero in-degree nodes of Eigenvector Centrality. These zero in-degree nodes become central hubs and contribute to other nodes. Yet we can still use a free centrality contribution constant like in Katz Centrality or other variants.\nAlthough these measures are generally used in a citation network or the internet, this can be pretty interesting in the context of our classroom frienship network. Maybe we are interested in knowing who are the non-popular students that nominate many popular students as friends, but receive no reciprocity in terms of nominations."
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#other-packages-that-i-like-for-visualization",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#other-packages-that-i-like-for-visualization",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Other Packages that I Like for Visualization",
    "text": "Other Packages that I Like for Visualization\nI want to conclude this post with some sample code to produce a nice network plot using the ggnetwork package. igraph is only one package that exists for network visualization and centrality calculations, I encourage you to check out additional packages which may be stronger than igraph for some purposes.\n\nlibrary(GGally)\nlibrary(network)\nlibrary(ggnetwork)\n\nnet &lt;- list(nodes=friendships[c('source', 'target', 'source_sex')], \n            edges=friendships[c('source', 'target', 'source_sex')])\n\n# create node attribute data\nnet.cet &lt;- as.character(net$nodes$source_sex)\nnames(net.cet) = net$nodes$source\nedges &lt;- net$edges\n\n# create network\nnet.net &lt;- edges[, c(\"source\", \"target\") ]\nnet.net &lt;- network::network(net.net, directed = TRUE)\n\n# create sourc sex node attribute\nnet.net %v% \"source_sex\" &lt;- net.cet[ network.vertex.names(net.net) ]\n\n\nset.seed(1)\nggnet2(net.net, color = \"source_sex\",\n       palette = c(\"Female\" = \"purple\", \"Male\" = \"maroon\"), size = 'indegree',\n       arrow.size = 3, arrow.gap = 0.04, alpha = 1,  label = TRUE, vjust = 2.5, label.size = 3.5,\n       edge.alpha = 0.5, mode = \"kamadakawai\",edge.color = 'grey50',\n       color.legend = \"Student Sex\") + theme_bw() + theme_blank()  + \n  theme(legend.position = \"bottom\", text = element_text(size = 15),\n        plot.caption = element_text(hjust = 0)) + guides(size=F) + \n  labs(title='Social Network Mapping of Friendship Nominations for Simulated Data', \n       caption = str_wrap(\"The size of the dots represent the number of \n                          friendship nominations by others. The maroon dots represent males \n                          while the purple dots represent females. Directed edges are used \n                          to indicate the direction of friendship nominations. \n                          Edges that are bi-directional indicated friendship reciprocity. \n                          \\n\\n This particular network represents the frienship nominations \n                          of 28 students from our simulated data.\", 128))\n\n\n\n\n\n\n\n\nThank you!\nJacob"
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "",
    "text": "Odds ratios have been a long-standing mainstay in health research. Their use can be found sprinkled throughout countless academic journals and research papers. But just because something is popular doesn’t mean it’s the best choice for all contexts. The purpose of this article is to shed light on the limitations of odds ratios, particularly when it comes to interpretation, and advocate for the more intuitive marginal effects and predicted probabilities."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#the-math-behind-odds-ratios",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#the-math-behind-odds-ratios",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "The math behind odds ratios",
    "text": "The math behind odds ratios\nTo understand why odds ratios can be confusing, let’s briefly discuss what they represent.\nIn a logistic regression, the fundamental equation is:\n\\[log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ...\\]\nWhere \\(p\\) is the probability of an event occurring.\nThe odds ratio for a given predictor (let’s say \\(X_1\\)) is simply the exponentiated coefficient for that predictor:\nOdds Ratio for \\(X_1 = E^{\\beta_1}\\)\nIt represents the multiplicative change in the odds of the outcome for a one-unit increase in \\(X_1\\) , holding other variables constant. If this sounds confusing, you’re not alone."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#when-odds-ratios-mislead",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#when-odds-ratios-mislead",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "When Odds Ratios Mislead",
    "text": "When Odds Ratios Mislead\nThe challenge with odds ratios arises particularly when the outcome of interest is common. The reason being, odds ratios tend to exaggerate relative risks for common outcomes.\nLet’s say we find an odds ratio of 2 for a drug reducing heart attack risk. Many might (mistakenly) interpret this as “patients on the drug are twice as likely to avoid heart attacks.” In reality, they have twice the odds, which doesn’t translate directly to actual probabilities, especially when the event is common."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#marginal-effects-clarity-over-confusion",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#marginal-effects-clarity-over-confusion",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Marginal Effects: Clarity over Confusion",
    "text": "Marginal Effects: Clarity over Confusion\nMarginal effects shine where odds ratios falter. They convey the change in the probability of an outcome for a one-unit change in the predictor, holding other variables constant. It’s a direct measure that offers a tangible understanding of impact.\nMathematically, for a binary predictor (like drug use: yes or no):\n\\[\\text{Marginal Effect} (dy/dx) = P(Y = 1|X=1) - P(Y=1|X=0)\\]\nFor continuous predictors, the marginal effect represents the derivative of the probability with respect to the predictor, which essentially gives us the rate of change."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#predicted-probabilities-concrete-scenarios-over-abstract-odds",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#predicted-probabilities-concrete-scenarios-over-abstract-odds",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Predicted Probabilities: Concrete Scenarios Over Abstract Odds",
    "text": "Predicted Probabilities: Concrete Scenarios Over Abstract Odds\nPredicted probabilities go a step further, providing the likelihood of an outcome under specific conditions. Instead of leaving things in the realm of odds and abstract increases, we can state tangible scenarios."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#demonstrating-with-real-world-data",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#demonstrating-with-real-world-data",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Demonstrating with “Real-World” Data",
    "text": "Demonstrating with “Real-World” Data\nTo provide concrete evidence of the limitations of odds ratios, let’s explore a simulated dataset that mirrors a health study on heart attacks, age, and drug use for high-risk individuals.\nFirst, we’ll generate a dataset with age as a continuous predictor and drug_use as a binary predictor:\nSimulate data\n\n\nShow the code\n# Setting the seed for reproducibility\nset.seed(1234)\n\nn &lt;- 1000\n\n# Age distribution: Normally distributed between 40 and 70\nage &lt;- rnorm(n, 55, 7.5)\n\n# Drug use: 50% of the participants use the drug\ndrug_use &lt;- rbinom(n, 1, 0.5)\n\n# Log odds of heart attack: Base risk, then increasing slightly with age, \n# and decreasing with drug use (this is where we're setting the odds ratio to roughly 2 for drug use)\nlog_odds_heart_attack &lt;- -2 + 0.04 * age - log(2) * drug_use\n\n# Converting log odds to probability\nprob_heart_attack &lt;- exp(log_odds_heart_attack) / (1 + exp(log_odds_heart_attack))\n\n# Simulating whether a heart attack occurs or not\nheart_attack &lt;- rbinom(n, 1, prob_heart_attack)\n\n# Create a dataframe\ndata_high_risk &lt;- data.frame(age, drug_use, heart_attack)\n\n\n\nnrow(data_high_risk)\n\n[1] 1000\n\n\n\ndata_high_risk\n\n\n\n\n\n\n\nage\ndrug_use\nheart_attack\n\n\n\n\n45.94701\n0\n1\n\n\n57.08072\n0\n1\n\n\n63.13331\n1\n0\n\n\n37.40727\n1\n0\n\n\n58.21844\n0\n1\n\n\n58.79542\n1\n0\n\n\n50.68945\n1\n0\n\n\n50.90026\n0\n0\n\n\n50.76661\n1\n0\n\n\n48.32472\n0\n1\n\n\n\n\n\n\n\n\n\nModeling and Results\nUsing a logistic regression, we’ll model the probability of a heart attack based on age and drug use:\n\nmodel_high_risk &lt;- glm(heart_attack ~ age + drug_use, \n                       data=data_high_risk, family = binomial)\n\nsummary(model_high_risk)\n\n\nCall:\nglm(formula = heart_attack ~ age + drug_use, family = binomial, \n    data = data_high_risk)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.662  -1.049  -0.854   1.188   1.685  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.24031    0.49848  -4.494 6.98e-06 ***\nage          0.04281    0.00896   4.777 1.78e-06 ***\ndrug_use    -0.66254    0.13064  -5.072 3.94e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1373.7  on 999  degrees of freedom\nResidual deviance: 1324.4  on 997  degrees of freedom\nAIC: 1330.4\n\nNumber of Fisher Scoring iterations: 4\n\n\nUpon examining the output, you’ll observe an odds ratio close to 2 for drug use (exponentiate the coeffecient). A common misinterpretation might be: “Patients on the drug have twice the likelihood to avoid heart attacks.” This is a misconception. They have twice the odds, not twice the probability. This distinction becomes especially murky when the event (heart attack) is common.\n\n\nMarginal Effects and Interpretation\nThe marginal effect of the drug can be found by computing the difference in predicted probabilities between drug users and non-users, holding other variables constant. This gives a more direct measure of the drug’s impact on the probability of heart attack.\nUsing our model, you can compute the marginal effect for any given age. For example, for someone aged 55:\n\npredicted_prob_on_drug &lt;- predict(model_high_risk, \n                                  newdata = data.frame(age = 55, drug_use = 1),\n                                  type = \"response\")\n\npredicted_prob_off_drug &lt;- predict(model_high_risk, \n                                   newdata = data.frame(age = 55, drug_use = 0),\n                                   type = \"response\")\n\nmarginal_effect_55 &lt;- predicted_prob_on_drug - predicted_prob_off_drug\n\nmarginal_effect_55\n\n         1 \n-0.1622666 \n\n\nThe result is a clear percentage point decrease in the probability of a heart attack for drug users compared to non-users.\nThis means that, for individuals aged 55, using the drug results in a 16 percentage point decrease in the probability of experiencing a heart attack compared to those not using the drug. This direct interpretation offers a tangible sense of the drug’s impact and is far more intuitive than grappling with odds ratios."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#average-marginal-effects-a-comprehensive-view",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#average-marginal-effects-a-comprehensive-view",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Average Marginal Effects: A Comprehensive View",
    "text": "Average Marginal Effects: A Comprehensive View\nBeyond just evaluating the marginal effect at specific values, we can compute the average marginal effect (AME) across our sample. AMEs provide a holistic view of the predictor’s impact by averaging its effects over all observations in our dataset. It’s particularly insightful when dealing with continuous predictors like age.\n\nComputing the Average Marginal Effects\nTo compute the AMEs in R, you can leverage the margins package:\n\n#install.packages(\"margins\")\nlibrary(margins)\n\name &lt;- margins(model_high_risk, variables = \"drug_use\")\nsummary(ame)\n\n   factor     AME     SE       z      p   lower   upper\n drug_use -0.1556 0.0291 -5.3429 0.0000 -0.2127 -0.0985\n\n\nThis will give you the average marginal effect of drug use on the probability of a heart attack across all ages in the dataset.\n\n\nInterpreting the AMEs\nSuppose the AME for drug use is -0.156. This means that, on average, across all ages in our sample, using the drug is associated with a 15.6 percentage point decrease in the probability of experiencing a heart attack relative to not using the drug.\nThis single number provides an overarching sense of the drug’s impact, making it immensely useful for policy and clinical decisions. For instance, in public health discussions or in communications with patients, being able to state the average effect of a treatment can be more practical than specifying its impact at particular ages or conditions.\nBy incorporating AMEs into your analysis, you’re providing an extra layer of depth to your findings. This approach shifts the discussion from “what might the effect be?” to “here’s the effect, on average, across our population.” When combined with specific marginal effects and predicted probabilities, this trio offers a robust, comprehensive, and intuitive understanding of your predictors’ impacts."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#visualizing-predicted-probabilities",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#visualizing-predicted-probabilities",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Visualizing Predicted Probabilities",
    "text": "Visualizing Predicted Probabilities\nTo provide a tangible representation, let’s plot the predicted probabilities of heart attacks by age from our model:\n\nlibrary(ggplot2)\n\ndata_high_risk$predicted_prob &lt;- predict(model_high_risk, type = \"response\")\npredictions &lt;- predict(model_high_risk, type = \"response\", se.fit = TRUE)\ndata_high_risk$lower_ci &lt;- predictions$fit - 1.96 * predictions$se.fit\ndata_high_risk$upper_ci &lt;- predictions$fit + 1.96 * predictions$se.fit\n\n\nggplot(data_high_risk, aes(x = age, \n                           y = predicted_prob, \n                           color = as.factor(drug_use))) +\n  geom_ribbon(aes(ymin = lower_ci, \n                  ymax = upper_ci, \n                  fill = as.factor(drug_use)), alpha = 0.3) +\n  geom_line() +\n  labs(title = \"Predicted Probability of Heart Attack by Age with 95% CI\",\n       y = \"Predicted Probability\",\n       x = \"Age\",\n       color = \"Drug Use\",\n       fill = \"Drug Use\") +\n  scale_color_discrete(labels = c(\"No Drug\", \"On Drug\")) +\n  scale_fill_discrete(labels = c(\"No Drug\", \"On Drug\")) +\n  theme_minimal()\n\n\n\n\nThe visual clearly illustrates the beneficial effect of the drug over time. It gives a tangible sense of how the risk of heart attacks evolves with age for both groups and the significant advantage of drug users. This can be especially useful for diagnostic purposes or to demonstrate the results of your model to stakeholders."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#conclusion-a-call-to-transition",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#conclusion-a-call-to-transition",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Conclusion: A Call to Transition",
    "text": "Conclusion: A Call to Transition\nThe era of odds ratios has provided us with valuable insights, but as the field of health research progresses, so should our statistical tools and reporting practices. Marginal effects and predicted probabilities provide clearer, more intuitive insights, pushing us away from the ambiguous realm of odds and into the tangible world of real impact.\nAfter all, the end goal of our research is to provide clear, actionable insights that can be readily applied to improve health outcomes. Let’s choose clarity over tradition, and marginal effects over odds ratios.\nThank you!\nJacob"
  },
  {
    "objectID": "posts/blog.html",
    "href": "posts/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog!\n\n\n\n\n\n\n\n\n  \n\n\n\n\nBeyond DiD: Autoregressive Models for Policy Evaluation When Parallel Trends Fail\n\n\n\n\n\n\n\ncausal inference\n\n\npolicy evaluation\n\n\nmethodology\n\n\n\n\nThis post demonstrates how autoregressive models can overcome the limitations of difference-in-differences analysis when evaluating health policies with non-parallel pre-treatment trends, providing researchers with practical tools to improve causal inference in observational studies.\n\n\n\n\n\n\nApr 29, 2025\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nA Gentle Introduction to Bayesian Linear Regression in R\n\n\n\n\n\n\n\nbayesian\n\n\nregression\n\n\n\n\nThis guide walks you through Bayesian linear regression in R and Stan, explaining how priors, likelihoods, and posteriors work in an intuitive and hands-on way.\n\n\n\n\n\n\nFeb 9, 2025\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nDIY Decision Trees\n\n\n\n\n\n\n\ndecison trees\n\n\nmachine learning\n\n\n\n\nEmbarking on a DIY Journey with Machine Learning Algorithms.\n\n\n\n\n\n\nMar 26, 2024\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nDoes it Spark Joy? Using PCA for Dimensionality Reduction in R\n\n\n\n\n\n\n\nPCA\n\n\n\n\nPrincipal Component Analysis (PCA) is a powerful tool for dimensionality reduction. In this post, we provide an overview of PCA, explaining its purpose and how it works. We then walk through a hands-on application of PCA using R, demonstrating the step-by-step process of PCA and its application in predictive modeling.\n\n\n\n\n\n\nFeb 27, 2024\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nWhat is Linear Discriminant Analysis (LDA)?\n\n\n\n\n\n\n\nLDA\n\n\n\n\nA brief overview of Linear Discriminant Analysis (LDA) and how it can be used to classify data.\n\n\n\n\n\n\nFeb 15, 2024\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nDemystifying KNN\n\n\n\n\n\n\n\nknn\n\n\n\n\nA deep dive into the K-Nearest Neighbors (KNN) algorithm, exploring its mathematical foundations and practical applications.\n\n\n\n\n\n\nFeb 1, 2024\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nThe ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research\n\n\n\n\n\n\n\nmarginal effects\n\n\n\n\nMarginal Effects &gt;&gt;&gt; Odds Ratios\n\n\n\n\n\n\nSep 1, 2023\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nBasic Network Analysis and Visualization for Directed Graphs in R\n\n\n\n\n\n\n\nnetworks\n\n\ncentrality\n\n\n\n\nChoosing the Right Centrality Measure.\n\n\n\n\n\n\nNov 1, 2022\n\n\nJacob Jameson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "api 222 files/section 7/section 7.2.html",
    "href": "api 222 files/section 7/section 7.2.html",
    "title": "Section 7.2 - Non-linear models",
    "section": "",
    "text": "Note: this section has several different types of models. We cannot cover all of them in 1.25 hours. We will go over a few examples but once you understand how the examples, you should be able to apply the logic to other types models.\nThis week, we will use the Wage data that is part of the ISLR package\nlibrary(ISLR)\nLoad the Wage data and drop the “Wage” columns, as we did in Section 6\nwage_data &lt;- Wage\nwage_data &lt;- wage_data[, -10]\nTo make life easier for later analyses, I will start by sorting the data on age. To do this, I will use a package called dplyr, which has a lot of great tools for data manipulation.\nlibrary(dplyr)\nThe first two functions we will use are: - %&gt;% which means “and then do” - arrange() which sorts the data on the column inside the parentheses\nwage_data &lt;- wage_data %&gt;% arrange(age)\nWe will start by splitting the data into training and test sets\nset.seed(222)\ntrain &lt;- sample(1:nrow(wage_data), round(nrow(wage_data) * 0.8))\ntrain &lt;- sort(train)\ntest &lt;- which(!(seq(nrow(wage_data)) %in% train))\nTo quickly and easily measure MSEP, write our own function\nmsep_func &lt;- function(predictions, true_vals) {\n  MSEP &lt;- mean((predictions - true_vals)^2)\n  return(MSEP)\n}"
  },
  {
    "objectID": "api 222 files/section 7/section 7.2.html#polynomial-regression",
    "href": "api 222 files/section 7/section 7.2.html#polynomial-regression",
    "title": "Section 7.2 - Non-linear models",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nWe can start by fitting a polynomial regression using only age\n\nage_poly &lt;- lm(wage ~ poly(age, 4), data = wage_data[train,])\n\nExtract the coefficients from the model\n\ncoef(summary(age_poly))\n\n                Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    111.88351  0.8086511 138.358203 0.000000e+00\npoly(age, 4)1  394.18926 39.6156514   9.950342 6.942194e-23\npoly(age, 4)2 -434.00566 39.6156514 -10.955409 2.748637e-27\npoly(age, 4)3  105.56550 39.6156514   2.664742 7.756432e-03\npoly(age, 4)4  -90.09776 39.6156514  -2.274297 2.303622e-02\n\n\nWhen you use poly(), it returns a matrix of “orthogonal polynomials” so the columns of the matrix are linear combinations of age, age\\(^2\\), age\\(^3\\), age\\(^4\\). Let’s take a look!\n\nhead(poly(wage_data$age, 4))\n\n              1          2           3          4\n[1,] -0.0386248 0.05590873 -0.07174058 0.08672985\n[2,] -0.0386248 0.05590873 -0.07174058 0.08672985\n[3,] -0.0386248 0.05590873 -0.07174058 0.08672985\n[4,] -0.0386248 0.05590873 -0.07174058 0.08672985\n[5,] -0.0386248 0.05590873 -0.07174058 0.08672985\n[6,] -0.0386248 0.05590873 -0.07174058 0.08672985\n\nhead(wage_data$age)\n\n[1] 18 18 18 18 18 18\n\n\nIf you want it to return the raw powers of age, you can add an argument to the function poly()\n\nhead(poly(wage_data$age, 4, raw = TRUE))\n\n      1   2    3      4\n[1,] 18 324 5832 104976\n[2,] 18 324 5832 104976\n[3,] 18 324 5832 104976\n[4,] 18 324 5832 104976\n[5,] 18 324 5832 104976\n[6,] 18 324 5832 104976\n\n\nAlthough the two forms give you different numbers, they result in the same predictions, because your model is still a linear combination of the original powers\n\nage_poly_TRUE &lt;- lm(wage ~ poly(age, 4, raw = TRUE), \n                    data = wage_data[train,])\n\nExtract the coefficients from the model\n\ncoef(summary(age_poly_TRUE))\n\n                               Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept)               -2.085312e+02 6.559430e+01 -3.179106 0.0014961570\npoly(age, 4, raw = TRUE)1  2.391179e+01 6.439356e+00  3.713383 0.0002091772\npoly(age, 4, raw = TRUE)2 -6.646745e-01 2.257678e-01 -2.944063 0.0032705193\npoly(age, 4, raw = TRUE)3  8.403840e-03 3.362741e-03  2.499105 0.0125172334\npoly(age, 4, raw = TRUE)4 -4.098605e-05 1.802141e-05 -2.274297 0.0230362203\n\n\nWe can see from the coefficient outputs of the two models that they have different coefficients. We will now check that they make the same predictions.\n\nage_poly_pred &lt;- predict(age_poly, newdata = wage_data[test,])\nage_poly_TRUE_pred  &lt;- predict(age_poly_TRUE, newdata = wage_data[test,])\n\nExplore the predictions\n\nhead(age_poly_pred)\n\n       5       19       26       32       37       41 \n51.23518 58.14597 64.50781 64.50781 64.50781 64.50781 \n\nhead(age_poly_TRUE_pred)\n\n       5       19       26       32       37       41 \n51.23518 58.14597 64.50781 64.50781 64.50781 64.50781 \n\n\nCalculate and print the MSEP\n\nprint(msep_func(age_poly_pred, wage_data[test, \"wage\"]))\n\n[1] 1689.522\n\n\ncbind() is a function that joins columns together by binding them next to each other. There is also a function rbind() that joins by binding new rows to the bottom of old ones. Let’s try using cbind() to look at both sets of predictions at the same time:\n\npred_comparison &lt;- cbind(age_poly_pred, age_poly_TRUE_pred)\n\nThe column names default to the variable names, but ifwe want to change them, we can use the colnames() function\n\ncolnames(pred_comparison) &lt;- c(\"pred1\", \"pred2\")\n\nWe can then generate a variable that flags any instances where the predictions do not line up. To do this, I will introduce you another function in dplyr: mutate() which means create a new variable\n\npred_comparison &lt;- pred_comparison %&gt;% \n  mutate(check_same = as.numeric(pred1 == pred2))\n\nError in UseMethod(\"mutate\") : no applicable method for 'mutate' applied to an object of class \"c('matrix', 'array', 'double', 'numeric')\"\nThe previous line errored because pred_comparison is a matrix and dplyr was designed to work on data frames.\nMake pred_comparison a data frame\n\npred_comparison &lt;- data.frame(pred_comparison)\n\nNow try running the code:\n\npred_comparison &lt;- pred_comparison %&gt;%\n  mutate(check_same = as.numeric(pred1 == pred2))\n\nView the results\n\nView(pred_comparison)"
  },
  {
    "objectID": "api 222 files/section 7/section 7.2.html#regression-splines",
    "href": "api 222 files/section 7/section 7.2.html#regression-splines",
    "title": "Section 7.2 - Non-linear models",
    "section": "Regression Splines",
    "text": "Regression Splines\n\nSplines\nWe’re going to start by making an age grid, so that we test our predictions on a grid of evenly spaced ages so that you capture the functional form well.\nA quick aside, na.rm = TRUE means exlucde missing observations. Then we are going to run a spline with knots at ages 25, 40, and 60.\n\nlibrary(splines)\n\nCreate the age grids\n\nage_grid &lt;- seq(from = min(wage_data$age, na.rm = TRUE),\n                to = max(wage_data$age, na.rm = TRUE))\n\nNow we use a basis function\n\nspline_age &lt;- lm(wage ~ bs(age, knots = c(25, 40, 60)), \n                 data = wage_data[train,])\n\n?bs # to check what the basis function does\n\nGet the predictions at the grid points we defined earlier\n\nspline_age_grid_pred  &lt;- predict(spline_age, \n                                 newdata = list(age = age_grid), \n                                 se = TRUE)\n\nPlot age on the x-axis and wage on the y-axis for the test data. Then add the predictions and the confidence intervals\n\nplot(wage_data[test, \"age\"], wage_data[test, \"wage\"],\ncex = 0.5, col = \"darkgrey\",\nxlab = \"age\", ylab = \"wage\")\nlines(age_grid, spline_age_grid_pred$fit, lwd = 2)\nlines(age_grid, spline_age_grid_pred$fit + \n        2 * spline_age_grid_pred$se, lty =\"dashed\")\nlines(age_grid, spline_age_grid_pred$fit - \n        2 * spline_age_grid_pred$se, lty =\"dashed\")\n\n\n\n\n\n\nNatural Splines\nIf we instead wanted to fit a natural spline, we use ns() instead of knots, we can specify degrees of freedom. In this case we are going to use 4 degrees of freedom.\n\nns_age_poly &lt;- lm(wage ~ ns(age, df = 4), data = wage_data[train,])\nns_age_grid_poly_pred &lt;- predict(ns_age_poly,\n                                 newdata = list(age = age_grid),\n                                 se = TRUE)\n\nplot(wage_data[test, \"age\"], wage_data[test, \"wage\"], \n     cex = 0.5, col = \"darkgrey\",\n     xlab = \"age\", ylab = \"wage\")\nlines(age_grid, ns_age_grid_poly_pred$fit, lwd = 2)\nlines(age_grid, ns_age_grid_poly_pred$fit + \n        2 * ns_age_grid_poly_pred$se, lty = \"dashed\")\nlines(age_grid, ns_age_grid_poly_pred$fit - \n        2 * ns_age_grid_poly_pred$se, lty = \"dashed\")\n\n\n\n\n\n\nSmoothing Splines\nTo fit a smoothing spline, we use smooth.spline(). We can specify our own df\n\nsmooth_age &lt;- smooth.spline(wage_data[train,\"age\"], \n                            wage_data[train, \"wage\"], \n                            df = 16)\n\nOr we can use cross Validation to get optimal df and penalty\n\nsmoothCV_age &lt;- smooth.spline(wage_data[train, \"age\"], \n                              wage_data[train, \"wage\"], \n                              cv = TRUE) # specify we want to use CV\nsmoothCV_age$df\n\n[1] 6.786486\n\n\nPlot the results as before\n\nplot(wage_data[test, \"age\"], wage_data[test, \"wage\"],\n     cex =.5, col = \"darkgrey\",\n     xlab = \"age\", ylab = \"wage\")\ntitle(\" Smoothing Spline \")\nlines(smooth_age, col =\"red\", lwd = 2)\nlines(smoothCV_age, col =\"blue\", lwd =2)\n\n\n\n\n\n\nLocal Regression\nLocal Regression use loess()\nNote: span = 0.2 makes neighborhoods with 20% of observations. Span = 0.5 creates neighborhoods with 50% of observations. So the larger the span, the smoother the fit\n\nlocal2_age &lt;- loess(wage ~ age, span = 0.2, data = wage_data)\nlocal5_age &lt;- loess(wage ~ age, span = 0.5, data = wage_data)\n\nGet the predictions\n\npred_local2_age &lt;- predict(local2_age, newdata = data.frame(age = age_grid))\npred_local5_age &lt;- predict(local5_age, newdata = data.frame(age = age_grid))\n\nPlot the results\n\nplot(wage_data[test, \"age\"], wage_data[test, \"wage\"],\n     cex =.5, col = \"darkgrey\",\n     xlab = \"age\", ylab = \"wage\")\nlines(age_grid, pred_local2_age, col = \"red\", lwd = 2)\nlines(age_grid, pred_local5_age, col = \"blue\", lwd = 2)\n\n\n\n\n\n\nGAMs\nGeneralized Additive Models (GAMs)\nStart with using natural spline functions of year and age and treating education as a qualitative variable does not require any special packages\n\nExample 1: GAMs with splines\n\n\ngam_yae &lt;- lm(wage ~ ns(year, 4) + ns(age, 4) + education, \n              data = wage_data[train,])\n\n\nExample 2: GAMs with smoothing splines\n\n\nlibrary(gam)\n\nIn the gam library s() indicates that we want to use a smoothing spline\n\ngam_smooth &lt;- gam(wage ~ s(year, 4) + s(age, 5) + education,\n                  data = wage_data[train,])\n\nPlot the model results\n\npar(mfrow = c(1, 3))\nplot(gam_smooth, se = TRUE, col =\"blue \")\n\n\n\n\nTo plot the GAM we created just using lm(), we can use plot.Gam()\n\nplot.Gam(gam_yae, se = TRUE, col = \"red\") # Note the capitalization\n\n\n\n\n\n\n\n\n\n\nMake predictions\n\ngam_yae_pred &lt;- predict(gam_yae, newdata = wage_data[test,])\ngam_smooth_pred &lt;- predict(gam_smooth, newdata = wage_data[test,])\n\nPrint out the MSEP for these two GAMs using the function we created at the start of class\n\nprint(msep_func(gam_yae_pred, wage_data[test, \"wage\"]))\n\n[1] 1304.807\n\nprint(msep_func(gam_smooth_pred, wage_data[test, \"wage\"]))\n\n[1] 1300.582\n\n\n\nExample 3: GAMs with local regression\n\nTo use local regression in GAMs, use lo()\n\ngam_lo &lt;- gam(wage ~ s(year, df = 4) + lo(age, span = 0.7) + education, \n              data = wage_data[train,])\n\nPlot the results\n\nplot.Gam(gam_lo, se = TRUE, col = \"green\")\n\n\n\n\n\n\n\n\n\n\nIf you want to do a local regression in two variables:\n\ngam_2lo &lt;- gam(wage ~ lo(year, age, span =0.5) + education,\n               data = wage_data[train,])\n\nWarning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, bf.maxit, :\nliv too small.  (Discovered by lowesd)\n\n\nWarning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, bf.maxit, : lv\ntoo small.  (Discovered by lowesd)\n\n\nWarning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, bf.maxit, :\nliv too small.  (Discovered by lowesd)\n\n\nWarning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, bf.maxit, : lv\ntoo small.  (Discovered by lowesd)"
  },
  {
    "objectID": "api 222 files/section 6/section 6.1.html",
    "href": "api 222 files/section 6/section 6.1.html",
    "title": "Section 6.1 - Regularization and Dimension Reduction Notes",
    "section": "",
    "text": "Note that the material in these notes draws on past TF’s notes (Ibou Dieye, Laura Morris, Emily Mower, Amy Wickett), and the more thorough treatment of these topics in Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.\nThe goal of this session is to review concepts related to regularization methods (LASSO and Ridge Regression) and dimension reduction techniques (PCA and PLS)."
  },
  {
    "objectID": "api 222 files/section 6/section 6.1.html#lasso-and-ridge-regression",
    "href": "api 222 files/section 6/section 6.1.html#lasso-and-ridge-regression",
    "title": "Section 6.1 - Regularization and Dimension Reduction Notes",
    "section": "LASSO and Ridge Regression",
    "text": "LASSO and Ridge Regression\n\nConcept\nLeast Absolute Shrinkage and Selection Operator (LASSO) and Ridge Regression both fall under a broader class of models called shrinkage models. Shrinkage models regularize or penalize coefficient estimates, which means that they shrink the coefficients toward zero. Shrinking the coefficient estimates can reduce their variance, so these methods are particularly useful for models where we are concerned about high variance (i.e. over-fitting), such as models with a large number of predictors relative to the number of observations.\nBoth LASSO and Ridge regression operate by adding a penalty to the normal OLS (Ordinary Least Squares) minimization problem. These penalties can be thought of as a budget, and sometimes the minimization problem is explicitly formulated as minimizing the Residual Sum of Squares (RSS) subject to a budget constraint. The idea of the budget is if you have a small budget, you can only afford a little “total” \\(\\beta\\), where the definition of “total” \\(\\beta\\) varies between LASSO and Ridge, but as your budget increases, you get closer and closer to the OLS \\(\\beta\\)s.\nLASSO and Ridge regression differ in exactly how they penalize the coefficients. In particular, LASSO penalizes the coefficients using an \\(L_1\\) penalty:\n\\[\n\\sum_{i=1}^n \\left(\n    y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}\n    \\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| = RSS + \\lambda \\sum_{j=1}^p |\\beta_j|\n\\]\nRidge regression penalizes the coefficients using an \\(L_2\\) penalty:\n\\[\n    \\sum_{i=1}^n \\left(\n    y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}\n    \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 = RSS + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\] You may notice that both the LASSO and Ridge Regression minimization problems feature a parameter \\(\\lambda\\). This is a tuning parameter that dictates how much we will penalize the “total” coefficients. Tuning parameters appear in many models and get their name from the fact that we tune (adjust / choose) them in order to improve our model’s performance.\nSince LASSO and Ridge Regression are used when we are concerned about variance (over-fitting), it should come as no surprise that increasing \\(\\lambda\\) decreases variance. It can be helpful to contextualize the \\(\\lambda\\) size by realizing that \\(\\lambda=0\\) is OLS and \\(\\lambda =\\infty\\) results in only \\(\\beta_0\\) being assigned a non-zero value.\nAs we increase \\(\\lambda\\) from zero, we decrease variance but increase bias (the classic bias-variance tradeoff). To determine the best \\(\\lambda\\) (e.g. the \\(\\lambda\\) that will lead to the best out of sample predictive performance), it is common to use cross validation. A good function in R that trains LASSO and Ridge Regression models and uses cross-validation to select a good \\(\\lambda\\) is cv.glmnet(), which is part of the glmnet package.\n\n\nKey difference between LASSO and Ridge\nAs noted above, both LASSO and Ridge regression shrink the coefficients toward zero. However, the penalty in Ridge (\\(\\lambda \\beta_j^2\\)) will not set any of coefficients exactly to zero (unless \\(\\lambda =\\infty\\)). Increasing the value of \\(\\lambda\\) will tend to reduce the magnitudes of the coefficients (which helps reduce variance), but will not result in exclusion of any of the variables. While this may not impact prediction accuracy, it can create a challenge in model interpretation in settings where the number of features is large. This is an obvious disadvantage.\nOn the other hand, the \\(L_1\\) penalty of LASSO forces some of the coefficient estimates to be exactly equal to zero when the tuning parameter \\(\\lambda\\) is sufficiently large. Therefore, LASSO performs variable selection, much like best subset selection.\n\n\nImplementation and Considerations\nLASSO and Ridge Regression are useful models to use when dealing with a large number of predictors \\(p\\) relative to the number of observations \\(n\\). It is good to standardize your features so that coefficients are not selected because of their scale rather than their relative importance.\nFor example, suppose you were predicting salary, and suppose you had a standardized test score that was highly predictive of salary and you also had parents’ income, which was only somewhat predictive of salary. Since standardized test scores are measured on a much smaller scale than the outcome and than parents’ income, we would expect the coefficient on test scores to be large relative to the coefficient on parents’ income. This means it would likely be shrunk by adding the penalty, even though it reflects a strong predictive relationship.\n\n\nInterpreting Coefficients\nCoefficients produced by LASSO and Ridge Regression should not be interpreted causally. These methods are used for prediction, and as such, our focus is on \\(\\hat{y}\\). This is in contrast to an inference problem, where we would be interested in \\(\\hat{\\beta}\\).\nThe intuition behind why we cannot interpret the coefficients causally is similar to the intuition underlying Omitted Variables Bias (OVB). In OVB, we said that if two variables \\(X_1\\) and \\(X_2\\) were correlated with each other and with the outcome \\(Y\\), then the coefficient on \\(X_1\\) in a regression of \\(Y\\) on \\(X_1\\) would differ from the coefficient on \\(X_1\\) in a regression where \\(X_2\\) was also included. This is because since \\(X_1\\) and \\(X_2\\) are correlated, when we omit \\(X_2\\), the coefficient on \\(X_1\\) picks up the effect of \\(X_1\\) on \\(Y\\) as well as some of the effect of \\(X_2\\) on \\(Y\\)."
  },
  {
    "objectID": "api 222 files/section 6/section 6.1.html#principal-components-analysis-and-regression",
    "href": "api 222 files/section 6/section 6.1.html#principal-components-analysis-and-regression",
    "title": "Section 6.1 - Regularization and Dimension Reduction Notes",
    "section": "Principal Components Analysis and Regression",
    "text": "Principal Components Analysis and Regression\n\nPrincipal Components Analysis\nPrincipal Components Analysis is a method of unsupervised learning. We have not yet covered unsupervised learning, though we will in the second half of the semester. The important thing to know at this point about unsupervised learning is that unsupervised learning methods do not use labels \\(Y\\).\nPrincipal Components Analysis (PCA), therefore, does not use \\(Y\\) in determining the principal components. Instead, the principal components are determined solely by looking at the predictors \\(X_1\\),…,\\(X_p\\). Each principal component is a weighted sum of the original predictors \\(X_1\\),…,\\(X_p\\). For clarity of notation, we will use \\(Z\\) to represent principal components and \\(X\\) to represent predictors.\nPrincipal components are created in an ordered way. The order can be described as follows: If you had to project all of the data onto only one line, the first principal component defines the line that would lead to points being as spread out as possible (e.g. having highest variance). If you had to project all of the data onto only one plane, the plane defined by the first two principal components (which are orthogonal by definition) would be the plane where the points were as spread out as possible (e.g. highest possible variance).\nSince variance is greatly affected by measurement scale, it is common practice and a good idea to scale your variables, so that results are not driven by the scale on which the variables were measured.\nWhen estimating principal components, you will estimate \\(p\\) components, where \\(p\\) is the number of predictors \\(X\\). However, you usually only pay attention to / use the first few components, since the last few components capture very little variance. Note that you can exactly recover your original data when using all \\(p\\) components.\n\n\nPrincipal Components Regression\nPrincipal Components Regression (PCR) regresses the outcome \\(Y\\) on the principal components \\(Z_1\\),…,\\(Z_M\\), where \\(M&lt;p\\) and \\(p\\) is the number of predictors \\(X_1\\),…,\\(X_p\\).\nNote that the principal components \\(Z_1\\),…,\\(Z_M\\) were defined by looking only at \\(X_1\\),…,\\(X_p\\) and not at \\(Y\\). Putting the principal components into a regression is the first time we are introducing any interaction between the principal components and the outcome \\(Y\\).\nThe main idea of Principal Components Regression is that hopefully only a few components explain most of the variance in the predictors overall and as is relevant to the relationship between the predictors and the response. In other words, when we use PCR, we assume that the directions in which \\(X\\) shows the most variance are also the directions most associated with \\(Y\\). When this assumption is true, we are able to use \\(M&lt;&lt;p\\) (e.g. \\(M\\) much smaller than \\(p\\)) parameters while still getting similar in-sample performance and hopefully better out-of-sample performance (due to not overfitting) to a regression of \\(Y\\) on all \\(p\\) predictors. Although the assumption is not guaranteed to be true, it is often a good enough approximation to give good results.\nPCA is one example of dimensionality reduction, because it reduces the dimension of the problem from \\(p\\) to \\(M\\). Note, though, that dimensionality reduction is different from feature selection. We are still using all features; we have just aggregated them into principal components.\nThe exact number \\(M\\) of principal components to use in PCR (the regression of \\(Y\\) on the principal components) is usually determined by cross-validation.\n\n\nImplementation and Considerations\nWhen using PCR, there are a few things to pay attention to. First, be sure to standardize variables or else the first few principal components will favor features measured on larger scales. Second, the number of principal components to use in PCR is determined using cross-validation. If the number is high and close to the number of features in your data, the assumption that the directions in which the predictors vary most are also the directions that explain the relationship between the predictors and response is false. In such a case, other methods, such as ridge and lasso, are likely to perform better."
  },
  {
    "objectID": "api 222 files/section 6/section 6.1.html#partial-least-squares",
    "href": "api 222 files/section 6/section 6.1.html#partial-least-squares",
    "title": "Section 6.1 - Regularization and Dimension Reduction Notes",
    "section": "Partial Least Squares",
    "text": "Partial Least Squares\nPartial Least Squares is a supervised alternative to PCR. Recall that for PCR, the principal components \\(Z_1,...,Z_M\\) were formed from the original features \\(X_1,...,X_p\\) without looking at \\(Y\\) (unsupervised).\nPartial Least Squares (PLS) also generates a new set of features \\(Z_1,...,Z_M\\) but it uses the correlation between the predictors \\(X_1,...,X_p\\) and the outcome \\(Y\\) to determine the weights on \\(X_1,...,X_p\\) for each \\(Z_1,...,Z_M\\). In this way, PLS attempts to find directions that explain both the response and the predictors.\nThe first feature \\(Z_1\\) is determined by weighting \\(X_1,...,X_p\\) proportional to each feature’s correlation with \\(Y\\). It then residualizes the predictors \\(X_1,...,X_p\\) by regressing them on \\(Z_1\\) and repeats the weighting procedure using the orthogonalized predictors. The process then repeats until we have \\(M\\) components, where \\(M\\) is determined by cross validation.\n\nImplementation and Considerations\nJust as with PCR, it’s best practice to scale the predictors before running PLS.\n\n\nComparison with PCR\nPLS directly uses \\(Y\\) in generating the features \\(Z_1,...,Z_M\\). It then regresses \\(Y\\) on these features. Therefore, PLS uses \\(Y\\) twice in the process of estimating the model.\nIn contrast, PCR only looks at the outcome \\(Y\\) when estimating the final regression. The components are estimated without ``peeking’’ at \\(Y\\).\nBecause \\(Y\\) is used twice in PLS and only once in PCR, in practice PLS exhibits lower bias (e.g. is better able to fit the training data) but higher variance (e.g. is more sensitive to the exact training data). Therefore, the two methods generally have similar out-of-sample predictive power in practice."
  },
  {
    "objectID": "api 222 files/section 1/section1.html",
    "href": "api 222 files/section 1/section1.html",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "",
    "text": "Notes build on previous TFs (Ibou Dieye, Laura Morris, Amy Wickett & Emily Mower)\nThe following code is meant as a first introduction to R. It is therefore helpful to run it one line at a time and see what happens. To run one line of code in RStudio, you can highlight the code you want to run and hit “Run” at the top of the script.\nOn a mac, you can highlight the code you want to run and hit Command + Enter. On a PC, you can highlight the code you want to run and hit Ctrl + Enter. If you ever forget how a function works, you can type ? followed immediately (e.g. with no space) by the function name to get the help file."
  },
  {
    "objectID": "api 222 files/section 1/section1.html#exercise-1-basic-operations-and-functions",
    "href": "api 222 files/section 1/section1.html#exercise-1-basic-operations-and-functions",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 1: Basic Operations and Functions",
    "text": "Exercise 1: Basic Operations and Functions\nCreate a new vector: Create a vector my.vector containing any five numbers. Print the vector. Basic calculations: Find the sum, product, and average of the numbers in my.vector.\n\n\nSample Solution\n# Create a vector with five numbers\nmy.vector &lt;- c(1, 3, 5, 7, 9)\nprint(my.vector)\n\n# Perform basic calculations\nsum(my.vector)\nprod(my.vector)\nmean(my.vector)\n\n\nUse a built-in function: Use the length() function to find the length of my.vector.\n\n\nSample Solution\n# Use a built-in function\nlength(my.vector)"
  },
  {
    "objectID": "api 222 files/section 1/section1.html#exercise-2-vector-manipulation",
    "href": "api 222 files/section 1/section1.html#exercise-2-vector-manipulation",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 2: Vector Manipulation",
    "text": "Exercise 2: Vector Manipulation\nCreate and modify a vector: Create a numeric vector numbers from 1 to 20. Then, extract and print the first 5 elements.\n\n\nSample Solution\n# Create and modify a vector\nnumbers &lt;- 1:20\nprint(numbers[1:5])\n\n\nLogical indexing: From numbers, create a new vector even.numbers that contains only the even numbers. Print even.numbers.\n\n\nSample Solution\n# Logical indexing for even numbers\neven.numbers &lt;- numbers[numbers %% 2 == 0]\nprint(even.numbers)\n\n\nVector arithmetic: Create a new vector that is the square of each element in numbers.\n\n\nSample Solution\nsquared.numbers &lt;- numbers^2\nprint(squared.numbers)"
  },
  {
    "objectID": "api 222 files/section 1/section1.html#exercise-3-matrices",
    "href": "api 222 files/section 1/section1.html#exercise-3-matrices",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 3: Matrices",
    "text": "Exercise 3: Matrices\nCreate a matrix: Convert numbers into a \\(4 \\times 5\\) matrix matrix.1. Print matrix.1.\n\n\nSample Solution\nmatrix.1 &lt;- matrix(numbers, nrow = 4, ncol = 5)\nprint(matrix.1)\n\n\nMatrix transposition: Print the transpose of matrix.1.\n\n\nSample Solution\n# Matrix transposition\nprint(t(matrix.1))\n\n\nMatrix indexing: Extract and print the element in the 2nd row and 3rd column of matrix.1.\n\n\nSample Solution\n# Matrix indexing\nprint(matrix.1[2, 3])"
  },
  {
    "objectID": "api 222 files/section 1/section1.html#exercise-4-logical-statements",
    "href": "api 222 files/section 1/section1.html#exercise-4-logical-statements",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 4: Logical Statements",
    "text": "Exercise 4: Logical Statements\nSimple if-else: Write an if-else statement that prints “Big” if the average of numbers is greater than 10, and “Small” otherwise.\n\n\nSample Solution\n# Simple if-else\nif (mean(numbers) &gt; 10) {\n  print(\"Big\")\n} else {\n  print(\"Small\")\n}\n\n\nNested if-else: Modify the above to include a check if the average is exactly 10, printing “Exactly 10”.\n\n\nSample Solution\n# Nested if-else\nif (mean(numbers) == 10) {\n  print(\"Exactly 10\")\n} else if (mean(numbers) &gt; 10) {\n  print(\"Big\")\n} else {\n  print(\"Small\")\n}"
  },
  {
    "objectID": "api 222 files/section 1/section1.html#exercise-5-random-numbers",
    "href": "api 222 files/section 1/section1.html#exercise-5-random-numbers",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 5: Random Numbers",
    "text": "Exercise 5: Random Numbers\nGenerate random numbers: Generate a vector of 5 random numbers drawn from a normal distribution with mean 0 and standard deviation 1. Print the vector.\n\n\nSample Solution\n# Generate random numbers\nrandom.numbers &lt;- rnorm(5)\nprint(random.numbers)\n\n\nReproducibility: Set a seed of your choice and generate the same vector of random numbers as above.\n\n\nSample Solution\n# Generate random numbers\nset.seed(222) # Set seed for reproducibility\nrandom.numbers &lt;- rnorm(5)\nprint(random.numbers)"
  },
  {
    "objectID": "api 222 files/section 1/section1.html#exercise-6-data-frames",
    "href": "api 222 files/section 1/section1.html#exercise-6-data-frames",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 6: Data Frames",
    "text": "Exercise 6: Data Frames\nExplore college.data: Print the first 6 rows of college.data.\n\n\nSample Solution\n# Explore `college.data`\nhead(college.data)\n\n\nColumn operations: Calculate the mean of the PhD column in college.data.\n\n\nSample Solution\n# Column operations\nmean(college.data$PhD)\n\n\nSubsetting: Create a new data frame small.college that only includes colleges with less than 5000 students (use college.data$Enroll for enrollment numbers).\n\n\nSample Solution\n# Subsetting data frame\nsmall.college &lt;- college.data[college.data$Enroll &lt; 5000, ]\nprint(small.college)"
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html",
    "href": "api 222 files/section 4/section 4.1.html",
    "title": "Section 4.1 - Classification Notes",
    "section": "",
    "text": "Note that the material in these notes draws on past TF’s notes (Ibou Dieye, Laura Morris, Emily Mower, Amy Wickett), and the more thorough treatment of these topics in Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani."
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html#concept",
    "href": "api 222 files/section 4/section 4.1.html#concept",
    "title": "Section 4.1 - Classification Notes",
    "section": "Concept",
    "text": "Concept\nLogistic regression is a parametric model that models the probability that \\(Y\\) belongs to a particular category. It is somewhat similar to linear regression, but the linear regression form of \\(\\beta_0+\\beta_1 X_1 + \\beta_2 X_2 + \\dots\\) undergoes a transformation that ensures the output will be bounded between 0 and 1 and can thus be interpreted as a probability.\n\\[\\begin{equation}\n     p(X) = \\frac{e^{\\beta_0+\\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\end{equation}\\]\nTherefore, while linear regression is best suited for regression problems, logistic regression is best suited for classification problems. Note that logistic regression produces a probability of class membership that then needs to be transformed to 0 or 1 using a decision rule, such as if \\(p(X)\\geq 0.5\\) then predict 1 otherwise predict 0."
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html#method",
    "href": "api 222 files/section 4/section 4.1.html#method",
    "title": "Section 4.1 - Classification Notes",
    "section": "Method",
    "text": "Method\nLogistic regression is estimated using Maximum Likelihood Estimation (MLE). MLE finds the values of \\(\\beta_0\\), \\(\\beta_1\\), etc. that maximize the probability of observing the observations in your training data given the values of the parameters \\(\\beta_0\\), \\(\\beta_1\\), etc. and the assumed functional form (e.g. see above)."
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html#implementation-and-considerations",
    "href": "api 222 files/section 4/section 4.1.html#implementation-and-considerations",
    "title": "Section 4.1 - Classification Notes",
    "section": "Implementation and Considerations",
    "text": "Implementation and Considerations\nWhen implementing logistic regression, the restrictions on the features are the same as for linear regression (e.g. no collinearity, number of features must be less than number of observations, etc.). The outcome should be a binary class membership. You can extend the logistic regression to cover a scenario with more than two classes, and this is called multinomial logistic regression, but we will not cover that in class.\nWhen you run logistic regression, the prediction output is a continuous value that reflects the predicted probability that the observation belongs to class 1. Therefore, a decision rule is required to convert the predicted probability to a predicted class (0 or 1). If you care equally about wrongly predicting positive for a True Negative (e.g. predicting class 1 for someone who is actually in class 0) and predicting negative for a True Positive, then a good decision rule is if \\(p(X)\\geq 0.5\\), predict 1 and otherwise predict 0. However, sometimes you care more about an error in one direction than the other. An example of this would be not wanting to offer a loan to someone who will default even if that means you deny more people who wouldn’t default. In that case, you might lower the threshold to 0.2 or some other value. We explore this more in the code."
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html#methods",
    "href": "api 222 files/section 4/section 4.1.html#methods",
    "title": "Section 4.1 - Classification Notes",
    "section": "Methods",
    "text": "Methods\nAt this point in the course, you have been introduced to three methods. The methods and their properties are summarized in the table below.\nWhen thinking about if a model is parametric or non-parametric, it can be helpful to think: Do I have a set of parameters that I can use to find the predicted value of any new observation? If yes, it’s parametric. When thinking about if a problem is a classification problem or a regression problem, it is helpful to think about the outcome in the training data. If the outcome is continuous, then it’s a regression problem. If the outcome is categorical, it’s a classification problem. The emphasis on the outcome in the training data is to avoid the confusion that arises when you look at prediction output. As we saw with logistic regression, even though it’s a classification problem, the output will be a probability (which is continuous and needs to be converted to 0 or 1 in order to measure performance)."
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html#classification",
    "href": "api 222 files/section 4/section 4.1.html#classification",
    "title": "Section 4.1 - Classification Notes",
    "section": "Classification",
    "text": "Classification\nClassification is really a two-step process. Usually, the model will predict something that looks like a probability that your observation belongs to each class. You then need to convert the probability to a class membership using a decision rule. A good general rule is: ``whichever class is assigned the highest probability is the predicted class.’’ However, when you have reason to prefer an error in one direction (e.g. predicting more people will default than actually will), you should change this threshold. Exactly which threshold is optimal will depend on domain knowledge and other factors (such as how costly defaults are or how profitable repaid loans are)."
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html#concept-1",
    "href": "api 222 files/section 4/section 4.1.html#concept-1",
    "title": "Section 4.1 - Classification Notes",
    "section": "Concept",
    "text": "Concept\nRecall that the Bayes’ Classifier is the unattainable gold standard classifier. It assumes knowledge of the true underlying distribution of the data, which you will not have in practice. Given features \\(X\\), it knows the true probability that \\(Y\\) belongs to each possible class. It predicts the most likely class, which is the best decision rule given the available features.\nLinear Discriminant Analysis (LDA) approximates the Bayes’ Classifier, given the information available and the assumption that features are Normally (Gaussian) distributed within each class. The result is decision boundaries that are linear in the included features.\nWhen there is one feature (predictor), LDA estimates class-specific means \\(\\hat{\\mu}_k\\) and a single variance \\(\\hat{\\sigma}^2\\) that is common to all classes. When there are multiple features, LDA estimates class-specific mean vectors \\(\\hat{\\mu}_k\\) and a single variance-covariance matrix \\(\\hat{\\Sigma}\\) that is assumed to be relevant to all classes. In both cases (one feature or many features), LDA also calculates the unconditional probability of belonging to each class \\(\\hat{\\pi}_k\\). LDA then takes these components (means, variance, and unconditional class probability) and calculates a discriminant function for each observation and each class. For each observation, the predicted class is determined by the largest discriminant.\nQuadratic Discriminant Analysis (QDA) is conceptually similar, though instead of requiring all classes to share the same variance or variance-covariance matrix, it allows for class-specific variances. This has the effect of allowing non-linear decision boundaries. The drawback, though, is that allowing for class-specific variances (and especially class-specific variance-covariance matrices) increases the number of parameters to estimate, increasing the likelihood of overfitting."
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html#method-1",
    "href": "api 222 files/section 4/section 4.1.html#method-1",
    "title": "Section 4.1 - Classification Notes",
    "section": "Method",
    "text": "Method\nTo estimate LDA or QDA, you estimate the feature means, feature variance(s), and unconditional (empirical) class probabilities for each class. Let \\(k\\) index the classes, then if there is only one feature, LDA calculates the following discriminant function for each observation for each class: \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x \\cdot \\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2}-\\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2}+\\log(\\hat{\\pi}_k)\n\\end{equation}\\] Where \\(\\hat{\\pi}_k\\) is the unconditional class probability, \\(\\hat{\\mu}_k\\) is the mean feature value for class \\(k\\), and \\(\\hat{\\sigma}^2\\) is the common feature variance. When \\(p&gt;1\\) (e.g. there are multiple predictors), then we use \\(\\hat{\\Sigma}\\) to represent the common feature variance-covariance matrix, \\(\\hat{\\mu}_k\\) becomes a vector, and thus the LDA discriminant function becomes: \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_k-\\frac{1}{2}\\hat{\\mu}_k^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_k+\\log(\\hat{\\pi}_k)\n\\end{equation}\\] For QDA, the one feature discriminant function is: \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x \\cdot \\frac{\\hat{\\mu}_k}{\\hat{\\sigma}_k^2}-\\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}_k^2}+\\log(\\hat{\\pi}_k)\n\\end{equation}\\] Note that \\(\\hat{\\sigma}\\) now has a subscript to indicate that the variance is class-specific. For multiple predictors, the QDA discriminant function is again just like the LDA one but with a subscripted \\(\\Sigma\\): \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x^T\\hat{\\Sigma}_k^{-1}\\hat{\\mu}_k-\\frac{1}{2}\\hat{\\mu}_k^T\\hat{\\Sigma}_k^{-1}\\hat{\\mu}_k+\\log(\\hat{\\pi}_k)\n\\end{equation}\\] In practice, we will use the lda() and qda() functions that are part of the MASS package in R."
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html#implementation-and-considerations-1",
    "href": "api 222 files/section 4/section 4.1.html#implementation-and-considerations-1",
    "title": "Section 4.1 - Classification Notes",
    "section": "Implementation and Considerations",
    "text": "Implementation and Considerations\nLDA and QDA both generalize easily to settings where there are more than two classes. They are also parametric, which means they are computationally efficient with large data sets compared to non-parametric KNN. However, they both make the assumption that the features are normally distributed, so you should pay attention to your data. For example, binary variables will never be normally distributed nor well approximated by a normal distribution, and so the methods are not appropriate to use in the presence of binary features.\nQDA differs from LDA by assuming the variance or variance-covariance matrix of the feature(s) varies from class to class. This allows for more flexible and non-linear decision boundaries, but requires estimation of more parameters. As with all other models we’ve seen, estimating more parameters increases the likelihood of overfitting and so should only be used when the number of observations is large relative to the number of features and classes."
  },
  {
    "objectID": "api 222 files/API222.html",
    "href": "api 222 files/API222.html",
    "title": "API 222 Section Materials",
    "section": "",
    "text": "Welcome to the Section Materials for API-222! This page contains all of the code and notes for the course. You can use the links below to navigate to the different sections of the course. If you have any questions or need help with anything, please don’t hesitate to reach out to me."
  },
  {
    "objectID": "api 222 files/API222.html#section-materials-introduction",
    "href": "api 222 files/API222.html#section-materials-introduction",
    "title": "API 222 Section Materials",
    "section": "",
    "text": "Welcome to the Section Materials for API-222! This page contains all of the code and notes for the course. You can use the links below to navigate to the different sections of the course. If you have any questions or need help with anything, please don’t hesitate to reach out to me."
  },
  {
    "objectID": "api 222 files/section 2/section2.1.html",
    "href": "api 222 files/section 2/section2.1.html",
    "title": "Section 2.2 - KNN and Linear Regression Code",
    "section": "",
    "text": "There are lots of great datasets available as part of R packages. Page 14 of Introduction to Statistical Learning with Applications in R Table 1.1 lays out 15 data sets available from R packages. We will use the College dataset from the ISLR package. The first time you ever use a package, you need to install it. Then, every time you want to use the package, you use library(package_name). We will use the college data. Note that details on this data are available online: https://cran.r-project.org/web/packages/ISLR/ISLR.pdf Page 5. You can also get the same information in R by typing: help(“College”) or ?College."
  },
  {
    "objectID": "api 222 files/section 2/section2.1.html#explore-data",
    "href": "api 222 files/section 2/section2.1.html#explore-data",
    "title": "Section 2.2 - KNN and Linear Regression Code",
    "section": "Explore Data",
    "text": "Explore Data\n\nlibrary(ISLR)\n\ndata(College)\ncollege_data  &lt;- College\n\nLet’s learn about our data. To get the names of the columns in the dataframe, we can use the function colnames()\n\ncolnames(college_data)\n\n [1] \"Private\"     \"Apps\"        \"Accept\"      \"Enroll\"      \"Top10perc\"  \n [6] \"Top25perc\"   \"F.Undergrad\" \"P.Undergrad\" \"Outstate\"    \"Room.Board\" \n[11] \"Books\"       \"Personal\"    \"PhD\"         \"Terminal\"    \"S.F.Ratio\"  \n[16] \"perc.alumni\" \"Expend\"      \"Grad.Rate\"  \n\n\nTo find out how many rows and columns are in the dataset, use dim() Recall that this gives us Rows followed by Columns\n\ndim(college_data)\n\n[1] 777  18\n\n\nYou can also look in the “environment” tab, press the blue arrow next to college_data and it will drop down showing the column names with their types and first few values. For college, all columns except the first are numeric. The first column is a factor column, which means it’s categorical. To get a better sense of the data, let’s look at it:\n\nView(college_data)\n\nSuppose we are interested in predicting whether a college is private or public based on available covariates, like Number accepted, enrolled, etc. Additionally, let’s suppose you don’t want certain variables included in your dataset. You can drop these functions using -c(). For example, let’s suppose you don’t want the Apps or Student to Faculty Ratio included in your dataset.\n\ncollege_data &lt;- college_data[, -c(15, 2)]\n\nBe careful when you are dropping multiple columns. You need to put the numbers in reverse order (from highest to lowest). This is because if you drop the second column first, then the 15th column becomes the the 14th column.\n\ncollege_data &lt;- College\ncollege_data &lt;- college_data[, -c(2)]\ncollege_data &lt;- college_data[, -c(15)]\n\nA less manual way of dropping columns is to use R to first use R to find the corresponding indices in the data columns. Go back to the original college data\n\ncollege_data &lt;- College\n\nFind the indices (i.e. column positions) of the columns to drop\n\nto_drop &lt;- which(names(college_data) %in% c(\"Apps\", \"S.F.Ratio\"))\nprint(to_drop)\n\n[1]  2 15\n\n\nReverse the indices as suggested above\n\nto_drop &lt;- rev(to_drop)\nprint(to_drop)\n\n[1] 15  2\n\n\nNow use the object you have defined to drop the columns\n\ncollege_data &lt;- college_data[, -c(to_drop)]\n\nAlso sometimes we have factor variables that we want to convert to numeric variables. To check variable types, you can use the “str” function\n\nstr(college_data)\n\n'data.frame':   777 obs. of  16 variables:\n $ Private    : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n $ Accept     : num  1232 1924 1097 349 146 ...\n $ Enroll     : num  721 512 336 137 55 158 103 489 227 172 ...\n $ Top10perc  : num  23 16 22 60 16 38 17 37 30 21 ...\n $ Top25perc  : num  52 29 50 89 44 62 45 68 63 44 ...\n $ F.Undergrad: num  2885 2683 1036 510 249 ...\n $ P.Undergrad: num  537 1227 99 63 869 ...\n $ Outstate   : num  7440 12280 11250 12960 7560 ...\n $ Room.Board : num  3300 6450 3750 5450 4120 ...\n $ Books      : num  450 750 400 450 800 500 500 450 300 660 ...\n $ Personal   : num  2200 1500 1165 875 1500 ...\n $ PhD        : num  70 29 53 92 76 67 90 89 79 40 ...\n $ Terminal   : num  78 30 66 97 72 73 93 100 84 41 ...\n $ perc.alumni: num  12 16 30 37 2 11 26 37 23 15 ...\n $ Expend     : num  7041 10527 8735 19016 10922 ...\n $ Grad.Rate  : num  60 56 54 59 15 55 63 73 80 52 ...\n\n\nYou can see that the Private variable is a factor. We can convert it to a numeric variable using the “as.numeric” function. I like my binary variables in R to be 0/1. In R, most factors automatically convert to a binary 1/2 format. I usually prefer a binary 0/1 format. To transform, I subtract 1.\n\ncollege_data$Private &lt;- as.numeric(college_data$Private) - 1 \nsummary(college_data$Private)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  1.0000  0.7272  1.0000  1.0000 \n\nsummary(College$Private)\n\n No Yes \n212 565 \n\n\nLet’s get back our original sample\n\ncollege_data &lt;- College"
  },
  {
    "objectID": "api 222 files/section 2/section2.1.html#testing-and-training-sets",
    "href": "api 222 files/section 2/section2.1.html#testing-and-training-sets",
    "title": "Section 2.2 - KNN and Linear Regression Code",
    "section": "Testing and Training Sets",
    "text": "Testing and Training Sets\nIn order to make this interesting, let’s split our data into a training set and a test set. To do this, we will use set.seed(), which will allow us to draw the same pseudorandom numbers the next time we run this code, and we will use the sample() function.\n\nset.seed(222)\n\nThe sample() function takes two arguments: The first is a vector of numbers from which to draw a random sample. The second is the number of random numbers to draw. The default is to sample without replacement, but you can sample with replacement by adding “, replace = TRUE” inside the function. Now, let’s generate a list of indices from the original dataset that will be designated part of the test set using sample()\n\ntest_ids &lt;- sample(1:(nrow(college_data)), round(0.2 * nrow(college_data)))\n\nTo identify the training_ids, we want all of the numbers from 1:nrow(college_data) that aren’t test IDs. Recall that which() returns the indices for which the statement inside the parentheses is true. which(!()) returns the indices for which the statement inside the parentheses is false. The “!” means “not”. Also, if you wanted to know which values of vector A were in vector B, you can use which(A %in% B). So if you want to know which values of vector A are NOT in vector B, you use which(!(A %in B)), so that’s what we will do – vector A is the vector of all integers between 1 and the number of rows in our data. vector B is the vector of test IDs\n\ntraining_ids &lt;- which(!(1:(nrow(college_data)) %in% test_ids))\n\nWe can use these indices to define our test and training sets by putting those vectors in the row position inside square brackets.\n\ntest_data &lt;- college_data[test_ids,]\ntraining_data &lt;- college_data[training_ids,]"
  },
  {
    "objectID": "api 222 files/section 2/section2.1.html#knn-classification",
    "href": "api 222 files/section 2/section2.1.html#knn-classification",
    "title": "Section 2.2 - KNN and Linear Regression Code",
    "section": "KNN Classification",
    "text": "KNN Classification\nLet’s develop a KNN model to try to predict whether it’s a private college using all available features.\nTo use KNN for classification, we need to install and load the library “class”\n\nlibrary(class)\n\nknn() is the function we will use to run the KNN model. It takes four arguments:\n\ntrain = training data features (no outcome)\ntest = test data features (no outcome)\ncl = training data outcome (class each observation belongs to)\nk = number of nearest neighbors to use\n\nFor two-class classification problems, k should be odd (avoids tied votes). Let’s run the model with 1 NN and 9 NNs. To exclude a column, use -# in the column position insider square brackets. (e.g. df[, -2] excludes the second column of dataframe df)\n\nknn_model1 &lt;- knn(train = training_data[, -1],\n                  test = test_data[, -1],\n                  cl = training_data[, 1],\n                  k = 1)\n\nknn_model9 &lt;- knn(train = training_data[, -1],\n                  test = test_data[, -1],\n                  cl = training_data[, 1],\n                  k = 9)\n\nWe are trying to predict Private Yes/No. knn() output predicted values for our test data, so we can compare actual v. predicted values. “prediction == actual” gives a vector with the same number of elements as there are observations in the test set. Each element will either be TRUE (the prediction was correct) or FALSE (the prediction was wrong). Applying which() to this vector will yield the index numbers for all the elements equal to TRUE. Applying length() to that vector tells us how many are TRUE (e.g. for how many observations prediction == actual). We can then divide by the number of observations in the test data to obtain the accuracy rate\n\naccuracy1  &lt;- length(which(knn_model1 == test_data$Private)) / nrow(test_data)\naccuracy9 &lt;- length(which(knn_model9 == test_data$Private)) / nrow(test_data)\n\nprint(accuracy1)\n\n[1] 0.9096774\n\nprint(accuracy9)\n\n[1] 0.9225806\n\n\nLet’s visualize what is happening in a KNN classification model. We will use the ggplot2 package to create a scatterplot of the training data, and then overlay the test data on top of it. We will color the points by whether the school is private or not.\n\nlibrary(ggplot2)\nggplot(data = training_data, \n       aes(x = Outstate, y = F.Undergrad, \n           color = as.factor(Private))) +\n  geom_point() +\n  geom_point(data = test_data, aes(x = Outstate, y = F.Undergrad), \n             color = \"black\", size = 1) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  theme_minimal() +\n  guides(color = guide_legend(title = \"Private\")) \n\n\n\n\nThis seems like excellent predictive performance. However, it’s good to think about the distribution of the data. As an extreme example, if all schools in the data were private, we would expect 100% prediction accuracy regardless of our model. Let’s see how well we do if our prediction is all schools are Private. Start by calculating the proportion of private schools\n\nprint(length(which(test_data$Private == \"Yes\")) / nrow(test_data))\n\n[1] 0.716129\n\n\nWe can also check our accuracy on Private schools v. Public schools. To do this, we need to figure out which schools are private in the test data. Specifically, get the indices for the private schools\n\nprivate_schools &lt;- which(test_data$Private == \"Yes\")\npublic_schools &lt;- which(test_data$Private == \"No\")\n\nprint(private_schools)\n\n  [1]   1   2   3   8  11  12  13  14  15  16  18  19  20  21  22  23  24  27\n [19]  28  29  30  31  32  33  34  35  37  38  39  41  42  43  44  45  47  48\n [37]  49  50  51  52  53  55  56  57  58  60  61  62  63  64  65  66  68  69\n [55]  76  77  78  80  81  82  84  85  86  90  91  92  93  94  96  97  99 100\n [73] 101 102 104 106 107 108 110 112 113 116 119 120 121 122 123 125 127 128\n [91] 129 130 133 134 135 136 137 138 139 140 142 145 146 147 148 149 151 152\n[109] 153 154 155\n\nprint(public_schools)\n\n [1]   4   5   6   7   9  10  17  25  26  36  40  46  54  59  67  70  71  72  73\n[20]  74  75  79  83  87  88  89  95  98 103 105 109 111 114 115 117 118 124 126\n[39] 131 132 141 143 144 150\n\n\nTo calculate the prediction accuracy for private schools, we need to know how many (true not predicted) private schools are in the test data. Likewise, we need to know how many public schools are in the test data.\n\nnum_private_schools &lt;- length(private_schools)\nnum_public_schools &lt;- length(public_schools)\n\nNow we will calculate the prediction accuracy separately for private and public schools.\n\nprivate_accuracy1 &lt;- length(\n  which(knn_model1[private_schools] == test_data$Private[private_schools])) /\n  num_private_schools\n\nprivate_accuracy9 &lt;- length(\n  which(knn_model9[private_schools] == test_data$Private[private_schools])) /\n  num_private_schools\n\nNow we will calculate the prediction accuracy separately for private and public schools.\n\n## Private schools (% correctly predicted):\nprivate_accuracy1 &lt;- length(\n  which(knn_model1[private_schools] == test_data$Private[private_schools])) /\n  num_private_schools\n\nprivate_accuracy9 &lt;- length(\n  which(knn_model9[private_schools] == test_data$Private[private_schools])) /\n  num_private_schools\n\n\n# Public schools (% correctly predicted): \npublic_accuracy1 &lt;- length(\n  which(knn_model1[public_schools] == test_data$Private[public_schools])) /\n  num_public_schools\n\npublic_accuracy9 &lt;- length(\n  which(knn_model9[public_schools] == test_data$Private[public_schools])) /\n  num_public_schools\n\nLet’s see how it did on different school types:\n\nprint(private_accuracy1)\n\n[1] 0.9459459\n\nprint(public_accuracy1)\n\n[1] 0.8181818\n\nprint(private_accuracy9)\n\n[1] 0.972973\n\nprint(public_accuracy9)\n\n[1] 0.7954545\n\n\nTherefore, we did better on private schools than public schools because our prediction accuracy was higher on private schools. Thinking about differential performance by label is related to fairness of machine learning algorithms. For an interesting discussion on ML fairness and different ways to define fairness, see the following academic paper:\nJon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. \nInherent Trade-Offs in the Fair\nDetermination of Risk Scores, November 2016"
  },
  {
    "objectID": "api 222 files/section 2/section2.1.html#knn-for-regression",
    "href": "api 222 files/section 2/section2.1.html#knn-for-regression",
    "title": "Section 2.2 - KNN and Linear Regression Code",
    "section": "KNN for Regression",
    "text": "KNN for Regression\nSuppose we wanted to predict how many students would enroll given the other features available in the data. In that case, the classification function we used above will not work. We will need a KNN function designed for regression problems. This function is knn.reg() in the FNN package, so we should install then read in the FNN package.\n\n#install.packages(\"FNN\")\nlibrary(FNN)\n\nknn.reg() takes four arguments: - training data with only features (no outcome) - test data with only features (no outcome) - training outcomes - k = number of neighbors\nEnrollment is the fourth column, so we will exclude that from the features. Because public / private is a factor, we either need to convert it to a numeric variable or exclude it. We will exclude it for now. Note that you can scale your features using scale(). Deciding to scale your features or not is problem dependent. We will not scale here. If you’re not sure whether or not to scale, you can always try it both ways and see how the performance changes.\n\nknn_reg1 &lt;- knn.reg(training_data[, -c(1, 4)],\n                    test_data[, -c(1, 4)],\n                    training_data$Enroll,\n                    k = 1)\n\nknn_reg5 &lt;- knn.reg(training_data[, -c(1, 4)],\n                    test_data[,-c(1, 4)],\n                    training_data$Enroll,\n                    k = 5)\n\nMSE is an appropriate loss function for regression whereas accuracy is only relevant for classification\n\nmse_knn1 &lt;- mean((knn_reg1$pred - test_data$Enroll)^2)\nmse_knn5 &lt;- mean((knn_reg5$pred - test_data$Enroll)^2)\n\nprint(mse_knn1)\n\n[1] 124500.9\n\nprint(mse_knn5)\n\n[1] 73296.56"
  },
  {
    "objectID": "api 222 files/section 2/section2.1.html#standard-linear-regression",
    "href": "api 222 files/section 2/section2.1.html#standard-linear-regression",
    "title": "Section 2.2 - KNN and Linear Regression Code",
    "section": "Standard Linear Regression",
    "text": "Standard Linear Regression\nWe will now do linear regression. To run a linear regression in R, we use the function lm(), which stands for linear model. lm() takes two main arguments. The first is the formula, which should be of the form Dependent Variable ~ Feature1 + Feature2 + … The second is the training data – including both features and the outcome. Note that “~.” means regress this variable on all other variables\n\nenroll_reg &lt;- lm(Enroll ~ ., training_data)\n\nlm() returns a list, which includes among other things coefficients, residuals, and fitted values for the training data. You can look at the elements in RStudio by using the blue arrow next to enroll_reg in the environment tab. In order to call one element of a list, you can use $\n\nenroll_reg$coefficients\n\n  (Intercept)    PrivateYes          Apps        Accept     Top10perc \n187.938169332   7.806939217  -0.027759639   0.146504112   4.016271367 \n    Top25perc   F.Undergrad   P.Undergrad      Outstate    Room.Board \n -2.268522370   0.144249348  -0.011850547  -0.003105257  -0.024152785 \n        Books      Personal           PhD      Terminal     S.F.Ratio \n -0.027184975   0.008447046  -0.431202139  -0.539993156  -0.253400149 \n  perc.alumni        Expend     Grad.Rate \n  2.319201329   0.003018132   0.135713594 \n\n\nIn order to see a more traditional regression output, use summary()\n\nsummary(enroll_reg)\n\n\nCall:\nlm(formula = Enroll ~ ., data = training_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1284.27   -60.18    -8.62    51.46  1544.82 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 187.938169  89.614941   2.097 0.036393 *  \nPrivateYes    7.806939  30.036405   0.260 0.795017    \nApps         -0.027760   0.008149  -3.406 0.000702 ***\nAccept        0.146504   0.014710   9.959  &lt; 2e-16 ***\nTop10perc     4.016271   1.283269   3.130 0.001834 ** \nTop25perc    -2.268522   0.996912  -2.276 0.023222 *  \nF.Undergrad   0.144249   0.004298  33.560  &lt; 2e-16 ***\nP.Undergrad  -0.011851   0.006753  -1.755 0.079809 .  \nOutstate     -0.003105   0.004185  -0.742 0.458327    \nRoom.Board   -0.024153   0.010712  -2.255 0.024500 *  \nBooks        -0.027185   0.049391  -0.550 0.582244    \nPersonal      0.008447   0.013655   0.619 0.536410    \nPhD          -0.431202   1.002234  -0.430 0.667174    \nTerminal     -0.539993   1.094362  -0.493 0.621887    \nS.F.Ratio    -0.253400   2.843330  -0.089 0.929015    \nperc.alumni   2.319201   0.879334   2.637 0.008568 ** \nExpend        0.003018   0.002639   1.144 0.253219    \nGrad.Rate     0.135714   0.647956   0.209 0.834168    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 202.3 on 604 degrees of freedom\nMultiple R-squared:  0.9558,    Adjusted R-squared:  0.9546 \nF-statistic: 768.8 on 17 and 604 DF,  p-value: &lt; 2.2e-16\n\n\nIf you want to use the coefficients from enroll_reg to predict enrollment values in the test data, you can use the function predict(). The first argument is the lm object (the whole thing – not just the coefficients) and the second argument is the test data frame without the outcome column\n\npredicted_enroll &lt;- predict(enroll_reg, test_data[, -4])\n\nLet’s see how well we did in terms of MSE\n\nMSE_lm_enroll &lt;- mean((predicted_enroll - test_data$Enroll)^2)\nprint(MSE_lm_enroll)\n\n[1] 39312.19\n\n\nWe can see how this compared to our training MSE\n\nprint(mean((enroll_reg$residuals)^2))\n\n[1] 39760.39\n\n\nTraining MSE as % of Test MSE:\n\nprint(mean((enroll_reg$residuals)^2) / MSE_lm_enroll)\n\n[1] 1.011401\n\n\nWe know that the coefficients might change if we exclude some variables. Let’s pretend we only had Apps and Accept (columns 2 and 3) as features\n\nsmall_enroll_reg  &lt;- lm(Enroll ~ Apps + Accept, training_data)\n\nWe can compare coefficients from the small regression and the full regression. If the coefficients in the small regression are different from the coefficients in the full regression, then the small regression suffers from Omitted Variables Bias (OVB).\n\nsmall_enroll_reg$coefficients\n\n(Intercept)        Apps      Accept \n86.88115150 -0.05243254  0.42420181 \n\nenroll_reg$coefficients\n\n  (Intercept)    PrivateYes          Apps        Accept     Top10perc \n187.938169332   7.806939217  -0.027759639   0.146504112   4.016271367 \n    Top25perc   F.Undergrad   P.Undergrad      Outstate    Room.Board \n -2.268522370   0.144249348  -0.011850547  -0.003105257  -0.024152785 \n        Books      Personal           PhD      Terminal     S.F.Ratio \n -0.027184975   0.008447046  -0.431202139  -0.539993156  -0.253400149 \n  perc.alumni        Expend     Grad.Rate \n  2.319201329   0.003018132   0.135713594"
  },
  {
    "objectID": "api 222 files/section 2/section2.1.html#stargazer-for-regression-output",
    "href": "api 222 files/section 2/section2.1.html#stargazer-for-regression-output",
    "title": "Section 2.2 - KNN and Linear Regression Code",
    "section": "Stargazer for Regression Output",
    "text": "Stargazer for Regression Output\nIf you want to compare the coefficients from different regressions, you can use the stargazer package. This package is not installed by default, so you will need to install it.\n\n#install.packages(\"stargazer\")\nlibrary(stargazer)\n\nstargazer(small_enroll_reg, enroll_reg, \n          type = \"text\", column.labels = c(\"Small Model\", \"Full Model\"))\n\n\n========================================================================\n                                    Dependent variable:                 \n                    ----------------------------------------------------\n                                           Enroll                       \n                           Small Model                Full Model        \n                               (1)                        (2)           \n------------------------------------------------------------------------\nPrivateYes                                               7.807          \n                                                       (30.036)         \n                                                                        \nApps                        -0.052***                  -0.028***        \n                             (0.013)                    (0.008)         \n                                                                        \nAccept                       0.424***                  0.147***         \n                             (0.021)                    (0.015)         \n                                                                        \nTop10perc                                              4.016***         \n                                                        (1.283)         \n                                                                        \nTop25perc                                              -2.269**         \n                                                        (0.997)         \n                                                                        \nF.Undergrad                                            0.144***         \n                                                        (0.004)         \n                                                                        \nP.Undergrad                                             -0.012*         \n                                                        (0.007)         \n                                                                        \nOutstate                                                -0.003          \n                                                        (0.004)         \n                                                                        \nRoom.Board                                             -0.024**         \n                                                        (0.011)         \n                                                                        \nBooks                                                   -0.027          \n                                                        (0.049)         \n                                                                        \nPersonal                                                 0.008          \n                                                        (0.014)         \n                                                                        \nPhD                                                     -0.431          \n                                                        (1.002)         \n                                                                        \nTerminal                                                -0.540          \n                                                        (1.094)         \n                                                                        \nS.F.Ratio                                               -0.253          \n                                                        (2.843)         \n                                                                        \nperc.alumni                                            2.319***         \n                                                        (0.879)         \n                                                                        \nExpend                                                   0.003          \n                                                        (0.003)         \n                                                                        \nGrad.Rate                                                0.136          \n                                                        (0.648)         \n                                                                        \nConstant                    86.881***                  187.938**        \n                             (20.984)                  (89.615)         \n                                                                        \n------------------------------------------------------------------------\nObservations                   622                        622           \nR2                            0.820                      0.956          \nAdjusted R2                   0.819                      0.955          \nResidual Std. Error     403.989 (df = 619)        202.349 (df = 604)    \nF Statistic         1,405.761*** (df = 2; 619) 768.822*** (df = 17; 604)\n========================================================================\nNote:                                        *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nYou can also also use stargazer to get the latex code for a table. This is useful if you want to include the table in a paper or a presentation.\n\nstargazer(small_enroll_reg, enroll_reg, \n          type = \"latex\", \n          column.labels = c(\"Small Model\", \"Full Model\"))\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nEnroll\n\n\n\n\n\n\nSmall Model\n\n\nFull Model\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nPrivateYes\n\n\n\n\n7.807\n\n\n\n\n\n\n\n\n(30.036)\n\n\n\n\n\n\n\n\n\n\n\n\nApps\n\n\n-0.052***\n\n\n-0.028***\n\n\n\n\n\n\n(0.013)\n\n\n(0.008)\n\n\n\n\n\n\n\n\n\n\n\n\nAccept\n\n\n0.424***\n\n\n0.147***\n\n\n\n\n\n\n(0.021)\n\n\n(0.015)\n\n\n\n\n\n\n\n\n\n\n\n\nTop10perc\n\n\n\n\n4.016***\n\n\n\n\n\n\n\n\n(1.283)\n\n\n\n\n\n\n\n\n\n\n\n\nTop25perc\n\n\n\n\n-2.269**\n\n\n\n\n\n\n\n\n(0.997)\n\n\n\n\n\n\n\n\n\n\n\n\nF.Undergrad\n\n\n\n\n0.144***\n\n\n\n\n\n\n\n\n(0.004)\n\n\n\n\n\n\n\n\n\n\n\n\nP.Undergrad\n\n\n\n\n-0.012*\n\n\n\n\n\n\n\n\n(0.007)\n\n\n\n\n\n\n\n\n\n\n\n\nOutstate\n\n\n\n\n-0.003\n\n\n\n\n\n\n\n\n(0.004)\n\n\n\n\n\n\n\n\n\n\n\n\nRoom.Board\n\n\n\n\n-0.024**\n\n\n\n\n\n\n\n\n(0.011)\n\n\n\n\n\n\n\n\n\n\n\n\nBooks\n\n\n\n\n-0.027\n\n\n\n\n\n\n\n\n(0.049)\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal\n\n\n\n\n0.008\n\n\n\n\n\n\n\n\n(0.014)\n\n\n\n\n\n\n\n\n\n\n\n\nPhD\n\n\n\n\n-0.431\n\n\n\n\n\n\n\n\n(1.002)\n\n\n\n\n\n\n\n\n\n\n\n\nTerminal\n\n\n\n\n-0.540\n\n\n\n\n\n\n\n\n(1.094)\n\n\n\n\n\n\n\n\n\n\n\n\nS.F.Ratio\n\n\n\n\n-0.253\n\n\n\n\n\n\n\n\n(2.843)\n\n\n\n\n\n\n\n\n\n\n\n\nperc.alumni\n\n\n\n\n2.319***\n\n\n\n\n\n\n\n\n(0.879)\n\n\n\n\n\n\n\n\n\n\n\n\nExpend\n\n\n\n\n0.003\n\n\n\n\n\n\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\n\n\nGrad.Rate\n\n\n\n\n0.136\n\n\n\n\n\n\n\n\n(0.648)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n86.881***\n\n\n187.938**\n\n\n\n\n\n\n(20.984)\n\n\n(89.615)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n622\n\n\n622\n\n\n\n\nR2\n\n\n0.820\n\n\n0.956\n\n\n\n\nAdjusted R2\n\n\n0.819\n\n\n0.955\n\n\n\n\nResidual Std. Error\n\n\n403.989 (df = 619)\n\n\n202.349 (df = 604)\n\n\n\n\nF Statistic\n\n\n1,405.761*** (df = 2; 619)\n\n\n768.822*** (df = 17; 604)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01"
  },
  {
    "objectID": "api 222 files/section 5/section 5.2.html",
    "href": "api 222 files/section 5/section 5.2.html",
    "title": "Section 5.2 - Cross-Validation, Ridge, Lasso",
    "section": "",
    "text": "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n\n\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\nDownload the dataset here:\n\ndiabetes_data &lt;- read.csv(\"diabetes.csv\", header = TRUE)\n\nLet’s get to know our data.\n\ndim(diabetes_data)\n\n[1] 768   9\n\nsummary(diabetes_data)\n\n  Pregnancies        Glucose      BloodPressure    SkinThickness  \n Min.   : 0.000   Min.   :  0.0   Min.   :  0.00   Min.   : 0.00  \n 1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.: 0.00  \n Median : 3.000   Median :117.0   Median : 72.00   Median :23.00  \n Mean   : 3.845   Mean   :120.9   Mean   : 69.11   Mean   :20.54  \n 3rd Qu.: 6.000   3rd Qu.:140.2   3rd Qu.: 80.00   3rd Qu.:32.00  \n Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00  \n    Insulin           BMI        DiabetesPedigreeFunction      Age       \n Min.   :  0.0   Min.   : 0.00   Min.   :0.0780           Min.   :21.00  \n 1st Qu.:  0.0   1st Qu.:27.30   1st Qu.:0.2437           1st Qu.:24.00  \n Median : 30.5   Median :32.00   Median :0.3725           Median :29.00  \n Mean   : 79.8   Mean   :31.99   Mean   :0.4719           Mean   :33.24  \n 3rd Qu.:127.2   3rd Qu.:36.60   3rd Qu.:0.6262           3rd Qu.:41.00  \n Max.   :846.0   Max.   :67.10   Max.   :2.4200           Max.   :81.00  \n    Outcome     \n Min.   :0.000  \n 1st Qu.:0.000  \n Median :0.000  \n Mean   :0.349  \n 3rd Qu.:1.000  \n Max.   :1.000  \n\n\nFrom the output, we see that we have one binary variable (Outcome) and eight continuous variables. We have 768 observations.\nToday, we will focus on predicting Outcome: (whether or not a patient has diabetes)\nLet’s start with KNN for classification, so load the required package:\n\nlibrary(class)\n\nWe are interested in using KNN on this dataset, but we aren’t sure which k will give the best out-of-sample accuracy. We can use cross-validation to find the best k in this sense.\n\n\n\nAn R function is created by using the keyword function. The basic syntax of an R function definition is as follows −\n\nfunction_name &lt;- function(arg_1, arg_2, ...) {\n   Function body \n}\n\nThe different parts of a function are:\n\nFunction Name: This is the actual name of the function. It is stored in R environment as an object with this name.\nArguments: An argument is a placeholder. When a function is invoked, you pass a value to the argument. Arguments are optional; that is, a function may contain no arguments. Also arguments can have default values.\nFunction Body: The function body contains a collection of statements that defines what the function does.\nReturn Value: The return value of a function is the last expression in the function body to be evaluated.\n\n\n## Example of a simple function\nadd_fun &lt;- function(a1, a2){\n  res = a1 + a2\n  return(res)\n}\n\nadd_fun(3, 4)\n\n[1] 7\n\n\n\n\n\nLet’s write a function to do cross-validation. We will name the function cross_validation_KNN() and it will take 3 arguments:\n\nThe feature columns of the data\nThe outcome column of the data (Y)\nA vector of all k values to test\nThe number of folds to use in k-fold cross validation\n\n\ncross_validation_KNN  &lt;- function(data_x, data_y, k_seq, kfolds) {\n    \n  ## We will start by assigning each observation to one and \n  ## only one fold for cross-validation. To do this, we use\n  ## an index vector. The vector's length equals the number\n  ## of observations in the data. The vector's entries are \n  ## equal numbers of 1s, 2s, etc. up to the number of folds\n  ## being used for k-fold cross validation.\n  ## Recall seq(5) is a vector (1, 2, 3, 4, 5)\n  ## ceiling() rounds a number up to the nearest integer\n  ## rep(a, b) repeats a b times (e.g. rep(1, 3) =&gt; (1, 1, 1))\n  ## So this says repeat the sequence from 1:kfolds\n  ## enough times to fill a vector that is as long or \n  ## slightly longer than the number of observations in\n  ## the data. Then, if the length of the vector exceeds\n  ## the number of observations, truncate it to the right\n  ## length\n  \n  fold_ids &lt;- rep(seq(kfolds), ceiling(nrow(data_x) / kfolds))\n  fold_ids &lt;- fold_ids[1:nrow(data_x)]\n  \n  ## To make the IDs random, randomly rearrange the vector\n  fold_ids &lt;- sample(fold_ids, length(fold_ids))\n  \n  ## In order to store the prediction performance for each fold\n  ## for each k, we initialize a matrix.\n  CV_error_mtx  &lt;- matrix(0, nrow = length(k_seq), ncol = kfolds)\n  \n  ## To run CV, we will loop over all values of k that we want to \n  ## consider for KNN. For each value of k, we will loop over all\n  ## folds. For each fold, we will estimate KNN for the given k on \n  ## all but one fold. We will then measure the model's accuracy \n  ## on the hold out fold and save it. After we finish looping \n  ## through all k's and all folds, we will find the average CV\n  ## error for each value of k and use that as a measure of the \n  ## model's performance.\n  for (k in k_seq) {\n    for (fold in 1:kfolds) {\n      ## Train the KNN model (Note: if it throws a weird error, make sure\n      ## all features are numeric variables -- not factors)\n      ## Note: usually the features are normalized/re-scaled\n      ## Otherwise KNN will give more weight to variables at the larger scale\n      ## when calculating the distance metric.\n      ## See page 165 of the textbook for a useful example\n      knn_fold_model &lt;- knn(train = scale(data_x[which(fold_ids != fold),]),\n                            test = scale(data_x[which(fold_ids == fold),]),\n                            cl = data_y[which(fold_ids != fold)],\n                            k = k)\n      \n      ## Measure and save error rate (% wrong)\n      CV_error_mtx[k, fold] &lt;- mean(knn_fold_model != \n                                      data_y[which(fold_ids == fold)])\n    }\n  }\n  \n  ## We want our function to return the accuracy matrix\n  return(CV_error_mtx)\n    \n}\n\n\n\n\nWe can now use the cross-validation function on real data. Note that the outcome is stored in the 9th column.\n\nset.seed(222)\nknn_cv_error5 &lt;- cross_validation_KNN(data_x = diabetes_data[, -9],\n                                      data_y = diabetes_data$Outcome,\n                                      k_seq = seq(20),\n                                      kfolds = 5)\nprint(knn_cv_error5)\n\n           [,1]      [,2]      [,3]      [,4]      [,5]\n [1,] 0.3636364 0.3246753 0.2857143 0.3137255 0.3137255\n [2,] 0.2857143 0.3051948 0.2727273 0.3137255 0.3137255\n [3,] 0.2727273 0.2727273 0.2337662 0.2745098 0.3071895\n [4,] 0.2597403 0.2727273 0.2857143 0.2614379 0.3464052\n [5,] 0.2402597 0.2922078 0.2532468 0.2483660 0.2941176\n [6,] 0.2662338 0.3116883 0.2662338 0.2549020 0.3071895\n [7,] 0.2207792 0.2987013 0.2792208 0.2549020 0.2941176\n [8,] 0.2467532 0.2922078 0.2857143 0.2549020 0.2875817\n [9,] 0.2207792 0.2987013 0.2792208 0.2614379 0.2875817\n[10,] 0.2402597 0.2922078 0.2597403 0.2745098 0.2549020\n[11,] 0.2272727 0.2857143 0.2597403 0.2549020 0.2745098\n[12,] 0.2207792 0.2857143 0.2662338 0.2549020 0.2875817\n[13,] 0.2402597 0.2727273 0.2727273 0.2549020 0.2745098\n[14,] 0.2337662 0.2727273 0.2727273 0.2352941 0.2745098\n[15,] 0.2337662 0.2597403 0.2467532 0.2418301 0.2614379\n[16,] 0.2662338 0.2467532 0.2337662 0.2483660 0.2549020\n[17,] 0.2597403 0.2597403 0.2467532 0.2418301 0.2614379\n[18,] 0.2597403 0.2272727 0.2662338 0.2222222 0.2549020\n[19,] 0.2662338 0.2402597 0.2532468 0.2222222 0.2810458\n[20,] 0.2597403 0.2337662 0.2402597 0.2483660 0.2745098\n\nknn_cv_error10 &lt;- cross_validation_KNN(data_x = diabetes_data[,-9],\n                                       data_y = diabetes_data$Outcome,\n                                       k_seq = seq(50),\n                                       kfolds = 10)\nprint(knn_cv_error10)\n\n           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n [1,] 0.2857143 0.2727273 0.3246753 0.2727273 0.3246753 0.2597403 0.3636364\n [2,] 0.3506494 0.2467532 0.2597403 0.2987013 0.2597403 0.2857143 0.4675325\n [3,] 0.2207792 0.2337662 0.3246753 0.2337662 0.3376623 0.1948052 0.3506494\n [4,] 0.2597403 0.2467532 0.2597403 0.2857143 0.2987013 0.1558442 0.3506494\n [5,] 0.2727273 0.2857143 0.3246753 0.2467532 0.3506494 0.1688312 0.3246753\n [6,] 0.2857143 0.2597403 0.3116883 0.2467532 0.3376623 0.1818182 0.2987013\n [7,] 0.2727273 0.2337662 0.3246753 0.2337662 0.2857143 0.1688312 0.2857143\n [8,] 0.3116883 0.2077922 0.2857143 0.2467532 0.3116883 0.1818182 0.3116883\n [9,] 0.2467532 0.1818182 0.2987013 0.2467532 0.2987013 0.1818182 0.2857143\n[10,] 0.2337662 0.1818182 0.2987013 0.2337662 0.2727273 0.2207792 0.2727273\n[11,] 0.2467532 0.2077922 0.3246753 0.2077922 0.3116883 0.1948052 0.2987013\n[12,] 0.2467532 0.1818182 0.2857143 0.2337662 0.3246753 0.1818182 0.2467532\n[13,] 0.2207792 0.1688312 0.2987013 0.2727273 0.2987013 0.2077922 0.2597403\n[14,] 0.2467532 0.1948052 0.2987013 0.2467532 0.2987013 0.2337662 0.2597403\n[15,] 0.2467532 0.1818182 0.2727273 0.2207792 0.2987013 0.2337662 0.2727273\n[16,] 0.2077922 0.1818182 0.2857143 0.2337662 0.3116883 0.2467532 0.2987013\n[17,] 0.2207792 0.1818182 0.2727273 0.2337662 0.3376623 0.2337662 0.2597403\n[18,] 0.2467532 0.1818182 0.2727273 0.2597403 0.3246753 0.2337662 0.2597403\n[19,] 0.2207792 0.1818182 0.2727273 0.2207792 0.3246753 0.2467532 0.2597403\n[20,] 0.2077922 0.1818182 0.2857143 0.2467532 0.3116883 0.2077922 0.2857143\n[21,] 0.2337662 0.1948052 0.2857143 0.2077922 0.3246753 0.2207792 0.2597403\n[22,] 0.2077922 0.1818182 0.3376623 0.2337662 0.3376623 0.2207792 0.2727273\n[23,] 0.1948052 0.1818182 0.3116883 0.2337662 0.3116883 0.2077922 0.2727273\n[24,] 0.2207792 0.1428571 0.3116883 0.2077922 0.3246753 0.2077922 0.2337662\n[25,] 0.2207792 0.1948052 0.3246753 0.2077922 0.3246753 0.2077922 0.2467532\n[26,] 0.2207792 0.1948052 0.3376623 0.2337662 0.2727273 0.1818182 0.2727273\n[27,] 0.2467532 0.1818182 0.3376623 0.2337662 0.2727273 0.1948052 0.2857143\n[28,] 0.2337662 0.1818182 0.3246753 0.2207792 0.2857143 0.1948052 0.2857143\n[29,] 0.2467532 0.1948052 0.3506494 0.2467532 0.2987013 0.2077922 0.2467532\n[30,] 0.2337662 0.1948052 0.3506494 0.2207792 0.3116883 0.2077922 0.2727273\n[31,] 0.2207792 0.1948052 0.3376623 0.2337662 0.2987013 0.2207792 0.2727273\n[32,] 0.2207792 0.1818182 0.3376623 0.2207792 0.2987013 0.2207792 0.2597403\n[33,] 0.1948052 0.1818182 0.3376623 0.2467532 0.2987013 0.2077922 0.2597403\n[34,] 0.1818182 0.1948052 0.3506494 0.2077922 0.2857143 0.1948052 0.2597403\n[35,] 0.1818182 0.1818182 0.3376623 0.2207792 0.2987013 0.2077922 0.2337662\n[36,] 0.2077922 0.1818182 0.3376623 0.1948052 0.3116883 0.1948052 0.2337662\n[37,] 0.1948052 0.1818182 0.3506494 0.2337662 0.2987013 0.2207792 0.2337662\n[38,] 0.2077922 0.1818182 0.3246753 0.2467532 0.2987013 0.2337662 0.2337662\n[39,] 0.2207792 0.1948052 0.3246753 0.2077922 0.2987013 0.2337662 0.2597403\n[40,] 0.2077922 0.1948052 0.3116883 0.2207792 0.2987013 0.2337662 0.2597403\n[41,] 0.2077922 0.1948052 0.3246753 0.2077922 0.2987013 0.2337662 0.2597403\n[42,] 0.2207792 0.1948052 0.3376623 0.1818182 0.3116883 0.2207792 0.2597403\n[43,] 0.2207792 0.1818182 0.3246753 0.1948052 0.3116883 0.2337662 0.2467532\n[44,] 0.1948052 0.1948052 0.3116883 0.2077922 0.3116883 0.2597403 0.2467532\n[45,] 0.2337662 0.1818182 0.3116883 0.1948052 0.2987013 0.2207792 0.2467532\n[46,] 0.2337662 0.1948052 0.2987013 0.1948052 0.2987013 0.2337662 0.2597403\n[47,] 0.2467532 0.1948052 0.3376623 0.2207792 0.2987013 0.2467532 0.2467532\n[48,] 0.2467532 0.1688312 0.3376623 0.2077922 0.2987013 0.2207792 0.2467532\n[49,] 0.2337662 0.1688312 0.3376623 0.2077922 0.2857143 0.2467532 0.2467532\n[50,] 0.2337662 0.1688312 0.3376623 0.2207792 0.2857143 0.2337662 0.2337662\n           [,8]      [,9]     [,10]\n [1,] 0.2727273 0.3026316 0.2631579\n [2,] 0.2467532 0.3289474 0.3157895\n [3,] 0.3246753 0.2105263 0.2500000\n [4,] 0.2207792 0.3026316 0.2368421\n [5,] 0.2337662 0.2894737 0.2368421\n [6,] 0.2597403 0.3157895 0.2368421\n [7,] 0.2597403 0.2763158 0.2763158\n [8,] 0.2597403 0.3157895 0.2105263\n [9,] 0.2207792 0.2631579 0.2368421\n[10,] 0.2077922 0.3026316 0.2500000\n[11,] 0.1948052 0.2631579 0.2894737\n[12,] 0.1948052 0.2894737 0.2763158\n[13,] 0.2077922 0.2631579 0.2368421\n[14,] 0.2077922 0.2763158 0.2763158\n[15,] 0.1818182 0.2500000 0.2631579\n[16,] 0.2207792 0.2105263 0.2500000\n[17,] 0.2207792 0.2236842 0.2631579\n[18,] 0.2337662 0.2631579 0.2894737\n[19,] 0.1948052 0.2368421 0.2763158\n[20,] 0.2077922 0.2368421 0.2631579\n[21,] 0.2077922 0.2500000 0.2631579\n[22,] 0.2207792 0.2500000 0.2631579\n[23,] 0.2077922 0.2500000 0.2631579\n[24,] 0.2077922 0.2631579 0.2894737\n[25,] 0.2077922 0.2500000 0.2763158\n[26,] 0.2207792 0.2631579 0.2894737\n[27,] 0.1948052 0.2631579 0.2763158\n[28,] 0.2207792 0.2368421 0.2894737\n[29,] 0.1948052 0.2500000 0.3026316\n[30,] 0.1948052 0.2500000 0.2894737\n[31,] 0.1818182 0.2631579 0.3026316\n[32,] 0.2077922 0.2500000 0.3157895\n[33,] 0.1818182 0.2500000 0.2894737\n[34,] 0.2077922 0.2631579 0.2894737\n[35,] 0.2207792 0.2500000 0.2894737\n[36,] 0.2207792 0.2368421 0.2763158\n[37,] 0.2207792 0.2500000 0.2763158\n[38,] 0.2077922 0.2368421 0.2894737\n[39,] 0.2207792 0.2500000 0.2763158\n[40,] 0.2207792 0.2631579 0.3157895\n[41,] 0.2077922 0.2500000 0.2763158\n[42,] 0.2207792 0.2500000 0.2894737\n[43,] 0.2077922 0.2500000 0.2894737\n[44,] 0.2207792 0.2631579 0.3026316\n[45,] 0.2077922 0.2631579 0.2894737\n[46,] 0.1948052 0.2631579 0.2894737\n[47,] 0.2077922 0.2763158 0.2894737\n[48,] 0.2077922 0.2631579 0.3026316\n[49,] 0.2077922 0.2631579 0.3026316\n[50,] 0.2077922 0.2631579 0.3026316\n\n\nWe are usually interested in the mean CV error or accuracy for each parameter considered (value of k in this case). To find the mean for each row, we can use the function rowMeans(). Note that an equivalent function exists for taking the means of columns: colMeans(). There are also functions for rowSums() and colSums().\n\nmean_cv_error5 &lt;- rowMeans(knn_cv_error5)\nmean_cv_error10 &lt;- rowMeans(knn_cv_error10)\n\nTo find the position of the smallest CV error, use which.min().\n\nwhich.min(mean_cv_error5)\n\n[1] 18\n\nwhich.min(mean_cv_error10)\n\n[1] 36\n\n\nTo understand how perform changes across the K values, a plot would be helpful.\n\nplot(seq(20), mean_cv_error5, type = \"l\",\n     main = \"Mean CV Error Rate as a Function of k\",\n     ylab = \"CV Error Rate\", xlab = \"k\")\n\n\n\n\nFrom this, we see that low k values have poor performance while large k values seem to have roughly equivalent performance."
  },
  {
    "objectID": "api 222 files/section 5/section 5.2.html#about-dataset",
    "href": "api 222 files/section 5/section 5.2.html#about-dataset",
    "title": "Section 5.2 - Cross-Validation, Ridge, Lasso",
    "section": "",
    "text": "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n\n\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\nDownload the dataset here:\n\ndiabetes_data &lt;- read.csv(\"diabetes.csv\", header = TRUE)\n\nLet’s get to know our data.\n\ndim(diabetes_data)\n\n[1] 768   9\n\nsummary(diabetes_data)\n\n  Pregnancies        Glucose      BloodPressure    SkinThickness  \n Min.   : 0.000   Min.   :  0.0   Min.   :  0.00   Min.   : 0.00  \n 1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.: 0.00  \n Median : 3.000   Median :117.0   Median : 72.00   Median :23.00  \n Mean   : 3.845   Mean   :120.9   Mean   : 69.11   Mean   :20.54  \n 3rd Qu.: 6.000   3rd Qu.:140.2   3rd Qu.: 80.00   3rd Qu.:32.00  \n Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00  \n    Insulin           BMI        DiabetesPedigreeFunction      Age       \n Min.   :  0.0   Min.   : 0.00   Min.   :0.0780           Min.   :21.00  \n 1st Qu.:  0.0   1st Qu.:27.30   1st Qu.:0.2437           1st Qu.:24.00  \n Median : 30.5   Median :32.00   Median :0.3725           Median :29.00  \n Mean   : 79.8   Mean   :31.99   Mean   :0.4719           Mean   :33.24  \n 3rd Qu.:127.2   3rd Qu.:36.60   3rd Qu.:0.6262           3rd Qu.:41.00  \n Max.   :846.0   Max.   :67.10   Max.   :2.4200           Max.   :81.00  \n    Outcome     \n Min.   :0.000  \n 1st Qu.:0.000  \n Median :0.000  \n Mean   :0.349  \n 3rd Qu.:1.000  \n Max.   :1.000  \n\n\nFrom the output, we see that we have one binary variable (Outcome) and eight continuous variables. We have 768 observations.\nToday, we will focus on predicting Outcome: (whether or not a patient has diabetes)\nLet’s start with KNN for classification, so load the required package:\n\nlibrary(class)\n\nWe are interested in using KNN on this dataset, but we aren’t sure which k will give the best out-of-sample accuracy. We can use cross-validation to find the best k in this sense.\n\n\n\nAn R function is created by using the keyword function. The basic syntax of an R function definition is as follows −\n\nfunction_name &lt;- function(arg_1, arg_2, ...) {\n   Function body \n}\n\nThe different parts of a function are:\n\nFunction Name: This is the actual name of the function. It is stored in R environment as an object with this name.\nArguments: An argument is a placeholder. When a function is invoked, you pass a value to the argument. Arguments are optional; that is, a function may contain no arguments. Also arguments can have default values.\nFunction Body: The function body contains a collection of statements that defines what the function does.\nReturn Value: The return value of a function is the last expression in the function body to be evaluated.\n\n\n## Example of a simple function\nadd_fun &lt;- function(a1, a2){\n  res = a1 + a2\n  return(res)\n}\n\nadd_fun(3, 4)\n\n[1] 7\n\n\n\n\n\nLet’s write a function to do cross-validation. We will name the function cross_validation_KNN() and it will take 3 arguments:\n\nThe feature columns of the data\nThe outcome column of the data (Y)\nA vector of all k values to test\nThe number of folds to use in k-fold cross validation\n\n\ncross_validation_KNN  &lt;- function(data_x, data_y, k_seq, kfolds) {\n    \n  ## We will start by assigning each observation to one and \n  ## only one fold for cross-validation. To do this, we use\n  ## an index vector. The vector's length equals the number\n  ## of observations in the data. The vector's entries are \n  ## equal numbers of 1s, 2s, etc. up to the number of folds\n  ## being used for k-fold cross validation.\n  ## Recall seq(5) is a vector (1, 2, 3, 4, 5)\n  ## ceiling() rounds a number up to the nearest integer\n  ## rep(a, b) repeats a b times (e.g. rep(1, 3) =&gt; (1, 1, 1))\n  ## So this says repeat the sequence from 1:kfolds\n  ## enough times to fill a vector that is as long or \n  ## slightly longer than the number of observations in\n  ## the data. Then, if the length of the vector exceeds\n  ## the number of observations, truncate it to the right\n  ## length\n  \n  fold_ids &lt;- rep(seq(kfolds), ceiling(nrow(data_x) / kfolds))\n  fold_ids &lt;- fold_ids[1:nrow(data_x)]\n  \n  ## To make the IDs random, randomly rearrange the vector\n  fold_ids &lt;- sample(fold_ids, length(fold_ids))\n  \n  ## In order to store the prediction performance for each fold\n  ## for each k, we initialize a matrix.\n  CV_error_mtx  &lt;- matrix(0, nrow = length(k_seq), ncol = kfolds)\n  \n  ## To run CV, we will loop over all values of k that we want to \n  ## consider for KNN. For each value of k, we will loop over all\n  ## folds. For each fold, we will estimate KNN for the given k on \n  ## all but one fold. We will then measure the model's accuracy \n  ## on the hold out fold and save it. After we finish looping \n  ## through all k's and all folds, we will find the average CV\n  ## error for each value of k and use that as a measure of the \n  ## model's performance.\n  for (k in k_seq) {\n    for (fold in 1:kfolds) {\n      ## Train the KNN model (Note: if it throws a weird error, make sure\n      ## all features are numeric variables -- not factors)\n      ## Note: usually the features are normalized/re-scaled\n      ## Otherwise KNN will give more weight to variables at the larger scale\n      ## when calculating the distance metric.\n      ## See page 165 of the textbook for a useful example\n      knn_fold_model &lt;- knn(train = scale(data_x[which(fold_ids != fold),]),\n                            test = scale(data_x[which(fold_ids == fold),]),\n                            cl = data_y[which(fold_ids != fold)],\n                            k = k)\n      \n      ## Measure and save error rate (% wrong)\n      CV_error_mtx[k, fold] &lt;- mean(knn_fold_model != \n                                      data_y[which(fold_ids == fold)])\n    }\n  }\n  \n  ## We want our function to return the accuracy matrix\n  return(CV_error_mtx)\n    \n}\n\n\n\n\nWe can now use the cross-validation function on real data. Note that the outcome is stored in the 9th column.\n\nset.seed(222)\nknn_cv_error5 &lt;- cross_validation_KNN(data_x = diabetes_data[, -9],\n                                      data_y = diabetes_data$Outcome,\n                                      k_seq = seq(20),\n                                      kfolds = 5)\nprint(knn_cv_error5)\n\n           [,1]      [,2]      [,3]      [,4]      [,5]\n [1,] 0.3636364 0.3246753 0.2857143 0.3137255 0.3137255\n [2,] 0.2857143 0.3051948 0.2727273 0.3137255 0.3137255\n [3,] 0.2727273 0.2727273 0.2337662 0.2745098 0.3071895\n [4,] 0.2597403 0.2727273 0.2857143 0.2614379 0.3464052\n [5,] 0.2402597 0.2922078 0.2532468 0.2483660 0.2941176\n [6,] 0.2662338 0.3116883 0.2662338 0.2549020 0.3071895\n [7,] 0.2207792 0.2987013 0.2792208 0.2549020 0.2941176\n [8,] 0.2467532 0.2922078 0.2857143 0.2549020 0.2875817\n [9,] 0.2207792 0.2987013 0.2792208 0.2614379 0.2875817\n[10,] 0.2402597 0.2922078 0.2597403 0.2745098 0.2549020\n[11,] 0.2272727 0.2857143 0.2597403 0.2549020 0.2745098\n[12,] 0.2207792 0.2857143 0.2662338 0.2549020 0.2875817\n[13,] 0.2402597 0.2727273 0.2727273 0.2549020 0.2745098\n[14,] 0.2337662 0.2727273 0.2727273 0.2352941 0.2745098\n[15,] 0.2337662 0.2597403 0.2467532 0.2418301 0.2614379\n[16,] 0.2662338 0.2467532 0.2337662 0.2483660 0.2549020\n[17,] 0.2597403 0.2597403 0.2467532 0.2418301 0.2614379\n[18,] 0.2597403 0.2272727 0.2662338 0.2222222 0.2549020\n[19,] 0.2662338 0.2402597 0.2532468 0.2222222 0.2810458\n[20,] 0.2597403 0.2337662 0.2402597 0.2483660 0.2745098\n\nknn_cv_error10 &lt;- cross_validation_KNN(data_x = diabetes_data[,-9],\n                                       data_y = diabetes_data$Outcome,\n                                       k_seq = seq(50),\n                                       kfolds = 10)\nprint(knn_cv_error10)\n\n           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n [1,] 0.2857143 0.2727273 0.3246753 0.2727273 0.3246753 0.2597403 0.3636364\n [2,] 0.3506494 0.2467532 0.2597403 0.2987013 0.2597403 0.2857143 0.4675325\n [3,] 0.2207792 0.2337662 0.3246753 0.2337662 0.3376623 0.1948052 0.3506494\n [4,] 0.2597403 0.2467532 0.2597403 0.2857143 0.2987013 0.1558442 0.3506494\n [5,] 0.2727273 0.2857143 0.3246753 0.2467532 0.3506494 0.1688312 0.3246753\n [6,] 0.2857143 0.2597403 0.3116883 0.2467532 0.3376623 0.1818182 0.2987013\n [7,] 0.2727273 0.2337662 0.3246753 0.2337662 0.2857143 0.1688312 0.2857143\n [8,] 0.3116883 0.2077922 0.2857143 0.2467532 0.3116883 0.1818182 0.3116883\n [9,] 0.2467532 0.1818182 0.2987013 0.2467532 0.2987013 0.1818182 0.2857143\n[10,] 0.2337662 0.1818182 0.2987013 0.2337662 0.2727273 0.2207792 0.2727273\n[11,] 0.2467532 0.2077922 0.3246753 0.2077922 0.3116883 0.1948052 0.2987013\n[12,] 0.2467532 0.1818182 0.2857143 0.2337662 0.3246753 0.1818182 0.2467532\n[13,] 0.2207792 0.1688312 0.2987013 0.2727273 0.2987013 0.2077922 0.2597403\n[14,] 0.2467532 0.1948052 0.2987013 0.2467532 0.2987013 0.2337662 0.2597403\n[15,] 0.2467532 0.1818182 0.2727273 0.2207792 0.2987013 0.2337662 0.2727273\n[16,] 0.2077922 0.1818182 0.2857143 0.2337662 0.3116883 0.2467532 0.2987013\n[17,] 0.2207792 0.1818182 0.2727273 0.2337662 0.3376623 0.2337662 0.2597403\n[18,] 0.2467532 0.1818182 0.2727273 0.2597403 0.3246753 0.2337662 0.2597403\n[19,] 0.2207792 0.1818182 0.2727273 0.2207792 0.3246753 0.2467532 0.2597403\n[20,] 0.2077922 0.1818182 0.2857143 0.2467532 0.3116883 0.2077922 0.2857143\n[21,] 0.2337662 0.1948052 0.2857143 0.2077922 0.3246753 0.2207792 0.2597403\n[22,] 0.2077922 0.1818182 0.3376623 0.2337662 0.3376623 0.2207792 0.2727273\n[23,] 0.1948052 0.1818182 0.3116883 0.2337662 0.3116883 0.2077922 0.2727273\n[24,] 0.2207792 0.1428571 0.3116883 0.2077922 0.3246753 0.2077922 0.2337662\n[25,] 0.2207792 0.1948052 0.3246753 0.2077922 0.3246753 0.2077922 0.2467532\n[26,] 0.2207792 0.1948052 0.3376623 0.2337662 0.2727273 0.1818182 0.2727273\n[27,] 0.2467532 0.1818182 0.3376623 0.2337662 0.2727273 0.1948052 0.2857143\n[28,] 0.2337662 0.1818182 0.3246753 0.2207792 0.2857143 0.1948052 0.2857143\n[29,] 0.2467532 0.1948052 0.3506494 0.2467532 0.2987013 0.2077922 0.2467532\n[30,] 0.2337662 0.1948052 0.3506494 0.2207792 0.3116883 0.2077922 0.2727273\n[31,] 0.2207792 0.1948052 0.3376623 0.2337662 0.2987013 0.2207792 0.2727273\n[32,] 0.2207792 0.1818182 0.3376623 0.2207792 0.2987013 0.2207792 0.2597403\n[33,] 0.1948052 0.1818182 0.3376623 0.2467532 0.2987013 0.2077922 0.2597403\n[34,] 0.1818182 0.1948052 0.3506494 0.2077922 0.2857143 0.1948052 0.2597403\n[35,] 0.1818182 0.1818182 0.3376623 0.2207792 0.2987013 0.2077922 0.2337662\n[36,] 0.2077922 0.1818182 0.3376623 0.1948052 0.3116883 0.1948052 0.2337662\n[37,] 0.1948052 0.1818182 0.3506494 0.2337662 0.2987013 0.2207792 0.2337662\n[38,] 0.2077922 0.1818182 0.3246753 0.2467532 0.2987013 0.2337662 0.2337662\n[39,] 0.2207792 0.1948052 0.3246753 0.2077922 0.2987013 0.2337662 0.2597403\n[40,] 0.2077922 0.1948052 0.3116883 0.2207792 0.2987013 0.2337662 0.2597403\n[41,] 0.2077922 0.1948052 0.3246753 0.2077922 0.2987013 0.2337662 0.2597403\n[42,] 0.2207792 0.1948052 0.3376623 0.1818182 0.3116883 0.2207792 0.2597403\n[43,] 0.2207792 0.1818182 0.3246753 0.1948052 0.3116883 0.2337662 0.2467532\n[44,] 0.1948052 0.1948052 0.3116883 0.2077922 0.3116883 0.2597403 0.2467532\n[45,] 0.2337662 0.1818182 0.3116883 0.1948052 0.2987013 0.2207792 0.2467532\n[46,] 0.2337662 0.1948052 0.2987013 0.1948052 0.2987013 0.2337662 0.2597403\n[47,] 0.2467532 0.1948052 0.3376623 0.2207792 0.2987013 0.2467532 0.2467532\n[48,] 0.2467532 0.1688312 0.3376623 0.2077922 0.2987013 0.2207792 0.2467532\n[49,] 0.2337662 0.1688312 0.3376623 0.2077922 0.2857143 0.2467532 0.2467532\n[50,] 0.2337662 0.1688312 0.3376623 0.2207792 0.2857143 0.2337662 0.2337662\n           [,8]      [,9]     [,10]\n [1,] 0.2727273 0.3026316 0.2631579\n [2,] 0.2467532 0.3289474 0.3157895\n [3,] 0.3246753 0.2105263 0.2500000\n [4,] 0.2207792 0.3026316 0.2368421\n [5,] 0.2337662 0.2894737 0.2368421\n [6,] 0.2597403 0.3157895 0.2368421\n [7,] 0.2597403 0.2763158 0.2763158\n [8,] 0.2597403 0.3157895 0.2105263\n [9,] 0.2207792 0.2631579 0.2368421\n[10,] 0.2077922 0.3026316 0.2500000\n[11,] 0.1948052 0.2631579 0.2894737\n[12,] 0.1948052 0.2894737 0.2763158\n[13,] 0.2077922 0.2631579 0.2368421\n[14,] 0.2077922 0.2763158 0.2763158\n[15,] 0.1818182 0.2500000 0.2631579\n[16,] 0.2207792 0.2105263 0.2500000\n[17,] 0.2207792 0.2236842 0.2631579\n[18,] 0.2337662 0.2631579 0.2894737\n[19,] 0.1948052 0.2368421 0.2763158\n[20,] 0.2077922 0.2368421 0.2631579\n[21,] 0.2077922 0.2500000 0.2631579\n[22,] 0.2207792 0.2500000 0.2631579\n[23,] 0.2077922 0.2500000 0.2631579\n[24,] 0.2077922 0.2631579 0.2894737\n[25,] 0.2077922 0.2500000 0.2763158\n[26,] 0.2207792 0.2631579 0.2894737\n[27,] 0.1948052 0.2631579 0.2763158\n[28,] 0.2207792 0.2368421 0.2894737\n[29,] 0.1948052 0.2500000 0.3026316\n[30,] 0.1948052 0.2500000 0.2894737\n[31,] 0.1818182 0.2631579 0.3026316\n[32,] 0.2077922 0.2500000 0.3157895\n[33,] 0.1818182 0.2500000 0.2894737\n[34,] 0.2077922 0.2631579 0.2894737\n[35,] 0.2207792 0.2500000 0.2894737\n[36,] 0.2207792 0.2368421 0.2763158\n[37,] 0.2207792 0.2500000 0.2763158\n[38,] 0.2077922 0.2368421 0.2894737\n[39,] 0.2207792 0.2500000 0.2763158\n[40,] 0.2207792 0.2631579 0.3157895\n[41,] 0.2077922 0.2500000 0.2763158\n[42,] 0.2207792 0.2500000 0.2894737\n[43,] 0.2077922 0.2500000 0.2894737\n[44,] 0.2207792 0.2631579 0.3026316\n[45,] 0.2077922 0.2631579 0.2894737\n[46,] 0.1948052 0.2631579 0.2894737\n[47,] 0.2077922 0.2763158 0.2894737\n[48,] 0.2077922 0.2631579 0.3026316\n[49,] 0.2077922 0.2631579 0.3026316\n[50,] 0.2077922 0.2631579 0.3026316\n\n\nWe are usually interested in the mean CV error or accuracy for each parameter considered (value of k in this case). To find the mean for each row, we can use the function rowMeans(). Note that an equivalent function exists for taking the means of columns: colMeans(). There are also functions for rowSums() and colSums().\n\nmean_cv_error5 &lt;- rowMeans(knn_cv_error5)\nmean_cv_error10 &lt;- rowMeans(knn_cv_error10)\n\nTo find the position of the smallest CV error, use which.min().\n\nwhich.min(mean_cv_error5)\n\n[1] 18\n\nwhich.min(mean_cv_error10)\n\n[1] 36\n\n\nTo understand how perform changes across the K values, a plot would be helpful.\n\nplot(seq(20), mean_cv_error5, type = \"l\",\n     main = \"Mean CV Error Rate as a Function of k\",\n     ylab = \"CV Error Rate\", xlab = \"k\")\n\n\n\n\nFrom this, we see that low k values have poor performance while large k values seem to have roughly equivalent performance."
  },
  {
    "objectID": "api 222 files/section 5/section 5.2.html#bonus-material",
    "href": "api 222 files/section 5/section 5.2.html#bonus-material",
    "title": "Section 5.2 - Cross-Validation, Ridge, Lasso",
    "section": "BONUS MATERIAL",
    "text": "BONUS MATERIAL\n\nLDA/QDA/Logistic CV\nGiven that our problem is a classification problem, we might be interested in seeing how well we could do with different methods, like logistic regression, LDA and QDA. Let’s see! We can edit the function we used for CV for KNN and make it relevant for these models. Note that logistic_formula = NULL means the function can run even if this formula is not specified (which it won’t be when we want to run LDA or QDA). Note also that we need to load the package MASS, because it contains the functions lda() and qda().\n\nlibrary(MASS)\ncross_validation &lt;- function(full_data, model_type, kfolds,\n                             logistic_formula = NULL) {\n  \n  ## Define fold_ids in exactly the same way as before\n  fold_ids &lt;- rep(seq(kfolds), \n                       ceiling(nrow(full_data) / kfolds))\n  fold_ids &lt;- fold_ids[1:nrow(full_data)]\n  fold_ids &lt;- sample(fold_ids, length(fold_ids))\n  \n  ## Initialize a vector to store CV error\n  CV_error_vec  &lt;- vector(length = kfolds, mode = \"numeric\")\n  \n  ## Loop through the folds\n  for (k in 1:kfolds){\n    if (model_type == \"logistic\") {\n      logistic_model &lt;- glm(logistic_formula,\n                            data = full_data[which(fold_ids != k),],\n                            family = binomial)\n      logistic_pred &lt;- predict(logistic_model,\n                               full_data[which(fold_ids == k),],\n                               type = \"response\")\n      class_pred &lt;- as.numeric(logistic_pred &gt; 0.5)\n      \n    } else if (model_type == \"LDA\") {\n      lda_model &lt;- lda(full_data[which(fold_ids != k),-9], \n                       full_data[which(fold_ids != k),9])\n      lda_pred &lt;- predict(lda_model, full_data[which(fold_ids == k), -9])\n      class_pred &lt;- lda_pred$class\n      \n    } else if (model_type == \"QDA\") {\n      qda_model &lt;- qda(full_data[which(fold_ids != k), -9], \n                       full_data[which(fold_ids != k), 9])\n      qda_pred &lt;- predict(qda_model, \n                          full_data[which(fold_ids == k), -9])\n      class_pred &lt;- qda_pred$class\n    }\n    \n    CV_error_vec[k] &lt;- mean(class_pred != full_data[which(fold_ids == k), 9])\n  }\n  return(CV_error_vec)\n}\n\n## Run CV for logistic regression:\nlogistic_formula &lt;- paste(\"Outcome\", paste(colnames(diabetes_data)[1:8],\n                                           collapse = \" + \"), \n                          sep = \" ~ \")\nlogistic_CV_error &lt;- cross_validation(diabetes_data, \"logistic\", 5, \n                                      as.formula(logistic_formula))\n\n## Run CV for LDA:\nlda_CV_error &lt;- cross_validation(diabetes_data, \"LDA\", 5)\n\n## Run CV for QDA:\nqda_CV_error &lt;- cross_validation(diabetes_data, \"QDA\", 5)\n\n## Determine the best model in terms of lowest average CV error\nprint(paste(\"Logistic Error Rate:\", round(mean(logistic_CV_error), 3)))\n\n[1] \"Logistic Error Rate: 0.221\"\n\nprint(paste(\"LDA Error Rate:\", round(mean(lda_CV_error), 3)))\n\n[1] \"LDA Error Rate: 0.231\"\n\nprint(paste(\"QDA Error Rate:\", round(mean(qda_CV_error), 3)))\n\n[1] \"QDA Error Rate: 0.249\""
  },
  {
    "objectID": "R files/9 Module/mod9.html",
    "href": "R files/9 Module/mod9.html",
    "title": "Module 9: Writing Functions",
    "section": "",
    "text": "Download a copy of Module 9 slides"
  },
  {
    "objectID": "R files/9 Module/mod9.html#writing-functions",
    "href": "R files/9 Module/mod9.html#writing-functions",
    "title": "Module 9: Writing Functions",
    "section": "",
    "text": "Download a copy of Module 9 slides"
  },
  {
    "objectID": "R files/9 Module/mod9.html#lab-9",
    "href": "R files/9 Module/mod9.html#lab-9",
    "title": "Module 9: Writing Functions",
    "section": "Lab 9",
    "text": "Lab 9\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\n\n\n\nQuestions\nRecall a function has the following form\n\nname &lt;- function(args) { \n  # body\n  do something (probably with args) \n  }\n\n\nWrite a function called calc_quadratic that takes an input x and calculates \\(f(x) = x^2 + 2x + 1\\). For example:\n\n\ncalc_quadratic(5)\n\n[1] 36\n\n\n\nWhat are the arguments to your function? What is the body of the function?\nThis function is vectorized! (Since binary operators are vectorized). Show this is true by running calc_quadratic with an input vector that is -10 to 10.\n\n\nYou realize you want to be able to work with any quadratic. Update your functions so that it can work with any quadratic in standard form \\(f(x) = ax^2 + bx + c\\).\n\n\nYour new function will take arguments x, a, b and c.\nSet the default arguments to a=1, b=2 and c=1.\n\n\nWrite a function called solve_quadratic that takes arguments a, b and c and provides the two roots using the quadratic formula.\n\nIn our outline, we suggest you:\n\nCalculate the determinant \\(\\sqrt{b^2 − 4ac}\\) and store as an intermediate value.\nReturn two values by putting them in a vector. If you stored the roots as root_1 and root_2, then the final line of code in the function should be c(root_1, root_2) or, if you prefer, return(c(root_1, root_2)).\n\n\n# fill in the ... with appropriate code\nsolve_quadratic &lt;- function(...){\n  determinant &lt;- ...\n  root_1 &lt;- ...\n  root_2 &lt;- ...\n  c(root_1, root_2)\n}\n\nThe code should work as follows:\n\nsolve_quadratic(a = -4, b = 0, c = 1)\n\n[1] -0.5  0.5\n\n\n\nWe “normalize” a variable by subtracting the mean and dividing by the standard deviation \\(\\frac{x - \\mu}{\\sigma}\\) .\n\nWrite a function called normalize that takes a vector as input and normalizes it. You should get the following output.\n\nnormalize(1:5)\n\n[1] -1.2649111 -0.6324555  0.0000000  0.6324555  1.2649111\n\n\n\nWhat output do you get when the input vector is 0:4? How about -100:-96? Why?\nWhat happens when your input vector is c(1,2,3,4,5, NA)? Rewrite the function so the result is:\nThe txhousing data set comes with ggplot. Use your normalize function in mutate to create normalized_annual_volume to make the following graph.\n\n\n# replace the ... with the appropriate code.\ntxhousing %&gt;%\n  group_by(year, city) %&gt;%\n  summarize(annual_volume = sum(volume, na.rm = TRUE)) %&gt;% \n  group_by(year) %&gt;%\n  mutate(...) %&gt;%\n  ggplot(aes(x = year, y = normalized_annual_volume)) + \n  geom_point() +\n  geom_line(aes(color = city))\n\n\n\n\n\n\n\n\nSimulating Data with Monte Carlo Simulations (Extension)\nYou will be asked to investigate statistical concepts using Monte Carlo simulations. We’ll try not to get too technical in the main body of the lab. There are some “technical notes” which you can ignore!\nIn a monte carlo simulation, you repeatedly:\n\nGenerate random samples of data using a known process.\nMake calculations based on the random sample.\nAggregate the results.\n\nFunctions and loops help us do these repetitious acts efficiently, without repeatedly writing similar code or copying and pasting.\nToday’s problem: Let us investigate how good the random number generator in R is.1 We hypothesize that rnorm(n, mean = true_mean) provides random sample of size n from the normal distribution with mean = true_mean and standard deviation = 1.\nThe lesson is organized as follows.\n\nWe do a single simulation.\nWe take the logic of the simulation, encapsulate it in functions and then run 1000s of simulations!\n\n\nA single simulation\nRecall our hypothesis is that rnorm() faithfully gives us random numbers from the normal distribution. If we test this with a single random draw, we might be misled. For example, let’s draw 30 numbers from a normal distribution with true mean of 0.5 and see if the observed mean appears statistically different from the true mean.\n\n# Setting a seed ensures replicability\nset.seed(4)\n# we set our parameters\ntrue_mean &lt;- .5\nN &lt;- 30\n# We simulate and observe outcomes\nsimulated_data &lt;- rnorm(N, mean = true_mean) # the standard deviation is 1 by default!\nobs_mean &lt;- mean(simulated_data)\nobs_mean\n\n[1] 0.9871873\n\n\nWow! The observed mean is twice what we expected given true_mean! Let’s calculate a z-score to put that in perspective. (Focus on the formulas, you’ll learn the intuition in stats class)\nA z-score is calculated \\(\\frac{\\bar{X}-\\mu}{\\frac{s_n}{\\sqrt{N}}}\\) where \\(\\bar{X}\\) is the sample mean, \\(\\mu\\) is the true mean, \\(s_n\\) is the sample standard deviation and \\(N\\) is the number of observations.\n\nobs_sd &lt;- sd(simulated_data)\nzscore &lt;- (obs_mean - true_mean) / (obs_sd / sqrt(N)) \nzscore\n\n[1] 3.303849\n\n\nWe expect the observed mean of this simulated data will be within 1.96 standard deviations of \\(\\mu\\) 95 out of 100 times.This observation is 3.3 standard deviations from Mu. The probability of that happening by chance is very small. To be more formal about this probability, we can calculate a p-value. Plug in the z-score below:\n\n(1 - pnorm(abs(zscore)))*2\n\n[1] 0.000953672\n\n\nThis says that the probability of getting this draw by chance is less than 0.1 percent or 1 in 1000.\nThat outcome seems surprising, but we could also just have made an unusual draw. In this workshop, we want to see how often we get such extreme results. We will repeat the steps above 1000 times each, but first we’ll write functions that will make this process smooth!\n\n\nWriting Helper Functions to Make Our Monte Carlo Simulation\nWe want to develop functions that automate repeated steps in our Monte Carlo. In that way, we can define a few important parameters and run the entire process without rewriting or copying and pasting code over and over again.\nAs you saw in the motivating example, we must do the following a 1000 times or B times if parameterize the number of iterations with B:\n\nSimulate data and calculate sample statistics.\nDetermine z-scores.\nTest whether the z-score is outside the threshold. Finally, we:\nMeasure to what extent our simulations match the theory.\n\nTo proceed, we’ll write the steps into their own functions, then call them in the right order in the function do_monte_carlo(). We are breaking a complicated process into smaller chunks and tackling them one by one!\nLet’s look at do_monte_carlo(). It takes a sample-size N, a true_mean, number of iterations B (1000 by default) and a significance level alpha (.05 by default). It returns the proportion of observed means that are significantly different from the true_mean with 95 percent confidence level\n\nBefore following our road map, think about how you would set up functions to automate this process. What would the inputs and outputs be of each step/function? Your processes will be different from ours, but that doesn’t mean ours is better.\n\nNow check out do_monte_carlo() below. It’s our road map.\n\ndo_monte_carlo &lt;- function(N, true_mean, B= 1000, alpha = .05){\n# step 1: Simulate B random samples and calculate sample statistics\n  sample_statistics &lt;- make_mc_sample(N, true_mean, B)\n# step 2: Determine z-scores\n  z_scores &lt;- get_zscores(sample_statistics$mean, true_mean, sample_statistics$sd, N)\n# step 3: Test whether the z-scores are outside the threshold.\n  significance &lt;- test_significance(z_scores, alpha)\n# step 4: Measure to what extent our simulations match the theory. (We expect a number close to alpha)\n  mean(significance)\n}\n\n\n\nDetermine z-scores\nWe’ll start with step 2 determine z-scores. Recall the formula for a zscore is\\(\\frac{\\bar{X}-\\mu}{\\frac{s_n}{\\sqrt{N}}}\\).\nWrite a function called get_zscores that takes the observed means and sds, the true mean and N as inputs and returns a z-score as an output. Name the arguments obs_mean, true_mean, obs_sd, and N.\nIf your functions works, it should return 4 for test.\n\ntest &lt;- get_zscores(obs_mean = 4.4, true_mean = 4.3, obs_sd = 0.25, N = 100)\ntest\n\n[1] 4\n\n\nThe function you wrote should also work on vectorized functions. Run the following code which takes estimates of the mean and standard deviation from 5 random draws and returns their associated z-scores:\n\n# before running set eval = TRUE (and delete this comment)\nmade_up_means &lt;- c(4.4, 4.1, 4.2, 4.4, 4.2)\nmade_up_sd &lt;- c(.25, .5, .4, 1, .4)\nmade_up_zscores &lt;- get_zscores(obs_mean = made_up_means,\n                               true_mean = 4.3,\n                               obs_sd = made_up_sd,\n                               N = 100)\nmade_up_zscores\n\n\nWhich observation from made_up_zscores is not statistically different from 4.3 with 95 percent confidence? In other words, which observed mean and standard deviation return \\(|z-score| &lt; 1.96\\)?\n\n\n\n\nCheck for Significance\nNow we write code for step 3. Test whether the z-scores are outside the threshold.\nThe threshold depends on alpha and the formula is abs(qnorm(alpha/2)).\n\nFor example, for a two-tailed z-test at the 95% confidence level, the cutoff is set at 1.96. Verify this using the formula above.\nWrite a function test_significance() that takes zscores and a given alpha and determines if there is a significant difference at the given level.\n\nRun the following code, and check that your code matches the expected output:\n\n# before knitting set eval = TRUE (and delete this comment)\ntest_significance(zscores = 2, alpha = 0.05)\n\nShould return TRUE. And:\n\n# before knitting set eval = TRUE (and delete this comment)\ntest_significance(zscores = c(1.9, -0.3, -3), alpha = 0.05)\n\nShould return FALSE, FALSE, and TRUE.\n\nBuilding make_mc_sample()\nNow we do step 1: simulate B random samples and calculate sample statistics.\nOur goal is make_mc_sample(N, true_mean, B) a function that produces sample statistics from B random samples from the normal distribution with mean true_mean of size N. When you think of doing something B times it suggest we need a loop. Let’s start with the body of the loop. And because we’re in a lesson about functions, let’s write a function.\n\nWrite a function called calc_mean_and_sd_from_sample() that\n\n\nGenerates a random sample with rnorm() of size N centered around true_mean\nCalculate the mean() and sd() of the random sample.\nReturn the mean and sd in a tibble with column names mean and sd.\n\nIdea: To return two values from a function, we need to put those values into a data structure like a vector or tibble.\nHere’s a test! Verify your function works. Remember, what guarantees that you get the same numbers from a random number generator as we did is that we’re setting a seed to 5\n\n# before knitting set eval = TRUE (and delete this comment)\nset.seed(5)\ncalc_mean_and_sd_from_sample(N = 30, true_mean = 0.5)\n\nNow, this function only does what we need once, while we’ll need it to do it B times. This is an appropriate time for a loop!\n\nWrite the function make_mc_sample. The inputs are described above. The output is a tibble with B rows of means and standard deviations.\n\nHere’s a test.\n\nset.seed(24601)\nmake_mc_sample(N = 30, true_mean = 100, B = 3 )\n\n# A tibble: 3 × 2\n   mean    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1  99.8 0.920\n2  99.8 0.952\n3 100.  1.04 \n\n\n\n\nFunctions, Assemble\nNow you have all the helper functions that are critical for our simulation. We want to simulate 1000 sets of 30 data points drawn from a normal distribution with true mean 0.5 and then see how often our random sample mean is significantly different from the true mean at a significance level of 0.05. If everything is working as expected, we should see about 5% of the random means to be statistically different.\n\ndo_monte_carlo &lt;- function(N, true_mean, B = 1000, alpha = .05){\n# step 1: Simulate B random samples and calculate sample statistics\nsample_statistics &lt;- make_mc_sample(N, true_mean, B)\n# step 2: Determine z-scores\nz_scores &lt;- get_zscores(sample_statistics$mean, true_mean, sample_statistics$sd, N)\n# step 3: Test whether the z-scores are outside the threshold.\nsignificance &lt;- test_significance(z_scores, alpha)\n# step 5: Measure to what extent our simulations match the theory. (We expect a number close to alpha)\nmean(significance)\n}\n\n\nTest out your function with N equals 30 and true_mean equals 0.5. The resulting number should be close to .05 (alpha).\nTry again with a different alpha and verify that do_monte_carlo returns a number in the ball park of alpha.\n\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/3 Module/mod3.html",
    "href": "R files/3 Module/mod3.html",
    "title": "Module 3: Vectors and Lists",
    "section": "",
    "text": "Download a copy of Module 3 slides\nDownload data for Module 3 lab and tutorial"
  },
  {
    "objectID": "R files/3 Module/mod3.html#vectors-and-lists",
    "href": "R files/3 Module/mod3.html#vectors-and-lists",
    "title": "Module 3: Vectors and Lists",
    "section": "",
    "text": "Download a copy of Module 3 slides\nDownload data for Module 3 lab and tutorial"
  },
  {
    "objectID": "R files/3 Module/mod3.html#lab-3",
    "href": "R files/3 Module/mod3.html#lab-3",
    "title": "Module 3: Vectors and Lists",
    "section": "Lab 3",
    "text": "Lab 3\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\n\n\n\nWarm-up\n\nIn the lecture, we covered c(), :, rep(), seq() among other ways to create vectors.\n\n\ndolly = c(9, 10, 11, 12, 13, 14, 15, 16, 17) \nbees = c(\"b\", \"b\", \"b\", \"b\", \"b\")\n\n\nRecreate dolly using :.\nCreate the same vector using seq().\nRecreate bees using rep().\n\n\nWe are now going to use the functions rnorm() and runif() to initialize vectors.\n\n\nrandom_norm = rnorm(100) \nrandom_unif = runif(1000)\n\n\nHow long are the vectors random_norm and random_unif? Use length() to verify.\nWhat are the largest and smallest values in random_norm and random_unif? Use min() and max().\nUse mean() and sd() to calculate the mean and standard deviation of the two distributions.\nCreate a new vector with 10000 draws from the standard normal distribution.\nrnorm() by default sets mean = 0 (see ?rnorm). Create a vector of 10000 draws from the normal distribution with mean = 1. Use mean() to verify.\n\nNotice the functions min(), max(), mean() and sd() all take a vector with many values and summarize them as one value. These are good to use with summarize() when doing data analysis on simple dataframes.\n\nData Types\n\nUse typeof() to verify the data types of dolly, bees, random_unif\nCoerce dolly to a character vector. Recall we have functions as.&lt;type&gt;() for this kind of coercion.\nTry to coerce bees to type numeric. What does R do when you ask it to turn “b” into a number?\n\n\n\nVectorized Math\n\na and b are vectors of length 10. Look at them in the console.\n\n\na &lt;- 1:10\nb &lt;- rep(c(2, 4), 5)\n\n\nAdd a and b element by element.\nSubtract a and b element by element.\nDivide a by b element by element.\nMultiply a and b element by element.\nRaise the element of a to the power of b element by element.\nMultiply each element of a by 3 then subtract b\nRaise each element of b to the third power.\nTake the square root of each element of a.\n\n\n\nCalculating Mean and Standard Deviation\n\nCalculating the Mean\nIn this exercise, we will calculate the mean of a vector of random numbers. Wewill practice assigning new variables and using functions in R.\nWe can run the following code to create a vector of 1000 random numbers. The function set.seed() ensures that the process used to generate random numbers is the same across computers.\nNote: rf() is a R command we use to generate 1000 random numbers according to the F distribution, and 10 and 100 are parameters that specify how “peaked” the distribution is.\n\nset.seed(1)\nrandom_numbers = rf(1000, 10, 100)\n\nWrite code that gives you the sum of random_numbers and saves it to a new variable called numbers_sum:\nHint: To sum the numbers in a vector, use the sum() function.\nNote: You don’t automatically see the output of numbers_sum when you assign it to a variable. Type numbers_sum into the console and run it to see the value that you assigned it.\nWrite code that gives you the number of items in the random_numbers vector and saves it to a new variable called numbers_count:\nHint: To count the number of items in a vector, use the length() function.\nNow write code that uses the above two variables to calculate the average of random_numbers and assign it to a new variable called this_mean.\nWhat number did you get? It should have been 1.018. If it isn’t, double check your code!\nR actually has a built in function to calculate the mean for you, so you don’t have to remember how to build it from scratch each time! Check your above answer by using the mean() function on the random_numbers vector.\n\n\nCalculating the Standard Deviation\nNow that you’ve got that under your fingers, let’s move on to standard deviation.\nWe will be converting the following formula for calculating the sample standard deviation into code:\n\\(s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2} {n-1}}\\)\nFor this, we’ll review the concept of vectorization. This means that an operation like subtraction will act on all numbers in a vector at the same time.\nSubtract this_mean from the random_numbers vector. Did each number in random_numbers change?\nTry to write the formula for standard deviation in R code using the sqrt(), sum(), and length() functions, along with other operators (^, /, -). Assign it to a new variable called this_sd. Watch out for your parentheses!\nWhat number did you get for this_sd, or the standard deviation of random_numbers? If you didn’t get 0.489704, recheck your code!\nR also has a built in function for standard deviation. Check if you calculated the standard deviation correctly by using the sd() function on the random_numbers vector.\n\n\n\nMaking a Histogram of Our Numbers\nWhat do these random numbers look like, anyway? We can use base plotting in R to visualize the distribution of our random numbers.\nRun the following code to visualize the original distribution of random_numbers as a histogram.\n\nhist(random_numbers)\n\nNotice how most of the values are concentrated on the left-hand side of the graph, while there is a longer “tail” to the right? Counterintuitively, this is known as a right-skewed distribution. When we see a distribution like this, one common thing to do is to normalize it.\nThis is also known as calculating a z-score, which we will cover next.\n\n\nCalculating a Z-Score\nThe formula for calculating a z-score for a single value, or normalizing that value, is as follows:\n\\(z = \\frac{x - \\bar{x}}{s}\\)\nThis can be calculated for each value in random_numbers in context of the larger set of values.\nCan you translate this formula into code?\nUsing random_numbers, this_mean, and this_sd that are already in your environment, write a formula to transform all the values in random_numbers into z-scores, and assign it to the new variable normalized_data.\nHint: R is vectorized, so you can subtract the mean from each random number in random_numbers in a straightforward way.\nTake the mean of normalized_data and assign it to a variable called normalized_mean.\nNote: If you see something that ends in “e-16”, that means that it’s a very small decimal number (16 places to the right of the decimal point), and is essentially 0.\nTake the standard deviation of normalized_data and assign it to a variable called normalized_sd.\nWhat is the value of normalized_mean? What is the value of normalized_sd? You should get a vector that is mean zero and has a standard deviation of one, because the data has been normalized.\n\nMaking a Histogram of Z-scores\nLet’s plot the z-scores and see if our values are still skewed. How does this compare to the histogram of random_numbers? Run the following code:\n\nhist(normalized_data)\n\nIs the resulting data skewed?\n\n\n\n\nCalculating a T-Score\nT-tests are used to determine if two sample means are equal. The formula for calculating a t-score is as follows:\n\\(t = \\frac{\\overline{x}_1 - \\overline{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\)\nwhere \\(\\overline{x}_i\\) is the mean of the first or second set of data, \\(s_i\\) is the sample standard deviation of the first or second set of data, and \\(n_i\\) is the sample size of the \\(i\\)th set of data.\nWe’ll first create two data sets of random numbers following a normal distribution:\n\nset.seed(1)\ndata_1 &lt;- rnorm(1000, 3)\ndata_2 &lt;- rnorm(100, 2)\n\nHere’s how we’ll calculate the mean (x_1), standard deviation (s_1), and sample size (n_1) of the first data set:\n\nx_1 &lt;- mean(data_1)\ns_1 &lt;- sd(data_1)\nn_1 &lt;- length(data_1)\n\nWhat numeric types do you get from doing this? Try running the typeof() function on each of x_1, s_1, and n_1. We have you started with x_1.\n\ntypeof(x_1)\n\n[1] \"double\"\n\n\nWhat object type is n_1?\nCan you calculate the same values for data_2, assigning mean, standard deviation, and length to the variables of x_2, s_2, and n_2, respectively?\nWhat values do you get for x_2 and s_2?\nNow, you should be able to translate the t-score formula (\\(\\frac{\\overline{x}_1 - \\overline{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\)) into code, based on the above calculated values.\nWhat did you get for the t-score? You should have gotten 9.243, if not, double check your code!\nThe t-score’s meaning depends on your sample size, but in general t-scores close to 0 imply that the means are not statistically distinguishable, and large t-scores (e.g. t &gt; 3) imply the data have different means.\n\nPerforming a T-Test\nOnce again, R has a built in function that will perform a T-test for us, aptly named t.test(). Look up the arguments the function t.test() takes, and perform a T-test on data_1 and data_2.\nWhat are the sample means, and are they distinguishable from each other?\nWell done! You’ve learned how to work with R to calculate basic statistics. We’ve had you generate a few by hand, but be sure to use the built-in functions in R in the future.\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/1 Module/mod1.html",
    "href": "R files/1 Module/mod1.html",
    "title": "Module 1: An Introduction and Motivation for R Programming",
    "section": "",
    "text": "Download a copy of Module 1 slides"
  },
  {
    "objectID": "R files/1 Module/mod1.html#an-introduction-and-motivation-for-r-programming",
    "href": "R files/1 Module/mod1.html#an-introduction-and-motivation-for-r-programming",
    "title": "Module 1: An Introduction and Motivation for R Programming",
    "section": "",
    "text": "Download a copy of Module 1 slides"
  },
  {
    "objectID": "R files/1 Module/mod1.html#lab-1",
    "href": "R files/1 Module/mod1.html#lab-1",
    "title": "Module 1: An Introduction and Motivation for R Programming",
    "section": "Lab 1",
    "text": "Lab 1\nWe expect you to watch the Module 1 material prior to lab.\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\n\n\n\nWarm-up\n\nThe most important warm up question: do you have R and RStudio installed?\nWhich of these allow you to pull up the documentation for a command in R?\n\n\n*\n?\nhelp()\ndocumentation()\n\n\nIn the code block below, run code that will pull up documentation for the function paste0().\n\n\n?paste0()\n\nWhat does this function do?\n\nWhat are the two ways that you can assign a value to a variable?\n\n\n\nGuess the Output: Algebra\n\nGuess the output of the following code:\n\n\na &lt;- 3\nb &lt;- a^2 + 1\n\nb\n\nNow, run the code block to check your answer.\n\nGuess the output of the following code:\n\n\na &lt;- 10\nb &lt;-3 %% a\n\nb + 5\n\nHint: If you are not sure what %% does you can try running ?'%%' to better understand.\n\nGuess the output of the following code:\n\n\na &lt;- c(1,2,3)\nb &lt;- a^2 + 1\n\nb\n\n\n\nGuess the Output: Boolean\n\nGuess the output of the following code:\n\n\n25 &gt;= 14\n\n\nGuess the output of the following code:\n\n\n10 != 100\n\n\nGuess the output of the following code:\n\n\n7%%5 == 2\n\n\nGuess the output of the following code:\n\n\n(5 &gt; 7) & (7 * 7 == 49)\n\n\nOk, let’s try some logic! Try to figure out each one before running the code!\n\n\n\n\n\nTRUE & FALSE\n\n\n\n\n\nFALSE & FALSE\n\n\n\n\n\nTRUE | (FALSE & TRUE)\n\n\n\n\n\nFALSE | (TRUE | FALSE)\n\n\n\n\n\n(TRUE & (TRUE | FALSE)) | FALSE\n\n\n\nData Types\n\nRun these lines to create these variables in your environment.\n\n\nitem_1 &lt;- \"Hi, my name is item 1!\"\nitem_2 &lt;- 7\nitem_3 &lt;- FALSE\n\nWhat are the type (or mode) of each of these items?\nHint: If you are not sure, you could apply the mode() function to each item and check the output. If you are unsure about how to apply the mode() function, you can always run ?mode().\n\nGuess the output of the following code:\n\n\n(item_2 + 19 &lt;= 25) == item_3\n\n\nDo you remember earlier when you ran ?paste0()? We are now going to try to use this function. In the code block below, initialize two variables that are of mode “character”. The output when you apply paste0() to these variables should be “Hello, world!”.\n\n\n#v1 &lt;-\n#v2 &lt;- \n\nWell done! You’ve learned how to work with R to perform simple variable assignment and operations!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/8 Module/mod8.html",
    "href": "R files/8 Module/mod8.html",
    "title": "Module 8: Iteration",
    "section": "",
    "text": "Download a copy of Module 8 slides"
  },
  {
    "objectID": "R files/8 Module/mod8.html#iteration",
    "href": "R files/8 Module/mod8.html#iteration",
    "title": "Module 8: Iteration",
    "section": "",
    "text": "Download a copy of Module 8 slides"
  },
  {
    "objectID": "R files/8 Module/mod8.html#lab-8",
    "href": "R files/8 Module/mod8.html#lab-8",
    "title": "Module 8: Iteration",
    "section": "Lab 8",
    "text": "Lab 8\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\n\n\n\nWarm-up\nRecall, for-loops are an iterator that help us repeat tasks while changing inputs. The most common structure for your code will look like the following code. This can be simplified if you are not storing results.\n\n# what are you iterating over? The vector from -10:10\nitems_to_iterate_over &lt;- c(-10:10) \n\n# pre-allocate the results\nout &lt;- rep(0, length(items_to_iterate_over))\n# write the iteration statement --\n# we'll use indices so we can store the output easily \nfor (i in seq_along(items_to_iterate_over)) {\n# do something\n# we capture the median of three random numbers\n# from normal distributions various means\n    out[[i]] &lt;- median(rnorm(n = 3,mean = items_to_iterate_over[[i]]))\n    }\n\n\nWriting for-loops\n\nWrite a for-loop that prints the numbers 5, 10, 15, 20, 250000.\nWrite a for-loop that iterates over the indices of x and prints the ith value of x.\n\n\nx &lt;- c(5, 10, 15, 20, 250000)\n# replace the ... with the relevant code\n\nfor (i in ... ){ \n  print(x[[...]])\n  }\n\n\nWrite a for-loop that simplifies the following code so that you don’t repeat yourself! Don’t worry about storing the output yet. Use print() so that you can see the output. What happens if you don’t use print()?\n\n\nsd(rnorm(5))\nsd(rnorm(10))\nsd(rnorm(15))\nsd(rnorm(20)) \nsd(rnorm(25000))\n\n\nadjust your for-loop to see how the sd changes when you use rnorm(n, mean = 4)\nadjust your for-loop to see how the sd changes when you use rnorm(n, sd = 4)\n\n\nNow store the results of your for-loop above in a vector. Pre-allocate a vector of length 5 to capture the standard deviations.\n\n\n\nvectorization vs for loops\nRecall, vectorized functions operate on a vector item by item. It’s like looping over the vector! The following for-loop is better written vectorized.\nCompare the loop version\n\nnames &lt;- c(\"Marlene\", \"Jacob\", \"Buddy\")\nout &lt;- character(length(names))\n\nfor (i in seq_along(names)) {\n  out[[i]] &lt;- paste0(\"Welcome \", names[[i]])\n  }\n\nto the vectorized version\n\nnames &lt;- c(\"Marlene\", \"Jacob\", \"Buddy\") \nout &lt;- paste0(\"Welcome \", names)\n\nThe vectorized code is preferred because it is easier to write and read, and is possibly more efficient.\n\nRewrite your first for-loop, where you printed 5, 10, 15, 20, 250000 as vectorised code\nRewrite this for-loop as vectorized code:\n\n\nradii &lt;- c(0:10)\n\narea &lt;- double(length(radii)) \n\nfor (i in seq_along(radii)) { \n  area[[i]] &lt;- pi * radii[[i]] ^ 2 \n  }\n\n\nRewrite this for-loop as vectorized code:\n\n\nradii &lt;- c(-1:10)\n\narea &lt;- double(length(radii))\n\nfor (i in seq_along(radii)) { \n  if (radii[[i]] &lt; 0) { \n    area[[i]] &lt;- NaN \n  } else {\n      area[[i]] &lt;- pi * radii[[i]] ^ 2 }\n  }\n\n\n\n\nExtension\n\nSimulating the Law of Large Numbers\nThe Law of Large Numbers says that as sample sizes increase, the mean of the sample will approach the true mean of the distribution. We are going to simulate this phenomenon!\nWe’ll start by making a vector of sample sizes from 1 to 50, to represent increasing sample sizes.\nCreate a vector called sample_sizes that is made up of the numbers 1 through 50. (Hint: You can use seq() or : notation).\nWe’ll make an empty tibble to store the results of the for loop:\n\nestimates &lt;- tibble(n = integer(), sample_mean = double())\n\nWrite a loop over the sample_sizes you specified above. In the loop, for each sample size you will:\n\nCalculate the mean of a sample from the random normal distribution with mean = 0 and sd = 5.\nMake an intermediate tibble to store the results\nAppend the intermediate tibble to your tibble using bind_rows().\n\n\nset.seed(60637) \nfor (___ in ___) {\n  # Calculate the mean of a sample from the random normal distribution with mean = 0 and s\n  ___ &lt;- ___\n  # Make a tibble with your estimates\n  this_estimate &lt;- tibble(n = ___, sample_mean = ___) \n  # Append the new rows to your tibble\n  ___ &lt;- bind_rows(estimates, ___)\n  }\n\nWe can use ggplot2 to view the results. Fill in the correct information for the data and x and y variables, so that the n column of the estimates tibble is plotted on the x-axis, while the sample_mean column of the estimates tibble is plotted on the y-axis.\n\n# your data goes in the first position\n___ %&gt;%\n  ggplot(aes(x = ___, y = ___)) + \n  geom_line()\n\n\nAs the sample size (n) increases, does the sample mean becomes closer to 0, or farther away from 0?\n\nRewrite the loop code without looking at your previous code and use a wider range of sample sizes. Try several different sample size combinations. What happens when you increase the sample size to 100? 500? 1000? Use the seq() function to generate a sensibly spaced sequence.\n\nset.seed(60637) \nsample_sizes &lt;- ___ \nestimates_larger_n &lt;- ___\n\nfor (___ in ___) {\n  ___ &lt;- ___\n  ___ &lt;- ___\n  ___ &lt;- ___\n}\n\n___ %&gt;%\n  ggplot(___(___ = ___, ___ = ___)) +\n  geom_line()\n\n\nHow does this compare to before?\n\n\n\nExtending Our Simulation\nLooking at your results, you might think a small sample size is sufficient for estimating a mean, but your data had a relatively small standard deviation compared to the mean. Let’s run the same simulation as before with different standard deviations.\nDo the following:\n\nCreate a vector called population_sd of length 4 with values 1, 5, 10, and 20 (you’re welcome to add larger numbers if you wish).\nMake an empty tibble to store the output. Compared to before, this has an extra column for the changing population standard deviations.\nWrite a loop inside a loop over population_sd and then sample_sizes.\nThen, make a ggplot graph where the x and y axes are the same, but we facet (aka we create small multiples of individual graphs) on population_sd.\n\n\nset.seed(60637)\npopulation_sd &lt;- ___\n# use what every you came up with in the previous part \nsample_sizes &lt;- ___\nestimates_adjust_sd &lt;- ___\nfor (___ in ___){ \n  for (___ in ___) {\n    ___ &lt;- ___\n    ___ &lt;- ___\n    ___ &lt;- ___\n  } \n}\n\n___ %&gt;%\n  ggplot(___) +\n  geom_line() + \n  facet_wrap(~population_sd) + \n  theme_minimal()\n\nHow do these estimates differ as you increase the standard deviation?\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/2 Module/mod2.html",
    "href": "R files/2 Module/mod2.html",
    "title": "Module 2: Installing Packages and Reading Data",
    "section": "",
    "text": "Download a copy of Module 2 slides\nDownload data for Module 2 lab and tutorial"
  },
  {
    "objectID": "R files/2 Module/mod2.html#installing-packages-and-reading-data",
    "href": "R files/2 Module/mod2.html#installing-packages-and-reading-data",
    "title": "Module 2: Installing Packages and Reading Data",
    "section": "",
    "text": "Download a copy of Module 2 slides\nDownload data for Module 2 lab and tutorial"
  },
  {
    "objectID": "R files/2 Module/mod2.html#lab-2",
    "href": "R files/2 Module/mod2.html#lab-2",
    "title": "Module 2: Installing Packages and Reading Data",
    "section": "Lab 2",
    "text": "Lab 2\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\n\n\n\nWarm-up\n\nCreate a new Rmd and add code to load the tidyverse package.\nYour classmate comes to you and says they can’t get data to load after restarting their R session. You see the code:\n\n\ninstall.packages(\"haven\")\nawesome_data &lt;- read_dta(\"awesome_data.dta\")\n\nError in read_dta(\"awesome_data.dta\") : could not find function \"read_dta\"\n\nDiagnose the problem.\nNote: If they say the code worked before, it’s likely they had loaded haven in the console or perhaps in an earlier script. R packages will stay attached as long as the R session is live.\n\nIn general, once you have successfully used install.packages(pkg) for a “pkg”, you won’t need to do it again. Install haven and readxl using the console.\nIn your script, load haven and readxl. Notice that if you had to restart R right now. You could reproduce the entire warm-up by running the script. We strive for reproducibility by keeping the code we want organized in scripts or Rmds.\nIt’s good practice when starting a new project to clear your R environment. This helps you make sure you are not relying on data or functions you wrote in another project. After you library() statements add the following code rm(list = ls()).\nrm() is short for remove. Find the examples in ?rm and run them in the console.\n\n\n\nISLR Chapter 2 Q8\nThis exercise relates to the College data set, which can be found in the file College.csv. It contains a number of variables for 777 different universities and colleges in the US. The variables are\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nPrivate\nPublic/private indicator\n\n\nApps\nNumber of applications received\n\n\nAccept\nNumber of applicants accepted\n\n\nEnroll\nNumber of new students enrolled\n\n\nTop10perc\nNew students from top 10 % of high school class\n\n\nTop25perc\nNew students from top 25 % of high school class\n\n\nF.Undergrad\nNumber of full-time undergraduates\n\n\nP.Undergrad\nNumber of part-time undergraduates\n\n\nOutstate\nOut-of-state tuition\n\n\nRoom.Board\nRoom and board costs\n\n\nBooks\nEstimated book costs\n\n\nPersonal\nEstimated personal spending\n\n\nPhD\nPercent of faculty with Ph.D.’s\n\n\nTerminal\nPercent of faculty with terminal degree\n\n\nS.F.Ratio\nStudent/faculty ratio\n\n\nperc.alumni\nPercent of alumni who donate\n\n\nExpend\nInstructional expenditure per student\n\n\nGrad.Rate\nGraduation rate\n\n\n\nBefore reading the data into R, it can be viewed in Excel or a text editor. Make sure that you have the directory set to the correct location for the data.\n\nUse the base R read.csv() function to read the data into R with option stringsAsFactors=T (this is needed later on for plotting figures). Call the loaded data college.\nLook at the data using the View() function. You should notice that the first column is just the name of each university. Load your data and then try the following commands:\n\n\n#set your working directory ,fill in your code after this line\n\n#read in the file College.csv using read.csv() with option `stringsAsFactors=T`\ncollege &lt;- read.csv('College.csv', stringsAsFactors = T)\n\n#Give data frame college rownames\nrownames(college) &lt;- college[,1] \n\n# Please comment out View function after using it. Otherwise you'll see some error when knit.\n# View(college)\n\n\nYou should see that there is now a row.names column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. Next, we will remove the first column in the data where the names are stored. Try\n\n\n#Use a negative number to generate a subset with all but one column\n# college[, -c(1, 2, 3)]  will generate a subset with all but the first three columns\ncollege &lt;- college[,-1]\n# as.factor can turn a character column to a factor column so that we can use it to plot later on\ncollege$Private &lt;- as.factor(college$Private)\n#View(college)\n\nNow you should see that the first data column is Private. Note that another column labeled row.names now appears before the Private column. However, this is not a data column but rather the name that R is giving to each row.\n\nUse the summary() function to produce a numerical summary of the variables in the data set. Hint: summary() takes in an object such as data.frame and return the summery results\nUse the pairs() function to produce a scatterplot matrix of the first five columns or variables of the data. Recall that you can reference the first five columns of a data frame dat using dat[,1:5]\nUse the plot() function to produce side-by-side boxplots of Outstate versus Private. Hint: plot() takes two arguments one vector for x axis and one vector for y axis. Try plot(dat$col_name, dat$col_name).\n\n\n# replicate \"No\" for the same times as the number of colleges using rep()\nElite &lt;- rep(\"No\",nrow(college))\n# change the values in Elite for colleges with proportion of students \n# coming from the top 10% of their high school classes \n# exceeds 50 % to \"Yes\"\nElite[college$Top10perc &gt;50] &lt;- \"Yes\"\n# as.factor change ELite, a character vector to a factor vector\n# (we will touch on factors later in class) \nElite &lt;- as.factor(Elite)\n# add the newly created vector to the college data frame\ncollege &lt;- data.frame(college ,Elite)\n\n\nUse the summary() function to see how many elite universities there are. Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite.\n\nContinue exploring the data, and provide a brief summary of what you discover.\n\n\nISLR Chapter 2 Q9\nThis exercise involves the Auto data set. na.omit() removes the missing values from the data and returns a new data frame.\n\n#load the Auto.csv into a variable called auto using read_csv()\n\n\n# remove all rows with missing values using na.omit()\nauto &lt;- na.omit(auto)\n\nWe can use class() to check which of the columns are quantitative (numeric or integer), and which are qualitative( logical or character). And sapply() function takes in a data frame and a function (in this case class()), apply the class function to each column. Try the following commands:\n\n#apply the class() function to each column of auto data frame\nsapply(auto, class)\n\n\nWhat is the range of each quantitative columns? You can answer this using the range() function. Hint: You can call range() function individually on each column. You can also subset the quantitative columns by creating a variable quant_cols equal to all columns with a numeric mode, then use sapply the function range() with the data frame with only quantitative columns. This is not required.\nUsing the functions mean() and sd(). Find out what is the mean and standard deviation of each quantitative columns?\nNow remove the 10th through 85th observations (rows). What is the range, mean, and standard deviation of each column in the subset of the data that remains? Hint: We’ve seen removing columns in question 8. To remove the rows, we can use the negative sign - again. For example, auto[-c(1,3),] removes the first and third row\nUsing the full data set, investigate the columns graphically, using scatterplots (pairs or plot) or other tools of your choice. Create some plots highlighting the relationships among the columns Comment on your findings.\nSuppose that we wish to predict gas mileage (mpg) on the basis of the other numerical variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.\n\n\n\nISLR Chapter 2 Q10\nThis exercise involves the Boston housing data set.\nTo begin, load in the Boston data set. The Boston data set is part of the MASS library in R. You may need to install the package using install.packages() function if you haven’t done so.\n\n# install.packages(MASS)\nlibrary(MASS)\n\nNow the data set is contained in the object Boston.\n\nBoston\n\nRead about the data set:\n\n?Boston\n\n\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\nMake some pairwise scatterplots of the columns in this data set. Describe your findings. Hint: Use function pairs()\nHow many of the suburbs in this data set bound the Charles river? Hint: Subset the data using a logical vector to check if variable chas==1, then use nrow() to see the number of suburbs.\nUsing median(), find out what is the median pupil-teacher ratio among the towns in this data set?\n\nWell done! You’ve learned how to work with R to read in data and perform some simple analysis and exploration!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "Data/viz/2023-01-17 Tidy Tuesday/TT_wk3.html",
    "href": "Data/viz/2023-01-17 Tidy Tuesday/TT_wk3.html",
    "title": "Tidy Tuesday, 2023 Week 3 🎨",
    "section": "",
    "text": "Data for this week’s TidyTuesday comes from Art History!\nHere is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-01-17 Tidy Tuesday/TT_wk3.html#my-plot",
    "href": "Data/viz/2023-01-17 Tidy Tuesday/TT_wk3.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 3 🎨",
    "section": "My plot",
    "text": "My plot\n\n🎨 Art Histpry\n\nCode here"
  },
  {
    "objectID": "Data/viz/2023-01-03 Tidy Tuesday/TT_wk1.html",
    "href": "Data/viz/2023-01-03 Tidy Tuesday/TT_wk1.html",
    "title": "Tidy Tuesday, 2023 Week 1 🏡",
    "section": "",
    "text": "The theme of this week’s #TidyTuesday is “Bring your own data from 2022!”\nI decided to use data from Tidy Tuesday 2022-07-05. Here is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-01-03 Tidy Tuesday/TT_wk1.html#my-plot",
    "href": "Data/viz/2023-01-03 Tidy Tuesday/TT_wk1.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 1 🏡",
    "section": "My plot",
    "text": "My plot\n\n🏡 SF Housing Prices by Kate Pennington’s Craigslist scrape of Bay Area housing posts\n\nCode here"
  },
  {
    "objectID": "Data/viz/2023-02-14 Tidy Tuesday/TT_wk7.html",
    "href": "Data/viz/2023-02-14 Tidy Tuesday/TT_wk7.html",
    "title": "Tidy Tuesday, 2023 Week 7 🎬",
    "section": "",
    "text": "Week\nDate\nData\nSource\nArticle\n\n\n\n\n7\n2023-02-14\nHollywood Age Gaps\nHollywood Age Gap Download Data\nHollywood Age Gap\nHere is a little more information about the data: # Hollywood Age Gaps\nThe data this week comes from Hollywood Age Gap via Data Is Plural.\nThe data follows certain rules:\nWe previously provided a dataset about the Bechdel Test. It might be interesting to see whether there is any correlation between these datasets! The Bechdel Test dataset also included additional information about the films that were used in that dataset.\nNote: The age gaps dataset includes “gender” columns, which always contain the values “man” or “woman”. These values appear to indicate how the characters in each film identify. Some of these values do not match how the actor identifies. We apologize if any characters are misgendered in the data!"
  },
  {
    "objectID": "Data/viz/2023-02-14 Tidy Tuesday/TT_wk7.html#my-plot",
    "href": "Data/viz/2023-02-14 Tidy Tuesday/TT_wk7.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 7 🎬",
    "section": "My plot",
    "text": "My plot\n\n🎬 Hollywood Age Gaps\n\nCode here"
  },
  {
    "objectID": "Data/viz/2023-01-24 Tidy Tuesday/TT_wk4.html",
    "href": "Data/viz/2023-01-24 Tidy Tuesday/TT_wk4.html",
    "title": "Tidy Tuesday, 2023 Week 4 🐻",
    "section": "",
    "text": "Data for this week’s TidyTuesday comes from the TV show ALONE!\nHere is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-01-24 Tidy Tuesday/TT_wk4.html#my-plot",
    "href": "Data/viz/2023-01-24 Tidy Tuesday/TT_wk4.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 4 🐻",
    "section": "My plot",
    "text": "My plot\n\n🐻 ALONE\n\nCode here"
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Data can’t speak for itself; it’s up to you to give it a voice. Try to speak truthfully. - Ronald Coase\n\n\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 7 🎬\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 7, 2023: Hollywood Age Gaps\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 6 📈\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 6, 2023: Big Tech Stock Prices\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 5 🐱\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 5, 2023: Pet Cats UK\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 4 🐻\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 4, 2023: ALONE\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 3 🎨\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 3, 2023: Art History\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 2 🐦\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 2, 2023: Project FeederWatch\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 1 🏡\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 1, 2023: Bring your own data from 2022!\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nHealthcare Area Chart\n\n\n\n\n\n\n\narea chart\n\n\nD3\n\n\n\n\nIn 2020, a greater percent of previously uninsured people in the U.S. with low incomes were enrolled in Medicare or Medicaid.\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-03-26-DT/2024-03-26-DT.html",
    "href": "posts/2024-03-26-DT/2024-03-26-DT.html",
    "title": "DIY Decision Trees",
    "section": "",
    "text": "Decision trees are a fundamental machine learning algorithm that’s easy to understand and interpret. They’re a popular choice for classification and regression tasks, offering a visual representation of the decision-making process. In this series, we’ll explore the inner workings of decision trees, starting with the basics and gradually building up to more advanced concepts.\n\n\nWhy go through the trouble of building a machine learning algorithm by hand, especially when libraries like sklearn in Python or rpart in R are just a function call away? The answer lies in the deeper understanding and insights that come from peeling back the layers of these algorithms ourselves.\n\n\n\nBeyond the theoretical, there’s immense practical value in understanding the inner workings of these models. It allows for more informed decisions about feature selection—knowing which to discard because they don’t fit the model’s assumptions and which to transform for better representation. This knowledge often translates to improved model performance on new, unseen data.\nAnd let’s not overlook the sheer joy of the process. Diving deep into algorithms offers a unique way to tackle problem-solving, enhancing our execution and grasp of machine learning concepts.\n\n\n\nHow do we navigate through the countless ways to split our data? We turn to optimization metrics, the compass guiding each iteration towards better performance. For decision trees, two key metrics stand out: Gini Impurity and Entropy.\n\n\nGini Impurity is an insightful metric focused on misclassification. It hinges on the probability of classifying a data point correctly within our dataset. For any given class \\(c\\) with frequency \\(f_c\\), we predict that class for a data point \\(f_c\\) proportion of the time. The error for each class is the chance of selecting the class multiplied by the likelihood of being incorrect:\n\\[Error_c = f_c \\times (1 - f_c)\\]\nThe Gini Impurity is the aggregate of these individual errors across all classes:\n\\[Gini\\ Impurity = \\sum_c f_c \\times (1 - f_c) = \\sum_c f_c - \\sum_c f_c^2 = 1 - \\sum_c f_c^2\\]\nImproving the Gini Impurity, specifically by minimizing it across data regions, directly leads to a reduction in misclassification.\n\n\n\nConsider the Gini Impurity of a coin flip, where the probability of heads is \\(p_H\\):\n\\[Gini\\ Impurity = 1 - p_H^2 - (1 - p_H)^2 = 2 \\times p_H \\times (1 - p_H)\\]\n\np &lt;- seq(0.01, 0.99, 0.01)\nplot(p, 1-p^2-(1-p)^2, type='l', ylim=c(0,1), \n     ylab = \"Gini Impurity\", xlab=\"Probability of Heads\", \n     col=\"blue\")\n\n\n\n\nWhen \\(p_H\\) is at 0 or 1, there’s no chance of error—our predictions are certain. Yet at \\(p_H = 0.5\\), Gini Impurity peaks, reflecting the highest error rate and aligning with our intuitive understanding.\n\n\n\nEntropy, on the other hand, takes a different tack. It measures the expected information content—or surprise—of a class in a dataset. If \\(f_c\\) is the proportion of a class, and \\(I(c)\\) represents the information content of the class, then entropy is defined as:\n\\[Entropy = \\sum_c f_c \\times I(c)\\]\nWithout delving too deep into the underlying theory, the formula for entropy, given that \\(I(c) = -\\log_2(f_c)\\), simplifies to:\n\\[Entropy = -\\sum_c f_c \\times \\log_2(f_c)\\]\nThough the logarithm base could be arbitrary, we align it with the binary nature of the decision trees we’re examining in this series.\n\n\n\nExtending our analogy to the coin toss, let’s examine how entropy changes with varying probabilities of heads, \\(p_H\\):\n\\[Entropy = -p_H \\log_2(p_H) - (1 - p_H) \\log_2(1 - p_H)\\]\n\np &lt;- seq(0.01, 0.99, 0.01)\nplot(p, -p*log2(p)-(1-p)*log2(1-p), type='l', ylim=c(0,1), \n     ylab = \"Entropy\", xlab=\"Probability of Heads\", col=\"blue\")\n\n\n\n\nEntropy achieves its maximum at \\(p_H = 0.5\\), where uncertainty—and the potential for error—is greatest. This resonates with the principle that the greatest entropy occurs where prediction is most challenging.\n\n\n\n\nWith our metrics in place to judge how well our dataset’s predictions are doing, it’s time to focus on improvement—how can we make our predictions even better? We’re going to talk about something called ‘Information Gain’, a concept which is the difference we want to maximize when deciding how to split our data.\n\n\nWhen we build a classifier—a kind of decision tree—our goal is to organize our data into groups that make our predictions as accurate as possible. The concept of ‘Information Gain’ is a bit like a treasure map; it helps us find these valuable groupings by measuring how much more ordered, or less chaotic, our data becomes after a split.\n\\[Information\\ Gain = Gini\\ Impurity_{before\\ split} - \\sum_{each\\ split} \\frac{|split\\ group|}{|total\\ data|} Gini\\ Impurity_{split\\ group}\\]\n\\[Information\\ Gain = Entropy_{before\\ split} - \\sum_{each\\ split} \\frac{|split\\ group|}{|total\\ data|} Entropy_{split\\ group}\\]\nIn these equations, \\(Gini\\ Impurity_{before\\ split}\\) and \\(Entropy_{before\\ split}\\) represent the disorder in our data before any splits. The sum of the Gini Impurity or Entropy of each split group is weighted by the proportion of data in that group. The Information Gain is the difference between the disorder before the split and the weighted sum of disorder after the split.\nIn essence, it tells us how much clearer our predictions become after we’ve sorted the data into separate buckets based on certain features.\n\n\n\nA common pitfall with decision trees is that they can get a little too enthusiastic, creating very specific rules that work perfectly for the data they were trained on but not so well for new data. This is known as overfitting.\nTo avoid this, we can trim or ‘prune’ the tree after it’s grown, or we can set some ground rules that stop the tree from getting overly complicated in the first place. These rules can include limits like:\n\nMaximum depth of the tree\nMinimum number of data points to justify creating a new branch\nMinimum improvement needed to make a new split\nA threshold below which we won’t consider any new splits\n\n\n\n\n\nWe’ll be using the well-loved Iris dataset for classification tasks. The Species attribute will be our target for prediction. Here’s a peek at the dataset:\n\nlibrary(datasets)\ndata(iris)\nhead(iris, 6)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nBy using the Iris dataset, we can focus on classifying flower species, a perfect starting point for practicing with decision trees.\nBuilding the Classifier: The Search for the First Split In our script, we’re not just taking wild guesses. We’re methodically examining each feature to find the value that offers the best division of our data, aiming to increase Information Gain.\n\nentropy &lt;- function(y) {\n  if(length(y) == 0) return(0)\n  p &lt;- table(y) / length(y)\n  sum(-p * log2(p + 1e-9))\n}\n\ngini_impurity &lt;- function(y) {\n  if(length(y) == 0) return(0)\n  p &lt;- table(y) / length(y)\n  1 - sum(p^2)\n}\n\nNow we can calculate the Information Gain, considering the improvements brought by different partitions in our data. We use a mask—a logical vector indicating if a row belongs to the first or second partition—to guide this process:\n\ninformation_gain &lt;- function(y, mask, metric_func = entropy) {\n  # We don't want partitions with no data points\n  if (sum(mask) == 0 || sum(!mask) == 0) return(0)\n  metric_func(y) - (sum(mask) / length(mask)) * metric_func(y[mask]) - \n    (sum(!mask) / length(!mask)) * metric_func(y[!mask])\n}\n\n\n\n\nWith our metric functions in hand, we’ll comb through all the features to find the ultimate split:\n\nmax_information_gain_split &lt;- function(y, x, metric_func = gini_impurity) {\n  # Initialize the best change and split value\n  best_change &lt;- NA\n  split_value &lt;- NA\n  # Check if the feature is numeric or categorical\n  is_numeric &lt;- !(is.factor(x) || is.logical(x) || is.character(x))\n\n  for(val in sort(unique(x))) {\n    mask &lt;- if (is_numeric) { x &lt; val } else { x == val }\n    change &lt;- information_gain(y, mask, metric_func)\n    if(is.na(best_change) || change &gt; best_change) {\n      best_change &lt;- change\n      split_value &lt;- val\n    }\n  }\n\n  return(list(\"best_change\" = best_change, \"split_value\" = split_value, \"is_numeric\" = is_numeric))\n}\n\nNow, let’s use this to find the best split for a single feature:\n\nprint(max_information_gain_split(iris$Species, iris$Petal.Width))\n\n$best_change\n[1] 0.3333333\n\n$split_value\n[1] 1\n\n$is_numeric\n[1] TRUE\n\n\n\nsapply(iris[, 1:4], function(x) max_information_gain_split(iris$Species, x))\n\n            Sepal.Length Sepal.Width Petal.Length Petal.Width\nbest_change 0.2277603    0.1269234   0.3333333    0.3333333  \nsplit_value 5.5          3.4         3            1          \nis_numeric  TRUE         TRUE        TRUE         TRUE       \n\n\nAfter determining the feature and value that provide the most significant information gain, we can then carve our dataset into two distinct sets. Think of it like sorting a deck of cards where one feature is the deciding factor.\n\nbest_feature_split &lt;- function(X, y) {\n  results &lt;- sapply(X, function(x) max_information_gain_split(y, x))\n  best_name &lt;- names(which.max(results['best_change',]))\n  best_result &lt;- results[, best_name]\n  best_result[[\"name\"]] &lt;- best_name\n  return(best_result)\n}\n\nLet’s run it for the iris dataset and see which feature provides the best split.\n\nas.data.frame(best_feature_split(iris[, 1:4], iris[, 5]))\n\n  best_change split_value is_numeric         name\n1   0.3333333           3       TRUE Petal.Length\n\n\nThis neat little function sifts through our data and finds the golden split—the point where dividing our dataset yields the most clarity according to our chosen metric.\nNow, imagine we’ve found our winning feature. The next step is to partition the iris dataset into two new datasets based on this feature. We’ll refer to them as ‘left’ and ‘right’ datasets, aligning with the branching structure of a tree.\n\nget_best_mask &lt;- function(X, best_feature_list) {\n  best_mask &lt;- X[, best_feature_list$name] &lt; best_feature_list$split_value\n  return(best_mask)\n}\n\nWe’ll get the best mask and split the iris dataset accordingly.\n\nbest_iris_split &lt;- best_feature_split(iris[, 1:4], iris[, 5])\nbest_iris_mask &lt;- get_best_mask(iris[, 1:4], best_iris_split)\n\nPartition the iris dataset into ‘left’ and ‘right’\n\nleft_iris &lt;- iris[best_iris_mask, ]\nright_iris &lt;- iris[!best_iris_mask, ]\n\nAnd just like that, with our data now split into ‘left’ and ‘right’, we’ve taken the first step in constructing our decision tree. This process will serve as the foundation for building a data structure that captures the essence of our tree, complete with branches that stem from each decision point."
  },
  {
    "objectID": "posts/2024-03-26-DT/2024-03-26-DT.html#a-small-example-health-dataset",
    "href": "posts/2024-03-26-DT/2024-03-26-DT.html#a-small-example-health-dataset",
    "title": "Decision Trees: A Visual Guide",
    "section": "",
    "text": "Let’s consider a small health-related dataset with 10 observations. Each observation represents a patient and includes four features:\n\nAge (A)\nBMI (B)\nSmoker (S) (1 for yes, 0 for no)\nExercise Regularly (E) (1 for yes, 0 for no)\na target variable representing the presence of a health condition (1 for present, 0 for absent).\n\n\n\n\nID\nA\nB\nS\nE\nCondition\n\n\n\n\n1\n23\n25\n0\n1\n0\n\n\n2\n45\n30\n1\n0\n1\n\n\n3\n50\n28\n1\n1\n1\n\n\n4\n35\n23\n0\n1\n0\n\n\n5\n40\n29\n1\n0\n1\n\n\n6\n60\n27\n0\n0\n1\n\n\n7\n55\n26\n0\n1\n0\n\n\n8\n30\n22\n0\n1\n0\n\n\n9\n20\n24\n0\n0\n0\n\n\n10\n50\n32\n1\n0\n1\n\n\n\nNow, let’s walk through the decision tree process, choosing to use Gini impurity as our measure for making splits.\n\n\nThe Gini impurity of a dataset is calculated as:\n\\[\n\\text{Gini} = 1 - \\sum (p_i)^2\n\\]\nWhere \\(p_i\\) is the proportion of the samples that belong to class i in a given node.\n\n\n\nTo demonstrate, let’s consider splitting on the “Smoker” feature and calculate the Gini impurity:\nGiven our dataset with 10 observations and the target variable indicating the presence (1) or absence (0) of a health condition, let’s calculate the Gini impurity for splitting the dataset based on the “Smoker” feature. For simplicity, let’s assume the distribution of our target variable within smokers and non-smokers is as follows:\n\nSmokers (S=1): 3 have the condition (1), 1 does not (0).\nNon-Smokers (S=0): 2 have the condition (1), 4 do not (0).\n\n\n\nFirst, calculate the proportions of each class within smokers:\n\nProportion of having the condition (1): \\(p_1 = \\frac{3}{4}\\)\nProportion of not having the condition (0): \\(p_0 = \\frac{1}{4}\\)\n\nThen, the Gini impurity for smokers can be calculated as:\n\\[\nGini_{\\text{Smokers}} = 1 - (p_1^2 + p_0^2) = 1 - \\left(\\left(\\frac{3}{4}\\right)^2 + \\left(\\frac{1}{4}\\right)^2\\right) = 1 - \\left(\\frac{9}{16} + \\frac{1}{16}\\right) = 1 - \\frac{10}{16} = \\frac{6}{16} = 0.375\n\\]\n\n\n\nNext, calculate the proportions of each class within non-smokers:\n\nProportion of having the condition (1): \\(p_1 = \\frac{2}{6} = \\frac{1}{3}\\)\nProportion of not having the condition (0): \\(p_0 = \\frac{4}{6} = \\frac{2}{3}\\)\n\nThen, the Gini impurity for non-smokers can be calculated as:\n\\[\nGini_{\\text{Non-Smokers}} = 1 - (p_1^2 + p_0^2) = 1 - \\left(\\left(\\frac{1}{3}\\right)^2 + \\left(\\frac{2}{3}\\right)^2\\right) = 1 - \\left(\\frac{1}{9} + \\frac{4}{9}\\right) = 1 - \\frac{5}{9} = \\frac{4}{9} \\approx 0.444\n\\]\n\n\n\nTo determine the optimal feature to split on, we calculate the weighted Gini impurity for the entire dataset before the split and compare it with the weighted average Gini impurity after the split. The split that results in the greatest decrease in impurity is considered optimal.\nGiven the Gini impurities calculated for both groups, we calculate the weighted average Gini impurity for splitting on “Smoker”:\n\\[\n\\text{Weighted Gini} = \\frac{4}{10} \\times Gini_{\\text{Smokers}} + \\frac{6}{10} \\times Gini_{\\text{Non-Smokers}} = \\frac{4}{10} \\times 0.375 + \\frac{6}{10} \\times 0.444 \\approx 0.416\n\\]\nAssuming this is the lowest weighted Gini impurity compared to other features, splitting on “Smoker” would be considered the optimal choice at this node.\nIn this detailed example, we’ve seen how to calculate Gini impurity by hand for a simple dataset, providing insights into the decision-making process within a decision tree algorithm. Next, we will implement this decision tree in R and verify our manual calculations."
  },
  {
    "objectID": "posts/2024-03-26-DT/2024-03-26-DT.html#metrics-navigating-improvement-in-decision-trees",
    "href": "posts/2024-03-26-DT/2024-03-26-DT.html#metrics-navigating-improvement-in-decision-trees",
    "title": "Jacob Jameson",
    "section": "Metrics: Navigating Improvement in Decision Trees",
    "text": "Metrics: Navigating Improvement in Decision Trees\nIn the journey of machine learning, each algorithm strives for perfection through iteration, continuously improving upon a specific measure or metric. Decision trees are no exception. To steer this algorithm correctly, we must first establish the criteria it aims to optimize.\n\nClassification Metrics for Decision Trees\n\nGini Impurity\nGini Impurity is an insightful metric focused on misclassification. It hinges on the probability of classifying a data point correctly within our dataset. For any given class \\(c\\) with frequency \\(f_c\\), we predict that class for a data point \\(f_c\\) proportion of the time. The error for each class is the chance of selecting the class multiplied by the likelihood of being incorrect:\n\\[Error_c = f_c \\times (1 - f_c)\\]\nThe Gini Impurity is the aggregate of these individual errors across all classes:\n\\[Gini\\ Impurity = \\sum_c f_c \\times (1 - f_c) = \\sum_c f_c - \\sum_c f_c^2 = 1 - \\sum_c f_c^2\\]\nImproving the Gini Impurity, specifically by minimizing it across data regions, directly leads to a reduction in misclassification.\n\n\nGini Impurity Illustrated with a Coin Toss\nConsider the Gini Impurity of a coin flip, where the probability of heads is \\(p_H\\):\n\\[Gini\\ Impurity = 1 - p_H^2 - (1 - p_H)^2 = 2 \\times p_H \\times (1 - p_H)\\]\n\np &lt;- seq(0.01, 0.99, 0.01)\nplot(p, 1-p^2-(1-p)^2, type='l', ylim=c(0,1), \n     ylab = \"Gini Impurity\", xlab=\"Probability of Heads\", \n     col=\"blue\")\n\n\n\n\nWhen \\(p_H\\) is at 0 or 1, there’s no chance of error—our predictions are certain. Yet at \\(p_H = 0.5\\), Gini Impurity peaks, reflecting the highest error rate and aligning with our intuitive understanding.\n\n\nEntropy\nEntropy, on the other hand, takes a different tack. It measures the expected information content—or surprise—of a class in a dataset. If \\(f_c\\) is the proportion of a class, and \\(I(c)\\) represents the information content of the class, then entropy is defined as:\n\\[Entropy = \\sum_c f_c \\times I(c)\\]\nWithout delving too deep into the underlying theory, the formula for entropy, given that \\(I(c) = -\\log_2(f_c)\\), simplifies to:\n\\[Entropy = -\\sum_c f_c \\times \\log_2(f_c)\\]\nThough the logarithm base could be arbitrary, we align it with the binary nature of the decision trees we’re examining in this series.\n\n\nEntropy in the Context of a Coin Toss\nExtending our analogy to the coin toss, let’s examine how entropy changes with varying probabilities of heads, \\(p_H\\):\n\\[Entropy = -p_H \\log_2(p_H) - (1 - p_H) \\log_2(1 - p_H)\\]\n\np &lt;- seq(0.01, 0.99, 0.01)\nplot(p, -p*log2(p)-(1-p)*log2(1-p), type='l', ylim=c(0,1), \n     ylab = \"Entropy\", xlab=\"Probability of Heads\", col=\"blue\")\n\n\n\n\nEntropy achieves its maximum at \\(p_H = 0.5\\), where uncertainty—and the potential for error—is greatest. This resonates with the principle that the greatest entropy occurs where prediction is most challenging.\nRefining Our Approach with Information Gain\nWith our metrics in place to judge how well our dataset’s predictions are doing, it’s time to focus on improvement—how can we make our predictions even better? We’re going to talk about something called ‘Information Gain’, a concept which is the difference we want to maximize when deciding how to split our data.\nFine-tuning a Decision Tree for Classification\nWhen we build a classifier—a kind of decision tree—our goal is to organize our data into groups that make our predictions as accurate as possible. The concept of ‘Information Gain’ is a bit like a treasure map; it helps us find these valuable groupings by measuring how much more ordered, or less chaotic, our data becomes after a split.\n\\[Information\\ Gain = Gini\\ Impurity_{before\\ split} - \\sum_{each\\ split} \\frac{|split\\ group|}{|total\\ data|} Gini\\ Impurity_{split\\ group}\\]\n\\[Information\\ Gain = Entropy_{before\\ split} - \\sum_{each\\ split} \\frac{|split\\ group|}{|total\\ data|} Entropy_{split\\ group}\\]\nIn these equations, \\(Gini\\ Impurity_{before\\ split}\\) and \\(Entropy_{before\\ split}\\) represent the disorder in our data before any splits. The sum of the Gini Impurity or Entropy of each split group is weighted by the proportion of data in that group. The Information Gain is the difference between the disorder before the split and the weighted sum of disorder after the split.\nIn essence, it tells us how much clearer our predictions become after we’ve sorted the data into separate buckets based on certain features.\nBalancing Decision Trees to Avoid Overfitting\nA common pitfall with decision trees is that they can get a little too enthusiastic, creating very specific rules that work perfectly for the data they were trained on but not so well for new data. This is known as overfitting.\nTo avoid this, we can trim or ‘prune’ the tree after it’s grown, or we can set some ground rules that stop the tree from getting overly complicated in the first place. These rules can include limits like:\n\nMaximum depth of the tree\nMinimum number of data points to justify creating a new branch\nMinimum improvement needed to make a new split\nA threshold below which we won’t consider any new splits\n\nThe Iris Dataset: Our Classification Playground We’ll be using the well-loved Iris dataset for classification tasks. The Species attribute will be our target for prediction. Here’s a peek at the dataset:\n\nlibrary(datasets)\ndata(iris)\nhead(iris, 6)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nBy using the Iris dataset, we can focus on classifying flower species, a perfect starting point for practicing with decision trees.\nBuilding the Classifier: The Search for the First Split In our script, we’re not just taking wild guesses. We’re methodically examining each feature to find the value that offers the best division of our data, aiming to increase Information Gain.\n\nentropy &lt;- function(y) {\n  if(length(y) == 0) return(0)\n  p &lt;- table(y) / length(y)\n  sum(-p * log2(p + 1e-9))\n}\n\ngini_impurity &lt;- function(y) {\n  if(length(y) == 0) return(0)\n  p &lt;- table(y) / length(y)\n  1 - sum(p^2)\n}\n\nNow we can calculate the Information Gain, considering the improvements brought by different partitions in our data. We use a mask—a logical vector indicating if a row belongs to the first or second partition—to guide this process:\n\ninformation_gain &lt;- function(y, mask, metric_func = entropy) {\n  # We don't want partitions with no data points\n  if (sum(mask) == 0 || sum(!mask) == 0) return(0)\n  metric_func(y) - (sum(mask) / length(mask)) * metric_func(y[mask]) - \n    (sum(!mask) / length(!mask)) * metric_func(y[!mask])\n}\n\nSeeking the Best Split Across All Features With our metric functions in hand, we’ll comb through all the features to find the ultimate split:\n\nmax_information_gain_split &lt;- function(y, x, metric_func = gini_impurity) {\n  # Initialize the best change and split value\n  best_change &lt;- NA\n  split_value &lt;- NA\n  # Check if the feature is numeric or categorical\n  is_numeric &lt;- !(is.factor(x) || is.logical(x) || is.character(x))\n\n  for(val in sort(unique(x))) {\n    mask &lt;- if (is_numeric) { x &lt; val } else { x == val }\n    change &lt;- information_gain(y, mask, metric_func)\n    if(is.na(best_change) || change &gt; best_change) {\n      best_change &lt;- change\n      split_value &lt;- val\n    }\n  }\n\n  return(list(\"best_change\" = best_change, \"split_value\" = split_value, \"is_numeric\" = is_numeric))\n}\n\nNow, let’s use this to find the best split for a single feature:\n\nprint(max_information_gain_split(iris$Species, iris$Petal.Width))\n\n$best_change\n[1] 0.3333333\n\n$split_value\n[1] 1\n\n$is_numeric\n[1] TRUE\n\n\n\nsapply(iris[, 1:4], function(x) max_information_gain_split(iris$Species, x))\n\n            Sepal.Length Sepal.Width Petal.Length Petal.Width\nbest_change 0.2277603    0.1269234   0.3333333    0.3333333  \nsplit_value 5.5          3.4         3            1          \nis_numeric  TRUE         TRUE        TRUE         TRUE       \n\n\nAfter determining the feature and value that provide the most significant information gain, we can then carve our dataset into two distinct sets. Think of it like sorting a deck of cards where one feature is the deciding factor.\n\nbest_feature_split &lt;- function(X, y) {\n  results &lt;- sapply(X, function(x) max_information_gain_split(y, x))\n  best_name &lt;- names(which.max(results['best_change',]))\n  best_result &lt;- results[, best_name]\n  best_result[[\"name\"]] &lt;- best_name\n  return(best_result)\n}\n\nLet’s run it for the iris dataset and see which feature provides the best split.\n\nas.data.frame(best_feature_split(iris[, 1:4], iris[, 5]))\n\n  best_change split_value is_numeric         name\n1   0.3333333           3       TRUE Petal.Length\n\n\nThis neat little function sifts through our data and finds the golden split—the point where dividing our dataset yields the most clarity according to our chosen metric.\nNow, imagine we’ve found our winning feature. The next step is to partition the iris dataset into two new datasets based on this feature. We’ll refer to them as ‘left’ and ‘right’ datasets, aligning with the branching structure of a tree.\n\nget_best_mask &lt;- function(X, best_feature_list) {\n  best_mask &lt;- X[, best_feature_list$name] &lt; best_feature_list$split_value\n  return(best_mask)\n}\n\nWe’ll get the best mask and split the iris dataset accordingly.\n\nbest_iris_split &lt;- best_feature_split(iris[, 1:4], iris[, 5])\nbest_iris_mask &lt;- get_best_mask(iris[, 1:4], best_iris_split)\n\nPartition the iris dataset into ‘left’ and ‘right’\n\nleft_iris &lt;- iris[best_iris_mask, ]\nright_iris &lt;- iris[!best_iris_mask, ]\n\nAnd just like that, with our data now split into ‘left’ and ‘right’, we’ve taken the first step in constructing our decision tree. This process will serve as the foundation for building a data structure that captures the essence of our tree, complete with branches that stem from each decision point.\n\ncalculate_entropy &lt;- function(y) {\n  if (length(y) == 0) return(0)\n  p &lt;- table(y) / length(y)\n  -sum(p * log2(p + 1e-9))\n}\n\ncalculate_gini_impurity &lt;- function(y) {\n  if (length(y) == 0) return(0)\n  p &lt;- table(y) / length(y)\n  1 - sum(p^2)\n}\n\ncalculate_information_gain &lt;- function(y, mask, metric_func) {\n  s1 &lt;- sum(mask)\n  s2 &lt;- length(mask) - s1\n  if (s1 == 0 || s2 == 0) return(0)\n  metric_func(y) - s1 / (s1 + s2) * metric_func(y[mask]) - s2 / (s1 + s2) * metric_func(y[!mask])\n}\n\nfind_best_split &lt;- function(X, y, metric_func) {\n  features &lt;- names(X)\n  best_gain &lt;- 0\n  best_split &lt;- NULL\n  best_feature &lt;- NULL\n  for (feature in features) {\n    unique_values &lt;- unique(X[[feature]])\n    for (value in unique_values) {\n      mask &lt;- X[[feature]] &lt; value\n      gain &lt;- calculate_information_gain(y, mask, metric_func)\n      if (gain &gt; best_gain) {\n        best_gain &lt;- gain\n        best_split &lt;- value\n        best_feature &lt;- feature\n      }\n    }\n  }\n  list(gain = best_gain, feature = best_feature, split = best_split)\n}\n\nsplit_data &lt;- function(X, y, best_split) {\n  mask &lt;- X[[best_split$feature]] &lt; best_split$split\n  list(\n    left_X = X[mask, ],\n    left_y = y[mask],\n    right_X = X[!mask, ],\n    right_y = y[!mask]\n  )\n}\n\ncreate_decision_tree &lt;- function(X, y, max_depth = 3, depth = 0, metric_func = calculate_gini_impurity) {\n  if (depth == max_depth || length(unique(y)) == 1) {\n    return(list(prediction = ifelse(is.factor(y), names(sort(-table(y)))[1], mean(y))))\n  }\n  \n  best_split &lt;- find_best_split(X, y, metric_func)\n  if (best_split$gain == 0) {\n    return(list(prediction = ifelse(is.factor(y), names(sort(-table(y)))[1], mean(y))))\n  }\n  \n  split_sets &lt;- split_data(X, y, best_split)\n  return(list(\n    feature = best_split$feature,\n    split = best_split$split,\n    left = create_decision_tree(split_sets$left_X, split_sets$left_y, max_depth, depth + 1, metric_func),\n    right = create_decision_tree(split_sets$right_X, split_sets$right_y, max_depth, depth + 1, metric_func)\n  ))\n}\n\n\nprint_decision_tree &lt;- function(node, prefix = \"\") {\n  if (!is.null(node$prediction)) {\n    cat(prefix, \"Predict:\", node$prediction, \"\\n\")\n  } else {\n    cat(prefix, \"If\", node$feature, \"&lt;\", node$split, \":\\n\")\n    print_decision_tree(node$left, paste0(prefix, \"  \"))\n    \n    cat(prefix, \"Else (\", node$feature, \"&gt;=\", node$split, \"):\\n\")\n    print_decision_tree(node$right, paste0(prefix, \"  \"))\n  }\n}\n\niris_tree &lt;- create_decision_tree(iris[, 1:4], iris[, 5], max_depth = 3)\n\nprint_decision_tree(iris_tree)\n\n If Petal.Length &lt; 3 :\n   Predict: setosa \n Else ( Petal.Length &gt;= 3 ):\n   If Petal.Width &lt; 1.8 :\n     If Petal.Length &lt; 5 :\n       Predict: versicolor \n     Else ( Petal.Length &gt;= 5 ):\n       Predict: virginica \n   Else ( Petal.Width &gt;= 1.8 ):\n     If Petal.Length &lt; 4.9 :\n       Predict: virginica \n     Else ( Petal.Length &gt;= 4.9 ):\n       Predict: virginica"
  },
  {
    "objectID": "posts/2024-03-26-DT/2024-03-26-DT.html#comparing-our-tree-to-rpart",
    "href": "posts/2024-03-26-DT/2024-03-26-DT.html#comparing-our-tree-to-rpart",
    "title": "DIY Decision Trees",
    "section": "Comparing Our Tree to rpart",
    "text": "Comparing Our Tree to rpart\nAfter meticulously constructing our decision tree from scratch and gaining insights into its inner workings, it’s illuminating to compare our results with those from the well-established rpart package. This comparison not only validates our approach but also offers perspective on how different methodologies might influence the model’s structure and performance."
  },
  {
    "objectID": "posts/2024-03-26-DT/2024-03-26-DT.html#embarking-on-a-diy-journey-with-machine-learning-algorithms",
    "href": "posts/2024-03-26-DT/2024-03-26-DT.html#embarking-on-a-diy-journey-with-machine-learning-algorithms",
    "title": "DIY Decision Trees",
    "section": "",
    "text": "Why go through the trouble of building a machine learning algorithm by hand, especially when libraries like sklearn in Python or rpart in R are just a function call away? The answer lies in the deeper understanding and insights that come from peeling back the layers of these algorithms ourselves."
  },
  {
    "objectID": "posts/2024-03-26-DT/2024-03-26-DT.html#practical-benefits-of-a-hands-on-approach",
    "href": "posts/2024-03-26-DT/2024-03-26-DT.html#practical-benefits-of-a-hands-on-approach",
    "title": "DIY Decision Trees",
    "section": "",
    "text": "Beyond the theoretical, there’s immense practical value in understanding the inner workings of these models. It allows for more informed decisions about feature selection—knowing which to discard because they don’t fit the model’s assumptions and which to transform for better representation. This knowledge often translates to improved model performance on new, unseen data.\nAnd let’s not overlook the sheer joy of the process. Diving deep into algorithms offers a unique way to tackle problem-solving, enhancing our execution and grasp of machine learning concepts."
  },
  {
    "objectID": "posts/2024-03-26-DT/2024-03-26-DT.html#metrics-the-compass-for-improvement",
    "href": "posts/2024-03-26-DT/2024-03-26-DT.html#metrics-the-compass-for-improvement",
    "title": "DIY Decision Trees",
    "section": "",
    "text": "How do we navigate through the countless ways to split our data? We turn to optimization metrics, the compass guiding each iteration towards better performance. For decision trees, two key metrics stand out: Gini Impurity and Entropy.\n\n\nGini Impurity is an insightful metric focused on misclassification. It hinges on the probability of classifying a data point correctly within our dataset. For any given class \\(c\\) with frequency \\(f_c\\), we predict that class for a data point \\(f_c\\) proportion of the time. The error for each class is the chance of selecting the class multiplied by the likelihood of being incorrect:\n\\[Error_c = f_c \\times (1 - f_c)\\]\nThe Gini Impurity is the aggregate of these individual errors across all classes:\n\\[Gini\\ Impurity = \\sum_c f_c \\times (1 - f_c) = \\sum_c f_c - \\sum_c f_c^2 = 1 - \\sum_c f_c^2\\]\nImproving the Gini Impurity, specifically by minimizing it across data regions, directly leads to a reduction in misclassification.\n\n\n\nConsider the Gini Impurity of a coin flip, where the probability of heads is \\(p_H\\):\n\\[Gini\\ Impurity = 1 - p_H^2 - (1 - p_H)^2 = 2 \\times p_H \\times (1 - p_H)\\]\n\np &lt;- seq(0.01, 0.99, 0.01)\nplot(p, 1-p^2-(1-p)^2, type='l', ylim=c(0,1), \n     ylab = \"Gini Impurity\", xlab=\"Probability of Heads\", \n     col=\"blue\")\n\n\n\n\nWhen \\(p_H\\) is at 0 or 1, there’s no chance of error—our predictions are certain. Yet at \\(p_H = 0.5\\), Gini Impurity peaks, reflecting the highest error rate and aligning with our intuitive understanding.\n\n\n\nEntropy, on the other hand, takes a different tack. It measures the expected information content—or surprise—of a class in a dataset. If \\(f_c\\) is the proportion of a class, and \\(I(c)\\) represents the information content of the class, then entropy is defined as:\n\\[Entropy = \\sum_c f_c \\times I(c)\\]\nWithout delving too deep into the underlying theory, the formula for entropy, given that \\(I(c) = -\\log_2(f_c)\\), simplifies to:\n\\[Entropy = -\\sum_c f_c \\times \\log_2(f_c)\\]\nThough the logarithm base could be arbitrary, we align it with the binary nature of the decision trees we’re examining in this series.\n\n\n\nExtending our analogy to the coin toss, let’s examine how entropy changes with varying probabilities of heads, \\(p_H\\):\n\\[Entropy = -p_H \\log_2(p_H) - (1 - p_H) \\log_2(1 - p_H)\\]\n\np &lt;- seq(0.01, 0.99, 0.01)\nplot(p, -p*log2(p)-(1-p)*log2(1-p), type='l', ylim=c(0,1), \n     ylab = \"Entropy\", xlab=\"Probability of Heads\", col=\"blue\")\n\n\n\n\nEntropy achieves its maximum at \\(p_H = 0.5\\), where uncertainty—and the potential for error—is greatest. This resonates with the principle that the greatest entropy occurs where prediction is most challenging."
  },
  {
    "objectID": "posts/2024-03-26-DT/2024-03-26-DT.html#building-the-tree-with-rpart",
    "href": "posts/2024-03-26-DT/2024-03-26-DT.html#building-the-tree-with-rpart",
    "title": "DIY Decision Trees",
    "section": "Building the Tree with rpart",
    "text": "Building the Tree with rpart\nTo make this comparison, we first need to construct a decision tree using rpart on the same dataset:\n\nlibrary(rpart)\n# Building the decision tree model with rpart\nrpart_tree &lt;- rpart(Species ~ ., data = iris, method = \"class\")\n\nSummary of the rpart tree\n\nsummary(rpart_tree)\n\nCall:\nrpart(formula = Species ~ ., data = iris, method = \"class\")\n  n= 150 \n\n    CP nsplit rel error xerror       xstd\n1 0.50      0      1.00   1.24 0.04636090\n2 0.44      1      0.50   0.85 0.06069047\n3 0.01      2      0.06   0.09 0.02908608\n\nVariable importance\n Petal.Width Petal.Length Sepal.Length  Sepal.Width \n          34           31           21           14 \n\nNode number 1: 150 observations,    complexity param=0.5\n  predicted class=setosa      expected loss=0.6666667  P(node) =1\n    class counts:    50    50    50\n   probabilities: 0.333 0.333 0.333 \n  left son=2 (50 obs) right son=3 (100 obs)\n  Primary splits:\n      Petal.Length &lt; 2.45 to the left,  improve=50.00000, (0 missing)\n      Petal.Width  &lt; 0.8  to the left,  improve=50.00000, (0 missing)\n      Sepal.Length &lt; 5.45 to the left,  improve=34.16405, (0 missing)\n      Sepal.Width  &lt; 3.35 to the right, improve=19.03851, (0 missing)\n  Surrogate splits:\n      Petal.Width  &lt; 0.8  to the left,  agree=1.000, adj=1.00, (0 split)\n      Sepal.Length &lt; 5.45 to the left,  agree=0.920, adj=0.76, (0 split)\n      Sepal.Width  &lt; 3.35 to the right, agree=0.833, adj=0.50, (0 split)\n\nNode number 2: 50 observations\n  predicted class=setosa      expected loss=0  P(node) =0.3333333\n    class counts:    50     0     0\n   probabilities: 1.000 0.000 0.000 \n\nNode number 3: 100 observations,    complexity param=0.44\n  predicted class=versicolor  expected loss=0.5  P(node) =0.6666667\n    class counts:     0    50    50\n   probabilities: 0.000 0.500 0.500 \n  left son=6 (54 obs) right son=7 (46 obs)\n  Primary splits:\n      Petal.Width  &lt; 1.75 to the left,  improve=38.969400, (0 missing)\n      Petal.Length &lt; 4.75 to the left,  improve=37.353540, (0 missing)\n      Sepal.Length &lt; 6.15 to the left,  improve=10.686870, (0 missing)\n      Sepal.Width  &lt; 2.45 to the left,  improve= 3.555556, (0 missing)\n  Surrogate splits:\n      Petal.Length &lt; 4.75 to the left,  agree=0.91, adj=0.804, (0 split)\n      Sepal.Length &lt; 6.15 to the left,  agree=0.73, adj=0.413, (0 split)\n      Sepal.Width  &lt; 2.95 to the left,  agree=0.67, adj=0.283, (0 split)\n\nNode number 6: 54 observations\n  predicted class=versicolor  expected loss=0.09259259  P(node) =0.36\n    class counts:     0    49     5\n   probabilities: 0.000 0.907 0.093 \n\nNode number 7: 46 observations\n  predicted class=virginica   expected loss=0.02173913  P(node) =0.3066667\n    class counts:     0     1    45\n   probabilities: 0.000 0.022 0.978 \n\n\nThis code snippet fits a decision tree to predict the Species from the iris dataset, mimicking the setup of our manually built tree. The summary provides detailed information about the tree’s splits, size, and predictive performance.\n\nVisualizing the rpart Tree\nVisualizing the tree helps in understanding the decisions it makes. The rpart.plot package offers tools for this purpose:\n\nlibrary(rpart.plot)\nrpart.plot(rpart_tree, type = 3, box.palette = \"RdBu\", extra = 104)\n\n\n\n\nThis visualization shows the splits made by the tree, the criteria for each decision, and the distribution of classes at each node. The type, color palette, and extra parameters are adjustable to enhance readability and insight."
  },
  {
    "objectID": "posts/2024-03-26-DT/2024-03-26-DT.html#evaluating-the-models",
    "href": "posts/2024-03-26-DT/2024-03-26-DT.html#evaluating-the-models",
    "title": "DIY Decision Trees",
    "section": "Evaluating the Models",
    "text": "Evaluating the Models\nWith both trees constructed, we can now evaluate their performance on the dataset. This comparison will shed light on the predictive power of each model and how they differ in their decision-making processes.\n\npredict_manual_tree &lt;- function(tree, newdata) {\n  if (!is.null(tree$prediction)) {\n    # If it's a leaf node, return the prediction\n    return(tree$prediction)\n  } else {\n    # Determine whether to follow the left or right branch\n    if (!is.null(tree$split) && newdata[[tree$feature]] &lt; tree$split) {\n      return(predict_manual_tree(tree$left, newdata))\n    } else {\n      return(predict_manual_tree(tree$right, newdata))\n    }\n  }\n}\n\npredicted_manual &lt;- sapply(1:nrow(iris), function(i) predict_manual_tree(iris_tree, iris[i, ]))\nmanual_accuracy &lt;- mean(predicted_manual == iris$Species)\n\n# For the rpart tree\npredicted_rpart &lt;- predict(rpart_tree, iris, type = \"class\")\nrpart_accuracy &lt;- mean(predicted_rpart == iris$Species)\n\ncat(\"Manual Tree Accuracy:\", manual_accuracy, \"\\n\")\n\nManual Tree Accuracy: 0.9733333 \n\ncat(\"Rpart Tree Accuracy:\", rpart_accuracy, \"\\n\")\n\nRpart Tree Accuracy: 0.96 \n\n\nInteresting! Our manually constructed tree had higher accuracy than the rpart tree on the iris dataset. How did that happen? The difference in accuracy could be due to the hyperparameters used, the splitting criterion, or the stopping criteria. It’s a reminder that the choice of algorithm and its parameters can significantly impact the model’s performance."
  },
  {
    "objectID": "posts/2024-03-26-DT/2024-03-26-DT.html#conclusion",
    "href": "posts/2024-03-26-DT/2024-03-26-DT.html#conclusion",
    "title": "DIY Decision Trees",
    "section": "Conclusion",
    "text": "Conclusion\nIn this project, we delved into the inner workings of decision trees, building one from scratch and comparing it with the rpart package. We explored the concepts of entropy, information gain, and Gini impurity, which are fundamental to decision tree algorithms. By constructing a decision tree manually, we gained a deeper understanding of how these models make decisions and split the data.\nThank you! I hope you learned as much as I did while working on this post! If you have any questions or feedback, please feel free to reach out. Happy learning!"
  },
  {
    "objectID": "posts/2024-03-26-DT/2024-03-26-DT.html#refining-our-approach-with-information-gain",
    "href": "posts/2024-03-26-DT/2024-03-26-DT.html#refining-our-approach-with-information-gain",
    "title": "DIY Decision Trees",
    "section": "",
    "text": "With our metrics in place to judge how well our dataset’s predictions are doing, it’s time to focus on improvement—how can we make our predictions even better? We’re going to talk about something called ‘Information Gain’, a concept which is the difference we want to maximize when deciding how to split our data.\n\n\nWhen we build a classifier—a kind of decision tree—our goal is to organize our data into groups that make our predictions as accurate as possible. The concept of ‘Information Gain’ is a bit like a treasure map; it helps us find these valuable groupings by measuring how much more ordered, or less chaotic, our data becomes after a split.\n\\[Information\\ Gain = Gini\\ Impurity_{before\\ split} - \\sum_{each\\ split} \\frac{|split\\ group|}{|total\\ data|} Gini\\ Impurity_{split\\ group}\\]\n\\[Information\\ Gain = Entropy_{before\\ split} - \\sum_{each\\ split} \\frac{|split\\ group|}{|total\\ data|} Entropy_{split\\ group}\\]\nIn these equations, \\(Gini\\ Impurity_{before\\ split}\\) and \\(Entropy_{before\\ split}\\) represent the disorder in our data before any splits. The sum of the Gini Impurity or Entropy of each split group is weighted by the proportion of data in that group. The Information Gain is the difference between the disorder before the split and the weighted sum of disorder after the split.\nIn essence, it tells us how much clearer our predictions become after we’ve sorted the data into separate buckets based on certain features.\n\n\n\nA common pitfall with decision trees is that they can get a little too enthusiastic, creating very specific rules that work perfectly for the data they were trained on but not so well for new data. This is known as overfitting.\nTo avoid this, we can trim or ‘prune’ the tree after it’s grown, or we can set some ground rules that stop the tree from getting overly complicated in the first place. These rules can include limits like:\n\nMaximum depth of the tree\nMinimum number of data points to justify creating a new branch\nMinimum improvement needed to make a new split\nA threshold below which we won’t consider any new splits"
  },
  {
    "objectID": "posts/2024-03-26-DT/2024-03-26-DT.html#the-iris-dataset-our-classification-playground",
    "href": "posts/2024-03-26-DT/2024-03-26-DT.html#the-iris-dataset-our-classification-playground",
    "title": "DIY Decision Trees",
    "section": "",
    "text": "We’ll be using the well-loved Iris dataset for classification tasks. The Species attribute will be our target for prediction. Here’s a peek at the dataset:\n\nlibrary(datasets)\ndata(iris)\nhead(iris, 6)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nBy using the Iris dataset, we can focus on classifying flower species, a perfect starting point for practicing with decision trees.\nBuilding the Classifier: The Search for the First Split In our script, we’re not just taking wild guesses. We’re methodically examining each feature to find the value that offers the best division of our data, aiming to increase Information Gain.\n\nentropy &lt;- function(y) {\n  if(length(y) == 0) return(0)\n  p &lt;- table(y) / length(y)\n  sum(-p * log2(p + 1e-9))\n}\n\ngini_impurity &lt;- function(y) {\n  if(length(y) == 0) return(0)\n  p &lt;- table(y) / length(y)\n  1 - sum(p^2)\n}\n\nNow we can calculate the Information Gain, considering the improvements brought by different partitions in our data. We use a mask—a logical vector indicating if a row belongs to the first or second partition—to guide this process:\n\ninformation_gain &lt;- function(y, mask, metric_func = entropy) {\n  # We don't want partitions with no data points\n  if (sum(mask) == 0 || sum(!mask) == 0) return(0)\n  metric_func(y) - (sum(mask) / length(mask)) * metric_func(y[mask]) - \n    (sum(!mask) / length(!mask)) * metric_func(y[!mask])\n}"
  },
  {
    "objectID": "posts/2024-03-26-DT/2024-03-26-DT.html#seeking-the-best-split-across-all-features",
    "href": "posts/2024-03-26-DT/2024-03-26-DT.html#seeking-the-best-split-across-all-features",
    "title": "DIY Decision Trees",
    "section": "",
    "text": "With our metric functions in hand, we’ll comb through all the features to find the ultimate split:\n\nmax_information_gain_split &lt;- function(y, x, metric_func = gini_impurity) {\n  # Initialize the best change and split value\n  best_change &lt;- NA\n  split_value &lt;- NA\n  # Check if the feature is numeric or categorical\n  is_numeric &lt;- !(is.factor(x) || is.logical(x) || is.character(x))\n\n  for(val in sort(unique(x))) {\n    mask &lt;- if (is_numeric) { x &lt; val } else { x == val }\n    change &lt;- information_gain(y, mask, metric_func)\n    if(is.na(best_change) || change &gt; best_change) {\n      best_change &lt;- change\n      split_value &lt;- val\n    }\n  }\n\n  return(list(\"best_change\" = best_change, \"split_value\" = split_value, \"is_numeric\" = is_numeric))\n}\n\nNow, let’s use this to find the best split for a single feature:\n\nprint(max_information_gain_split(iris$Species, iris$Petal.Width))\n\n$best_change\n[1] 0.3333333\n\n$split_value\n[1] 1\n\n$is_numeric\n[1] TRUE\n\n\n\nsapply(iris[, 1:4], function(x) max_information_gain_split(iris$Species, x))\n\n            Sepal.Length Sepal.Width Petal.Length Petal.Width\nbest_change 0.2277603    0.1269234   0.3333333    0.3333333  \nsplit_value 5.5          3.4         3            1          \nis_numeric  TRUE         TRUE        TRUE         TRUE       \n\n\nAfter determining the feature and value that provide the most significant information gain, we can then carve our dataset into two distinct sets. Think of it like sorting a deck of cards where one feature is the deciding factor.\n\nbest_feature_split &lt;- function(X, y) {\n  results &lt;- sapply(X, function(x) max_information_gain_split(y, x))\n  best_name &lt;- names(which.max(results['best_change',]))\n  best_result &lt;- results[, best_name]\n  best_result[[\"name\"]] &lt;- best_name\n  return(best_result)\n}\n\nLet’s run it for the iris dataset and see which feature provides the best split.\n\nas.data.frame(best_feature_split(iris[, 1:4], iris[, 5]))\n\n  best_change split_value is_numeric         name\n1   0.3333333           3       TRUE Petal.Length\n\n\nThis neat little function sifts through our data and finds the golden split—the point where dividing our dataset yields the most clarity according to our chosen metric.\nNow, imagine we’ve found our winning feature. The next step is to partition the iris dataset into two new datasets based on this feature. We’ll refer to them as ‘left’ and ‘right’ datasets, aligning with the branching structure of a tree.\n\nget_best_mask &lt;- function(X, best_feature_list) {\n  best_mask &lt;- X[, best_feature_list$name] &lt; best_feature_list$split_value\n  return(best_mask)\n}\n\nWe’ll get the best mask and split the iris dataset accordingly.\n\nbest_iris_split &lt;- best_feature_split(iris[, 1:4], iris[, 5])\nbest_iris_mask &lt;- get_best_mask(iris[, 1:4], best_iris_split)\n\nPartition the iris dataset into ‘left’ and ‘right’\n\nleft_iris &lt;- iris[best_iris_mask, ]\nright_iris &lt;- iris[!best_iris_mask, ]\n\nAnd just like that, with our data now split into ‘left’ and ‘right’, we’ve taken the first step in constructing our decision tree. This process will serve as the foundation for building a data structure that captures the essence of our tree, complete with branches that stem from each decision point."
  },
  {
    "objectID": "JSI@HKS/syllabus/syllabus.html",
    "href": "JSI@HKS/syllabus/syllabus.html",
    "title": "Office Hours",
    "section": "",
    "text": "Instructor: TBA\\ Teaching Assistant: TBA"
  },
  {
    "objectID": "api 222 files/section 8/section 8.1.html",
    "href": "api 222 files/section 8/section 8.1.html",
    "title": "Section 8.1 - Tree-Based Methods",
    "section": "",
    "text": "Note that more thorough treatment of these topics in Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani."
  },
  {
    "objectID": "api 222 files/section 8/section 8.1.html#tree-based-methods",
    "href": "api 222 files/section 8/section 8.1.html#tree-based-methods",
    "title": "Section 8.1 - Tree-Based Methods",
    "section": "Tree-Based Methods",
    "text": "Tree-Based Methods\nTree-based methods are non-parametric supervised learning methods that stratify or segment the predictor space into a number of simple regions. They can be used for both regression and clas- sification. After building a tree using the training data, a prediction can be made by using the training observations in the region to which the new observation belongs. For a regression tree, the predicted response for an observation is given by the mean response of the training observations that belong to the same terminal node. In contrast, for a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs. Decision trees are simple and useful for interpretation. However, they typically have lower predictive power than other supervised learning methods. Bagging, random forests, and boosting are approaches to improve decision trees by involving multiple trees, which are then combined to yield a single consensus prediction. These approaches can dramatically improve the prediction accuracy of decision trees, at the expense of interpretability.\n\nDecision Trees\nTo interpret a decision tree, suppose we have below tree from the Hitters data. The Figure represents a regression tree for predicting the log salary of a baseball player, based on the two predictors—the number of years that he has played in the major leagues and the number of hits that he made in the previous year. We can read the tree sequentially from top to bottom. At a given node (where the branches split), the label (of the form \\(X_j &lt; t_k\\)) indicates the left-hand branch emanating from that split, and the right-hand branch corresponds to \\(X_j \\geq t_k\\). For instance, the split at the top of the tree results in two large branches. The left-hand branch corresponds to \\(Years &lt; 4.5\\) and the right-hand branch corresponds to \\(Years \\geq 4.5\\). The number in each leaf (terminal nodes) is the mean of the response (outcomes) for the observations (in the training data) that fall there.\n\n\n\nDecision Tree Example\n\n\nDecision trees are easier to interpret and have a nice graphical representation. Unfortunately, fitting a decision tree is not simple. It is computationally infeasible to consider every possible partition of the feature space into \\(J\\) boxes. For this reason, we take a top-down, greedy approach that is known as recursive binary splitting. The approach is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. It is greedy because, at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step. In order to perform recursive binary splitting, we first select the predictor \\(X_j\\) and the cutpoint \\(s\\) such that splitting the predictor space into the regions \\(\\{X|X_j &lt; s\\}\\) and \\(\\{X|X_j \\geq s\\}\\) leads to the greatest possible reduction in RSS. For building a classification tree, alternatives to RSS are the classification error rate, Gini index, and entropy. Instead of the error rate, either the Gini index or the entropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate.\nAnother problem with the decision tree is that the process described above may produce good predictions on the training set but is likely to overfit the data, leading to poor test set performance. A better strategy is to grow a very large tree and then prune it back in order to obtain a subtree. We can use the cross validation to prune the tree.\nThe advantages of trees are the following: (1) trees are very easy to explain; (2) trees can be displayed graphically and are easily interpreted even by a non-expert; (3) trees can easily handle qualitative predictors without the need to create dummy variables. Disadvantages of trees are the following: (1) trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches; (2) trees can be very non-robust, i.e., a small change in the data can cause a large change in the final estimated tree.\n\n\nBagging\nBagging, random forests, and boosting are approaches to improve decision trees by involving multiple trees, which are then combined to yield a single consensus prediction. To apply bagging to regression trees, we simply construct \\(B\\) regression trees using \\(B\\) bootstrapped training sets and average the resulting predictions. These trees are grown deep and are not pruned. Because the tree is not pruned, it means each tree is more flexible, hence high variance but low bias. Averaging these \\(B\\) trees reduces the variance because in statistics, averaging a set of observations reduces variance. For bagging classification trees, we can record the class predicted by each of the \\(B\\) trees, and take a majority vote (i.e., the most commonly occurring class among the \\(B\\) predictions).\nWe can also obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). In the case of bagging regression trees, we can record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all \\(B\\) trees. A large value indicates an important predictor.\n\n\nRandom forests\nThe main difference between bagging and random forests is the choice of predictor subset size m. When building decision trees in random forests, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors. If a random forest is built using \\(m = p\\), then this amounts simply to bagging. Random forests can provide an improvement over bagged trees by decorrelating the trees, i.e., forcing the tree to consider different splits and thus avoid the situation when all trees to have similar structures due to a small subset of strong predictors. In general, using a small value of m in building a random forest will typically be helpful when we have a large number of correlated predictors.\n\n\nBoosting\nIn boosting, each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling, but instead, each tree is fit on a modified version of the original data set. Unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown. Boosting involves the parameters that we have to determine. The shrinkage parameter \\(\\lambda\\), a small positive number, controls the rate at which boosting learns. Very small \\(\\lambda\\) can require using a very large value of B and thus achieve good performance. We also need to determine the number \\(d\\) of splits in each tree, which controls the complexity of the boosted ensemble. Similar to \\(\\lambda\\), a small \\(d\\) can typically achieve a slower learn, which means better performance."
  },
  {
    "objectID": "api 222 files/section 8/section 8.2.html",
    "href": "api 222 files/section 8/section 8.2.html",
    "title": "Section 8.2 - Decision Trees",
    "section": "",
    "text": "#install.packages(\"tree\")\nlibrary(ISLR)\nlibrary(tree)\nlibrary(ggplot2)\n\nWe will use the Carseats data. Sales in this data set is a continuous variable. We start by converting it to a binary one that equals “Yes” if Sales \\(&gt; 8\\) and “No” otherwise.\n\ncarseat_data &lt;- Carseats\nhigh_sales &lt;- as.factor(ifelse(carseat_data$Sales &gt; 8, \"Yes\", \"No\"))\ncarseat_data &lt;- data.frame(carseat_data, high_sales)\ncarseat_data = carseat_data[, -1]\n\nLet’s again split the data into training and test sets\n\nset.seed(222)  \ntrain &lt;- sample(seq(nrow(carseat_data)),\n                round(nrow(carseat_data) * 0.5))\n\ntrain &lt;- sort(train)\ntest &lt;- which(!(seq(nrow(carseat_data)) %in% train))\n\nWe can now train a decision tree using the function tree()\n\n?tree\n\nHelp on topic 'tree' was found in the following packages:\n\n  Package               Library\n  cli                   /opt/homebrew/lib/R/4.3/site-library\n  xfun                  /opt/homebrew/lib/R/4.3/site-library\n  tree                  /opt/homebrew/lib/R/4.3/site-library\n\n\nUsing the first match ...\n\ncarseats_tree &lt;- tree(high_sales ~ ., data = carseat_data[train,])\n\nPlot the results\n\nplot(carseats_tree)\ntext(carseats_tree, pretty = 0)\n\n\n\n\nFrom this, we see that shelving location seems to be the most important determinant and price is the second most. Beyond that, this tree is very hard to read. If we just type the tree object name, we get:\n\nThe split criterion (e.g. Price &lt; 92.5)\nThe number of observations in that branch\nThe deviance\nThe overall prediction for the branch\nThe fraction of observations in that branch that are Yes/No\nBranches with terminal nodes are indicated by *\n\n\ncarseats_tree\n\nnode), split, n, deviance, yval, (yprob)\n      * denotes terminal node\n\n  1) root 200 271.500 No ( 0.58500 0.41500 )  \n    2) ShelveLoc: Bad,Medium 154 189.500 No ( 0.69481 0.30519 )  \n      4) Price &lt; 106.5 55  75.790 Yes ( 0.45455 0.54545 )  \n        8) Price &lt; 81 9   0.000 Yes ( 0.00000 1.00000 ) *\n        9) Price &gt; 81 46  63.420 No ( 0.54348 0.45652 )  \n         18) Age &lt; 63.5 31  40.320 Yes ( 0.35484 0.64516 )  \n           36) ShelveLoc: Bad 10  10.010 No ( 0.80000 0.20000 )  \n             72) CompPrice &lt; 112 5   0.000 No ( 1.00000 0.00000 ) *\n             73) CompPrice &gt; 112 5   6.730 No ( 0.60000 0.40000 ) *\n           37) ShelveLoc: Medium 21  17.220 Yes ( 0.14286 0.85714 )  \n             74) Income &lt; 48.5 7   9.561 Yes ( 0.42857 0.57143 ) *\n             75) Income &gt; 48.5 14   0.000 Yes ( 0.00000 1.00000 ) *\n         19) Age &gt; 63.5 15   7.348 No ( 0.93333 0.06667 ) *\n      5) Price &gt; 106.5 99  90.800 No ( 0.82828 0.17172 )  \n       10) CompPrice &lt; 142.5 84  57.200 No ( 0.89286 0.10714 )  \n         20) Price &lt; 127 46  45.480 No ( 0.80435 0.19565 )  \n           40) Age &lt; 52.5 20  26.920 No ( 0.60000 0.40000 )  \n             80) CompPrice &lt; 116 5   0.000 No ( 1.00000 0.00000 ) *\n             81) CompPrice &gt; 116 15  20.730 Yes ( 0.46667 0.53333 )  \n              162) Advertising &lt; 10.5 9  11.460 No ( 0.66667 0.33333 ) *\n              163) Advertising &gt; 10.5 6   5.407 Yes ( 0.16667 0.83333 ) *\n           41) Age &gt; 52.5 26   8.477 No ( 0.96154 0.03846 )  \n             82) CompPrice &lt; 132.5 21   0.000 No ( 1.00000 0.00000 ) *\n             83) CompPrice &gt; 132.5 5   5.004 No ( 0.80000 0.20000 ) *\n         21) Price &gt; 127 38   0.000 No ( 1.00000 0.00000 ) *\n       11) CompPrice &gt; 142.5 15  20.730 Yes ( 0.46667 0.53333 )  \n         22) Education &lt; 15.5 5   0.000 Yes ( 0.00000 1.00000 ) *\n         23) Education &gt; 15.5 10  12.220 No ( 0.70000 0.30000 )  \n           46) Income &lt; 52 5   0.000 No ( 1.00000 0.00000 ) *\n           47) Income &gt; 52 5   6.730 Yes ( 0.40000 0.60000 ) *\n    3) ShelveLoc: Good 46  48.170 Yes ( 0.21739 0.78261 )  \n      6) Price &lt; 136.5 39  29.870 Yes ( 0.12821 0.87179 )  \n       12) Advertising &lt; 6 17  20.600 Yes ( 0.29412 0.70588 )  \n         24) Price &lt; 109 8   0.000 Yes ( 0.00000 1.00000 ) *\n         25) Price &gt; 109 9  12.370 No ( 0.55556 0.44444 ) *\n       13) Advertising &gt; 6 22   0.000 Yes ( 0.00000 1.00000 ) *\n      7) Price &gt; 136.5 7   8.376 No ( 0.71429 0.28571 ) *\n\n\nGiven how deep our tree is grown, we may be worried about overfitting. We can start by evaluating the error rate on the test set for the current tree. We can write a helper function to compute the error rate\n\nerror_rate_func &lt;- function(predictions, true_vals) {\n  error_rate &lt;- mean(as.numeric(predictions != true_vals))\n  return(error_rate)\n}\n\nNow generate predictions from the model\n\ndeep_tree_preds &lt;- predict(carseats_tree, carseat_data[test,], \n                           type = \"class\")\n\nCalculate and summarize the error rate\n\nerror_rate_func(predictions = deep_tree_preds, \n                true_vals = carseat_data[test, \"high_sales\"])\n\n[1] 0.28\n\nsummary(carseats_tree)\n\n\nClassification tree:\ntree(formula = high_sales ~ ., data = carseat_data[train, ])\nVariables actually used in tree construction:\n[1] \"ShelveLoc\"   \"Price\"       \"Age\"         \"CompPrice\"   \"Income\"     \n[6] \"Advertising\" \"Education\"  \nNumber of terminal nodes:  19 \nResidual mean deviance:  0.4032 = 72.98 / 181 \nMisclassification error rate: 0.095 = 19 / 200 \n\n\nThe difference in our error rate between the training and test sets indicates that we overfit. To address this, we want to prune the tree. cv.tree() uses cross-validation to determine how much to prune the tree.\n\nset.seed(222)\ncv_carseats_tree  &lt;- cv.tree(carseats_tree, FUN = prune.misclass)\nnames(cv_carseats_tree)\n\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n\ncv_carseats_tree\n\n$size\n[1] 19 16 14  9  7  6  2  1\n\n$dev\n[1] 50 51 50 53 50 52 58 83\n\n$k\n[1] -Inf  0.0  0.5  1.0  2.5  3.0  6.0 26.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n\nSize tells us the number of terminal nodes on each of the trees considered; dev gives us the CV errors; k gives us the cost-complexity parameter. We can plot the error as a function of size and k\n\npar(mfrow = c(1, 2))\nplot(cv_carseats_tree$size, cv_carseats_tree$dev, type = \"b\")\nplot(cv_carseats_tree$k, cv_carseats_tree$dev, type = \"b\")\n\n\n\n\nFind and print the optimal size\n\nopt_indx &lt;- which.min(cv_carseats_tree$dev)\nopt_size &lt;- cv_carseats_tree$size[opt_indx]\nprint(opt_size)\n\n[1] 19\n\nopt_size &lt;- 7\n\nNow we can prune the tree using prune.misclass()\n\npruned_carseats_tree &lt;- prune.misclass(carseats_tree, best = opt_size)\nplot(pruned_carseats_tree)\ntext(pruned_carseats_tree, pretty = 0)\n\n\n\n\nNow evaluate model performance\n\npruned_tree_preds = predict(pruned_carseats_tree, carseat_data[test, ], \n                            type = \"class\")\n\nerror_rate_func(predictions = pruned_tree_preds, \n                true_vals = carseat_data[test, \"high_sales\"])\n\n[1] 0.25"
  },
  {
    "objectID": "api 222 files/section 8/section 8.2.html#polynomial-regression",
    "href": "api 222 files/section 8/section 8.2.html#polynomial-regression",
    "title": "Section 7.2 - Non-linear models",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nWe can start by fitting a polynomial regression using only age\n\nage_poly &lt;- lm(wage ~ poly(age, 4), data = wage_data[train,])\n\nExtract the coefficients from the model\n\ncoef(summary(age_poly))\n\n                Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    111.88351  0.8086511 138.358203 0.000000e+00\npoly(age, 4)1  394.18926 39.6156514   9.950342 6.942194e-23\npoly(age, 4)2 -434.00566 39.6156514 -10.955409 2.748637e-27\npoly(age, 4)3  105.56550 39.6156514   2.664742 7.756432e-03\npoly(age, 4)4  -90.09776 39.6156514  -2.274297 2.303622e-02\n\n\nWhen you use poly(), it returns a matrix of “orthogonal polynomials” so the columns of the matrix are linear combinations of age, age\\(^2\\), age\\(^3\\), age\\(^4\\). Let’s take a look!\n\nhead(poly(wage_data$age, 4))\n\n              1          2           3          4\n[1,] -0.0386248 0.05590873 -0.07174058 0.08672985\n[2,] -0.0386248 0.05590873 -0.07174058 0.08672985\n[3,] -0.0386248 0.05590873 -0.07174058 0.08672985\n[4,] -0.0386248 0.05590873 -0.07174058 0.08672985\n[5,] -0.0386248 0.05590873 -0.07174058 0.08672985\n[6,] -0.0386248 0.05590873 -0.07174058 0.08672985\n\nhead(wage_data$age)\n\n[1] 18 18 18 18 18 18\n\n\nIf you want it to return the raw powers of age, you can add an argument to the function poly()\n\nhead(poly(wage_data$age, 4, raw = TRUE))\n\n      1   2    3      4\n[1,] 18 324 5832 104976\n[2,] 18 324 5832 104976\n[3,] 18 324 5832 104976\n[4,] 18 324 5832 104976\n[5,] 18 324 5832 104976\n[6,] 18 324 5832 104976\n\n\nAlthough the two forms give you different numbers, they result in the same predictions, because your model is still a linear combination of the original powers\n\nage_poly_TRUE &lt;- lm(wage ~ poly(age, 4, raw = TRUE), \n                    data = wage_data[train,])\n\nExtract the coefficients from the model\n\ncoef(summary(age_poly_TRUE))\n\n                               Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept)               -2.085312e+02 6.559430e+01 -3.179106 0.0014961570\npoly(age, 4, raw = TRUE)1  2.391179e+01 6.439356e+00  3.713383 0.0002091772\npoly(age, 4, raw = TRUE)2 -6.646745e-01 2.257678e-01 -2.944063 0.0032705193\npoly(age, 4, raw = TRUE)3  8.403840e-03 3.362741e-03  2.499105 0.0125172334\npoly(age, 4, raw = TRUE)4 -4.098605e-05 1.802141e-05 -2.274297 0.0230362203\n\n\nWe can see from the coefficient outputs of the two models that they have different coefficients. We will now check that they make the same predictions.\n\nage_poly_pred &lt;- predict(age_poly, newdata = wage_data[test,])\nage_poly_TRUE_pred  &lt;- predict(age_poly_TRUE, newdata = wage_data[test,])\n\nExplore the predictions\n\nhead(age_poly_pred)\n\n       5       19       26       32       37       41 \n51.23518 58.14597 64.50781 64.50781 64.50781 64.50781 \n\nhead(age_poly_TRUE_pred)\n\n       5       19       26       32       37       41 \n51.23518 58.14597 64.50781 64.50781 64.50781 64.50781 \n\n\nCalculate and print the MSEP\n\nprint(msep_func(age_poly_pred, wage_data[test, \"wage\"]))\n\n[1] 1689.522\n\n\ncbind() is a function that joins columns together by binding them next to each other. There is also a function rbind() that joins by binding new rows to the bottom of old ones. Let’s try using cbind() to look at both sets of predictions at the same time:\n\npred_comparison &lt;- cbind(age_poly_pred, age_poly_TRUE_pred)\n\nThe column names default to the variable names, but ifwe want to change them, we can use the colnames() function\n\ncolnames(pred_comparison) &lt;- c(\"pred1\", \"pred2\")\n\nWe can then generate a variable that flags any instances where the predictions do not line up. To do this, I will introduce you another function in dplyr: mutate() which means create a new variable\n\npred_comparison &lt;- pred_comparison %&gt;% \n  mutate(check_same = as.numeric(pred1 == pred2))\n\nError in UseMethod(\"mutate\") : no applicable method for 'mutate' applied to an object of class \"c('matrix', 'array', 'double', 'numeric')\"\nThe previous line errored because pred_comparison is a matrix and dplyr was designed to work on data frames.\nMake pred_comparison a data frame\n\npred_comparison &lt;- data.frame(pred_comparison)\n\nNow try running the code:\n\npred_comparison &lt;- pred_comparison %&gt;%\n  mutate(check_same = as.numeric(pred1 == pred2))\n\nView the results\n\nView(pred_comparison)"
  },
  {
    "objectID": "api 222 files/section 8/section 8.2.html#regression-splines",
    "href": "api 222 files/section 8/section 8.2.html#regression-splines",
    "title": "Section 7.2 - Non-linear models",
    "section": "Regression Splines",
    "text": "Regression Splines\n\nSplines\nWe’re going to start by making an age grid, so that we test our predictions on a grid of evenly spaced ages so that you capture the functional form well.\nA quick aside, na.rm = TRUE means exlucde missing observations. Then we are going to run a spline with knots at ages 25, 40, and 60.\n\nlibrary(splines)\n\nCreate the age grids\n\nage_grid &lt;- seq(from = min(wage_data$age, na.rm = TRUE),\n                to = max(wage_data$age, na.rm = TRUE))\n\nNow we use a basis function\n\nspline_age &lt;- lm(wage ~ bs(age, knots = c(25, 40, 60)), \n                 data = wage_data[train,])\n\n?bs # to check what the basis function does\n\nGet the predictions at the grid points we defined earlier\n\nspline_age_grid_pred  &lt;- predict(spline_age, \n                                 newdata = list(age = age_grid), \n                                 se = TRUE)\n\nPlot age on the x-axis and wage on the y-axis for the test data. Then add the predictions and the confidence intervals\n\nplot(wage_data[test, \"age\"], wage_data[test, \"wage\"],\ncex = 0.5, col = \"darkgrey\",\nxlab = \"age\", ylab = \"wage\")\nlines(age_grid, spline_age_grid_pred$fit, lwd = 2)\nlines(age_grid, spline_age_grid_pred$fit + \n        2 * spline_age_grid_pred$se, lty =\"dashed\")\nlines(age_grid, spline_age_grid_pred$fit - \n        2 * spline_age_grid_pred$se, lty =\"dashed\")\n\n\n\n\n\n\nNatural Splines\nIf we instead wanted to fit a natural spline, we use ns() instead of knots, we can specify degrees of freedom. In this case we are going to use 4 degrees of freedom.\n\nns_age_poly &lt;- lm(wage ~ ns(age, df = 4), data = wage_data[train,])\nns_age_grid_poly_pred &lt;- predict(ns_age_poly,\n                                 newdata = list(age = age_grid),\n                                 se = TRUE)\n\nplot(wage_data[test, \"age\"], wage_data[test, \"wage\"], \n     cex = 0.5, col = \"darkgrey\",\n     xlab = \"age\", ylab = \"wage\")\nlines(age_grid, ns_age_grid_poly_pred$fit, lwd = 2)\nlines(age_grid, ns_age_grid_poly_pred$fit + \n        2 * ns_age_grid_poly_pred$se, lty = \"dashed\")\nlines(age_grid, ns_age_grid_poly_pred$fit - \n        2 * ns_age_grid_poly_pred$se, lty = \"dashed\")\n\n\n\n\n\n\nSmoothing Splines\nTo fit a smoothing spline, we use smooth.spline(). We can specify our own df\n\nsmooth_age &lt;- smooth.spline(wage_data[train,\"age\"], \n                            wage_data[train, \"wage\"], \n                            df = 16)\n\nOr we can use cross Validation to get optimal df and penalty\n\nsmoothCV_age &lt;- smooth.spline(wage_data[train, \"age\"], \n                              wage_data[train, \"wage\"], \n                              cv = TRUE) # specify we want to use CV\nsmoothCV_age$df\n\n[1] 6.786486\n\n\nPlot the results as before\n\nplot(wage_data[test, \"age\"], wage_data[test, \"wage\"],\n     cex =.5, col = \"darkgrey\",\n     xlab = \"age\", ylab = \"wage\")\ntitle(\" Smoothing Spline \")\nlines(smooth_age, col =\"red\", lwd = 2)\nlines(smoothCV_age, col =\"blue\", lwd =2)\n\n\n\n\n\n\nLocal Regression\nLocal Regression use loess()\nNote: span = 0.2 makes neighborhoods with 20% of observations. Span = 0.5 creates neighborhoods with 50% of observations. So the larger the span, the smoother the fit\n\nlocal2_age &lt;- loess(wage ~ age, span = 0.2, data = wage_data)\nlocal5_age &lt;- loess(wage ~ age, span = 0.5, data = wage_data)\n\nGet the predictions\n\npred_local2_age &lt;- predict(local2_age, newdata = data.frame(age = age_grid))\npred_local5_age &lt;- predict(local5_age, newdata = data.frame(age = age_grid))\n\nPlot the results\n\nplot(wage_data[test, \"age\"], wage_data[test, \"wage\"],\n     cex =.5, col = \"darkgrey\",\n     xlab = \"age\", ylab = \"wage\")\nlines(age_grid, pred_local2_age, col = \"red\", lwd = 2)\nlines(age_grid, pred_local5_age, col = \"blue\", lwd = 2)\n\n\n\n\n\n\nGAMs\nGeneralized Additive Models (GAMs)\nStart with using natural spline functions of year and age and treating education as a qualitative variable does not require any special packages\n\nExample 1: GAMs with splines\n\n\ngam_yae &lt;- lm(wage ~ ns(year, 4) + ns(age, 4) + education, \n              data = wage_data[train,])\n\n\nExample 2: GAMs with smoothing splines\n\n\nlibrary(gam)\n\nIn the gam library s() indicates that we want to use a smoothing spline\n\ngam_smooth &lt;- gam(wage ~ s(year, 4) + s(age, 5) + education,\n                  data = wage_data[train,])\n\nPlot the model results\n\npar(mfrow = c(1, 3))\nplot(gam_smooth, se = TRUE, col =\"blue \")\n\n\n\n\nTo plot the GAM we created just using lm(), we can use plot.Gam()\n\nplot.Gam(gam_yae, se = TRUE, col = \"red\") # Note the capitalization\n\n\n\n\n\n\n\n\n\n\nMake predictions\n\ngam_yae_pred &lt;- predict(gam_yae, newdata = wage_data[test,])\ngam_smooth_pred &lt;- predict(gam_smooth, newdata = wage_data[test,])\n\nPrint out the MSEP for these two GAMs using the function we created at the start of class\n\nprint(msep_func(gam_yae_pred, wage_data[test, \"wage\"]))\n\n[1] 1304.807\n\nprint(msep_func(gam_smooth_pred, wage_data[test, \"wage\"]))\n\n[1] 1300.582\n\n\n\nExample 3: GAMs with local regression\n\nTo use local regression in GAMs, use lo()\n\ngam_lo &lt;- gam(wage ~ s(year, df = 4) + lo(age, span = 0.7) + education, \n              data = wage_data[train,])\n\nPlot the results\n\nplot.Gam(gam_lo, se = TRUE, col = \"green\")\n\n\n\n\n\n\n\n\n\n\nIf you want to do a local regression in two variables:\n\ngam_2lo &lt;- gam(wage ~ lo(year, age, span =0.5) + education,\n               data = wage_data[train,])\n\nWarning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, bf.maxit, :\nliv too small.  (Discovered by lowesd)\n\n\nWarning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, bf.maxit, : lv\ntoo small.  (Discovered by lowesd)\n\n\nWarning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, bf.maxit, :\nliv too small.  (Discovered by lowesd)\n\n\nWarning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, bf.maxit, : lv\ntoo small.  (Discovered by lowesd)"
  },
  {
    "objectID": "api 222 files/section 8/section 8.2.html#decision-trees",
    "href": "api 222 files/section 8/section 8.2.html#decision-trees",
    "title": "Section 8.2 - Decision Trees",
    "section": "",
    "text": "#install.packages(\"tree\")\nlibrary(ISLR)\nlibrary(tree)\nlibrary(ggplot2)\n\nWe will use the Carseats data. Sales in this data set is a continuous variable. We start by converting it to a binary one that equals “Yes” if Sales \\(&gt; 8\\) and “No” otherwise.\n\ncarseat_data &lt;- Carseats\nhigh_sales &lt;- as.factor(ifelse(carseat_data$Sales &gt; 8, \"Yes\", \"No\"))\ncarseat_data &lt;- data.frame(carseat_data, high_sales)\ncarseat_data = carseat_data[, -1]\n\nLet’s again split the data into training and test sets\n\nset.seed(222)  \ntrain &lt;- sample(seq(nrow(carseat_data)),\n                round(nrow(carseat_data) * 0.5))\n\ntrain &lt;- sort(train)\ntest &lt;- which(!(seq(nrow(carseat_data)) %in% train))\n\nWe can now train a decision tree using the function tree()\n\n?tree\n\nHelp on topic 'tree' was found in the following packages:\n\n  Package               Library\n  cli                   /opt/homebrew/lib/R/4.3/site-library\n  xfun                  /opt/homebrew/lib/R/4.3/site-library\n  tree                  /opt/homebrew/lib/R/4.3/site-library\n\n\nUsing the first match ...\n\ncarseats_tree &lt;- tree(high_sales ~ ., data = carseat_data[train,])\n\nPlot the results\n\nplot(carseats_tree)\ntext(carseats_tree, pretty = 0)\n\n\n\n\nFrom this, we see that shelving location seems to be the most important determinant and price is the second most. Beyond that, this tree is very hard to read. If we just type the tree object name, we get:\n\nThe split criterion (e.g. Price &lt; 92.5)\nThe number of observations in that branch\nThe deviance\nThe overall prediction for the branch\nThe fraction of observations in that branch that are Yes/No\nBranches with terminal nodes are indicated by *\n\n\ncarseats_tree\n\nnode), split, n, deviance, yval, (yprob)\n      * denotes terminal node\n\n  1) root 200 271.500 No ( 0.58500 0.41500 )  \n    2) ShelveLoc: Bad,Medium 154 189.500 No ( 0.69481 0.30519 )  \n      4) Price &lt; 106.5 55  75.790 Yes ( 0.45455 0.54545 )  \n        8) Price &lt; 81 9   0.000 Yes ( 0.00000 1.00000 ) *\n        9) Price &gt; 81 46  63.420 No ( 0.54348 0.45652 )  \n         18) Age &lt; 63.5 31  40.320 Yes ( 0.35484 0.64516 )  \n           36) ShelveLoc: Bad 10  10.010 No ( 0.80000 0.20000 )  \n             72) CompPrice &lt; 112 5   0.000 No ( 1.00000 0.00000 ) *\n             73) CompPrice &gt; 112 5   6.730 No ( 0.60000 0.40000 ) *\n           37) ShelveLoc: Medium 21  17.220 Yes ( 0.14286 0.85714 )  \n             74) Income &lt; 48.5 7   9.561 Yes ( 0.42857 0.57143 ) *\n             75) Income &gt; 48.5 14   0.000 Yes ( 0.00000 1.00000 ) *\n         19) Age &gt; 63.5 15   7.348 No ( 0.93333 0.06667 ) *\n      5) Price &gt; 106.5 99  90.800 No ( 0.82828 0.17172 )  \n       10) CompPrice &lt; 142.5 84  57.200 No ( 0.89286 0.10714 )  \n         20) Price &lt; 127 46  45.480 No ( 0.80435 0.19565 )  \n           40) Age &lt; 52.5 20  26.920 No ( 0.60000 0.40000 )  \n             80) CompPrice &lt; 116 5   0.000 No ( 1.00000 0.00000 ) *\n             81) CompPrice &gt; 116 15  20.730 Yes ( 0.46667 0.53333 )  \n              162) Advertising &lt; 10.5 9  11.460 No ( 0.66667 0.33333 ) *\n              163) Advertising &gt; 10.5 6   5.407 Yes ( 0.16667 0.83333 ) *\n           41) Age &gt; 52.5 26   8.477 No ( 0.96154 0.03846 )  \n             82) CompPrice &lt; 132.5 21   0.000 No ( 1.00000 0.00000 ) *\n             83) CompPrice &gt; 132.5 5   5.004 No ( 0.80000 0.20000 ) *\n         21) Price &gt; 127 38   0.000 No ( 1.00000 0.00000 ) *\n       11) CompPrice &gt; 142.5 15  20.730 Yes ( 0.46667 0.53333 )  \n         22) Education &lt; 15.5 5   0.000 Yes ( 0.00000 1.00000 ) *\n         23) Education &gt; 15.5 10  12.220 No ( 0.70000 0.30000 )  \n           46) Income &lt; 52 5   0.000 No ( 1.00000 0.00000 ) *\n           47) Income &gt; 52 5   6.730 Yes ( 0.40000 0.60000 ) *\n    3) ShelveLoc: Good 46  48.170 Yes ( 0.21739 0.78261 )  \n      6) Price &lt; 136.5 39  29.870 Yes ( 0.12821 0.87179 )  \n       12) Advertising &lt; 6 17  20.600 Yes ( 0.29412 0.70588 )  \n         24) Price &lt; 109 8   0.000 Yes ( 0.00000 1.00000 ) *\n         25) Price &gt; 109 9  12.370 No ( 0.55556 0.44444 ) *\n       13) Advertising &gt; 6 22   0.000 Yes ( 0.00000 1.00000 ) *\n      7) Price &gt; 136.5 7   8.376 No ( 0.71429 0.28571 ) *\n\n\nGiven how deep our tree is grown, we may be worried about overfitting. We can start by evaluating the error rate on the test set for the current tree. We can write a helper function to compute the error rate\n\nerror_rate_func &lt;- function(predictions, true_vals) {\n  error_rate &lt;- mean(as.numeric(predictions != true_vals))\n  return(error_rate)\n}\n\nNow generate predictions from the model\n\ndeep_tree_preds &lt;- predict(carseats_tree, carseat_data[test,], \n                           type = \"class\")\n\nCalculate and summarize the error rate\n\nerror_rate_func(predictions = deep_tree_preds, \n                true_vals = carseat_data[test, \"high_sales\"])\n\n[1] 0.28\n\nsummary(carseats_tree)\n\n\nClassification tree:\ntree(formula = high_sales ~ ., data = carseat_data[train, ])\nVariables actually used in tree construction:\n[1] \"ShelveLoc\"   \"Price\"       \"Age\"         \"CompPrice\"   \"Income\"     \n[6] \"Advertising\" \"Education\"  \nNumber of terminal nodes:  19 \nResidual mean deviance:  0.4032 = 72.98 / 181 \nMisclassification error rate: 0.095 = 19 / 200 \n\n\nThe difference in our error rate between the training and test sets indicates that we overfit. To address this, we want to prune the tree. cv.tree() uses cross-validation to determine how much to prune the tree.\n\nset.seed(222)\ncv_carseats_tree  &lt;- cv.tree(carseats_tree, FUN = prune.misclass)\nnames(cv_carseats_tree)\n\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n\ncv_carseats_tree\n\n$size\n[1] 19 16 14  9  7  6  2  1\n\n$dev\n[1] 50 51 50 53 50 52 58 83\n\n$k\n[1] -Inf  0.0  0.5  1.0  2.5  3.0  6.0 26.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n\nSize tells us the number of terminal nodes on each of the trees considered; dev gives us the CV errors; k gives us the cost-complexity parameter. We can plot the error as a function of size and k\n\npar(mfrow = c(1, 2))\nplot(cv_carseats_tree$size, cv_carseats_tree$dev, type = \"b\")\nplot(cv_carseats_tree$k, cv_carseats_tree$dev, type = \"b\")\n\n\n\n\nFind and print the optimal size\n\nopt_indx &lt;- which.min(cv_carseats_tree$dev)\nopt_size &lt;- cv_carseats_tree$size[opt_indx]\nprint(opt_size)\n\n[1] 19\n\nopt_size &lt;- 7\n\nNow we can prune the tree using prune.misclass()\n\npruned_carseats_tree &lt;- prune.misclass(carseats_tree, best = opt_size)\nplot(pruned_carseats_tree)\ntext(pruned_carseats_tree, pretty = 0)\n\n\n\n\nNow evaluate model performance\n\npruned_tree_preds = predict(pruned_carseats_tree, carseat_data[test, ], \n                            type = \"class\")\n\nerror_rate_func(predictions = pruned_tree_preds, \n                true_vals = carseat_data[test, \"high_sales\"])\n\n[1] 0.25"
  },
  {
    "objectID": "api 222 files/section 8/section 8.2.html#regression-trees",
    "href": "api 222 files/section 8/section 8.2.html#regression-trees",
    "title": "Section 8.2 - Decision Trees",
    "section": "Regression Trees",
    "text": "Regression Trees\nFor this, we will use the Boston data\n\nlibrary(MASS)\nboston_data &lt;- Boston\n\nSplit the data into training and test sets\n\nset.seed(222)  \n\ntrain &lt;- sample(seq(nrow(boston_data)),\n                round(nrow(boston_data) * 0.8))\n\ntrain &lt;- sort(train)\n\ntest &lt;- which(!(seq(nrow(boston_data)) %in% train))\n\nboston_tree = tree(medv ~ ., Boston, subset = train)\n\nsummary(boston_tree)\n\n\nRegression tree:\ntree(formula = medv ~ ., data = Boston, subset = train)\nVariables actually used in tree construction:\n[1] \"rm\"      \"lstat\"   \"dis\"     \"crim\"    \"ptratio\"\nNumber of terminal nodes:  9 \nResidual mean deviance:  14.99 = 5935 / 396 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-23.5800  -2.1540   0.1416   0.0000   2.1420  16.0500 \n\n\nPlot the tree\n\nplot(boston_tree)\ntext(boston_tree)\n\n\n\n\nCalculate the MSE for the Predicted Values\n\nboston_preds &lt;- predict(boston_tree, newdata = boston_data[test,])\n\nCreate a helper function to calculate MSEP\n\nmsep_func &lt;- function(predictions, true_vals) {\n  MSEP &lt;- mean((predictions - true_vals)^2)\n  return(MSEP)\n}\n\nEvaluate model performance\n\nprint(msep_func(predictions = boston_preds, \n                true_vals = boston_data[test, \"medv\"]))\n\n[1] 18.4033\n\n\n\nCreate an object called cv_boston_tree that runs CV on boston_tree to find the best size according to CV error\n\n\ncv_boston_tree = cv.tree(boston_tree)\n\nPlot it\n\nplot(cv_boston_tree$size, cv_boston_tree$dev, type = 'b')\n\n\n\n\nLet’s see what the best size is\n\ncv_boston_tree\n\n$size\n[1] 9 8 7 6 5 4 3 2 1\n\n$dev\n[1]  8323.323  8585.479  8350.749 11323.362 11332.607 11363.410 13838.938\n[8] 20524.030 34561.890\n\n$k\n[1]       -Inf   395.2714   504.8779  1071.0363  1111.0632  1158.0037  2377.8966\n[8]  5952.7426 15916.5248\n\n$method\n[1] \"deviance\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n\n\nFind which size had the lowest CV error and save in a variable called best_size\n\n\nbest_indx &lt;- which.min(cv_boston_tree$dev)\nbest_size &lt;- cv_boston_tree$size[best_indx]\n\nPrune the tree using the best size as found above\n\nprune_boston = prune.tree(boston_tree, best = best_size)\n\nEvaluate model performance\n\nboston_prune_preds &lt;- predict(prune_boston, newdata = boston_data[test,])\nprint(msep_func(boston_prune_preds, boston_data[test, \"medv\"]))\n\n[1] 18.4033\n\n\nThere is a another popular package in R for decision trees called “rpart”. We don’t have time to go into it in class, but you can find more information using the link below.\nhttps://cran.r-project.org/web/packages/rpart/rpart.pdf\nYou can also find several helpful tutorials online."
  },
  {
    "objectID": "api 222 files/section 8/section 8.2.html#random-forest-bagging-and-boosting",
    "href": "api 222 files/section 8/section 8.2.html#random-forest-bagging-and-boosting",
    "title": "Section 8.2 - Decision Trees",
    "section": "Random Forest, Bagging and Boosting",
    "text": "Random Forest, Bagging and Boosting\n\nboston_data &lt;- Boston\n\nCreate a training and a test set\n\nset.seed(222)\ntrain &lt;- sample(seq(nrow(boston_data)),\n                round(nrow(boston_data) * 0.8))\ntrain &lt;- sort(train)\ntest &lt;- which(!(seq(nrow(boston_data)) %in% train))\n\n\nFit a random forest model to the Boston data using the randomForest function. Set the number of trees to 5000.\n\n\n## install.packages(\"randomForest\")\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nrf.boston &lt;- randomForest(medv ~ ., data = data.frame(boston_data[-test,]), \n                          importance = TRUE, n.trees = 5000)\n\n\nMake predictions on the test set\n\n\n## Predictions\nyhat.rf &lt;- predict (rf.boston, newdata = Boston[-train ,])\nboston.test = Boston[-train, \"medv\"]\nmean((yhat.rf - boston.test)^2)\n\n[1] 8.401946\n\n\nThe “mtry” parameter of the “randomForest” function controls the number of variables to include at each branch. By setting this value to equal 13, we are performing bagging. You may be interested in the relative importance of each variable. By setting importance = TRUE, R will store the importance matrix. You can call this by “name of random forest”$importance\n\nbag.boston &lt;- randomForest(medv ~ ., data = data.frame(boston_data[-test,]), \n                           mtry = 13, importance = TRUE)\nbag.boston$importance\n\n             %IncMSE IncNodePurity\ncrim     8.131043576    1598.82211\nzn       0.059260389      29.93039\nindus    0.784813091     151.24889\nchas     0.008483515      24.82692\nnox      4.672183652     672.51423\nrm      49.500433509   15547.73299\nage      2.495468402     475.06678\ndis      9.568228252    2114.52073\nrad      0.712166829     158.24255\ntax      2.163008968     473.14011\nptratio  3.299043553     570.50397\nblack    0.804476128     378.60153\nlstat   59.197744696   11971.04313\n\n\nNow let’s make some predictions\n\nyhat.bag &lt;- predict(bag.boston, newdata = Boston[-train,])\nmean((yhat.bag - boston.test)^2)\n\n[1] 8.725522\n\n\nWe are going to compare the outcome with boosting. Boosting has the same general form except instead of randomForest, you will use “gbm”. We list the distribution as gaussian” since this is a regression problem; if it were a binary classification problem, we would use distribution=“bernoulli”. The argument n.trees=5000 indicates that we want 5000 t trees, and the option interaction.depth=4 limits the depth of each tree. Just as before, we can see the relative importance by looking at the summary. lstat and rm are the most important variables.\n\n## install.packages(\"gbm\")  \nlibrary(gbm)\n\nLoaded gbm 2.1.9\n\n\nThis version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n\nset.seed(222)\n\n## Boosting model\nboost.boston &lt;- gbm(medv ~ ., data = data.frame(boston_data[-test,]), \n                    distribution = \"gaussian\", n.trees = 5000, \n                    interaction.depth = 4)\n\nplot &lt;- summary(boost.boston, plot = F) \n\n## create a ggplot bar plot with labels of plot object\nggplot(plot, aes(x = reorder(var, -rel.inf), y = rel.inf)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  xlab(\"Variable\") +\n  ylab(\"Relative Importance\") +\n  ggtitle(\"Relative Importance of Variables in Boosting Model\") +\n  theme_minimal()\n\n\n\n\nNow let’s make some predictions\n\nyhat.boost &lt;- predict(boost.boston, newdata = Boston[-train ,], \n                      n.trees = 5000)\n\nmean((yhat.boost - boston.test)^2)\n\n[1] 6.541105"
  },
  {
    "objectID": "api 222 files/section 2/section2.html#explore-data",
    "href": "api 222 files/section 2/section2.html#explore-data",
    "title": "Section 2 - KNN and Linear Regression",
    "section": "Explore Data",
    "text": "Explore Data\n\nlibrary(ISLR)\n\ndata(College)\ncollege_data  &lt;- College\n\nLet’s learn about our data. To get the names of the columns in the dataframe, we can use the function colnames()\n\ncolnames(college_data)\n\n [1] \"Private\"     \"Apps\"        \"Accept\"      \"Enroll\"      \"Top10perc\"  \n [6] \"Top25perc\"   \"F.Undergrad\" \"P.Undergrad\" \"Outstate\"    \"Room.Board\" \n[11] \"Books\"       \"Personal\"    \"PhD\"         \"Terminal\"    \"S.F.Ratio\"  \n[16] \"perc.alumni\" \"Expend\"      \"Grad.Rate\"  \n\n\nTo find out how many rows and columns are in the dataset, use dim() Recall that this gives us Rows followed by Columns\n\ndim(college_data)\n\n[1] 777  18\n\n\nYou can also look in the “environment” tab, press the blue arrow next to college_data and it will drop down showing the column names with their types and first few values. For college, all columns except the first are numeric. The first column is a factor column, which means it’s categorical. To get a better sense of the data, let’s look at it:\n\nView(college_data)\n\nSuppose we are interested in predicting whether a college is private or public based on available covariates, like Number accepted, enrolled, etc. Additionally, let’s suppose you don’t want certain variables included in your dataset. You can drop these functions using -c(). For example, let’s suppose you don’t want the Apps or Student to Faculty Ratio included in your dataset.\n\ncollege_data &lt;- college_data[, -c(15, 2)]\n\nBe careful when you are dropping multiple columns. You need to put the numbers in reverse order (from highest to lowest). This is because if you drop the second column first, then the 15th column becomes the the 14th column.\n\ncollege_data &lt;- College\ncollege_data &lt;- college_data[, -c(2)]\ncollege_data &lt;- college_data[, -c(15)]\n\nA less manual way of dropping columns is to use R to first use R to find the corresponding indices in the data columns. Go back to the original college data\n\ncollege_data &lt;- College\n\nFind the indices (i.e. column positions) of the columns to drop\n\nto_drop &lt;- which(names(college_data) %in% c(\"Apps\", \"S.F.Ratio\"))\nprint(to_drop)\n\n[1]  2 15\n\n\nReverse the indices as suggested above\n\nto_drop &lt;- rev(to_drop)\nprint(to_drop)\n\n[1] 15  2\n\n\nNow use the object you have defined to drop the columns\n\ncollege_data &lt;- college_data[, -c(to_drop)]\n\nAlso sometimes we have factor variables that we want to convert to numeric variables. To check variable types, you can use the “str” function\n\nstr(college_data)\n\n'data.frame':   777 obs. of  16 variables:\n $ Private    : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n $ Accept     : num  1232 1924 1097 349 146 ...\n $ Enroll     : num  721 512 336 137 55 158 103 489 227 172 ...\n $ Top10perc  : num  23 16 22 60 16 38 17 37 30 21 ...\n $ Top25perc  : num  52 29 50 89 44 62 45 68 63 44 ...\n $ F.Undergrad: num  2885 2683 1036 510 249 ...\n $ P.Undergrad: num  537 1227 99 63 869 ...\n $ Outstate   : num  7440 12280 11250 12960 7560 ...\n $ Room.Board : num  3300 6450 3750 5450 4120 ...\n $ Books      : num  450 750 400 450 800 500 500 450 300 660 ...\n $ Personal   : num  2200 1500 1165 875 1500 ...\n $ PhD        : num  70 29 53 92 76 67 90 89 79 40 ...\n $ Terminal   : num  78 30 66 97 72 73 93 100 84 41 ...\n $ perc.alumni: num  12 16 30 37 2 11 26 37 23 15 ...\n $ Expend     : num  7041 10527 8735 19016 10922 ...\n $ Grad.Rate  : num  60 56 54 59 15 55 63 73 80 52 ...\n\n\nYou can see that the Private variable is a factor. We can convert it to a numeric variable using the “as.numeric” function. I like my binary variables in R to be 0/1. In R, most factors automatically convert to a binary 1/2 format. I usually prefer a binary 0/1 format. To transform, I subtract 1.\n\ncollege_data$Private &lt;- as.numeric(college_data$Private) - 1 \nsummary(college_data$Private)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  1.0000  0.7272  1.0000  1.0000 \n\nsummary(College$Private)\n\n No Yes \n212 565 \n\n\nLet’s get back our original sample\n\ncollege_data &lt;- College"
  },
  {
    "objectID": "api 222 files/section 2/section2.html#testing-and-training-sets",
    "href": "api 222 files/section 2/section2.html#testing-and-training-sets",
    "title": "Section 2 - KNN and Linear Regression",
    "section": "Testing and Training Sets",
    "text": "Testing and Training Sets\nIn order to make this interesting, let’s split our data into a training set and a test set. To do this, we will use set.seed(), which will allow us to draw the same pseudorandom numbers the next time we run this code, and we will use the sample() function.\n\nset.seed(222)\n\nThe sample() function takes two arguments: The first is a vector of numbers from which to draw a random sample. The second is the number of random numbers to draw. The default is to sample without replacement, but you can sample with replacement by adding “, replace = TRUE” inside the function. Now, let’s generate a list of indices from the original dataset that will be designated part of the test set using sample()\n\ntest_ids &lt;- sample(1:(nrow(college_data)), round(0.2 * nrow(college_data)))\n\nTo identify the training_ids, we want all of the numbers from 1:nrow(college_data) that aren’t test IDs. Recall that which() returns the indices for which the statement inside the parentheses is true. which(!()) returns the indices for which the statement inside the parentheses is false. The “!” means “not”. Also, if you wanted to know which values of vector A were in vector B, you can use which(A %in% B). So if you want to know which values of vector A are NOT in vector B, you use which(!(A %in B)), so that’s what we will do – vector A is the vector of all integers between 1 and the number of rows in our data. vector B is the vector of test IDs\n\ntraining_ids &lt;- which(!(1:(nrow(college_data)) %in% test_ids))\n\nWe can use these indices to define our test and training sets by putting those vectors in the row position inside square brackets.\n\ntest_data &lt;- college_data[test_ids,]\ntraining_data &lt;- college_data[training_ids,]"
  },
  {
    "objectID": "api 222 files/section 2/section2.html#knn-classification",
    "href": "api 222 files/section 2/section2.html#knn-classification",
    "title": "Section 2 - KNN and Linear Regression",
    "section": "KNN Classification",
    "text": "KNN Classification\nLet’s develop a KNN model to try to predict whether it’s a private college using all available features.\nTo use KNN for classification, we need to install and load the library “class”\n\nlibrary(class)\n\nknn() is the function we will use to run the KNN model. It takes four arguments:\n\ntrain = training data features (no outcome)\ntest = test data features (no outcome)\ncl = training data outcome (class each observation belongs to)\nk = number of nearest neighbors to use\n\nFor two-class classification problems, k should be odd (avoids tied votes). Let’s run the model with 1 NN and 9 NNs. To exclude a column, use -# in the column position insider square brackets. (e.g. df[, -2] excludes the second column of dataframe df)\n\nknn_model1 &lt;- knn(train = training_data[, -1],\n                  test = test_data[, -1],\n                  cl = training_data$Private,\n                  k = 1)\n\nknn_model9 &lt;- knn(train = training_data[, -1],\n                  test = test_data[, -1],\n                  cl = training_data$Private,\n                  k = 9)\n\nWe are trying to predict Private Yes/No. knn() output predicted values for our test data, so we can compare actual v. predicted values. “prediction == actual” gives a vector with the same number of elements as there are observations in the test set. Each element will either be TRUE (the prediction was correct) or FALSE (the prediction was wrong). Applying which() to this vector will yield the index numbers for all the elements equal to TRUE. Applying length() to that vector tells us how many are TRUE (e.g. for how many observations prediction == actual). We can then divide by the number of observations in the test data to obtain the accuracy rate\n\naccuracy1  &lt;- length(which(knn_model1 == test_data$Private)) / nrow(test_data)\naccuracy9 &lt;- length(which(knn_model9 == test_data$Private)) / nrow(test_data)\n\nprint(accuracy1)\n\n[1] 0.9096774\n\nprint(accuracy9)\n\n[1] 0.9225806\n\n\nLet’s visualize what is happening in a KNN classification model. We will use the ggplot2 package to create a scatterplot of the training data, and then overlay the test data on top of it. We will color the points by whether the school is private or not.\n\nlibrary(ggplot2)\nggplot(data = training_data, \n       aes(x = Outstate, y = F.Undergrad, \n           color = as.factor(Private))) +\n  geom_point() +\n  geom_point(data = test_data, aes(x = Outstate, y = F.Undergrad), \n             color = \"black\", size = 1) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  theme_minimal() +\n  guides(color = guide_legend(title = \"Private\")) \n\n\n\n\nThis seems like excellent predictive performance. However, it’s good to think about the distribution of the data. As an extreme example, if all schools in the data were private, we would expect 100% prediction accuracy regardless of our model. Let’s see how well we do if our prediction is all schools are Private. Start by calculating the proportion of private schools\n\nprint(length(which(test_data$Private == \"Yes\")) / nrow(test_data))\n\n[1] 0.716129\n\n\nWe can also check our accuracy on Private schools v. Public schools. To do this, we need to figure out which schools are private in the test data. Specifically, get the indices for the private schools\n\nprivate_schools &lt;- which(test_data$Private == \"Yes\")\npublic_schools &lt;- which(test_data$Private == \"No\")\n\nprint(private_schools)\n\n  [1]   1   2   3   8  11  12  13  14  15  16  18  19  20  21  22  23  24  27\n [19]  28  29  30  31  32  33  34  35  37  38  39  41  42  43  44  45  47  48\n [37]  49  50  51  52  53  55  56  57  58  60  61  62  63  64  65  66  68  69\n [55]  76  77  78  80  81  82  84  85  86  90  91  92  93  94  96  97  99 100\n [73] 101 102 104 106 107 108 110 112 113 116 119 120 121 122 123 125 127 128\n [91] 129 130 133 134 135 136 137 138 139 140 142 145 146 147 148 149 151 152\n[109] 153 154 155\n\nprint(public_schools)\n\n [1]   4   5   6   7   9  10  17  25  26  36  40  46  54  59  67  70  71  72  73\n[20]  74  75  79  83  87  88  89  95  98 103 105 109 111 114 115 117 118 124 126\n[39] 131 132 141 143 144 150\n\n\nTo calculate the prediction accuracy for private schools, we need to know how many (true not predicted) private schools are in the test data. Likewise, we need to know how many public schools are in the test data.\n\nnum_private_schools &lt;- length(private_schools)\nnum_public_schools &lt;- length(public_schools)\n\nNow we will calculate the prediction accuracy separately for private and public schools.\n\nprivate_accuracy1 &lt;- length(\n  which(knn_model1[private_schools] == test_data$Private[private_schools])) /\n  num_private_schools\n\nprivate_accuracy9 &lt;- length(\n  which(knn_model9[private_schools] == test_data$Private[private_schools])) /\n  num_private_schools\n\nNow we will calculate the prediction accuracy separately for private and public schools.\n\n## Private schools (% correctly predicted):\nprivate_accuracy1 &lt;- length(\n  which(knn_model1[private_schools] == test_data$Private[private_schools])) /\n  num_private_schools\n\nprivate_accuracy9 &lt;- length(\n  which(knn_model9[private_schools] == test_data$Private[private_schools])) /\n  num_private_schools\n\n\n# Public schools (% correctly predicted): \npublic_accuracy1 &lt;- length(\n  which(knn_model1[public_schools] == test_data$Private[public_schools])) /\n  num_public_schools\n\npublic_accuracy9 &lt;- length(\n  which(knn_model9[public_schools] == test_data$Private[public_schools])) /\n  num_public_schools\n\nLet’s see how it did on different school types:\n\nprint(private_accuracy1)\n\n[1] 0.9459459\n\nprint(public_accuracy1)\n\n[1] 0.8181818\n\nprint(private_accuracy9)\n\n[1] 0.972973\n\nprint(public_accuracy9)\n\n[1] 0.7954545\n\n\nTherefore, we did better on private schools than public schools because our prediction accuracy was higher on private schools. Thinking about differential performance by label is related to fairness of machine learning algorithms. For an interesting discussion on ML fairness and different ways to define fairness, see the following academic paper:\nJon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. \nInherent Trade-Offs in the Fair\nDetermination of Risk Scores, November 2016"
  },
  {
    "objectID": "api 222 files/section 2/section2.html#knn-for-regression",
    "href": "api 222 files/section 2/section2.html#knn-for-regression",
    "title": "Section 2 - KNN and Linear Regression",
    "section": "KNN for Regression",
    "text": "KNN for Regression\nSuppose we wanted to predict how many students would enroll given the other features available in the data. In that case, the classification function we used above will not work. We will need a KNN function designed for regression problems. This function is knn.reg() in the FNN package, so we should install then read in the FNN package.\n\n#install.packages(\"FNN\")\nlibrary(FNN)\n\nknn.reg() takes four arguments: - training data with only features (no outcome) - test data with only features (no outcome) - training outcomes - k = number of neighbors\nEnrollment is the fourth column, so we will exclude that from the features. Because public / private is a factor, we either need to convert it to a numeric variable or exclude it. We will exclude it for now. Note that you can scale your features using scale(). Deciding to scale your features or not is problem dependent. We will not scale here. If you’re not sure whether or not to scale, you can always try it both ways and see how the performance changes.\n\nknn_reg1 &lt;- knn.reg(training_data[, -c(1, 4)],\n                    test_data[, -c(1, 4)],\n                    training_data$Enroll,\n                    k = 1)\n\nknn_reg5 &lt;- knn.reg(training_data[, -c(1, 4)],\n                    test_data[,-c(1, 4)],\n                    training_data$Enroll,\n                    k = 5)\n\n\nknn_reg1$pred\n\n  [1] 1492  185  302 2529 1464 1408 1930  177 1547 1652  278  363  452  572  176\n [16]  276 1973  323  137  197  200  489  456  108 1025  208  172 1194 1685  419\n [31]  156  361  432  502  659 1561  220  157  210 5873  144  753 1499  294  452\n [46]  951  375  306  437  217  380  543  695 1561  383  481  146  910  443  456\n [61]  185  235  151  228  363 1492 2367  350  452 1030  819  326 2678  265 1697\n [76]  465  337  575 1071  176  514  575  510  366  579  210  361 2408 1016  352\n [91]   91  276  806  314 1515  215  276 2133  227  306 1191   96 2408  688 1436\n[106]  177  298  691  376  500 4893  363  484  560  985  246  695 2408  691  504\n[121]  157  167   55 1036 1368 1515  177  244  456  306 2940  363  361  557  910\n[136]  125  167  137  298  248 3147  266 6180 1627  167  334  300  354 3087  217\n[151]  383  776  328  477  458\n\n\nMSE is an appropriate loss function for regression whereas accuracy is only relevant for classification\n\nmse_knn1 &lt;- mean((knn_reg1$pred - test_data$Enroll)^2)\nmse_knn5 &lt;- mean((knn_reg5$pred - test_data$Enroll)^2)\n\nprint(mse_knn1)\n\n[1] 124500.9\n\nprint(mse_knn5)\n\n[1] 73296.56"
  },
  {
    "objectID": "api 222 files/section 2/section2.html#standard-linear-regression",
    "href": "api 222 files/section 2/section2.html#standard-linear-regression",
    "title": "Section 2 - KNN and Linear Regression",
    "section": "Standard Linear Regression",
    "text": "Standard Linear Regression\nWe will now do linear regression. To run a linear regression in R, we use the function lm(), which stands for linear model. lm() takes two main arguments. The first is the formula, which should be of the form Dependent Variable ~ Feature1 + Feature2 + … The second is the training data – including both features and the outcome. Note that “~.” means regress this variable on all other variables\n\nenroll_reg &lt;- lm(Enroll ~ ., training_data)\n\n\nlibrary(stargazer)\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\nstargazer(enroll_reg, type = \"text\", single.row = TRUE)\n\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                              Enroll           \n-----------------------------------------------\nPrivateYes                7.807 (30.036)       \nApps                     -0.028*** (0.008)     \nAccept                   0.147*** (0.015)      \nTop10perc                4.016*** (1.283)      \nTop25perc                -2.269** (0.997)      \nF.Undergrad              0.144*** (0.004)      \nP.Undergrad               -0.012* (0.007)      \nOutstate                  -0.003 (0.004)       \nRoom.Board               -0.024** (0.011)      \nBooks                     -0.027 (0.049)       \nPersonal                   0.008 (0.014)       \nPhD                       -0.431 (1.002)       \nTerminal                  -0.540 (1.094)       \nS.F.Ratio                 -0.253 (2.843)       \nperc.alumni              2.319*** (0.879)      \nExpend                     0.003 (0.003)       \nGrad.Rate                  0.136 (0.648)       \nConstant                187.938** (89.615)     \n-----------------------------------------------\nObservations                    622            \nR2                             0.956           \nAdjusted R2                    0.955           \nResidual Std. Error     202.349 (df = 604)     \nF Statistic          768.822*** (df = 17; 604) \n===============================================\nNote:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nlm() returns a list, which includes among other things coefficients, residuals, and fitted values for the training data. You can look at the elements in RStudio by using the blue arrow next to enroll_reg in the environment tab. In order to call one element of a list, you can use $\n\nenroll_reg$coefficients\n\n  (Intercept)    PrivateYes          Apps        Accept     Top10perc \n187.938169332   7.806939217  -0.027759639   0.146504112   4.016271367 \n    Top25perc   F.Undergrad   P.Undergrad      Outstate    Room.Board \n -2.268522370   0.144249348  -0.011850547  -0.003105257  -0.024152785 \n        Books      Personal           PhD      Terminal     S.F.Ratio \n -0.027184975   0.008447046  -0.431202139  -0.539993156  -0.253400149 \n  perc.alumni        Expend     Grad.Rate \n  2.319201329   0.003018132   0.135713594 \n\n\nIn order to see a more traditional regression output, use summary()\n\nsummary(enroll_reg)\n\n\nCall:\nlm(formula = Enroll ~ ., data = training_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1284.27   -60.18    -8.62    51.46  1544.82 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 187.938169  89.614941   2.097 0.036393 *  \nPrivateYes    7.806939  30.036405   0.260 0.795017    \nApps         -0.027760   0.008149  -3.406 0.000702 ***\nAccept        0.146504   0.014710   9.959  &lt; 2e-16 ***\nTop10perc     4.016271   1.283269   3.130 0.001834 ** \nTop25perc    -2.268522   0.996912  -2.276 0.023222 *  \nF.Undergrad   0.144249   0.004298  33.560  &lt; 2e-16 ***\nP.Undergrad  -0.011851   0.006753  -1.755 0.079809 .  \nOutstate     -0.003105   0.004185  -0.742 0.458327    \nRoom.Board   -0.024153   0.010712  -2.255 0.024500 *  \nBooks        -0.027185   0.049391  -0.550 0.582244    \nPersonal      0.008447   0.013655   0.619 0.536410    \nPhD          -0.431202   1.002234  -0.430 0.667174    \nTerminal     -0.539993   1.094362  -0.493 0.621887    \nS.F.Ratio    -0.253400   2.843330  -0.089 0.929015    \nperc.alumni   2.319201   0.879334   2.637 0.008568 ** \nExpend        0.003018   0.002639   1.144 0.253219    \nGrad.Rate     0.135714   0.647956   0.209 0.834168    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 202.3 on 604 degrees of freedom\nMultiple R-squared:  0.9558,    Adjusted R-squared:  0.9546 \nF-statistic: 768.8 on 17 and 604 DF,  p-value: &lt; 2.2e-16\n\n\nIf you want to use the coefficients from enroll_reg to predict enrollment values in the test data, you can use the function predict(). The first argument is the lm object (the whole thing – not just the coefficients) and the second argument is the test data frame without the outcome column\n\npredicted_enroll &lt;- predict(enroll_reg, test_data[, -4])\n\nLet’s see how well we did in terms of MSE\n\nMSE_lm_enroll &lt;- mean((predicted_enroll - test_data$Enroll)^2)\nprint(MSE_lm_enroll)\n\n[1] 39312.19\n\n\nWe can see how this compared to our training MSE\n\nprint(mean((enroll_reg$residuals)^2))\n\n[1] 39760.39\n\n\nTraining MSE as % of Test MSE:\n\nprint(mean((enroll_reg$residuals)^2) / MSE_lm_enroll)\n\n[1] 1.011401\n\n\nWe know that the coefficients might change if we exclude some variables. Let’s pretend we only had Apps and Accept (columns 2 and 3) as features\n\nsmall_enroll_reg  &lt;- lm(Enroll ~ Apps + Accept, training_data)\n\nWe can compare coefficients from the small regression and the full regression. If the coefficients in the small regression are different from the coefficients in the full regression, then the small regression suffers from Omitted Variables Bias (OVB).\n\nsmall_enroll_reg$coefficients\n\n(Intercept)        Apps      Accept \n86.88115150 -0.05243254  0.42420181 \n\nenroll_reg$coefficients\n\n  (Intercept)    PrivateYes          Apps        Accept     Top10perc \n187.938169332   7.806939217  -0.027759639   0.146504112   4.016271367 \n    Top25perc   F.Undergrad   P.Undergrad      Outstate    Room.Board \n -2.268522370   0.144249348  -0.011850547  -0.003105257  -0.024152785 \n        Books      Personal           PhD      Terminal     S.F.Ratio \n -0.027184975   0.008447046  -0.431202139  -0.539993156  -0.253400149 \n  perc.alumni        Expend     Grad.Rate \n  2.319201329   0.003018132   0.135713594"
  },
  {
    "objectID": "api 222 files/section 2/section2.html#stargazer-for-regression-output",
    "href": "api 222 files/section 2/section2.html#stargazer-for-regression-output",
    "title": "Section 2 - KNN and Linear Regression",
    "section": "Stargazer for Regression Output",
    "text": "Stargazer for Regression Output\nIf you want to compare the coefficients from different regressions, you can use the stargazer package. This package is not installed by default, so you will need to install it.\n\n#install.packages(\"stargazer\")\nlibrary(stargazer)\n\nstargazer(small_enroll_reg, enroll_reg, \n          type = \"text\", column.labels = c(\"Small Model\", \"Full Model\"))\n\n\n========================================================================\n                                    Dependent variable:                 \n                    ----------------------------------------------------\n                                           Enroll                       \n                           Small Model                Full Model        \n                               (1)                        (2)           \n------------------------------------------------------------------------\nPrivateYes                                               7.807          \n                                                       (30.036)         \n                                                                        \nApps                        -0.052***                  -0.028***        \n                             (0.013)                    (0.008)         \n                                                                        \nAccept                       0.424***                  0.147***         \n                             (0.021)                    (0.015)         \n                                                                        \nTop10perc                                              4.016***         \n                                                        (1.283)         \n                                                                        \nTop25perc                                              -2.269**         \n                                                        (0.997)         \n                                                                        \nF.Undergrad                                            0.144***         \n                                                        (0.004)         \n                                                                        \nP.Undergrad                                             -0.012*         \n                                                        (0.007)         \n                                                                        \nOutstate                                                -0.003          \n                                                        (0.004)         \n                                                                        \nRoom.Board                                             -0.024**         \n                                                        (0.011)         \n                                                                        \nBooks                                                   -0.027          \n                                                        (0.049)         \n                                                                        \nPersonal                                                 0.008          \n                                                        (0.014)         \n                                                                        \nPhD                                                     -0.431          \n                                                        (1.002)         \n                                                                        \nTerminal                                                -0.540          \n                                                        (1.094)         \n                                                                        \nS.F.Ratio                                               -0.253          \n                                                        (2.843)         \n                                                                        \nperc.alumni                                            2.319***         \n                                                        (0.879)         \n                                                                        \nExpend                                                   0.003          \n                                                        (0.003)         \n                                                                        \nGrad.Rate                                                0.136          \n                                                        (0.648)         \n                                                                        \nConstant                    86.881***                  187.938**        \n                             (20.984)                  (89.615)         \n                                                                        \n------------------------------------------------------------------------\nObservations                   622                        622           \nR2                            0.820                      0.956          \nAdjusted R2                   0.819                      0.955          \nResidual Std. Error     403.989 (df = 619)        202.349 (df = 604)    \nF Statistic         1,405.761*** (df = 2; 619) 768.822*** (df = 17; 604)\n========================================================================\nNote:                                        *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nYou can also also use stargazer to get the latex code for a table. This is useful if you want to include the table in a paper or a presentation.\n\nstargazer(small_enroll_reg, enroll_reg, \n          type = \"latex\", \n          column.labels = c(\"Small Model\", \"Full Model\"))\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nEnroll\n\n\n\n\n\n\nSmall Model\n\n\nFull Model\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nPrivateYes\n\n\n\n\n7.807\n\n\n\n\n\n\n\n\n(30.036)\n\n\n\n\n\n\n\n\n\n\n\n\nApps\n\n\n-0.052***\n\n\n-0.028***\n\n\n\n\n\n\n(0.013)\n\n\n(0.008)\n\n\n\n\n\n\n\n\n\n\n\n\nAccept\n\n\n0.424***\n\n\n0.147***\n\n\n\n\n\n\n(0.021)\n\n\n(0.015)\n\n\n\n\n\n\n\n\n\n\n\n\nTop10perc\n\n\n\n\n4.016***\n\n\n\n\n\n\n\n\n(1.283)\n\n\n\n\n\n\n\n\n\n\n\n\nTop25perc\n\n\n\n\n-2.269**\n\n\n\n\n\n\n\n\n(0.997)\n\n\n\n\n\n\n\n\n\n\n\n\nF.Undergrad\n\n\n\n\n0.144***\n\n\n\n\n\n\n\n\n(0.004)\n\n\n\n\n\n\n\n\n\n\n\n\nP.Undergrad\n\n\n\n\n-0.012*\n\n\n\n\n\n\n\n\n(0.007)\n\n\n\n\n\n\n\n\n\n\n\n\nOutstate\n\n\n\n\n-0.003\n\n\n\n\n\n\n\n\n(0.004)\n\n\n\n\n\n\n\n\n\n\n\n\nRoom.Board\n\n\n\n\n-0.024**\n\n\n\n\n\n\n\n\n(0.011)\n\n\n\n\n\n\n\n\n\n\n\n\nBooks\n\n\n\n\n-0.027\n\n\n\n\n\n\n\n\n(0.049)\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal\n\n\n\n\n0.008\n\n\n\n\n\n\n\n\n(0.014)\n\n\n\n\n\n\n\n\n\n\n\n\nPhD\n\n\n\n\n-0.431\n\n\n\n\n\n\n\n\n(1.002)\n\n\n\n\n\n\n\n\n\n\n\n\nTerminal\n\n\n\n\n-0.540\n\n\n\n\n\n\n\n\n(1.094)\n\n\n\n\n\n\n\n\n\n\n\n\nS.F.Ratio\n\n\n\n\n-0.253\n\n\n\n\n\n\n\n\n(2.843)\n\n\n\n\n\n\n\n\n\n\n\n\nperc.alumni\n\n\n\n\n2.319***\n\n\n\n\n\n\n\n\n(0.879)\n\n\n\n\n\n\n\n\n\n\n\n\nExpend\n\n\n\n\n0.003\n\n\n\n\n\n\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\n\n\nGrad.Rate\n\n\n\n\n0.136\n\n\n\n\n\n\n\n\n(0.648)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n86.881***\n\n\n187.938**\n\n\n\n\n\n\n(20.984)\n\n\n(89.615)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n622\n\n\n622\n\n\n\n\nR2\n\n\n0.820\n\n\n0.956\n\n\n\n\nAdjusted R2\n\n\n0.819\n\n\n0.955\n\n\n\n\nResidual Std. Error\n\n\n403.989 (df = 619)\n\n\n202.349 (df = 604)\n\n\n\n\nF Statistic\n\n\n1,405.761*** (df = 2; 619)\n\n\n768.822*** (df = 17; 604)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01"
  },
  {
    "objectID": "api 222 files/section 9/section 9.html",
    "href": "api 222 files/section 9/section 9.html",
    "title": "Section 9 - Support Vector Machines",
    "section": "",
    "text": "Note that the material in these notes draws on the excellent past notes by TFs Laura Morris, Emily Mower and more thorough treatment of these topics in by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.\n\n\nIn this class, we will see a series of methods that fall under the support vector umbrella but which vary significantly in their flexibility. We start with maximal margin classifiers, which are the most rigid and only work when the data can be perfectly separated with a linear decision boundary. Then, we will cover support vector classifiers, which still uses a linear decision boundary, but which can accommodate data that cannot be perfectly separated by such a decision boundary. We will then cover support vector machines, which flexibly transform the original data to allow for decision boundaries that are non-linear in the original feature space (though they remain linear in the transformed feature space). Like support vector classifiers, support vector machines will also accommodate data that is not perfectly separable. Finally, we will touch upon extending the support vector ideas to multiclass settings.\n\n\n\nMaximal margin classifiers use a separating hyperplane to divide the feature space in two, with the idea being that all observations in one class lie on one side of the separating hyperplane while all observations of the other class lie on the other side.\n\n\n\nA hyperplane is a flat affine subspace that has one fewer dimensions than the feature space. The first part of the definition (“flat affine subspace”) means that the subspace can be described by a linear equation and does not need to pass through the origin. The second part of the definition (about the dimensionality) means that with \\(p\\) features, the hyperplane will have \\(p-1\\) dimensions. When we say “separating hyperplane,” we mean a hyperplane that separates the observations in one class from observations in the other by slicing the feature space in two.\nImagine a dataset with only two features, \\(X_1\\) and \\(X_2\\). The feature space is a plane, which can be divided by a line (which is a hyperplane). Suppose you had three features, \\(X_1\\), \\(X_2\\), and \\(X_3\\). The feature space is 3D and can be divided in two by a plane (which is also a hyperplane). We can continue to generalize to higher dimensional feature spaces.\nThe separating hyperplane that divides the feature space with \\(p\\) features can be described by a linear function of the following form\n\\[\n\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p = 0\n\\]\n\n\nWe can then use this equation as our classifier. Specifically, let \\(f(x)\\) represent the left hand side of the previous equation. To identify the class for observation \\(x^\\star\\), you first calculate \\(f(x^\\star)\\)\n\\[\nf(x^*) = \\beta_0 + \\beta_1 x_1^* + \\beta_2 x_2^* + ... + \\beta_p x_p^*\n\\]\nIf the resulting value is negative, the point lies to one side of the hyperplane and is assigned to class \\(y=-1\\). If it’s positive, the point lies on the other side of the hyperplane and is assigned to class \\(y=+1\\). If the point is far from the separating hyperplane, we are quite confident in our classification. If it’s close, we have much more uncertainty. In this sense, the sign of \\(f(x^\\star)\\) gives us the class and the magnitude of \\(f(x^\\star)\\) gives us our level of confidence. Given the linear form of the separating hyperplane, the decision boundary will also be linear.\nNote that we are now using \\(+1\\) and \\(-1\\) for our binary classes, whereas up to this point we have used \\(0\\) and \\(1\\).\n\n\n\nThe maximal margin classifier can be used when the data is perfectly separable, meaning that there exists a separating hyperplane such that all observations from one class fall on one side of the hyperplane and all observations from the other class fall on the other side of the hyperplane. However, when one such separating hyperplane exists, usually infinite such separating hyperplanes exist. The maximal margin classifier provides a disciplined way to choose among them.\nSpecifically, it measures the distance between each point and the separating hyperplane. The minimum distance between all points and the separating hyperplane is referred to as the margin. The maximal margin classifier selects that separating hyperplane that leads to the largest possible margin, that is, the largest possible distance between the separating hyperplane and the closest training points. Note that as you increase the distance between the decision boundary and the closest point from one class, you must decrease the distance between the decision boundary and the closest point(s) from the other class. Therefore, the margin will always be defined by at least one point from each class. These points are called ``support’’ points or vectors and the model is fully defined by them, so if they move the model changes. Other points can move however they like outside the margins, and as long as they do not cross the margins, the model will not change.\nWhen the margin is large, it suggests the classes are very well separated and performance on a test set should be good. When the margin is small, the classes are only barely separated and the exact decision boundary may not do as well in the test set.\n\n\n\n\nSometimes the data cannot be perfectly separated by a separating hyperplane. In such cases, the support vector classifier offers a generalization of the maximal margin classifier.\nThe support vector classifier is extremely similar to the maximal margin classifier, except that it allows some points to cross the margin and even the decision boundary (e.g. be misclassified). Because some points can cross the margin under the support vector classifier, we call the margin a “soft margin” compared to the “hard margin” of the maximal margin classifier, which does not permit any points to cross it.\nThe support vector classifier improves empirically relative to the maximal margin classifier in three key ways. One, it works for data that cannot be perfectly separated by a hyperplane. Two, it is more robust to individual points, meaning that because it allows points to cross the margin, movement in points near the margin will not have the same dramatic impact on the margin and decision boundary that they would have had under the maximal margin classifier. Third, related to this point, the support vector classifier yields better predictions for most training observations.\nThe support vector classifier works by creating slack variables \\(\\epsilon_i\\) and allowing a misclassification “budget” \\(C\\). The slack variables \\(\\epsilon_i\\) will be zero for all observations that fall on the correct side of the margin. If the observation is on the wrong side of the margin, \\(\\epsilon_i &gt; 0\\) and if it is on the wrong side of the hyperplane, \\(\\epsilon_i &gt; 1\\). The sum of the \\(\\epsilon_i\\)s must be no greater than the budget \\(C\\), so when the budget \\(C=0\\), no observations will be allowed on the wrong side of the margin. In this way, the maximal margin classifier is a special case of the support vector classifier. More generally, no more than \\(C\\) observations can be misclassified, because \\(\\epsilon_i&gt;1\\) when the observation is misclassified and \\(C\\) is the sum of the \\(\\epsilon_i\\)’s.\nThe budget \\(C\\) can be seen as a tuning parameter and is thus generally found through cross-validation. As \\(C\\) goes to zero, the classifier converges to the maximal margin classifier, so it is less tolerant of violations across the margin and thus the margins will shrink. As \\(C\\) gets large, it will become more tolerant of violations and the margin will increase.\nIn the support vector classifier, only observations that lie on the margin or that violate the margin will affect the hyperplane. Just as with the maximal margin classifier, points that lie on the correct side of the margin will not be used to define the hyperplane. As \\(C\\) increases, there are more violations and so more support vectors compared to when \\(C\\) is small. Given that there are more support vectors used when \\(C\\) is large, large \\(C\\) leads to lower variance though higher bias compared to small \\(C\\) (which uses a small number of support vectors). Intuitively, this is because the model is not as sensitive to the exact training points since more of them are used in defining the hyperplane when \\(C\\) is large.\n\n\n\nThe support vector machine extends the support vector classifier to the case where the decision boundary is non-linear. It is somewhat analogous to using polynomial regression when the linearity assumption of linear regression does not hold. Specifically, it works with a transformed feature space and finds a decision boundary that is linear in the transformed space, but which is non-linear in the original space.\nHowever, the way in which the support vector machine transforms the original space is new. It does the transformation using a kernel \\(K(x_i, x_{i^\\prime})\\), which is a generalization of the inner product \\(&lt;x_i,x_{i^\\prime}&gt;\\) (e.g. dot product). The hyperplane is then defined by\n\\[\n    f(x) = \\beta_0 + \\sum_{i\\in \\mathcal{S}}\\alpha_i K(x,x_i)\n\\]\nThe values of \\(\\alpha_i\\) are only non-zero for support vectors (points that lie on or across the margin). Some popular kernels include the polynomial kernel and the radial kernel. Note that the support vector classifier is a special case of the support vector machine where the kernel is a polynomial kernel of degree \\(d=1\\). Just like the support vector classifier, it maintains the principle of a budget \\(C\\), though now we use \\(\\alpha_i\\) instead of \\(\\epsilon_i\\).\nThe argument for why we use a kernel rather than an enlarged feature space has to do with computational efficiency. Using the kernel only requires computing the kernel (the generalized inner product) for each unique pair $(x_i, x_{i^}) $ in the training data; it does not require explicitly working in a transformed feature space, which may be computationally intractable. For example, suppose we wanted to expand our feature space from \\(x_1\\) and \\(x_2\\) to also include \\(x_3 = x_1^2 + x_2^2\\). Instead of explicitly calculating \\(x_3\\), we only need to adjust the inner product between two points \\(a\\) and \\(b\\) from\n\\[\nK(a,b) = x_{1,a}x_{1,b}+x_{2,a}x_{2,b}\n\\]\nto\n\\[\nK(a,b) = x_{1,a}x_{1,b}+x_{2,a}x_{2,b} +\n    (x_{1,a}^2 + x_{1,b}^2)(x_{2,a}^2 + x_{2,b}^2)\n\\]\nThe kernel function can take many forms, including polynomial and radial functions. Different kernel functions allow for different levels of model flexibility. However, note that as with many other models we’ve seen this semester, the more flexible we make the kernel, the more likely we are to fit the training data well (low bias) but risk overfitting the training data (high variance). Therefore, in the support vector machine, a very flexible kernel function leads to low bias but high variance.\n\n\n\nThe idea of support vector machines does not generalize easily to the multiclass setting, but two options have been proposed.\nOne is called the one-versus-one approach, where a collection of models is built that each evaluate the question of whether the observation belongs to class \\(a\\) or class \\(b\\). This is repeated with all possible pairs of classes in the data. For a given test point, you tally how many times the observation is assigned to each of the \\(K\\) classes and assign whichever class was assigned most often.\nThe other is called the one-versus-all approach, where \\(K\\) models are built and each model compares the class at hand to a collection of the other \\(K-1\\) classes, coded collectively as -1. For a given test point, you determine which \\(f_k(x)\\) is largest (most confidence) and assign that class."
  },
  {
    "objectID": "api 222 files/section 9/section 9.html#tree-based-methods",
    "href": "api 222 files/section 9/section 9.html#tree-based-methods",
    "title": "Section 8.1 - Tree-Based Methods",
    "section": "Tree-Based Methods",
    "text": "Tree-Based Methods\nTree-based methods are non-parametric supervised learning methods that stratify or segment the predictor space into a number of simple regions. They can be used for both regression and clas- sification. After building a tree using the training data, a prediction can be made by using the training observations in the region to which the new observation belongs. For a regression tree, the predicted response for an observation is given by the mean response of the training observations that belong to the same terminal node. In contrast, for a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs. Decision trees are simple and useful for interpretation. However, they typically have lower predictive power than other supervised learning methods. Bagging, random forests, and boosting are approaches to improve decision trees by involving multiple trees, which are then combined to yield a single consensus prediction. These approaches can dramatically improve the prediction accuracy of decision trees, at the expense of interpretability.\n\nDecision Trees\nTo interpret a decision tree, suppose we have below tree from the Hitters data. The Figure represents a regression tree for predicting the log salary of a baseball player, based on the two predictors—the number of years that he has played in the major leagues and the number of hits that he made in the previous year. We can read the tree sequentially from top to bottom. At a given node (where the branches split), the label (of the form \\(X_j &lt; t_k\\)) indicates the left-hand branch emanating from that split, and the right-hand branch corresponds to \\(X_j \\geq t_k\\). For instance, the split at the top of the tree results in two large branches. The left-hand branch corresponds to \\(Years &lt; 4.5\\) and the right-hand branch corresponds to \\(Years \\geq 4.5\\). The number in each leaf (terminal nodes) is the mean of the response (outcomes) for the observations (in the training data) that fall there.\n\n\n\nDecision Tree Example\n\n\nDecision trees are easier to interpret and have a nice graphical representation. Unfortunately, fitting a decision tree is not simple. It is computationally infeasible to consider every possible partition of the feature space into \\(J\\) boxes. For this reason, we take a top-down, greedy approach that is known as recursive binary splitting. The approach is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. It is greedy because, at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step. In order to perform recursive binary splitting, we first select the predictor \\(X_j\\) and the cutpoint \\(s\\) such that splitting the predictor space into the regions \\(\\{X|X_j &lt; s\\}\\) and \\(\\{X|X_j \\geq s\\}\\) leads to the greatest possible reduction in RSS. For building a classification tree, alternatives to RSS are the classification error rate, Gini index, and entropy. Instead of the error rate, either the Gini index or the entropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate.\nAnother problem with the decision tree is that the process described above may produce good predictions on the training set but is likely to overfit the data, leading to poor test set performance. A better strategy is to grow a very large tree and then prune it back in order to obtain a subtree. We can use the cross validation to prune the tree.\nThe advantages of trees are the following: (1) trees are very easy to explain; (2) trees can be displayed graphically and are easily interpreted even by a non-expert; (3) trees can easily handle qualitative predictors without the need to create dummy variables. Disadvantages of trees are the following: (1) trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches; (2) trees can be very non-robust, i.e., a small change in the data can cause a large change in the final estimated tree.\n\n\nBagging\nBagging, random forests, and boosting are approaches to improve decision trees by involving multiple trees, which are then combined to yield a single consensus prediction. To apply bagging to regression trees, we simply construct \\(B\\) regression trees using \\(B\\) bootstrapped training sets and average the resulting predictions. These trees are grown deep and are not pruned. Because the tree is not pruned, it means each tree is more flexible, hence high variance but low bias. Averaging these \\(B\\) trees reduces the variance because in statistics, averaging a set of observations reduces variance. For bagging classification trees, we can record the class predicted by each of the \\(B\\) trees, and take a majority vote (i.e., the most commonly occurring class among the \\(B\\) predictions).\nWe can also obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). In the case of bagging regression trees, we can record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all \\(B\\) trees. A large value indicates an important predictor.\n\n\nRandom forests\nThe main difference between bagging and random forests is the choice of predictor subset size m. When building decision trees in random forests, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors. If a random forest is built using \\(m = p\\), then this amounts simply to bagging. Random forests can provide an improvement over bagged trees by decorrelating the trees, i.e., forcing the tree to consider different splits and thus avoid the situation when all trees to have similar structures due to a small subset of strong predictors. In general, using a small value of m in building a random forest will typically be helpful when we have a large number of correlated predictors.\n\n\nBoosting\nIn boosting, each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling, but instead, each tree is fit on a modified version of the original data set. Unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown. Boosting involves the parameters that we have to determine. The shrinkage parameter \\(\\lambda\\), a small positive number, controls the rate at which boosting learns. Very small \\(\\lambda\\) can require using a very large value of B and thus achieve good performance. We also need to determine the number \\(d\\) of splits in each tree, which controls the complexity of the boosted ensemble. Similar to \\(\\lambda\\), a small \\(d\\) can typically achieve a slower learn, which means better performance."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html",
    "href": "api 222 files/section 4/section 4.html",
    "title": "Section 4 - Classification",
    "section": "",
    "text": "Note that the material in these notes draws on past TF’s notes (Ibou Dieye, Laura Morris, Emily Mower, Amy Wickett), and the more thorough treatment of these topics in Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.\n\n\n\n\n\nLogistic regression is a parametric model that models the probability that \\(Y\\) belongs to a particular category. It is somewhat similar to linear regression, but the linear regression form of \\(\\beta_0+\\beta_1 X_1 + \\beta_2 X_2 + \\dots\\) undergoes a transformation that ensures the output will be bounded between 0 and 1 and can thus be interpreted as a probability.\n\\[\\begin{equation}\n     p(X) = \\frac{e^{\\beta_0+\\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\end{equation}\\]\nTherefore, while linear regression is best suited for regression problems, logistic regression is best suited for classification problems. Note that logistic regression produces a probability of class membership that then needs to be transformed to 0 or 1 using a decision rule, such as if \\(p(X)\\geq 0.5\\) then predict 1 otherwise predict 0.\n\n\n\nLogistic regression is estimated using Maximum Likelihood Estimation (MLE). MLE finds the values of \\(\\beta_0\\), \\(\\beta_1\\), etc. that maximize the probability of observing the observations in your training data given the values of the parameters \\(\\beta_0\\), \\(\\beta_1\\), etc. and the assumed functional form (e.g. see above).\n\n\n\nWhen implementing logistic regression, the restrictions on the features are the same as for linear regression (e.g. no collinearity, number of features must be less than number of observations, etc.). The outcome should be a binary class membership. You can extend the logistic regression to cover a scenario with more than two classes, and this is called multinomial logistic regression, but we will not cover that in class.\nWhen you run logistic regression, the prediction output is a continuous value that reflects the predicted probability that the observation belongs to class 1. Therefore, a decision rule is required to convert the predicted probability to a predicted class (0 or 1). If you care equally about wrongly predicting positive for a True Negative (e.g. predicting class 1 for someone who is actually in class 0) and predicting negative for a True Positive, then a good decision rule is if \\(p(X)\\geq 0.5\\), predict 1 and otherwise predict 0. However, sometimes you care more about an error in one direction than the other. An example of this would be not wanting to offer a loan to someone who will default even if that means you deny more people who wouldn’t default. In that case, you might lower the threshold to 0.2 or some other value. We explore this more in the code."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#concept",
    "href": "api 222 files/section 4/section 4.html#concept",
    "title": "Section 4 - Classification",
    "section": "",
    "text": "Logistic regression is a parametric model that models the probability that \\(Y\\) belongs to a particular category. It is somewhat similar to linear regression, but the linear regression form of \\(\\beta_0+\\beta_1 X_1 + \\beta_2 X_2 + \\dots\\) undergoes a transformation that ensures the output will be bounded between 0 and 1 and can thus be interpreted as a probability.\n\\[\\begin{equation}\n     p(X) = \\frac{e^{\\beta_0+\\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\end{equation}\\]\nTherefore, while linear regression is best suited for regression problems, logistic regression is best suited for classification problems. Note that logistic regression produces a probability of class membership that then needs to be transformed to 0 or 1 using a decision rule, such as if \\(p(X)\\geq 0.5\\) then predict 1 otherwise predict 0."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#method",
    "href": "api 222 files/section 4/section 4.html#method",
    "title": "Section 4 - Classification",
    "section": "",
    "text": "Logistic regression is estimated using Maximum Likelihood Estimation (MLE). MLE finds the values of \\(\\beta_0\\), \\(\\beta_1\\), etc. that maximize the probability of observing the observations in your training data given the values of the parameters \\(\\beta_0\\), \\(\\beta_1\\), etc. and the assumed functional form (e.g. see above)."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#implementation-and-considerations",
    "href": "api 222 files/section 4/section 4.html#implementation-and-considerations",
    "title": "Section 4 - Classification",
    "section": "",
    "text": "When implementing logistic regression, the restrictions on the features are the same as for linear regression (e.g. no collinearity, number of features must be less than number of observations, etc.). The outcome should be a binary class membership. You can extend the logistic regression to cover a scenario with more than two classes, and this is called multinomial logistic regression, but we will not cover that in class.\nWhen you run logistic regression, the prediction output is a continuous value that reflects the predicted probability that the observation belongs to class 1. Therefore, a decision rule is required to convert the predicted probability to a predicted class (0 or 1). If you care equally about wrongly predicting positive for a True Negative (e.g. predicting class 1 for someone who is actually in class 0) and predicting negative for a True Positive, then a good decision rule is if \\(p(X)\\geq 0.5\\), predict 1 and otherwise predict 0. However, sometimes you care more about an error in one direction than the other. An example of this would be not wanting to offer a loan to someone who will default even if that means you deny more people who wouldn’t default. In that case, you might lower the threshold to 0.2 or some other value. We explore this more in the code."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#methods",
    "href": "api 222 files/section 4/section 4.html#methods",
    "title": "Section 4 - Classification",
    "section": "Methods",
    "text": "Methods\nAt this point in the course, you have been introduced to three methods. The methods and their properties are summarized in the table below.\nWhen thinking about if a model is parametric or non-parametric, it can be helpful to think: Do I have a set of parameters that I can use to find the predicted value of any new observation? If yes, it’s parametric. When thinking about if a problem is a classification problem or a regression problem, it is helpful to think about the outcome in the training data. If the outcome is continuous, then it’s a regression problem. If the outcome is categorical, it’s a classification problem. The emphasis on the outcome in the training data is to avoid the confusion that arises when you look at prediction output. As we saw with logistic regression, even though it’s a classification problem, the output will be a probability (which is continuous and needs to be converted to 0 or 1 in order to measure performance)."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#classification",
    "href": "api 222 files/section 4/section 4.html#classification",
    "title": "Section 4 - Classification",
    "section": "Classification",
    "text": "Classification\nClassification is really a two-step process. Usually, the model will predict something that looks like a probability that your observation belongs to each class. You then need to convert the probability to a class membership using a decision rule. A good general rule is: ``whichever class is assigned the highest probability is the predicted class.’’ However, when you have reason to prefer an error in one direction (e.g. predicting more people will default than actually will), you should change this threshold. Exactly which threshold is optimal will depend on domain knowledge and other factors (such as how costly defaults are or how profitable repaid loans are)."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#concept-1",
    "href": "api 222 files/section 4/section 4.html#concept-1",
    "title": "Section 4 - Classification",
    "section": "Concept",
    "text": "Concept\nRecall that the Bayes’ Classifier is the unattainable gold standard classifier. It assumes knowledge of the true underlying distribution of the data, which you will not have in practice. Given features \\(X\\), it knows the true probability that \\(Y\\) belongs to each possible class. It predicts the most likely class, which is the best decision rule given the available features.\nLinear Discriminant Analysis (LDA) approximates the Bayes’ Classifier, given the information available and the assumption that features are Normally (Gaussian) distributed within each class. The result is decision boundaries that are linear in the included features.\nWhen there is one feature (predictor), LDA estimates class-specific means \\(\\hat{\\mu}_k\\) and a single variance \\(\\hat{\\sigma}^2\\) that is common to all classes. When there are multiple features, LDA estimates class-specific mean vectors \\(\\hat{\\mu}_k\\) and a single variance-covariance matrix \\(\\hat{\\Sigma}\\) that is assumed to be relevant to all classes. In both cases (one feature or many features), LDA also calculates the unconditional probability of belonging to each class \\(\\hat{\\pi}_k\\). LDA then takes these components (means, variance, and unconditional class probability) and calculates a discriminant function for each observation and each class. For each observation, the predicted class is determined by the largest discriminant.\nQuadratic Discriminant Analysis (QDA) is conceptually similar, though instead of requiring all classes to share the same variance or variance-covariance matrix, it allows for class-specific variances. This has the effect of allowing non-linear decision boundaries. The drawback, though, is that allowing for class-specific variances (and especially class-specific variance-covariance matrices) increases the number of parameters to estimate, increasing the likelihood of overfitting."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#method-1",
    "href": "api 222 files/section 4/section 4.html#method-1",
    "title": "Section 4 - Classification",
    "section": "Method",
    "text": "Method\nTo estimate LDA or QDA, you estimate the feature means, feature variance(s), and unconditional (empirical) class probabilities for each class. Let \\(k\\) index the classes, then if there is only one feature, LDA calculates the following discriminant function for each observation for each class: \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x \\cdot \\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2}-\\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2}+\\log(\\hat{\\pi}_k)\n\\end{equation}\\] Where \\(\\hat{\\pi}_k\\) is the unconditional class probability, \\(\\hat{\\mu}_k\\) is the mean feature value for class \\(k\\), and \\(\\hat{\\sigma}^2\\) is the common feature variance. When \\(p&gt;1\\) (e.g. there are multiple predictors), then we use \\(\\hat{\\Sigma}\\) to represent the common feature variance-covariance matrix, \\(\\hat{\\mu}_k\\) becomes a vector, and thus the LDA discriminant function becomes: \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_k-\\frac{1}{2}\\hat{\\mu}_k^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_k+\\log(\\hat{\\pi}_k)\n\\end{equation}\\] For QDA, the one feature discriminant function is: \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x \\cdot \\frac{\\hat{\\mu}_k}{\\hat{\\sigma}_k^2}-\\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}_k^2}+\\log(\\hat{\\pi}_k)\n\\end{equation}\\] Note that \\(\\hat{\\sigma}\\) now has a subscript to indicate that the variance is class-specific. For multiple predictors, the QDA discriminant function is again just like the LDA one but with a subscripted \\(\\Sigma\\): \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x^T\\hat{\\Sigma}_k^{-1}\\hat{\\mu}_k-\\frac{1}{2}\\hat{\\mu}_k^T\\hat{\\Sigma}_k^{-1}\\hat{\\mu}_k+\\log(\\hat{\\pi}_k)\n\\end{equation}\\] In practice, we will use the lda() and qda() functions that are part of the MASS package in R."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#implementation-and-considerations-1",
    "href": "api 222 files/section 4/section 4.html#implementation-and-considerations-1",
    "title": "Section 4 - Classification",
    "section": "Implementation and Considerations",
    "text": "Implementation and Considerations\nLDA and QDA both generalize easily to settings where there are more than two classes. They are also parametric, which means they are computationally efficient with large data sets compared to non-parametric KNN. However, they both make the assumption that the features are normally distributed, so you should pay attention to your data. For example, binary variables will never be normally distributed nor well approximated by a normal distribution, and so the methods are not appropriate to use in the presence of binary features.\nQDA differs from LDA by assuming the variance or variance-covariance matrix of the feature(s) varies from class to class. This allows for more flexible and non-linear decision boundaries, but requires estimation of more parameters. As with all other models we’ve seen, estimating more parameters increases the likelihood of overfitting and so should only be used when the number of observations is large relative to the number of features and classes."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#reminders",
    "href": "api 222 files/section 4/section 4.html#reminders",
    "title": "Section 4 - Classification",
    "section": "Reminders",
    "text": "Reminders\nTo run one line of code in RStudio, you can highlight the code you want to run and hit “Run” at the top of the script. Alternatively, on a mac, you can highlight the code to run and hit Command + Enter. Or on a PC, you can highlight the code to run and hit Ctrl + Enter. If you ever forget how a function works, you can type ? followed immediately (e.g. with no space) by the function name to get the help file\nLet’s start by loading the necessary packages and data. We will use the “Default” dataset available from the ISLR package. This dataset contains information on credit card defaults, including a binary response variable “default” and three predictors: “student”, “balance”, and “income”. We will use this dataset to build and evaluate classification models.\n\n## Load the packages\nlibrary(ISLR)\nlibrary(FNN)\n\nData preparation\nNow extract the data and name it “default_data”\n\ndefault_data &lt;- Default\n\nLet’s get to know our data\n\nsummary(default_data)\n\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n\n\nIt looks like we have two categorical variables. We can convert them both to numeric\n\ndefault_data$default &lt;- as.numeric(default_data$default == \"Yes\")\ndefault_data$student &lt;- as.numeric(default_data$student == \"Yes\")\n\nLet’s again split our data into test and training data sets with a 20/80 split. We use set.seed() to ensure replicability.\n\nset.seed(222)\n\nThen we can use the sample function to split the data (as before)\n\n## First pick the test observations (20% of the data)\ntest_obs &lt;- sample(seq(nrow(default_data)), \n                   round(0.2 * nrow(default_data)))\n\n## The training observations are the remaining observations\ntrain_obs &lt;- setdiff(seq(nrow(default_data)), test_obs)\n\n## Use the indices now to extract the corresponding subsets of the data\ntest_data &lt;- default_data[test_obs,]\ntrain_data &lt;- default_data[train_obs,]"
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#logistic-regression-1",
    "href": "api 222 files/section 4/section 4.html#logistic-regression-1",
    "title": "Section 4 - Classification",
    "section": "Logistic regression",
    "text": "Logistic regression\nNow, let’s say we are interested in using a logistic regression. For a base case, let’s try to predict default from the other available variables using logistic regression. To run logistic regression in R, use glm(), which requires three arguments: - 1st: Your formula (y ~ x1 + x2) - 2nd: Family = binomial tells it to use logistic regression - 3rd: You data, including both x and y columns.\nWe will train the model on the training data, make predictions for the test data using predict(), and measure performance with Accuracy.\n\nlogistic_default &lt;- glm(default ~ student + balance + income,\n                        family = binomial,\n                        data = train_data)\n\nTo view information about the logistic regression, including coefficients, use summary()\n\nsummary(logistic_default)\n\n\nCall:\nglm(formula = default ~ student + balance + income, family = binomial, \n    data = train_data)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.108e+01  5.654e-01 -19.588   &lt;2e-16 ***\nstudent     -5.036e-01  2.638e-01  -1.909   0.0563 .  \nbalance      5.739e-03  2.619e-04  21.916   &lt;2e-16 ***\nincome       6.727e-06  9.312e-06   0.722   0.4701    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2320.3  on 7999  degrees of freedom\nResidual deviance: 1252.6  on 7996  degrees of freedom\nAIC: 1260.6\n\nNumber of Fisher Scoring iterations: 8\n\n\nTo predict outcomes on the test_data using logistic regression, use:\n\nlogistic_predict &lt;-predict(logistic_default,\n                           test_data,\n                           type = \"response\")\n\nhist(logistic_predict)\n\n\n\n# plot auc curves\nlibrary(ROCR)\n\npred &lt;- prediction(logistic_predict, test_data$default)\nperf &lt;- performance(pred, \"tpr\", \"fpr\")\nplot(perf)\n\n\n\n\nLet’s look at what we got from this prediction model. We will use the head() function, which prints the first few values of the object inside the parentheses. If you want to change the number of observations that are printed to 100, you can use head(object, n = 100)\n\nhead(logistic_predict)\n\n        4350           18         6678         4788         9481         9994 \n5.133197e-04 2.176129e-04 2.515555e-03 3.317590e-02 2.508148e-05 4.935658e-03 \n\n\nWe see that the prediction outputs are probabilities, so in order to make predictions we have to decide on a decision rule. A common one is if the predicted probability is &gt; 0.5, predict 1 otherwise 0. Let’s see how we would do using this rule. Because it’s a classification problem, accuracy is a good measure (% correct)\n\nclass_predictions &lt;- as.numeric(logistic_predict &gt; 0.5)\nlogistic_accuracy &lt;- mean(class_predictions == test_data$default)\nprint(logistic_accuracy)\n\n[1] 0.9745\n\n\nThe accuracy looks great! But, we might care more about different types of errors than overall error rate. For example, we may not want to give loans to people who will default even if this means denying loans to some people who wouldn’t default. We can measure error rate by true default status. Note that for defaulters, default = 1. Here, I pull out all the predictions for the true defaulters and see what fraction of those equal 1.\n\ntrue_pos_accuracy &lt;- mean(class_predictions[which(test_data$default == 1)] == 1)\nprint(true_pos_accuracy)\n\n[1] 0.3333333\n\n\nLike-wise for the non-defaulters, I see what fraction of those equal 0. This gives class-specific accuracy rates.\n\ntrue_neg_accuracy &lt;- mean(class_predictions[which(test_data$default == 0)] == 0)\nprint(true_neg_accuracy)\n\n[1] 0.9974107\n\n\nThese values summarise what can also be seen in the following table. Where the columns correspond to the true values and the rows correspond to the predicted values.\n\ntable(class_predictions, test_data$default)\n\n                 \nclass_predictions    0    1\n                0 1926   46\n                1    5   23\n\n\nSuppose instead of the accuracy, you wanted to directly calculate the error rate. How would you do it? Hint, errors are ones where the prediction does not equal the true value. In R, we use != for “does not equal”\nWe can also calculate the error rate for the true defaulters and non-defaulters.\n\ntrue_pos_error &lt;- mean(class_predictions\n                       [which(test_data$default == 1)] != 1)\nprint(true_pos_error)\n\n[1] 0.6666667\n\ntrue_neg_error &lt;- mean(class_predictions\n                       [which(test_data$default == 0)] != 0)\nprint(true_neg_error)\n\n[1] 0.002589332\n\n\nWe see that we did a lot better on the true negatives than the true positives. Among all the people who will default, we only predicted about 1/3% of them would default. If we want to do a better job identifying these people, we can do this by lowering the default threshold from a predicted probability of 0.5 to something lower, say 0.2. Note, though, that lowering this threshold means increasing the number of default predictions for people who don’t default as well. Since we aren’t really sure how low we want to make this threshold, we can try for a bunch of threshold values and then see how the performance changes to pick the one that is best for our setting. This involves domain knowledge, such as the cost of default and the earnings on loans extended to people who repay, so there’s not one right answer, but we can more clearly see the tradeoffs by trying many values and plotting the error rates in each group as a function of the threshold.\nTo do this, we can use a loop to try a bunch of threshold values and then calculate the error rates for each threshold. We can then plot the error rates as a function of the threshold to see how the error rates change as we change the threshold.\n\n## First, we need to specify the list of threshold values to assess\nthreshold_values &lt;- seq(from = 0.00, to = 0.50, by = 0.01)\n\nThen we initialize a matrix of error rates. This matrix will have a number of rows corresponding to the length of the list of threshold values and 2 columns corresponding to the true positive and true negative accuracy for each value that we test\n\nerror_rates &lt;- matrix(0, nrow = length(threshold_values), ncol = 2)\n\nNow we can start the loop. We initialize a tracker for the row index, then for each threshold value in our specified list of values, we update the tracker to reflect the row, generate the predicted classes using the specific threshold, calculate the true positive accuracy, calculate the true negative accuracy, and add the results to our matrix.\n\nindx &lt;- 0\n\nfor(threshold in threshold_values) {\n  \n  ## Update the tracker to reflect the row\n  indx &lt;- indx + 1\n  \n  ## Then generate the predicted classes using the specific threshold\n  class_predictions &lt;- as.numeric(logistic_predict &gt; threshold)\n  \n  ## Then calculate the true positive accuracy\n  true_pos_accuracy &lt;- mean(class_predictions[which(test_data$default == 1)] == 1)\n  \n  ## Then calculate the true negative accuracy\n  true_neg_accuracy &lt;- mean(class_predictions[which(test_data$default == 0)] == 0)\n  \n  ## Now we can add the results to our matrix \n  error_rates[indx,] &lt;- c(true_pos_accuracy, true_neg_accuracy)\n}\nerror_rates\n\n           [,1]      [,2]\n [1,] 1.0000000 0.0000000\n [2,] 0.9420290 0.7560849\n [3,] 0.8695652 0.8311756\n [4,] 0.8405797 0.8653547\n [5,] 0.8260870 0.8886587\n [6,] 0.7971014 0.9124806\n [7,] 0.7826087 0.9254272\n [8,] 0.7681159 0.9347488\n [9,] 0.7681159 0.9404454\n[10,] 0.7391304 0.9466598\n[11,] 0.7391304 0.9523563\n[12,] 0.7246377 0.9575350\n[13,] 0.7101449 0.9611600\n[14,] 0.6956522 0.9627136\n[15,] 0.6811594 0.9684102\n[16,] 0.6811594 0.9699637\n[17,] 0.6811594 0.9715173\n[18,] 0.6666667 0.9730709\n[19,] 0.6666667 0.9761781\n[20,] 0.6521739 0.9782496\n[21,] 0.6086957 0.9808389\n[22,] 0.6086957 0.9813568\n[23,] 0.5942029 0.9823925\n[24,] 0.5797101 0.9829104\n[25,] 0.5797101 0.9849819\n[26,] 0.5652174 0.9860176\n[27,] 0.5507246 0.9865355\n[28,] 0.5362319 0.9865355\n[29,] 0.5362319 0.9886069\n[30,] 0.5362319 0.9891248\n[31,] 0.5362319 0.9906784\n[32,] 0.5362319 0.9917141\n[33,] 0.5362319 0.9922320\n[34,] 0.5362319 0.9927499\n[35,] 0.5217391 0.9927499\n[36,] 0.5217391 0.9932677\n[37,] 0.5072464 0.9932677\n[38,] 0.4927536 0.9932677\n[39,] 0.4637681 0.9937856\n[40,] 0.4637681 0.9943035\n[41,] 0.4492754 0.9953392\n[42,] 0.4347826 0.9958571\n[43,] 0.4347826 0.9963749\n[44,] 0.4347826 0.9963749\n[45,] 0.4057971 0.9968928\n[46,] 0.4057971 0.9968928\n[47,] 0.3768116 0.9968928\n[48,] 0.3768116 0.9968928\n[49,] 0.3768116 0.9968928\n[50,] 0.3623188 0.9974107\n[51,] 0.3333333 0.9974107\n\n\nLet’s plot each of these as a function of the threshold\n\nmatplot(x = threshold_values,\n        y = error_rates, \n        type = \"l\",\n        col = 3:4,\n        xlab = \"Threshold\",\n        ylab = \"Accuracy\",\n        main = \"Accuracy as a Function of Threshold\")\n        legend(\"topright\", legend = c(\"Defaulters\", \"Non-defaulters\"), \n               col = 3:4, pch = 1)"
  },
  {
    "objectID": "api 222 files/section 5/section 5.html",
    "href": "api 222 files/section 5/section 5.html",
    "title": "Section 5 - Cross-Validation, Ridge, Lasso, and Bootstrapping",
    "section": "",
    "text": "Note that the material in these notes draws on past TF’s notes (Ibou Dieye, Laura Morris, Emily Mower, Amy Wickett), and the more thorough treatment of these topics in Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.\nIn the absence of a designated test data set, a number of methods can be used to estimate out-of-sample performance using the available training data. In this review session, we consider two popular types of resampling methods: Cross-validation and Bootstrap.\n\n\nCross-validation is a method that works by holding out a subset of the training observations from the model fitting process, and then applying the model to those held out observations.\n\n\nThis is the type of cross-validation method we have been using in review sessions so far. It involves randomly dividing the available set of observations into two parts, a training set and a validation set. The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The validation error rate provides an estimate of the test error rate.\nThe validation set approach is simple and is easy to implement, but it has a few drawbacks. First, it can yield estimates of the test error rate that are highly variable. In addition, since it trains the model using only a subset of the data, it may tend to overestimate the test error rate for the model fit on the entire data set.\n\n\n\nLeave-one-out cross-validation (LOOCV) attempts to address the drawbacks of the validation set method. Instead of creating two subsets, a single observation is used for the validation set, and the remaining (\\(n - 1\\)) observations constitute the training set. The statistical learning method is fit on the \\(n - 1\\) training observations, and a prediction is made for the excluded observation to estimate a test error rate. The procedure is repeated \\(n\\) times, by selecting each individual observation for the validation data, training the procedure on the \\(n - 1\\) observations, and computing the test error. The LOOCV estimate for the test error is the average of these \\(n\\) test error estimates.\nCompared to the validation set approach, LOOCV is more expensive to implement. However, it has less bias since it uses almost all the training observations and yields estimates of the test error rate that are less variable through its averaging feature.\n\n\n\n\\(k\\)-fold Cross-validation is an alternative to LOOCV. For \\(j=1,...,k\\), the model is fit on all folds except fold \\(j\\). Then, the model’s predictive performance on the data is assessed in fold \\(j\\). Thus, for a given iteration, the “training” data is data from all the folds except for fold \\(j\\), and the ``test’’ data is data from fold \\(j\\). This process is repeated for \\(j=1,...,k\\) and a test error is estimated for each fold. The \\(k\\)-fold cross-validation estimate for the test error is the average of these \\(k\\) test error estimates.\nLOOCV can be thought of as a special type \\(k\\)-fold cross-validation, where \\(k\\) equals the number of observations in the data. In practice, one typically performs \\(k\\)-fold CV using \\(k = 5\\) or \\(k = 10\\), which provides computational advantage over \\(k = n\\). Beyond computational issues, LOOCV has lower bias but higher variance (why?), compared to \\(k\\)-fold CV with \\(k &lt; n\\). Overall, \\(k\\)-fold CV tends to win in terms of the bias-variance trade-off.\n\n\n\n\nBootstrapping involves resampling a data set with replacement. It is a very useful statistical tool that allows one to quantify the uncertainty of an estimator, such as a coefficient. This is especially useful for models where there is not a convenient standard error formula. When the goal is to estimate the uncertainty of an estimate, one can bootstrap the data and generate the estimate many times (e.g. 1,000 times). Then, one can look at the distribution of the estimates and take the middle 95% as the 95% confidence interval.\nBootstrapping is also an important component of some very powerful machine learning models. We will learn about some of these models in the next few weeks."
  },
  {
    "objectID": "api 222 files/section 5/section 5.html#cross-validation",
    "href": "api 222 files/section 5/section 5.html#cross-validation",
    "title": "Section 5 - Cross-Validation, Ridge, Lasso, and Bootstrapping",
    "section": "",
    "text": "Cross-validation is a method that works by holding out a subset of the training observations from the model fitting process, and then applying the model to those held out observations.\n\n\nThis is the type of cross-validation method we have been using in review sessions so far. It involves randomly dividing the available set of observations into two parts, a training set and a validation set. The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The validation error rate provides an estimate of the test error rate.\nThe validation set approach is simple and is easy to implement, but it has a few drawbacks. First, it can yield estimates of the test error rate that are highly variable. In addition, since it trains the model using only a subset of the data, it may tend to overestimate the test error rate for the model fit on the entire data set.\n\n\n\nLeave-one-out cross-validation (LOOCV) attempts to address the drawbacks of the validation set method. Instead of creating two subsets, a single observation is used for the validation set, and the remaining (\\(n - 1\\)) observations constitute the training set. The statistical learning method is fit on the \\(n - 1\\) training observations, and a prediction is made for the excluded observation to estimate a test error rate. The procedure is repeated \\(n\\) times, by selecting each individual observation for the validation data, training the procedure on the \\(n - 1\\) observations, and computing the test error. The LOOCV estimate for the test error is the average of these \\(n\\) test error estimates.\nCompared to the validation set approach, LOOCV is more expensive to implement. However, it has less bias since it uses almost all the training observations and yields estimates of the test error rate that are less variable through its averaging feature.\n\n\n\n\\(k\\)-fold Cross-validation is an alternative to LOOCV. For \\(j=1,...,k\\), the model is fit on all folds except fold \\(j\\). Then, the model’s predictive performance on the data is assessed in fold \\(j\\). Thus, for a given iteration, the “training” data is data from all the folds except for fold \\(j\\), and the ``test’’ data is data from fold \\(j\\). This process is repeated for \\(j=1,...,k\\) and a test error is estimated for each fold. The \\(k\\)-fold cross-validation estimate for the test error is the average of these \\(k\\) test error estimates.\nLOOCV can be thought of as a special type \\(k\\)-fold cross-validation, where \\(k\\) equals the number of observations in the data. In practice, one typically performs \\(k\\)-fold CV using \\(k = 5\\) or \\(k = 10\\), which provides computational advantage over \\(k = n\\). Beyond computational issues, LOOCV has lower bias but higher variance (why?), compared to \\(k\\)-fold CV with \\(k &lt; n\\). Overall, \\(k\\)-fold CV tends to win in terms of the bias-variance trade-off."
  },
  {
    "objectID": "api 222 files/section 5/section 5.html#bootstrapping",
    "href": "api 222 files/section 5/section 5.html#bootstrapping",
    "title": "Section 5 - Cross-Validation, Ridge, Lasso, and Bootstrapping",
    "section": "",
    "text": "Bootstrapping involves resampling a data set with replacement. It is a very useful statistical tool that allows one to quantify the uncertainty of an estimator, such as a coefficient. This is especially useful for models where there is not a convenient standard error formula. When the goal is to estimate the uncertainty of an estimate, one can bootstrap the data and generate the estimate many times (e.g. 1,000 times). Then, one can look at the distribution of the estimates and take the middle 95% as the 95% confidence interval.\nBootstrapping is also an important component of some very powerful machine learning models. We will learn about some of these models in the next few weeks."
  },
  {
    "objectID": "api 222 files/section 5/section 5.html#about-dataset",
    "href": "api 222 files/section 5/section 5.html#about-dataset",
    "title": "Section 5 - Cross-Validation, Ridge, Lasso, and Bootstrapping",
    "section": "About Dataset",
    "text": "About Dataset\n\nContext\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n\nContent\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\nDownload the dataset here:\n\ndiabetes_data &lt;- read.csv(\"diabetes.csv\", header = TRUE)\n\nLet’s get to know our data.\n\ndim(diabetes_data)\n\n[1] 768   9\n\nsummary(diabetes_data)\n\n  Pregnancies        Glucose      BloodPressure    SkinThickness  \n Min.   : 0.000   Min.   :  0.0   Min.   :  0.00   Min.   : 0.00  \n 1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.: 0.00  \n Median : 3.000   Median :117.0   Median : 72.00   Median :23.00  \n Mean   : 3.845   Mean   :120.9   Mean   : 69.11   Mean   :20.54  \n 3rd Qu.: 6.000   3rd Qu.:140.2   3rd Qu.: 80.00   3rd Qu.:32.00  \n Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00  \n    Insulin           BMI        DiabetesPedigreeFunction      Age       \n Min.   :  0.0   Min.   : 0.00   Min.   :0.0780           Min.   :21.00  \n 1st Qu.:  0.0   1st Qu.:27.30   1st Qu.:0.2437           1st Qu.:24.00  \n Median : 30.5   Median :32.00   Median :0.3725           Median :29.00  \n Mean   : 79.8   Mean   :31.99   Mean   :0.4719           Mean   :33.24  \n 3rd Qu.:127.2   3rd Qu.:36.60   3rd Qu.:0.6262           3rd Qu.:41.00  \n Max.   :846.0   Max.   :67.10   Max.   :2.4200           Max.   :81.00  \n    Outcome     \n Min.   :0.000  \n 1st Qu.:0.000  \n Median :0.000  \n Mean   :0.349  \n 3rd Qu.:1.000  \n Max.   :1.000  \n\n\nFrom the output, we see that we have one binary variable (Outcome) and eight continuous variables. We have 768 observations.\nToday, we will focus on predicting Outcome: (whether or not a patient has diabetes)\nLet’s start with KNN for classification, so load the required package:\n\nlibrary(class)\n\nWe are interested in using KNN on this dataset, but we aren’t sure which k will give the best out-of-sample accuracy. We can use cross-validation to find the best k in this sense.\n\n\nBrief intro to functions in R\nAn R function is created by using the keyword function. The basic syntax of an R function definition is as follows −\n\nfunction_name &lt;- function(arg_1, arg_2, ...) {\n   Function body \n}\n\nThe different parts of a function are:\n\nFunction Name: This is the actual name of the function. It is stored in R environment as an object with this name.\nArguments: An argument is a placeholder. When a function is invoked, you pass a value to the argument. Arguments are optional; that is, a function may contain no arguments. Also arguments can have default values.\nFunction Body: The function body contains a collection of statements that defines what the function does.\nReturn Value: The return value of a function is the last expression in the function body to be evaluated.\n\n\n## Example of a simple function\nadd_fun &lt;- function(a1, a2){\n  res = a1 + a2\n  return(res)\n}\n\nadd_fun(3, 4)\n\n[1] 7\n\n\n\n\nFunction to perform cross-validation\nLet’s write a function to do cross-validation. We will name the function cross_validation_KNN() and it will take arguments:\n\nThe feature columns of the data\nThe outcome column of the data (Y)\nA vector of all k values to test\nThe number of folds to use in k-fold cross validation\n\n\ncross_validation_KNN  &lt;- function(data_x, data_y, k_seq, kfolds) {\n    \n  ## We will start by assigning each observation to one and \n  ## only one fold for cross-validation. To do this, we use\n  ## an index vector. The vector's length equals the number\n  ## of observations in the data. The vector's entries are \n  ## equal numbers of 1s, 2s, etc. up to the number of folds\n  ## being used for k-fold cross validation.\n  ## Recall seq(5) is a vector (1, 2, 3, 4, 5)\n  ## ceiling() rounds a number up to the nearest integer\n  ## rep(a, b) repeats a b times (e.g. rep(1, 3) =&gt; (1, 1, 1))\n  ## So this says repeat the sequence from 1:kfolds\n  ## enough times to fill a vector that is as long or \n  ## slightly longer than the number of observations in\n  ## the data. Then, if the length of the vector exceeds\n  ## the number of observations, truncate it to the right\n  ## length\n  \n  fold_ids &lt;- rep(seq(kfolds), ceiling(nrow(data_x) / kfolds))\n  fold_ids &lt;- fold_ids[1:nrow(data_x)]\n  \n  ## To make the IDs random, randomly rearrange the vector\n  fold_ids &lt;- sample(fold_ids, length(fold_ids))\n  \n  ## In order to store the prediction performance for each fold\n  ## for each k, we initialize a matrix.\n  CV_error_mtx  &lt;- matrix(0, nrow = length(k_seq), ncol = kfolds)\n  \n  ## To run CV, we will loop over all values of k that we want to \n  ## consider for KNN. For each value of k, we will loop over all\n  ## folds. For each fold, we will estimate KNN for the given k on \n  ## all but one fold. We will then measure the model's accuracy \n  ## on the hold out fold and save it. After we finish looping \n  ## through all k's and all folds, we will find the average CV\n  ## error for each value of k and use that as a measure of the \n  ## model's performance.\n  for (k in k_seq) {\n    for (fold in 1:kfolds) {\n      ## Train the KNN model (Note: if it throws a weird error, make sure\n      ## all features are numeric variables -- not factors)\n      ## Note: usually the features are normalized/re-scaled\n      ## Otherwise KNN will give more weight to variables at the larger scale\n      ## when calculating the distance metric.\n      ## See page 165 of the textbook for a useful example\n      knn_fold_model &lt;- knn(train = scale(data_x[which(fold_ids != fold),]),\n                            test = scale(data_x[which(fold_ids == fold),]),\n                            cl = data_y[which(fold_ids != fold)],\n                            k = k)\n      \n      ## Measure and save error rate (% wrong)\n      CV_error_mtx[k, fold] &lt;- mean(knn_fold_model != \n                                      data_y[which(fold_ids == fold)])\n    }\n  }\n  \n  ## We want our function to return the accuracy matrix\n  return(CV_error_mtx)\n    \n}\n\n\n\nApplying the function\nWe can now use the cross-validation function on real data. Note that the outcome is stored in the 9th column.\n\nset.seed(222)\nknn_cv_error5 &lt;- cross_validation_KNN(data_x = diabetes_data[, -9],\n                                      data_y = diabetes_data$Outcome,\n                                      k_seq = seq(20),\n                                      kfolds = 5)\nprint(knn_cv_error5)\n\n           [,1]      [,2]      [,3]      [,4]      [,5]\n [1,] 0.3636364 0.3246753 0.2857143 0.3137255 0.3137255\n [2,] 0.2857143 0.3051948 0.2727273 0.3137255 0.3137255\n [3,] 0.2727273 0.2727273 0.2337662 0.2745098 0.3071895\n [4,] 0.2597403 0.2727273 0.2857143 0.2614379 0.3464052\n [5,] 0.2402597 0.2922078 0.2532468 0.2483660 0.2941176\n [6,] 0.2662338 0.3116883 0.2662338 0.2549020 0.3071895\n [7,] 0.2207792 0.2987013 0.2792208 0.2549020 0.2941176\n [8,] 0.2467532 0.2922078 0.2857143 0.2549020 0.2875817\n [9,] 0.2207792 0.2987013 0.2792208 0.2614379 0.2875817\n[10,] 0.2402597 0.2922078 0.2597403 0.2745098 0.2549020\n[11,] 0.2272727 0.2857143 0.2597403 0.2549020 0.2745098\n[12,] 0.2207792 0.2857143 0.2662338 0.2549020 0.2875817\n[13,] 0.2402597 0.2727273 0.2727273 0.2549020 0.2745098\n[14,] 0.2337662 0.2727273 0.2727273 0.2352941 0.2745098\n[15,] 0.2337662 0.2597403 0.2467532 0.2418301 0.2614379\n[16,] 0.2662338 0.2467532 0.2337662 0.2483660 0.2549020\n[17,] 0.2597403 0.2597403 0.2467532 0.2418301 0.2614379\n[18,] 0.2597403 0.2272727 0.2662338 0.2222222 0.2549020\n[19,] 0.2662338 0.2402597 0.2532468 0.2222222 0.2810458\n[20,] 0.2597403 0.2337662 0.2402597 0.2483660 0.2745098\n\nknn_cv_error10 &lt;- cross_validation_KNN(data_x = diabetes_data[,-9],\n                                       data_y = diabetes_data$Outcome,\n                                       k_seq = seq(50),\n                                       kfolds = 10)\nprint(knn_cv_error10)\n\n           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n [1,] 0.2857143 0.2727273 0.3246753 0.2727273 0.3246753 0.2597403 0.3636364\n [2,] 0.3506494 0.2467532 0.2597403 0.2987013 0.2597403 0.2857143 0.4675325\n [3,] 0.2207792 0.2337662 0.3246753 0.2337662 0.3376623 0.1948052 0.3506494\n [4,] 0.2597403 0.2467532 0.2597403 0.2857143 0.2987013 0.1558442 0.3506494\n [5,] 0.2727273 0.2857143 0.3246753 0.2467532 0.3506494 0.1688312 0.3246753\n [6,] 0.2857143 0.2597403 0.3116883 0.2467532 0.3376623 0.1818182 0.2987013\n [7,] 0.2727273 0.2337662 0.3246753 0.2337662 0.2857143 0.1688312 0.2857143\n [8,] 0.3116883 0.2077922 0.2857143 0.2467532 0.3116883 0.1818182 0.3116883\n [9,] 0.2467532 0.1818182 0.2987013 0.2467532 0.2987013 0.1818182 0.2857143\n[10,] 0.2337662 0.1818182 0.2987013 0.2337662 0.2727273 0.2207792 0.2727273\n[11,] 0.2467532 0.2077922 0.3246753 0.2077922 0.3116883 0.1948052 0.2987013\n[12,] 0.2467532 0.1818182 0.2857143 0.2337662 0.3246753 0.1818182 0.2467532\n[13,] 0.2207792 0.1688312 0.2987013 0.2727273 0.2987013 0.2077922 0.2597403\n[14,] 0.2467532 0.1948052 0.2987013 0.2467532 0.2987013 0.2337662 0.2597403\n[15,] 0.2467532 0.1818182 0.2727273 0.2207792 0.2987013 0.2337662 0.2727273\n[16,] 0.2077922 0.1818182 0.2857143 0.2337662 0.3116883 0.2467532 0.2987013\n[17,] 0.2207792 0.1818182 0.2727273 0.2337662 0.3376623 0.2337662 0.2597403\n[18,] 0.2467532 0.1818182 0.2727273 0.2597403 0.3246753 0.2337662 0.2597403\n[19,] 0.2207792 0.1818182 0.2727273 0.2207792 0.3246753 0.2467532 0.2597403\n[20,] 0.2077922 0.1818182 0.2857143 0.2467532 0.3116883 0.2077922 0.2857143\n[21,] 0.2337662 0.1948052 0.2857143 0.2077922 0.3246753 0.2207792 0.2597403\n[22,] 0.2077922 0.1818182 0.3376623 0.2337662 0.3376623 0.2207792 0.2727273\n[23,] 0.1948052 0.1818182 0.3116883 0.2337662 0.3116883 0.2077922 0.2727273\n[24,] 0.2207792 0.1428571 0.3116883 0.2077922 0.3246753 0.2077922 0.2337662\n[25,] 0.2207792 0.1948052 0.3246753 0.2077922 0.3246753 0.2077922 0.2467532\n[26,] 0.2207792 0.1948052 0.3376623 0.2337662 0.2727273 0.1818182 0.2727273\n[27,] 0.2467532 0.1818182 0.3376623 0.2337662 0.2727273 0.1948052 0.2857143\n[28,] 0.2337662 0.1818182 0.3246753 0.2207792 0.2857143 0.1948052 0.2857143\n[29,] 0.2467532 0.1948052 0.3506494 0.2467532 0.2987013 0.2077922 0.2467532\n[30,] 0.2337662 0.1948052 0.3506494 0.2207792 0.3116883 0.2077922 0.2727273\n[31,] 0.2207792 0.1948052 0.3376623 0.2337662 0.2987013 0.2207792 0.2727273\n[32,] 0.2207792 0.1818182 0.3376623 0.2207792 0.2987013 0.2207792 0.2597403\n[33,] 0.1948052 0.1818182 0.3376623 0.2467532 0.2987013 0.2077922 0.2597403\n[34,] 0.1818182 0.1948052 0.3506494 0.2077922 0.2857143 0.1948052 0.2597403\n[35,] 0.1818182 0.1818182 0.3376623 0.2207792 0.2987013 0.2077922 0.2337662\n[36,] 0.2077922 0.1818182 0.3376623 0.1948052 0.3116883 0.1948052 0.2337662\n[37,] 0.1948052 0.1818182 0.3506494 0.2337662 0.2987013 0.2207792 0.2337662\n[38,] 0.2077922 0.1818182 0.3246753 0.2467532 0.2987013 0.2337662 0.2337662\n[39,] 0.2207792 0.1948052 0.3246753 0.2077922 0.2987013 0.2337662 0.2597403\n[40,] 0.2077922 0.1948052 0.3116883 0.2207792 0.2987013 0.2337662 0.2597403\n[41,] 0.2077922 0.1948052 0.3246753 0.2077922 0.2987013 0.2337662 0.2597403\n[42,] 0.2207792 0.1948052 0.3376623 0.1818182 0.3116883 0.2207792 0.2597403\n[43,] 0.2207792 0.1818182 0.3246753 0.1948052 0.3116883 0.2337662 0.2467532\n[44,] 0.1948052 0.1948052 0.3116883 0.2077922 0.3116883 0.2597403 0.2467532\n[45,] 0.2337662 0.1818182 0.3116883 0.1948052 0.2987013 0.2207792 0.2467532\n[46,] 0.2337662 0.1948052 0.2987013 0.1948052 0.2987013 0.2337662 0.2597403\n[47,] 0.2467532 0.1948052 0.3376623 0.2207792 0.2987013 0.2467532 0.2467532\n[48,] 0.2467532 0.1688312 0.3376623 0.2077922 0.2987013 0.2207792 0.2467532\n[49,] 0.2337662 0.1688312 0.3376623 0.2077922 0.2857143 0.2467532 0.2467532\n[50,] 0.2337662 0.1688312 0.3376623 0.2207792 0.2857143 0.2337662 0.2337662\n           [,8]      [,9]     [,10]\n [1,] 0.2727273 0.3026316 0.2631579\n [2,] 0.2467532 0.3289474 0.3157895\n [3,] 0.3246753 0.2105263 0.2500000\n [4,] 0.2207792 0.3026316 0.2368421\n [5,] 0.2337662 0.2894737 0.2368421\n [6,] 0.2597403 0.3157895 0.2368421\n [7,] 0.2597403 0.2763158 0.2763158\n [8,] 0.2597403 0.3157895 0.2105263\n [9,] 0.2207792 0.2631579 0.2368421\n[10,] 0.2077922 0.3026316 0.2500000\n[11,] 0.1948052 0.2631579 0.2894737\n[12,] 0.1948052 0.2894737 0.2763158\n[13,] 0.2077922 0.2631579 0.2368421\n[14,] 0.2077922 0.2763158 0.2763158\n[15,] 0.1818182 0.2500000 0.2631579\n[16,] 0.2207792 0.2105263 0.2500000\n[17,] 0.2207792 0.2236842 0.2631579\n[18,] 0.2337662 0.2631579 0.2894737\n[19,] 0.1948052 0.2368421 0.2763158\n[20,] 0.2077922 0.2368421 0.2631579\n[21,] 0.2077922 0.2500000 0.2631579\n[22,] 0.2207792 0.2500000 0.2631579\n[23,] 0.2077922 0.2500000 0.2631579\n[24,] 0.2077922 0.2631579 0.2894737\n[25,] 0.2077922 0.2500000 0.2763158\n[26,] 0.2207792 0.2631579 0.2894737\n[27,] 0.1948052 0.2631579 0.2763158\n[28,] 0.2207792 0.2368421 0.2894737\n[29,] 0.1948052 0.2500000 0.3026316\n[30,] 0.1948052 0.2500000 0.2894737\n[31,] 0.1818182 0.2631579 0.3026316\n[32,] 0.2077922 0.2500000 0.3157895\n[33,] 0.1818182 0.2500000 0.2894737\n[34,] 0.2077922 0.2631579 0.2894737\n[35,] 0.2207792 0.2500000 0.2894737\n[36,] 0.2207792 0.2368421 0.2763158\n[37,] 0.2207792 0.2500000 0.2763158\n[38,] 0.2077922 0.2368421 0.2894737\n[39,] 0.2207792 0.2500000 0.2763158\n[40,] 0.2207792 0.2631579 0.3157895\n[41,] 0.2077922 0.2500000 0.2763158\n[42,] 0.2207792 0.2500000 0.2894737\n[43,] 0.2077922 0.2500000 0.2894737\n[44,] 0.2207792 0.2631579 0.3026316\n[45,] 0.2077922 0.2631579 0.2894737\n[46,] 0.1948052 0.2631579 0.2894737\n[47,] 0.2077922 0.2763158 0.2894737\n[48,] 0.2077922 0.2631579 0.3026316\n[49,] 0.2077922 0.2631579 0.3026316\n[50,] 0.2077922 0.2631579 0.3026316\n\n\nWe are usually interested in the mean CV error or accuracy for each parameter considered (value of k in this case). To find the mean for each row, we can use the function rowMeans(). Note that an equivalent function exists for taking the means of columns: colMeans(). There are also functions for rowSums() and colSums().\n\nmean_cv_error5 &lt;- rowMeans(knn_cv_error5)\nmean_cv_error10 &lt;- rowMeans(knn_cv_error10)\n\nTo find the position of the smallest CV error, use which.min().\n\nwhich.min(mean_cv_error5)\n\n[1] 18\n\nwhich.min(mean_cv_error10)\n\n[1] 36\n\n\nTo understand how perform changes across the K values, a plot would be helpful.\n\nplot(seq(50), mean_cv_error10, type = \"l\",\n     main = \"Mean CV Error Rate as a Function of k\",\n     ylab = \"CV Error Rate\", xlab = \"k\")\n\n\n\n\nFrom this, we see that low k values have poor performance while large k values seem to have roughly equivalent performance."
  },
  {
    "objectID": "api 222 files/section 5/section 5.html#bonus-material",
    "href": "api 222 files/section 5/section 5.html#bonus-material",
    "title": "Section 5 - Cross-Validation, Ridge, Lasso, and Bootstrapping",
    "section": "BONUS MATERIAL",
    "text": "BONUS MATERIAL\n\nLDA/QDA/Logistic CV\nGiven that our problem is a classification problem, we might be interested in seeing how well we could do with different methods, like logistic regression, LDA and QDA. Let’s see! We can edit the function we used for CV for KNN and make it relevant for these models. Note that logistic_formula = NULL means the function can run even if this formula is not specified (which it won’t be when we want to run LDA or QDA). Note also that we need to load the package MASS, because it contains the functions lda() and qda().\n\nlibrary(MASS)\ncross_validation &lt;- function(full_data, model_type, kfolds,\n                             logistic_formula = NULL) {\n  \n  ## Define fold_ids in exactly the same way as before\n  fold_ids &lt;- rep(seq(kfolds), ceiling(nrow(full_data) / kfolds))\n  fold_ids &lt;- fold_ids[1:nrow(full_data)]\n  fold_ids &lt;- sample(fold_ids, length(fold_ids))\n  \n  ## Initialize a vector to store CV error\n  CV_error_vec  &lt;- vector(length = kfolds, mode = \"numeric\")\n  \n  ## Loop through the folds\n  for (k in 1:kfolds){\n    if (model_type == \"logistic\") {\n      logistic_model &lt;- glm(logistic_formula,\n                            data = full_data[which(fold_ids != k),],\n                            family = binomial)\n      logistic_pred &lt;- predict(logistic_model,\n                               full_data[which(fold_ids == k),],\n                               type = \"response\")\n      class_pred &lt;- as.numeric(logistic_pred &gt; 0.5)\n      \n    } else if (model_type == \"LDA\") {\n      lda_model &lt;- lda(full_data[which(fold_ids != k),-9], \n                       full_data[which(fold_ids != k),9])\n      lda_pred &lt;- predict(lda_model, full_data[which(fold_ids == k), -9])\n      class_pred &lt;- lda_pred$class\n      \n    } else if (model_type == \"QDA\") {\n      qda_model &lt;- qda(full_data[which(fold_ids != k), -9], \n                       full_data[which(fold_ids != k), 9])\n      qda_pred &lt;- predict(qda_model, \n                          full_data[which(fold_ids == k), -9])\n      class_pred &lt;- qda_pred$class\n    }\n    \n    CV_error_vec[k] &lt;- mean(class_pred != full_data[which(fold_ids == k), 9])\n  }\n  return(CV_error_vec)\n}\n\n## Run CV for logistic regression:\nlogistic_formula &lt;- paste(\"Outcome\", paste(colnames(diabetes_data)[1:8],\n                                           collapse = \" + \"), \n                          sep = \" ~ \")\n\nlogistic_CV_error &lt;- cross_validation(diabetes_data, \"logistic\", 5, \n                                      as.formula(logistic_formula))\n\n## Run CV for LDA:\nlda_CV_error &lt;- cross_validation(diabetes_data, \"LDA\", 5)\n\n## Run CV for QDA:\nqda_CV_error &lt;- cross_validation(diabetes_data, \"QDA\", 5)\n\n## Determine the best model in terms of lowest average CV error\nprint(paste(\"Logistic Error Rate:\", round(mean(logistic_CV_error), 3)))\n\n[1] \"Logistic Error Rate: 0.221\"\n\nprint(paste(\"LDA Error Rate:\", round(mean(lda_CV_error), 3)))\n\n[1] \"LDA Error Rate: 0.231\"\n\nprint(paste(\"QDA Error Rate:\", round(mean(qda_CV_error), 3)))\n\n[1] \"QDA Error Rate: 0.249\""
  },
  {
    "objectID": "api 222 files/section 6/section 6.html",
    "href": "api 222 files/section 6/section 6.html",
    "title": "Section 6 - Regularization and Dimension Reduction",
    "section": "",
    "text": "Note that the material in these notes draws on past TF’s notes (Ibou Dieye, Laura Morris, Emily Mower, Amy Wickett), and the more thorough treatment of these topics in Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.\nThe goal of this session is to review concepts related to regularization methods (LASSO and Ridge Regression) and dimension reduction techniques (PCA and PLS).\n\n\n\n\nLeast Absolute Shrinkage and Selection Operator (LASSO) and Ridge Regression both fall under a broader class of models called shrinkage models. Shrinkage models regularize or penalize coefficient estimates, which means that they shrink the coefficients toward zero. Shrinking the coefficient estimates can reduce their variance, so these methods are particularly useful for models where we are concerned about high variance (i.e. over-fitting), such as models with a large number of predictors relative to the number of observations.\nBoth LASSO and Ridge regression operate by adding a penalty to the normal OLS (Ordinary Least Squares) minimization problem. These penalties can be thought of as a budget, and sometimes the minimization problem is explicitly formulated as minimizing the Residual Sum of Squares (RSS) subject to a budget constraint. The idea of the budget is if you have a small budget, you can only afford a little “total” \\(\\beta\\), where the definition of “total” \\(\\beta\\) varies between LASSO and Ridge, but as your budget increases, you get closer and closer to the OLS \\(\\beta\\)s.\nLASSO and Ridge regression differ in exactly how they penalize the coefficients. In particular, LASSO penalizes the coefficients using an \\(L_1\\) penalty:\n\\[\n\\sum_{i=1}^n \\left(\n    y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}\n    \\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| = RSS + \\lambda \\sum_{j=1}^p |\\beta_j|\n\\]\nRidge regression penalizes the coefficients using an \\(L_2\\) penalty:\n\\[\n    \\sum_{i=1}^n \\left(\n    y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}\n    \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 = RSS + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\] You may notice that both the LASSO and Ridge Regression minimization problems feature a parameter \\(\\lambda\\). This is a tuning parameter that dictates how much we will penalize the “total” coefficients. Tuning parameters appear in many models and get their name from the fact that we tune (adjust / choose) them in order to improve our model’s performance.\nSince LASSO and Ridge Regression are used when we are concerned about variance (over-fitting), it should come as no surprise that increasing \\(\\lambda\\) decreases variance. It can be helpful to contextualize the \\(\\lambda\\) size by realizing that \\(\\lambda=0\\) is OLS and \\(\\lambda =\\infty\\) results in only \\(\\beta_0\\) being assigned a non-zero value.\nAs we increase \\(\\lambda\\) from zero, we decrease variance but increase bias (the classic bias-variance tradeoff). To determine the best \\(\\lambda\\) (e.g. the \\(\\lambda\\) that will lead to the best out of sample predictive performance), it is common to use cross validation. A good function in R that trains LASSO and Ridge Regression models and uses cross-validation to select a good \\(\\lambda\\) is cv.glmnet(), which is part of the glmnet package.\n\n\n\nAs noted above, both LASSO and Ridge regression shrink the coefficients toward zero. However, the penalty in Ridge (\\(\\lambda \\beta_j^2\\)) will not set any of coefficients exactly to zero (unless \\(\\lambda =\\infty\\)). Increasing the value of \\(\\lambda\\) will tend to reduce the magnitudes of the coefficients (which helps reduce variance), but will not result in exclusion of any of the variables. While this may not impact prediction accuracy, it can create a challenge in model interpretation in settings where the number of features is large. This is an obvious disadvantage.\nOn the other hand, the \\(L_1\\) penalty of LASSO forces some of the coefficient estimates to be exactly equal to zero when the tuning parameter \\(\\lambda\\) is sufficiently large. Therefore, LASSO performs variable selection, much like best subset selection.\n\n\n\nLASSO and Ridge Regression are useful models to use when dealing with a large number of predictors \\(p\\) relative to the number of observations \\(n\\). It is good to standardize your features so that coefficients are not selected because of their scale rather than their relative importance.\nFor example, suppose you were predicting salary, and suppose you had a standardized test score that was highly predictive of salary and you also had parents’ income, which was only somewhat predictive of salary. Since standardized test scores are measured on a much smaller scale than the outcome and than parents’ income, we would expect the coefficient on test scores to be large relative to the coefficient on parents’ income. This means it would likely be shrunk by adding the penalty, even though it reflects a strong predictive relationship.\n\n\n\nCoefficients produced by LASSO and Ridge Regression should not be interpreted causally. These methods are used for prediction, and as such, our focus is on \\(\\hat{y}\\). This is in contrast to an inference problem, where we would be interested in \\(\\hat{\\beta}\\).\nThe intuition behind why we cannot interpret the coefficients causally is similar to the intuition underlying Omitted Variables Bias (OVB). In OVB, we said that if two variables \\(X_1\\) and \\(X_2\\) were correlated with each other and with the outcome \\(Y\\), then the coefficient on \\(X_1\\) in a regression of \\(Y\\) on \\(X_1\\) would differ from the coefficient on \\(X_1\\) in a regression where \\(X_2\\) was also included. This is because since \\(X_1\\) and \\(X_2\\) are correlated, when we omit \\(X_2\\), the coefficient on \\(X_1\\) picks up the effect of \\(X_1\\) on \\(Y\\) as well as some of the effect of \\(X_2\\) on \\(Y\\).\n\n\n\n\n\n\nPrincipal Components Analysis is a method of unsupervised learning. We have not yet covered unsupervised learning, though we will in the second half of the semester. The important thing to know at this point about unsupervised learning is that unsupervised learning methods do not use labels \\(Y\\).\nPrincipal Components Analysis (PCA), therefore, does not use \\(Y\\) in determining the principal components. Instead, the principal components are determined solely by looking at the predictors \\(X_1\\),…,\\(X_p\\). Each principal component is a weighted sum of the original predictors \\(X_1\\),…,\\(X_p\\). For clarity of notation, we will use \\(Z\\) to represent principal components and \\(X\\) to represent predictors.\nPrincipal components are created in an ordered way. The order can be described as follows: If you had to project all of the data onto only one line, the first principal component defines the line that would lead to points being as spread out as possible (e.g. having highest variance). If you had to project all of the data onto only one plane, the plane defined by the first two principal components (which are orthogonal by definition) would be the plane where the points were as spread out as possible (e.g. highest possible variance).\nSince variance is greatly affected by measurement scale, it is common practice and a good idea to scale your variables, so that results are not driven by the scale on which the variables were measured.\nWhen estimating principal components, you will estimate \\(p\\) components, where \\(p\\) is the number of predictors \\(X\\). However, you usually only pay attention to / use the first few components, since the last few components capture very little variance. Note that you can exactly recover your original data when using all \\(p\\) components.\n\n\n\nPrincipal Components Regression (PCR) regresses the outcome \\(Y\\) on the principal components \\(Z_1\\),…,\\(Z_M\\), where \\(M&lt;p\\) and \\(p\\) is the number of predictors \\(X_1\\),…,\\(X_p\\).\nNote that the principal components \\(Z_1\\),…,\\(Z_M\\) were defined by looking only at \\(X_1\\),…,\\(X_p\\) and not at \\(Y\\). Putting the principal components into a regression is the first time we are introducing any interaction between the principal components and the outcome \\(Y\\).\nThe main idea of Principal Components Regression is that hopefully only a few components explain most of the variance in the predictors overall and as is relevant to the relationship between the predictors and the response. In other words, when we use PCR, we assume that the directions in which \\(X\\) shows the most variance are also the directions most associated with \\(Y\\). When this assumption is true, we are able to use \\(M&lt;&lt;p\\) (e.g. \\(M\\) much smaller than \\(p\\)) parameters while still getting similar in-sample performance and hopefully better out-of-sample performance (due to not overfitting) to a regression of \\(Y\\) on all \\(p\\) predictors. Although the assumption is not guaranteed to be true, it is often a good enough approximation to give good results.\nPCA is one example of dimensionality reduction, because it reduces the dimension of the problem from \\(p\\) to \\(M\\). Note, though, that dimensionality reduction is different from feature selection. We are still using all features; we have just aggregated them into principal components.\nThe exact number \\(M\\) of principal components to use in PCR (the regression of \\(Y\\) on the principal components) is usually determined by cross-validation.\n\n\n\nWhen using PCR, there are a few things to pay attention to. First, be sure to standardize variables or else the first few principal components will favor features measured on larger scales. Second, the number of principal components to use in PCR is determined using cross-validation. If the number is high and close to the number of features in your data, the assumption that the directions in which the predictors vary most are also the directions that explain the relationship between the predictors and response is false. In such a case, other methods, such as ridge and lasso, are likely to perform better.\n\n\n\n\nPartial Least Squares is a supervised alternative to PCR. Recall that for PCR, the principal components \\(Z_1,...,Z_M\\) were formed from the original features \\(X_1,...,X_p\\) without looking at \\(Y\\) (unsupervised).\nPartial Least Squares (PLS) also generates a new set of features \\(Z_1,...,Z_M\\) but it uses the correlation between the predictors \\(X_1,...,X_p\\) and the outcome \\(Y\\) to determine the weights on \\(X_1,...,X_p\\) for each \\(Z_1,...,Z_M\\). In this way, PLS attempts to find directions that explain both the response and the predictors.\nThe first feature \\(Z_1\\) is determined by weighting \\(X_1,...,X_p\\) proportional to each feature’s correlation with \\(Y\\). It then residualizes the predictors \\(X_1,...,X_p\\) by regressing them on \\(Z_1\\) and repeats the weighting procedure using the orthogonalized predictors. The process then repeats until we have \\(M\\) components, where \\(M\\) is determined by cross validation.\n\n\nJust as with PCR, it’s best practice to scale the predictors before running PLS.\n\n\n\nPLS directly uses \\(Y\\) in generating the features \\(Z_1,...,Z_M\\). It then regresses \\(Y\\) on these features. Therefore, PLS uses \\(Y\\) twice in the process of estimating the model.\nIn contrast, PCR only looks at the outcome \\(Y\\) when estimating the final regression. The components are estimated without ``peeking’’ at \\(Y\\).\nBecause \\(Y\\) is used twice in PLS and only once in PCR, in practice PLS exhibits lower bias (e.g. is better able to fit the training data) but higher variance (e.g. is more sensitive to the exact training data). Therefore, the two methods generally have similar out-of-sample predictive power in practice."
  },
  {
    "objectID": "api 222 files/section 6/section 6.html#lasso-and-ridge-regression",
    "href": "api 222 files/section 6/section 6.html#lasso-and-ridge-regression",
    "title": "Section 6 - Regularization and Dimension Reduction",
    "section": "",
    "text": "Least Absolute Shrinkage and Selection Operator (LASSO) and Ridge Regression both fall under a broader class of models called shrinkage models. Shrinkage models regularize or penalize coefficient estimates, which means that they shrink the coefficients toward zero. Shrinking the coefficient estimates can reduce their variance, so these methods are particularly useful for models where we are concerned about high variance (i.e. over-fitting), such as models with a large number of predictors relative to the number of observations.\nBoth LASSO and Ridge regression operate by adding a penalty to the normal OLS (Ordinary Least Squares) minimization problem. These penalties can be thought of as a budget, and sometimes the minimization problem is explicitly formulated as minimizing the Residual Sum of Squares (RSS) subject to a budget constraint. The idea of the budget is if you have a small budget, you can only afford a little “total” \\(\\beta\\), where the definition of “total” \\(\\beta\\) varies between LASSO and Ridge, but as your budget increases, you get closer and closer to the OLS \\(\\beta\\)s.\nLASSO and Ridge regression differ in exactly how they penalize the coefficients. In particular, LASSO penalizes the coefficients using an \\(L_1\\) penalty:\n\\[\n\\sum_{i=1}^n \\left(\n    y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}\n    \\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| = RSS + \\lambda \\sum_{j=1}^p |\\beta_j|\n\\]\nRidge regression penalizes the coefficients using an \\(L_2\\) penalty:\n\\[\n    \\sum_{i=1}^n \\left(\n    y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}\n    \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 = RSS + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\] You may notice that both the LASSO and Ridge Regression minimization problems feature a parameter \\(\\lambda\\). This is a tuning parameter that dictates how much we will penalize the “total” coefficients. Tuning parameters appear in many models and get their name from the fact that we tune (adjust / choose) them in order to improve our model’s performance.\nSince LASSO and Ridge Regression are used when we are concerned about variance (over-fitting), it should come as no surprise that increasing \\(\\lambda\\) decreases variance. It can be helpful to contextualize the \\(\\lambda\\) size by realizing that \\(\\lambda=0\\) is OLS and \\(\\lambda =\\infty\\) results in only \\(\\beta_0\\) being assigned a non-zero value.\nAs we increase \\(\\lambda\\) from zero, we decrease variance but increase bias (the classic bias-variance tradeoff). To determine the best \\(\\lambda\\) (e.g. the \\(\\lambda\\) that will lead to the best out of sample predictive performance), it is common to use cross validation. A good function in R that trains LASSO and Ridge Regression models and uses cross-validation to select a good \\(\\lambda\\) is cv.glmnet(), which is part of the glmnet package.\n\n\n\nAs noted above, both LASSO and Ridge regression shrink the coefficients toward zero. However, the penalty in Ridge (\\(\\lambda \\beta_j^2\\)) will not set any of coefficients exactly to zero (unless \\(\\lambda =\\infty\\)). Increasing the value of \\(\\lambda\\) will tend to reduce the magnitudes of the coefficients (which helps reduce variance), but will not result in exclusion of any of the variables. While this may not impact prediction accuracy, it can create a challenge in model interpretation in settings where the number of features is large. This is an obvious disadvantage.\nOn the other hand, the \\(L_1\\) penalty of LASSO forces some of the coefficient estimates to be exactly equal to zero when the tuning parameter \\(\\lambda\\) is sufficiently large. Therefore, LASSO performs variable selection, much like best subset selection.\n\n\n\nLASSO and Ridge Regression are useful models to use when dealing with a large number of predictors \\(p\\) relative to the number of observations \\(n\\). It is good to standardize your features so that coefficients are not selected because of their scale rather than their relative importance.\nFor example, suppose you were predicting salary, and suppose you had a standardized test score that was highly predictive of salary and you also had parents’ income, which was only somewhat predictive of salary. Since standardized test scores are measured on a much smaller scale than the outcome and than parents’ income, we would expect the coefficient on test scores to be large relative to the coefficient on parents’ income. This means it would likely be shrunk by adding the penalty, even though it reflects a strong predictive relationship.\n\n\n\nCoefficients produced by LASSO and Ridge Regression should not be interpreted causally. These methods are used for prediction, and as such, our focus is on \\(\\hat{y}\\). This is in contrast to an inference problem, where we would be interested in \\(\\hat{\\beta}\\).\nThe intuition behind why we cannot interpret the coefficients causally is similar to the intuition underlying Omitted Variables Bias (OVB). In OVB, we said that if two variables \\(X_1\\) and \\(X_2\\) were correlated with each other and with the outcome \\(Y\\), then the coefficient on \\(X_1\\) in a regression of \\(Y\\) on \\(X_1\\) would differ from the coefficient on \\(X_1\\) in a regression where \\(X_2\\) was also included. This is because since \\(X_1\\) and \\(X_2\\) are correlated, when we omit \\(X_2\\), the coefficient on \\(X_1\\) picks up the effect of \\(X_1\\) on \\(Y\\) as well as some of the effect of \\(X_2\\) on \\(Y\\)."
  },
  {
    "objectID": "api 222 files/section 6/section 6.html#principal-components-analysis-and-regression",
    "href": "api 222 files/section 6/section 6.html#principal-components-analysis-and-regression",
    "title": "Section 6 - Regularization and Dimension Reduction",
    "section": "",
    "text": "Principal Components Analysis is a method of unsupervised learning. We have not yet covered unsupervised learning, though we will in the second half of the semester. The important thing to know at this point about unsupervised learning is that unsupervised learning methods do not use labels \\(Y\\).\nPrincipal Components Analysis (PCA), therefore, does not use \\(Y\\) in determining the principal components. Instead, the principal components are determined solely by looking at the predictors \\(X_1\\),…,\\(X_p\\). Each principal component is a weighted sum of the original predictors \\(X_1\\),…,\\(X_p\\). For clarity of notation, we will use \\(Z\\) to represent principal components and \\(X\\) to represent predictors.\nPrincipal components are created in an ordered way. The order can be described as follows: If you had to project all of the data onto only one line, the first principal component defines the line that would lead to points being as spread out as possible (e.g. having highest variance). If you had to project all of the data onto only one plane, the plane defined by the first two principal components (which are orthogonal by definition) would be the plane where the points were as spread out as possible (e.g. highest possible variance).\nSince variance is greatly affected by measurement scale, it is common practice and a good idea to scale your variables, so that results are not driven by the scale on which the variables were measured.\nWhen estimating principal components, you will estimate \\(p\\) components, where \\(p\\) is the number of predictors \\(X\\). However, you usually only pay attention to / use the first few components, since the last few components capture very little variance. Note that you can exactly recover your original data when using all \\(p\\) components.\n\n\n\nPrincipal Components Regression (PCR) regresses the outcome \\(Y\\) on the principal components \\(Z_1\\),…,\\(Z_M\\), where \\(M&lt;p\\) and \\(p\\) is the number of predictors \\(X_1\\),…,\\(X_p\\).\nNote that the principal components \\(Z_1\\),…,\\(Z_M\\) were defined by looking only at \\(X_1\\),…,\\(X_p\\) and not at \\(Y\\). Putting the principal components into a regression is the first time we are introducing any interaction between the principal components and the outcome \\(Y\\).\nThe main idea of Principal Components Regression is that hopefully only a few components explain most of the variance in the predictors overall and as is relevant to the relationship between the predictors and the response. In other words, when we use PCR, we assume that the directions in which \\(X\\) shows the most variance are also the directions most associated with \\(Y\\). When this assumption is true, we are able to use \\(M&lt;&lt;p\\) (e.g. \\(M\\) much smaller than \\(p\\)) parameters while still getting similar in-sample performance and hopefully better out-of-sample performance (due to not overfitting) to a regression of \\(Y\\) on all \\(p\\) predictors. Although the assumption is not guaranteed to be true, it is often a good enough approximation to give good results.\nPCA is one example of dimensionality reduction, because it reduces the dimension of the problem from \\(p\\) to \\(M\\). Note, though, that dimensionality reduction is different from feature selection. We are still using all features; we have just aggregated them into principal components.\nThe exact number \\(M\\) of principal components to use in PCR (the regression of \\(Y\\) on the principal components) is usually determined by cross-validation.\n\n\n\nWhen using PCR, there are a few things to pay attention to. First, be sure to standardize variables or else the first few principal components will favor features measured on larger scales. Second, the number of principal components to use in PCR is determined using cross-validation. If the number is high and close to the number of features in your data, the assumption that the directions in which the predictors vary most are also the directions that explain the relationship between the predictors and response is false. In such a case, other methods, such as ridge and lasso, are likely to perform better."
  },
  {
    "objectID": "api 222 files/section 6/section 6.html#partial-least-squares",
    "href": "api 222 files/section 6/section 6.html#partial-least-squares",
    "title": "Section 6 - Regularization and Dimension Reduction",
    "section": "",
    "text": "Partial Least Squares is a supervised alternative to PCR. Recall that for PCR, the principal components \\(Z_1,...,Z_M\\) were formed from the original features \\(X_1,...,X_p\\) without looking at \\(Y\\) (unsupervised).\nPartial Least Squares (PLS) also generates a new set of features \\(Z_1,...,Z_M\\) but it uses the correlation between the predictors \\(X_1,...,X_p\\) and the outcome \\(Y\\) to determine the weights on \\(X_1,...,X_p\\) for each \\(Z_1,...,Z_M\\). In this way, PLS attempts to find directions that explain both the response and the predictors.\nThe first feature \\(Z_1\\) is determined by weighting \\(X_1,...,X_p\\) proportional to each feature’s correlation with \\(Y\\). It then residualizes the predictors \\(X_1,...,X_p\\) by regressing them on \\(Z_1\\) and repeats the weighting procedure using the orthogonalized predictors. The process then repeats until we have \\(M\\) components, where \\(M\\) is determined by cross validation.\n\n\nJust as with PCR, it’s best practice to scale the predictors before running PLS.\n\n\n\nPLS directly uses \\(Y\\) in generating the features \\(Z_1,...,Z_M\\). It then regresses \\(Y\\) on these features. Therefore, PLS uses \\(Y\\) twice in the process of estimating the model.\nIn contrast, PCR only looks at the outcome \\(Y\\) when estimating the final regression. The components are estimated without ``peeking’’ at \\(Y\\).\nBecause \\(Y\\) is used twice in PLS and only once in PCR, in practice PLS exhibits lower bias (e.g. is better able to fit the training data) but higher variance (e.g. is more sensitive to the exact training data). Therefore, the two methods generally have similar out-of-sample predictive power in practice."
  },
  {
    "objectID": "api 222 files/section 6/section 6.html#lasso-and-ridge-regression-1",
    "href": "api 222 files/section 6/section 6.html#lasso-and-ridge-regression-1",
    "title": "Section 6 - Regularization and Dimension Reduction",
    "section": "LASSO and Ridge Regression",
    "text": "LASSO and Ridge Regression\nWe will use the Caravan data set that is included in the package ISLR. This data set contains information on people offered Caravan insurance.\n\nlibrary(ISLR)\ninsurance_data &lt;- Caravan\n\nLet’s learn a little more about the data\n\n?Caravan\n\nLet’s try to predict CARAVAN. Note: although this is a binary variable, ridge and lasso are regression algorithms – regression can often give you a good sense of the ordinal distribution of likelihood that the outcome will be 1 even if the resulting value cannot be viewed as a probability). When you run lasso and ridge, you will need to provide a penalty parameter. Since we don’t know which penalty parameter is best, we will use a built in cross-validation function to find the best penalty parameter (lambda) in the package glmnet\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\n?cv.glmnet\n\nWe will start by using the function’s built-in sequence of lambdas and glmnet standardization\n\nset.seed(222) # Important for replicability\nlasso_ins &lt;- cv.glmnet(x = as.matrix(insurance_data[, 1:85]), # the features\n                       y = as.numeric(insurance_data[, 86]), # the outcome\n                       standardize = TRUE, # Why do we do this?\n                       alpha = 1) # Corresponds to LASSO\n\nWe can see which lambda sequence was used\n\nprint(lasso_ins$lambda)\n\n  [1] 3.577561e-02 3.259740e-02 2.970154e-02 2.706294e-02 2.465874e-02\n  [6] 2.246812e-02 2.047212e-02 1.865343e-02 1.699631e-02 1.548641e-02\n [11] 1.411064e-02 1.285709e-02 1.171490e-02 1.067418e-02 9.725915e-03\n [16] 8.861891e-03 8.074625e-03 7.357298e-03 6.703696e-03 6.108158e-03\n [21] 5.565526e-03 5.071100e-03 4.620597e-03 4.210116e-03 3.836101e-03\n [26] 3.495312e-03 3.184799e-03 2.901870e-03 2.644076e-03 2.409183e-03\n [31] 2.195158e-03 2.000146e-03 1.822459e-03 1.660557e-03 1.513037e-03\n [36] 1.378623e-03 1.256150e-03 1.144557e-03 1.042878e-03 9.502315e-04\n [41] 8.658156e-04 7.888989e-04 7.188153e-04 6.549577e-04 5.967731e-04\n [46] 5.437574e-04 4.954515e-04 4.514370e-04 4.113325e-04 3.747909e-04\n [51] 3.414955e-04 3.111580e-04 2.835156e-04 2.583288e-04 2.353796e-04\n [56] 2.144691e-04 1.954163e-04 1.780560e-04 1.622380e-04 1.478253e-04\n [61] 1.346929e-04 1.227271e-04 1.118244e-04 1.018902e-04 9.283857e-05\n [66] 8.459104e-05 7.707621e-05 7.022897e-05 6.399002e-05 5.830533e-05\n [71] 5.312564e-05 4.840611e-05 4.410584e-05 4.018760e-05 3.661744e-05\n [76] 3.336445e-05 3.040045e-05 2.769975e-05 2.523898e-05 2.299682e-05\n [81] 2.095385e-05 1.909237e-05 1.739625e-05 1.585082e-05 1.444267e-05\n [86] 1.315963e-05 1.199056e-05 1.092535e-05 9.954775e-06 9.070420e-06\n [91] 8.264629e-06 7.530422e-06 6.861440e-06 6.251889e-06 5.696488e-06\n [96] 5.190428e-06 4.729325e-06 4.309185e-06 3.926368e-06 3.577561e-06\n\n\nLet’s find the lambda that does the best as far as CV error goes\n\nprint(lasso_ins$lambda.min)\n\n[1] 0.006703696\n\n\nYou can plot the model results\n\nplot(lasso_ins)\n\n\n\n\nYou can also plot the CV-relevant outputs\n\nLassoCV &lt;- lasso_ins$glmnet.fit\n\nplot(LassoCV, label = TRUE, xvar = \"lambda\")\n\nabline(\n  v = log(c(lasso_ins$lambda.min, lasso_ins$lambda.1se))\n  ) # Adds lines to mark the two key lambda values\n\n\n\n\nYou can see the coefficients corresponding to the two key lambda values using the predict function\n\npredict(lasso_ins, type = \"coefficients\",\n        s = c(lasso_ins$lambda.min, lasso_ins$lambda.1se))\n\n86 x 2 sparse Matrix of class \"dgCMatrix\"\n                       s1          s2\n(Intercept)  0.9896239538 1.053595130\nMOSTYPE      .            .          \nMAANTHUI     .            .          \nMGEMOMV      .            .          \nMGEMLEEF     .            .          \nMOSHOOFD     .            .          \nMGODRK       .            .          \nMGODPR       .            .          \nMGODOV       .            .          \nMGODGE       .            .          \nMRELGE       0.0018180263 .          \nMRELSA       .            .          \nMRELOV       .            .          \nMFALLEEN     .            .          \nMFGEKIND     .            .          \nMFWEKIND     .            .          \nMOPLHOOG     0.0022611683 .          \nMOPLMIDD     .            .          \nMOPLLAAG    -0.0024753875 .          \nMBERHOOG     .            .          \nMBERZELF     .            .          \nMBERBOER    -0.0030373898 .          \nMBERMIDD     .            .          \nMBERARBG     .            .          \nMBERARBO     .            .          \nMSKA         .            .          \nMSKB1        .            .          \nMSKB2        .            .          \nMSKC         .            .          \nMSKD         .            .          \nMHHUUR      -0.0007565131 .          \nMHKOOP       .            .          \nMAUT1        0.0013643467 .          \nMAUT2        .            .          \nMAUT0        .            .          \nMZFONDS      .            .          \nMZPART       .            .          \nMINKM30      .            .          \nMINK3045     .            .          \nMINK4575     .            .          \nMINK7512     .            .          \nMINK123M     .            .          \nMINKGEM      0.0028225067 .          \nMKOOPKLA     0.0025557889 .          \nPWAPART      0.0076586029 .          \nPWABEDR      .            .          \nPWALAND     -0.0008184860 .          \nPPERSAUT     0.0087316235 0.002079863\nPBESAUT      .            .          \nPMOTSCO      .            .          \nPVRAAUT      .            .          \nPAANHANG     .            .          \nPTRACTOR     .            .          \nPWERKT       .            .          \nPBROM        .            .          \nPLEVEN       .            .          \nPPERSONG     .            .          \nPGEZONG      .            .          \nPWAOREG      0.0010472132 .          \nPBRAND       0.0042630439 .          \nPZEILPL      .            .          \nPPLEZIER     .            .          \nPFIETS       .            .          \nPINBOED      .            .          \nPBYSTAND     .            .          \nAWAPART      .            .          \nAWABEDR      .            .          \nAWALAND      .            .          \nAPERSAUT     0.0006293082 .          \nABESAUT      .            .          \nAMOTSCO      .            .          \nAVRAAUT      .            .          \nAAANHANG     .            .          \nATRACTOR     .            .          \nAWERKT       .            .          \nABROM        .            .          \nALEVEN       .            .          \nAPERSONG     .            .          \nAGEZONG      .            .          \nAWAOREG      .            .          \nABRAND       .            .          \nAZEILPL      .            .          \nAPLEZIER     0.2083116609 .          \nAFIETS       0.0076146528 .          \nAINBOED      .            .          \nABYSTAND     0.0361586998 .          \n\n\nQuestions you should be able to answer :\n\nWhich lambda in the sequence had the lowest CV error?\n\n\nlasso_ins$lambda.min\n\n[1] 0.006703696\n\n\n\nWhat is the CV error of the “best” lambda?\n\n\nlasso_ins$cvm[lasso_ins$lambda == lasso_ins$lambda.min]\n\n[1] 0.05384566\n\n\n\nWhat is the standard deviation of the CV error for the “best” lambda?\n\n\nlasso_ins$cvsd[lasso_ins$lambda == lasso_ins$lambda.min]\n\n[1] 0.002099205\n\n\n\nWhat is the largest lambda whose CV error was within 1 standard error of the lowest CV error?\n\n\nlasso_ins$lambda.1se\n\n[1] 0.02970154\n\n\nWhen you do ridge regression, the code is almost exactly the same as for lasso in R. You just need to change the alpha parameter from 1 to 0. I’ll leave this to you as an exercise."
  },
  {
    "objectID": "api 222 files/section 7/section 7.html",
    "href": "api 222 files/section 7/section 7.html",
    "title": "Section 7 - Non-linear Models",
    "section": "",
    "text": "Note: this section has several different types of models. We cannot cover all of them in 1.25 hours. We will go over a few examples but once you understand how the examples, you should be able to apply the logic to other types models.\nThis week, we will use the Wage data that is part of the ISLR package\nlibrary(ISLR)\nLoad the Wage data and drop the “Wage” columns, as we did in Section 6\nwage_data &lt;- Wage\nwage_data &lt;- wage_data[, -10]\nTo make life easier for later analyses, I will start by sorting the data on age. To do this, I will use a package called dplyr, which has a lot of great tools for data manipulation.\nlibrary(dplyr)\nThe first two functions we will use are: - %&gt;% which means “and then do” - arrange() which sorts the data on the column inside the parentheses\nwage_data &lt;- wage_data %&gt;% arrange(age)\nWe will start by splitting the data into training and test sets\nset.seed(222)\ntrain &lt;- sample(1:nrow(wage_data), round(nrow(wage_data) * 0.8))\ntrain &lt;- sort(train)\ntest &lt;- which(!(seq(nrow(wage_data)) %in% train))\nTo quickly and easily measure MSEP, write our own function\nmsep_func &lt;- function(predictions, true_vals) {\n  MSEP &lt;- mean((predictions - true_vals)^2)\n  return(MSEP)\n}"
  },
  {
    "objectID": "api 222 files/section 7/section 7.html#polynomial-regression",
    "href": "api 222 files/section 7/section 7.html#polynomial-regression",
    "title": "Section 7 - Non-linear Models",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nWe can start by fitting a polynomial regression using only age\n\nage_poly &lt;- lm(wage ~ poly(age, 4), data = wage_data[train,])\n\nExtract the coefficients from the model\n\ncoef(summary(age_poly))\n\n                Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    111.88351  0.8086511 138.358203 0.000000e+00\npoly(age, 4)1  394.18926 39.6156514   9.950342 6.942194e-23\npoly(age, 4)2 -434.00566 39.6156514 -10.955409 2.748637e-27\npoly(age, 4)3  105.56550 39.6156514   2.664742 7.756432e-03\npoly(age, 4)4  -90.09776 39.6156514  -2.274297 2.303622e-02\n\n\nWhen you use poly(), it returns a matrix of “orthogonal polynomials” so the columns of the matrix are linear combinations of age, age\\(^2\\), age\\(^3\\), age\\(^4\\). Let’s take a look!\n\nhead(poly(wage_data$age, 4))\n\n              1          2           3          4\n[1,] -0.0386248 0.05590873 -0.07174058 0.08672985\n[2,] -0.0386248 0.05590873 -0.07174058 0.08672985\n[3,] -0.0386248 0.05590873 -0.07174058 0.08672985\n[4,] -0.0386248 0.05590873 -0.07174058 0.08672985\n[5,] -0.0386248 0.05590873 -0.07174058 0.08672985\n[6,] -0.0386248 0.05590873 -0.07174058 0.08672985\n\nhead(wage_data$age)\n\n[1] 18 18 18 18 18 18\n\n\nIf you want it to return the raw powers of age, you can add an argument to the function poly()\n\nhead(poly(wage_data$age, 4, raw = TRUE))\n\n      1   2    3      4\n[1,] 18 324 5832 104976\n[2,] 18 324 5832 104976\n[3,] 18 324 5832 104976\n[4,] 18 324 5832 104976\n[5,] 18 324 5832 104976\n[6,] 18 324 5832 104976\n\n\nAlthough the two forms give you different numbers, they result in the same predictions, because your model is still a linear combination of the original powers\n\nage_poly_TRUE &lt;- lm(wage ~ poly(age, 4, raw = TRUE), \n                    data = wage_data[train,])\n\nExtract the coefficients from the model\n\ncoef(summary(age_poly_TRUE))\n\n                               Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept)               -2.085312e+02 6.559430e+01 -3.179106 0.0014961570\npoly(age, 4, raw = TRUE)1  2.391179e+01 6.439356e+00  3.713383 0.0002091772\npoly(age, 4, raw = TRUE)2 -6.646745e-01 2.257678e-01 -2.944063 0.0032705193\npoly(age, 4, raw = TRUE)3  8.403840e-03 3.362741e-03  2.499105 0.0125172334\npoly(age, 4, raw = TRUE)4 -4.098605e-05 1.802141e-05 -2.274297 0.0230362203\n\n\nWe can see from the coefficient outputs of the two models that they have different coefficients. We will now check that they make the same predictions.\n\nage_poly_pred &lt;- predict(age_poly, newdata = wage_data[test,])\nage_poly_TRUE_pred  &lt;- predict(age_poly_TRUE, newdata = wage_data[test,])\n\nExplore the predictions\n\nhead(age_poly_pred)\n\n       5       19       26       32       37       41 \n51.23518 58.14597 64.50781 64.50781 64.50781 64.50781 \n\nhead(age_poly_TRUE_pred)\n\n       5       19       26       32       37       41 \n51.23518 58.14597 64.50781 64.50781 64.50781 64.50781 \n\n\nCalculate and print the MSEP\n\nprint(msep_func(age_poly_pred, wage_data[test, \"wage\"]))\n\n[1] 1689.522\n\n\ncbind() is a function that joins columns together by binding them next to each other. There is also a function rbind() that joins by binding new rows to the bottom of old ones. Let’s try using cbind() to look at both sets of predictions at the same time:\n\npred_comparison &lt;- cbind(age_poly_pred, age_poly_TRUE_pred)\n\nThe column names default to the variable names, but ifwe want to change them, we can use the colnames() function\n\ncolnames(pred_comparison) &lt;- c(\"pred1\", \"pred2\")\n\nWe can then generate a variable that flags any instances where the predictions do not line up. To do this, I will introduce you another function in dplyr: mutate() which means create a new variable\n\npred_comparison &lt;- pred_comparison %&gt;% \n  mutate(check_same = as.numeric(pred1 == pred2))\n\nError in UseMethod(\"mutate\") : no applicable method for 'mutate' applied to an object of class \"c('matrix', 'array', 'double', 'numeric')\"\nThe previous line errored because pred_comparison is a matrix and dplyr was designed to work on data frames.\nMake pred_comparison a data frame\n\npred_comparison &lt;- data.frame(pred_comparison)\n\nNow try running the code:\n\npred_comparison &lt;- pred_comparison %&gt;%\n  mutate(check_same = as.numeric(pred1 == pred2))\n\nView the results\n\nView(pred_comparison)"
  },
  {
    "objectID": "api 222 files/section 8/section 8.html",
    "href": "api 222 files/section 8/section 8.html",
    "title": "Section 8 - Tree-Based Methods",
    "section": "",
    "text": "Note that more thorough treatment of these topics in Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.\n\n\nTree-based methods are non-parametric supervised learning methods that stratify or segment the predictor space into a number of simple regions. They can be used for both regression and clas- sification. After building a tree using the training data, a prediction can be made by using the training observations in the region to which the new observation belongs. For a regression tree, the predicted response for an observation is given by the mean response of the training observations that belong to the same terminal node. In contrast, for a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs. Decision trees are simple and useful for interpretation. However, they typically have lower predictive power than other supervised learning methods. Bagging, random forests, and boosting are approaches to improve decision trees by involving multiple trees, which are then combined to yield a single consensus prediction. These approaches can dramatically improve the prediction accuracy of decision trees, at the expense of interpretability.\n\n\nTo interpret a decision tree, suppose we have below tree from the Hitters data. The Figure represents a regression tree for predicting the log salary of a baseball player, based on the two predictors—the number of years that he has played in the major leagues and the number of hits that he made in the previous year. We can read the tree sequentially from top to bottom. At a given node (where the branches split), the label (of the form \\(X_j &lt; t_k\\)) indicates the left-hand branch emanating from that split, and the right-hand branch corresponds to \\(X_j \\geq t_k\\). For instance, the split at the top of the tree results in two large branches. The left-hand branch corresponds to \\(Years &lt; 4.5\\) and the right-hand branch corresponds to \\(Years \\geq 4.5\\). The number in each leaf (terminal nodes) is the mean of the response (outcomes) for the observations (in the training data) that fall there.\n\n\n\nDecision Tree Example\n\n\nDecision trees are easier to interpret and have a nice graphical representation. Unfortunately, fitting a decision tree is not simple. It is computationally infeasible to consider every possible partition of the feature space into \\(J\\) boxes. For this reason, we take a top-down, greedy approach that is known as recursive binary splitting. The approach is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. It is greedy because, at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step. In order to perform recursive binary splitting, we first select the predictor \\(X_j\\) and the cutpoint \\(s\\) such that splitting the predictor space into the regions \\(\\{X|X_j &lt; s\\}\\) and \\(\\{X|X_j \\geq s\\}\\) leads to the greatest possible reduction in RSS. For building a classification tree, alternatives to RSS are the classification error rate, Gini index, and entropy. Instead of the error rate, either the Gini index or the entropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate.\nAnother problem with the decision tree is that the process described above may produce good predictions on the training set but is likely to overfit the data, leading to poor test set performance. A better strategy is to grow a very large tree and then prune it back in order to obtain a subtree. We can use the cross validation to prune the tree.\nThe advantages of trees are the following: (1) trees are very easy to explain; (2) trees can be displayed graphically and are easily interpreted even by a non-expert; (3) trees can easily handle qualitative predictors without the need to create dummy variables. Disadvantages of trees are the following: (1) trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches; (2) trees can be very non-robust, i.e., a small change in the data can cause a large change in the final estimated tree.\n\n\n\nBagging, random forests, and boosting are approaches to improve decision trees by involving multiple trees, which are then combined to yield a single consensus prediction. To apply bagging to regression trees, we simply construct \\(B\\) regression trees using \\(B\\) bootstrapped training sets and average the resulting predictions. These trees are grown deep and are not pruned. Because the tree is not pruned, it means each tree is more flexible, hence high variance but low bias. Averaging these \\(B\\) trees reduces the variance because in statistics, averaging a set of observations reduces variance. For bagging classification trees, we can record the class predicted by each of the \\(B\\) trees, and take a majority vote (i.e., the most commonly occurring class among the \\(B\\) predictions).\nWe can also obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). In the case of bagging regression trees, we can record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all \\(B\\) trees. A large value indicates an important predictor.\n\n\n\nThe main difference between bagging and random forests is the choice of predictor subset size m. When building decision trees in random forests, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors. If a random forest is built using \\(m = p\\), then this amounts simply to bagging. Random forests can provide an improvement over bagged trees by decorrelating the trees, i.e., forcing the tree to consider different splits and thus avoid the situation when all trees to have similar structures due to a small subset of strong predictors. In general, using a small value of m in building a random forest will typically be helpful when we have a large number of correlated predictors.\n\n\n\nIn boosting, each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling, but instead, each tree is fit on a modified version of the original data set. Unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown. Boosting involves the parameters that we have to determine. The shrinkage parameter \\(\\lambda\\), a small positive number, controls the rate at which boosting learns. Very small \\(\\lambda\\) can require using a very large value of B and thus achieve good performance. We also need to determine the number \\(d\\) of splits in each tree, which controls the complexity of the boosted ensemble. Similar to \\(\\lambda\\), a small \\(d\\) can typically achieve a slower learn, which means better performance."
  },
  {
    "objectID": "api 222 files/section 8/section 8.html#tree-based-methods",
    "href": "api 222 files/section 8/section 8.html#tree-based-methods",
    "title": "Section 8 - Tree-Based Methods",
    "section": "",
    "text": "Tree-based methods are non-parametric supervised learning methods that stratify or segment the predictor space into a number of simple regions. They can be used for both regression and clas- sification. After building a tree using the training data, a prediction can be made by using the training observations in the region to which the new observation belongs. For a regression tree, the predicted response for an observation is given by the mean response of the training observations that belong to the same terminal node. In contrast, for a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs. Decision trees are simple and useful for interpretation. However, they typically have lower predictive power than other supervised learning methods. Bagging, random forests, and boosting are approaches to improve decision trees by involving multiple trees, which are then combined to yield a single consensus prediction. These approaches can dramatically improve the prediction accuracy of decision trees, at the expense of interpretability.\n\n\nTo interpret a decision tree, suppose we have below tree from the Hitters data. The Figure represents a regression tree for predicting the log salary of a baseball player, based on the two predictors—the number of years that he has played in the major leagues and the number of hits that he made in the previous year. We can read the tree sequentially from top to bottom. At a given node (where the branches split), the label (of the form \\(X_j &lt; t_k\\)) indicates the left-hand branch emanating from that split, and the right-hand branch corresponds to \\(X_j \\geq t_k\\). For instance, the split at the top of the tree results in two large branches. The left-hand branch corresponds to \\(Years &lt; 4.5\\) and the right-hand branch corresponds to \\(Years \\geq 4.5\\). The number in each leaf (terminal nodes) is the mean of the response (outcomes) for the observations (in the training data) that fall there.\n\n\n\nDecision Tree Example\n\n\nDecision trees are easier to interpret and have a nice graphical representation. Unfortunately, fitting a decision tree is not simple. It is computationally infeasible to consider every possible partition of the feature space into \\(J\\) boxes. For this reason, we take a top-down, greedy approach that is known as recursive binary splitting. The approach is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. It is greedy because, at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step. In order to perform recursive binary splitting, we first select the predictor \\(X_j\\) and the cutpoint \\(s\\) such that splitting the predictor space into the regions \\(\\{X|X_j &lt; s\\}\\) and \\(\\{X|X_j \\geq s\\}\\) leads to the greatest possible reduction in RSS. For building a classification tree, alternatives to RSS are the classification error rate, Gini index, and entropy. Instead of the error rate, either the Gini index or the entropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate.\nAnother problem with the decision tree is that the process described above may produce good predictions on the training set but is likely to overfit the data, leading to poor test set performance. A better strategy is to grow a very large tree and then prune it back in order to obtain a subtree. We can use the cross validation to prune the tree.\nThe advantages of trees are the following: (1) trees are very easy to explain; (2) trees can be displayed graphically and are easily interpreted even by a non-expert; (3) trees can easily handle qualitative predictors without the need to create dummy variables. Disadvantages of trees are the following: (1) trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches; (2) trees can be very non-robust, i.e., a small change in the data can cause a large change in the final estimated tree.\n\n\n\nBagging, random forests, and boosting are approaches to improve decision trees by involving multiple trees, which are then combined to yield a single consensus prediction. To apply bagging to regression trees, we simply construct \\(B\\) regression trees using \\(B\\) bootstrapped training sets and average the resulting predictions. These trees are grown deep and are not pruned. Because the tree is not pruned, it means each tree is more flexible, hence high variance but low bias. Averaging these \\(B\\) trees reduces the variance because in statistics, averaging a set of observations reduces variance. For bagging classification trees, we can record the class predicted by each of the \\(B\\) trees, and take a majority vote (i.e., the most commonly occurring class among the \\(B\\) predictions).\nWe can also obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). In the case of bagging regression trees, we can record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all \\(B\\) trees. A large value indicates an important predictor.\n\n\n\nThe main difference between bagging and random forests is the choice of predictor subset size m. When building decision trees in random forests, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors. If a random forest is built using \\(m = p\\), then this amounts simply to bagging. Random forests can provide an improvement over bagged trees by decorrelating the trees, i.e., forcing the tree to consider different splits and thus avoid the situation when all trees to have similar structures due to a small subset of strong predictors. In general, using a small value of m in building a random forest will typically be helpful when we have a large number of correlated predictors.\n\n\n\nIn boosting, each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling, but instead, each tree is fit on a modified version of the original data set. Unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown. Boosting involves the parameters that we have to determine. The shrinkage parameter \\(\\lambda\\), a small positive number, controls the rate at which boosting learns. Very small \\(\\lambda\\) can require using a very large value of B and thus achieve good performance. We also need to determine the number \\(d\\) of splits in each tree, which controls the complexity of the boosted ensemble. Similar to \\(\\lambda\\), a small \\(d\\) can typically achieve a slower learn, which means better performance."
  },
  {
    "objectID": "api 222 files/section 8/section 8.html#decision-trees-1",
    "href": "api 222 files/section 8/section 8.html#decision-trees-1",
    "title": "Section 8 - Tree-Based Methods",
    "section": "Decision Trees",
    "text": "Decision Trees\n\n#install.packages(\"tree\")\nlibrary(ISLR)\nlibrary(tree)\nlibrary(ggplot2)\n\nWe will use the Carseats data. Sales in this data set is a continuous variable. We start by converting it to a binary one that equals “Yes” if Sales \\(&gt; 8\\) and “No” otherwise.\n\ncarseat_data &lt;- Carseats\nhigh_sales &lt;- as.factor(ifelse(carseat_data$Sales &gt; 8, \"Yes\", \"No\"))\ncarseat_data &lt;- data.frame(carseat_data, high_sales)\ncarseat_data = carseat_data[, -1]\n\nLet’s again split the data into training and test sets\n\nset.seed(222)  \ntrain &lt;- sample(seq(nrow(carseat_data)),\n                round(nrow(carseat_data) * 0.5))\n\ntrain &lt;- sort(train)\ntest &lt;- which(!(seq(nrow(carseat_data)) %in% train))\n\nWe can now train a decision tree using the function tree()\n\n?tree\n\nHelp on topic 'tree' was found in the following packages:\n\n  Package               Library\n  cli                   /opt/homebrew/lib/R/4.3/site-library\n  xfun                  /opt/homebrew/lib/R/4.3/site-library\n  tree                  /opt/homebrew/lib/R/4.3/site-library\n\n\nUsing the first match ...\n\ncarseats_tree &lt;- tree(high_sales ~ ., data = carseat_data[train,])\n\nPlot the results\n\nplot(carseats_tree)\ntext(carseats_tree, pretty = 0)\n\n\n\n\nFrom this, we see that shelving location seems to be the most important determinant and price is the second most. Beyond that, this tree is very hard to read. If we just type the tree object name, we get:\n\nThe split criterion (e.g. Price &lt; 92.5)\nThe number of observations in that branch\nThe deviance\nThe overall prediction for the branch\nThe fraction of observations in that branch that are Yes/No\nBranches with terminal nodes are indicated by *\n\n\ncarseats_tree\n\nnode), split, n, deviance, yval, (yprob)\n      * denotes terminal node\n\n  1) root 200 271.500 No ( 0.58500 0.41500 )  \n    2) ShelveLoc: Bad,Medium 154 189.500 No ( 0.69481 0.30519 )  \n      4) Price &lt; 106.5 55  75.790 Yes ( 0.45455 0.54545 )  \n        8) Price &lt; 81 9   0.000 Yes ( 0.00000 1.00000 ) *\n        9) Price &gt; 81 46  63.420 No ( 0.54348 0.45652 )  \n         18) Age &lt; 63.5 31  40.320 Yes ( 0.35484 0.64516 )  \n           36) ShelveLoc: Bad 10  10.010 No ( 0.80000 0.20000 )  \n             72) CompPrice &lt; 112 5   0.000 No ( 1.00000 0.00000 ) *\n             73) CompPrice &gt; 112 5   6.730 No ( 0.60000 0.40000 ) *\n           37) ShelveLoc: Medium 21  17.220 Yes ( 0.14286 0.85714 )  \n             74) Income &lt; 48.5 7   9.561 Yes ( 0.42857 0.57143 ) *\n             75) Income &gt; 48.5 14   0.000 Yes ( 0.00000 1.00000 ) *\n         19) Age &gt; 63.5 15   7.348 No ( 0.93333 0.06667 ) *\n      5) Price &gt; 106.5 99  90.800 No ( 0.82828 0.17172 )  \n       10) CompPrice &lt; 142.5 84  57.200 No ( 0.89286 0.10714 )  \n         20) Price &lt; 127 46  45.480 No ( 0.80435 0.19565 )  \n           40) Age &lt; 52.5 20  26.920 No ( 0.60000 0.40000 )  \n             80) CompPrice &lt; 116 5   0.000 No ( 1.00000 0.00000 ) *\n             81) CompPrice &gt; 116 15  20.730 Yes ( 0.46667 0.53333 )  \n              162) Advertising &lt; 10.5 9  11.460 No ( 0.66667 0.33333 ) *\n              163) Advertising &gt; 10.5 6   5.407 Yes ( 0.16667 0.83333 ) *\n           41) Age &gt; 52.5 26   8.477 No ( 0.96154 0.03846 )  \n             82) CompPrice &lt; 132.5 21   0.000 No ( 1.00000 0.00000 ) *\n             83) CompPrice &gt; 132.5 5   5.004 No ( 0.80000 0.20000 ) *\n         21) Price &gt; 127 38   0.000 No ( 1.00000 0.00000 ) *\n       11) CompPrice &gt; 142.5 15  20.730 Yes ( 0.46667 0.53333 )  \n         22) Education &lt; 15.5 5   0.000 Yes ( 0.00000 1.00000 ) *\n         23) Education &gt; 15.5 10  12.220 No ( 0.70000 0.30000 )  \n           46) Income &lt; 52 5   0.000 No ( 1.00000 0.00000 ) *\n           47) Income &gt; 52 5   6.730 Yes ( 0.40000 0.60000 ) *\n    3) ShelveLoc: Good 46  48.170 Yes ( 0.21739 0.78261 )  \n      6) Price &lt; 136.5 39  29.870 Yes ( 0.12821 0.87179 )  \n       12) Advertising &lt; 6 17  20.600 Yes ( 0.29412 0.70588 )  \n         24) Price &lt; 109 8   0.000 Yes ( 0.00000 1.00000 ) *\n         25) Price &gt; 109 9  12.370 No ( 0.55556 0.44444 ) *\n       13) Advertising &gt; 6 22   0.000 Yes ( 0.00000 1.00000 ) *\n      7) Price &gt; 136.5 7   8.376 No ( 0.71429 0.28571 ) *\n\n\nGiven how deep our tree is grown, we may be worried about overfitting. We can start by evaluating the error rate on the test set for the current tree. We can write a helper function to compute the error rate\n\nerror_rate_func &lt;- function(predictions, true_vals) {\n  error_rate &lt;- mean(as.numeric(predictions != true_vals))\n  return(error_rate)\n}\n\nNow generate predictions from the model\n\ndeep_tree_preds &lt;- predict(carseats_tree, carseat_data[test,], \n                           type = \"class\")\n\nCalculate and summarize the error rate\n\nerror_rate_func(predictions = deep_tree_preds, \n                true_vals = carseat_data[test, \"high_sales\"])\n\n[1] 0.28\n\nsummary(carseats_tree)\n\n\nClassification tree:\ntree(formula = high_sales ~ ., data = carseat_data[train, ])\nVariables actually used in tree construction:\n[1] \"ShelveLoc\"   \"Price\"       \"Age\"         \"CompPrice\"   \"Income\"     \n[6] \"Advertising\" \"Education\"  \nNumber of terminal nodes:  19 \nResidual mean deviance:  0.4032 = 72.98 / 181 \nMisclassification error rate: 0.095 = 19 / 200 \n\n\nThe difference in our error rate between the training and test sets indicates that we overfit. To address this, we want to prune the tree. cv.tree() uses cross-validation to determine how much to prune the tree.\n\nset.seed(222)\ncv_carseats_tree  &lt;- cv.tree(carseats_tree, FUN = prune.misclass)\nnames(cv_carseats_tree)\n\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n\ncv_carseats_tree\n\n$size\n[1] 19 16 14  9  7  6  2  1\n\n$dev\n[1] 50 51 50 53 50 52 58 83\n\n$k\n[1] -Inf  0.0  0.5  1.0  2.5  3.0  6.0 26.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n\nSize tells us the number of terminal nodes on each of the trees considered; dev gives us the CV errors; k gives us the cost-complexity parameter. We can plot the error as a function of size and k\n\npar(mfrow = c(1, 2))\nplot(cv_carseats_tree$size, cv_carseats_tree$dev, type = \"b\")\nplot(cv_carseats_tree$k, cv_carseats_tree$dev, type = \"b\")\n\n\n\n\nFind and print the optimal size\n\nopt_indx &lt;- which.min(cv_carseats_tree$dev)\nopt_size &lt;- cv_carseats_tree$size[opt_indx]\nprint(opt_size)\n\n[1] 19\n\nopt_size &lt;- 7\n\nNow we can prune the tree using prune.misclass()\n\npruned_carseats_tree &lt;- prune.misclass(carseats_tree, best = opt_size)\nplot(pruned_carseats_tree)\ntext(pruned_carseats_tree, pretty = 0)\n\n\n\n\nNow evaluate model performance\n\npruned_tree_preds = predict(pruned_carseats_tree, carseat_data[test, ], \n                            type = \"class\")\n\nerror_rate_func(predictions = pruned_tree_preds, \n                true_vals = carseat_data[test, \"high_sales\"])\n\n[1] 0.25"
  },
  {
    "objectID": "api 222 files/section 8/section 8.html#regression-trees",
    "href": "api 222 files/section 8/section 8.html#regression-trees",
    "title": "Section 8 - Tree-Based Methods",
    "section": "Regression Trees",
    "text": "Regression Trees\nFor this, we will use the Boston data\n\nlibrary(MASS)\nboston_data &lt;- Boston\n\nSplit the data into training and test sets\n\nset.seed(222)  \n\ntrain &lt;- sample(seq(nrow(boston_data)),\n                round(nrow(boston_data) * 0.8))\n\ntrain &lt;- sort(train)\n\ntest &lt;- which(!(seq(nrow(boston_data)) %in% train))\n\nboston_tree = tree(medv ~ ., Boston, subset = train)\n\nsummary(boston_tree)\n\n\nRegression tree:\ntree(formula = medv ~ ., data = Boston, subset = train)\nVariables actually used in tree construction:\n[1] \"rm\"      \"lstat\"   \"dis\"     \"crim\"    \"ptratio\"\nNumber of terminal nodes:  9 \nResidual mean deviance:  14.99 = 5935 / 396 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-23.5800  -2.1540   0.1416   0.0000   2.1420  16.0500 \n\n\nPlot the tree\n\nplot(boston_tree)\ntext(boston_tree)\n\n\n\n\nCalculate the MSE for the Predicted Values\n\nboston_preds &lt;- predict(boston_tree, newdata = boston_data[test,])\n\nCreate a helper function to calculate MSEP\n\nmsep_func &lt;- function(predictions, true_vals) {\n  MSEP &lt;- mean((predictions - true_vals)^2)\n  return(MSEP)\n}\n\nEvaluate model performance\n\nprint(msep_func(predictions = boston_preds, \n                true_vals = boston_data[test, \"medv\"]))\n\n[1] 18.4033\n\n\n\nCreate an object called cv_boston_tree that runs CV on boston_tree to find the best size according to CV error\n\n\ncv_boston_tree = cv.tree(boston_tree)\n\nPlot it\n\nplot(cv_boston_tree$size, cv_boston_tree$dev, type = 'b')\n\n\n\n\nLet’s see what the best size is\n\ncv_boston_tree\n\n$size\n[1] 9 8 7 6 5 4 3 2 1\n\n$dev\n[1]  8323.323  8585.479  8350.749 11323.362 11332.607 11363.410 13838.938\n[8] 20524.030 34561.890\n\n$k\n[1]       -Inf   395.2714   504.8779  1071.0363  1111.0632  1158.0037  2377.8966\n[8]  5952.7426 15916.5248\n\n$method\n[1] \"deviance\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n\n\nFind which size had the lowest CV error and save in a variable called best_size\n\n\nbest_indx &lt;- which.min(cv_boston_tree$dev)\nbest_size &lt;- cv_boston_tree$size[best_indx]\n\nPrune the tree using the best size as found above\n\nprune_boston = prune.tree(boston_tree, best = best_size)\n\nEvaluate model performance\n\nboston_prune_preds &lt;- predict(prune_boston, newdata = boston_data[test,])\nprint(msep_func(boston_prune_preds, boston_data[test, \"medv\"]))\n\n[1] 18.4033\n\n\nThere is a another popular package in R for decision trees called “rpart”. We don’t have time to go into it in class, but you can find more information using the link below.\nhttps://cran.r-project.org/web/packages/rpart/rpart.pdf\nYou can also find several helpful tutorials online."
  },
  {
    "objectID": "api 222 files/section 8/section 8.html#random-forest-bagging-and-boosting",
    "href": "api 222 files/section 8/section 8.html#random-forest-bagging-and-boosting",
    "title": "Section 8 - Tree-Based Methods",
    "section": "Random Forest, Bagging and Boosting",
    "text": "Random Forest, Bagging and Boosting\n\nboston_data &lt;- Boston\n\nCreate a training and a test set\n\nset.seed(222)\ntrain &lt;- sample(seq(nrow(boston_data)),\n                round(nrow(boston_data) * 0.8))\ntrain &lt;- sort(train)\ntest &lt;- which(!(seq(nrow(boston_data)) %in% train))\n\n\nFit a random forest model to the Boston data using the randomForest function. Set the number of trees to 5000.\n\n\n## install.packages(\"randomForest\")\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nrf.boston &lt;- randomForest(medv ~ ., data = data.frame(boston_data[-test,]), \n                          importance = TRUE, n.trees = 5000)\n\n\nMake predictions on the test set\n\n\n## Predictions\nyhat.rf &lt;- predict (rf.boston, newdata = Boston[-train ,])\nboston.test = Boston[-train, \"medv\"]\nmean((yhat.rf - boston.test)^2)\n\n[1] 8.401946\n\n\nThe “mtry” parameter of the “randomForest” function controls the number of variables to include at each branch. By setting this value to equal 13, we are performing bagging. You may be interested in the relative importance of each variable. By setting importance = TRUE, R will store the importance matrix. You can call this by “name of random forest”$importance\n\nbag.boston &lt;- randomForest(medv ~ ., data = data.frame(boston_data[-test,]), \n                           mtry = 13, importance = TRUE)\nbag.boston$importance\n\n             %IncMSE IncNodePurity\ncrim     8.131043576    1598.82211\nzn       0.059260389      29.93039\nindus    0.784813091     151.24889\nchas     0.008483515      24.82692\nnox      4.672183652     672.51423\nrm      49.500433509   15547.73299\nage      2.495468402     475.06678\ndis      9.568228252    2114.52073\nrad      0.712166829     158.24255\ntax      2.163008968     473.14011\nptratio  3.299043553     570.50397\nblack    0.804476128     378.60153\nlstat   59.197744696   11971.04313\n\n\nNow let’s make some predictions\n\nyhat.bag &lt;- predict(bag.boston, newdata = Boston[-train,])\nmean((yhat.bag - boston.test)^2)\n\n[1] 8.725522\n\n\nWe are going to compare the outcome with boosting. Boosting has the same general form except instead of randomForest, you will use “gbm”. We list the distribution as gaussian” since this is a regression problem; if it were a binary classification problem, we would use distribution=“bernoulli”. The argument n.trees=5000 indicates that we want 5000 t trees, and the option interaction.depth=4 limits the depth of each tree. Just as before, we can see the relative importance by looking at the summary. lstat and rm are the most important variables.\n\n## install.packages(\"gbm\")  \nlibrary(gbm)\n\nLoaded gbm 2.1.9\n\n\nThis version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n\nset.seed(222)\n\n## Boosting model\nboost.boston &lt;- gbm(medv ~ ., data = data.frame(boston_data[-test,]), \n                    distribution = \"gaussian\", n.trees = 5000, \n                    interaction.depth = 4)\n\nplot &lt;- summary(boost.boston, plot = F) \n\n## create a ggplot bar plot with labels of plot object\nggplot(plot, aes(x = reorder(var, -rel.inf), y = rel.inf)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  xlab(\"Variable\") +\n  ylab(\"Relative Importance\") +\n  ggtitle(\"Relative Importance of Variables in Boosting Model\") +\n  theme_minimal()\n\n\n\n\nNow let’s make some predictions\n\nyhat.boost &lt;- predict(boost.boston, newdata = Boston[-train ,], \n                      n.trees = 5000)\n\nmean((yhat.boost - boston.test)^2)\n\n[1] 6.541105"
  },
  {
    "objectID": "api 222 files/section 9/section 9.html#support-vector-machines",
    "href": "api 222 files/section 9/section 9.html#support-vector-machines",
    "title": "Section 9 - Support Vector Machines",
    "section": "",
    "text": "In this class, we will see a series of methods that fall under the support vector umbrella but which vary significantly in their flexibility. We start with maximal margin classifiers, which are the most rigid and only work when the data can be perfectly separated with a linear decision boundary. Then, we will cover support vector classifiers, which still uses a linear decision boundary, but which can accommodate data that cannot be perfectly separated by such a decision boundary. We will then cover support vector machines, which flexibly transform the original data to allow for decision boundaries that are non-linear in the original feature space (though they remain linear in the transformed feature space). Like support vector classifiers, support vector machines will also accommodate data that is not perfectly separable. Finally, we will touch upon extending the support vector ideas to multiclass settings."
  },
  {
    "objectID": "api 222 files/section 9/section 9.html#maximal-margin-classifier",
    "href": "api 222 files/section 9/section 9.html#maximal-margin-classifier",
    "title": "Section 9 - Support Vector Machines",
    "section": "",
    "text": "Maximal margin classifiers use a separating hyperplane to divide the feature space in two, with the idea being that all observations in one class lie on one side of the separating hyperplane while all observations of the other class lie on the other side."
  },
  {
    "objectID": "api 222 files/section 9/section 9.html#separating-hyperplanes",
    "href": "api 222 files/section 9/section 9.html#separating-hyperplanes",
    "title": "Section 9 - Support Vector Machines",
    "section": "",
    "text": "A hyperplane is a flat affine subspace that has one fewer dimensions than the feature space. The first part of the definition (“flat affine subspace”) means that the subspace can be described by a linear equation and does not need to pass through the origin. The second part of the definition (about the dimensionality) means that with \\(p\\) features, the hyperplane will have \\(p-1\\) dimensions. When we say “separating hyperplane,” we mean a hyperplane that separates the observations in one class from observations in the other by slicing the feature space in two.\nImagine a dataset with only two features, \\(X_1\\) and \\(X_2\\). The feature space is a plane, which can be divided by a line (which is a hyperplane). Suppose you had three features, \\(X_1\\), \\(X_2\\), and \\(X_3\\). The feature space is 3D and can be divided in two by a plane (which is also a hyperplane). We can continue to generalize to higher dimensional feature spaces.\nThe separating hyperplane that divides the feature space with \\(p\\) features can be described by a linear function of the following form\n\\[\n\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p = 0\n\\]\n\n\nWe can then use this equation as our classifier. Specifically, let \\(f(x)\\) represent the left hand side of the previous equation. To identify the class for observation \\(x^\\star\\), you first calculate \\(f(x^\\star)\\)\n\\[\nf(x^*) = \\beta_0 + \\beta_1 x_1^* + \\beta_2 x_2^* + ... + \\beta_p x_p^*\n\\]\nIf the resulting value is negative, the point lies to one side of the hyperplane and is assigned to class \\(y=-1\\). If it’s positive, the point lies on the other side of the hyperplane and is assigned to class \\(y=+1\\). If the point is far from the separating hyperplane, we are quite confident in our classification. If it’s close, we have much more uncertainty. In this sense, the sign of \\(f(x^\\star)\\) gives us the class and the magnitude of \\(f(x^\\star)\\) gives us our level of confidence. Given the linear form of the separating hyperplane, the decision boundary will also be linear.\nNote that we are now using \\(+1\\) and \\(-1\\) for our binary classes, whereas up to this point we have used \\(0\\) and \\(1\\).\n\n\n\nThe maximal margin classifier can be used when the data is perfectly separable, meaning that there exists a separating hyperplane such that all observations from one class fall on one side of the hyperplane and all observations from the other class fall on the other side of the hyperplane. However, when one such separating hyperplane exists, usually infinite such separating hyperplanes exist. The maximal margin classifier provides a disciplined way to choose among them.\nSpecifically, it measures the distance between each point and the separating hyperplane. The minimum distance between all points and the separating hyperplane is referred to as the margin. The maximal margin classifier selects that separating hyperplane that leads to the largest possible margin, that is, the largest possible distance between the separating hyperplane and the closest training points. Note that as you increase the distance between the decision boundary and the closest point from one class, you must decrease the distance between the decision boundary and the closest point(s) from the other class. Therefore, the margin will always be defined by at least one point from each class. These points are called ``support’’ points or vectors and the model is fully defined by them, so if they move the model changes. Other points can move however they like outside the margins, and as long as they do not cross the margins, the model will not change.\nWhen the margin is large, it suggests the classes are very well separated and performance on a test set should be good. When the margin is small, the classes are only barely separated and the exact decision boundary may not do as well in the test set."
  },
  {
    "objectID": "api 222 files/section 9/section 9.html#support-vector-classifier",
    "href": "api 222 files/section 9/section 9.html#support-vector-classifier",
    "title": "Section 9 - Support Vector Machines",
    "section": "",
    "text": "Sometimes the data cannot be perfectly separated by a separating hyperplane. In such cases, the support vector classifier offers a generalization of the maximal margin classifier.\nThe support vector classifier is extremely similar to the maximal margin classifier, except that it allows some points to cross the margin and even the decision boundary (e.g. be misclassified). Because some points can cross the margin under the support vector classifier, we call the margin a “soft margin” compared to the “hard margin” of the maximal margin classifier, which does not permit any points to cross it.\nThe support vector classifier improves empirically relative to the maximal margin classifier in three key ways. One, it works for data that cannot be perfectly separated by a hyperplane. Two, it is more robust to individual points, meaning that because it allows points to cross the margin, movement in points near the margin will not have the same dramatic impact on the margin and decision boundary that they would have had under the maximal margin classifier. Third, related to this point, the support vector classifier yields better predictions for most training observations.\nThe support vector classifier works by creating slack variables \\(\\epsilon_i\\) and allowing a misclassification “budget” \\(C\\). The slack variables \\(\\epsilon_i\\) will be zero for all observations that fall on the correct side of the margin. If the observation is on the wrong side of the margin, \\(\\epsilon_i &gt; 0\\) and if it is on the wrong side of the hyperplane, \\(\\epsilon_i &gt; 1\\). The sum of the \\(\\epsilon_i\\)s must be no greater than the budget \\(C\\), so when the budget \\(C=0\\), no observations will be allowed on the wrong side of the margin. In this way, the maximal margin classifier is a special case of the support vector classifier. More generally, no more than \\(C\\) observations can be misclassified, because \\(\\epsilon_i&gt;1\\) when the observation is misclassified and \\(C\\) is the sum of the \\(\\epsilon_i\\)’s.\nThe budget \\(C\\) can be seen as a tuning parameter and is thus generally found through cross-validation. As \\(C\\) goes to zero, the classifier converges to the maximal margin classifier, so it is less tolerant of violations across the margin and thus the margins will shrink. As \\(C\\) gets large, it will become more tolerant of violations and the margin will increase.\nIn the support vector classifier, only observations that lie on the margin or that violate the margin will affect the hyperplane. Just as with the maximal margin classifier, points that lie on the correct side of the margin will not be used to define the hyperplane. As \\(C\\) increases, there are more violations and so more support vectors compared to when \\(C\\) is small. Given that there are more support vectors used when \\(C\\) is large, large \\(C\\) leads to lower variance though higher bias compared to small \\(C\\) (which uses a small number of support vectors). Intuitively, this is because the model is not as sensitive to the exact training points since more of them are used in defining the hyperplane when \\(C\\) is large."
  },
  {
    "objectID": "api 222 files/section 9/section 9.html#support-vector-machine",
    "href": "api 222 files/section 9/section 9.html#support-vector-machine",
    "title": "Section 9 - Support Vector Machines",
    "section": "",
    "text": "The support vector machine extends the support vector classifier to the case where the decision boundary is non-linear. It is somewhat analogous to using polynomial regression when the linearity assumption of linear regression does not hold. Specifically, it works with a transformed feature space and finds a decision boundary that is linear in the transformed space, but which is non-linear in the original space.\nHowever, the way in which the support vector machine transforms the original space is new. It does the transformation using a kernel \\(K(x_i, x_{i^\\prime})\\), which is a generalization of the inner product \\(&lt;x_i,x_{i^\\prime}&gt;\\) (e.g. dot product). The hyperplane is then defined by\n\\[\n    f(x) = \\beta_0 + \\sum_{i\\in \\mathcal{S}}\\alpha_i K(x,x_i)\n\\]\nThe values of \\(\\alpha_i\\) are only non-zero for support vectors (points that lie on or across the margin). Some popular kernels include the polynomial kernel and the radial kernel. Note that the support vector classifier is a special case of the support vector machine where the kernel is a polynomial kernel of degree \\(d=1\\). Just like the support vector classifier, it maintains the principle of a budget \\(C\\), though now we use \\(\\alpha_i\\) instead of \\(\\epsilon_i\\).\nThe argument for why we use a kernel rather than an enlarged feature space has to do with computational efficiency. Using the kernel only requires computing the kernel (the generalized inner product) for each unique pair $(x_i, x_{i^}) $ in the training data; it does not require explicitly working in a transformed feature space, which may be computationally intractable. For example, suppose we wanted to expand our feature space from \\(x_1\\) and \\(x_2\\) to also include \\(x_3 = x_1^2 + x_2^2\\). Instead of explicitly calculating \\(x_3\\), we only need to adjust the inner product between two points \\(a\\) and \\(b\\) from\n\\[\nK(a,b) = x_{1,a}x_{1,b}+x_{2,a}x_{2,b}\n\\]\nto\n\\[\nK(a,b) = x_{1,a}x_{1,b}+x_{2,a}x_{2,b} +\n    (x_{1,a}^2 + x_{1,b}^2)(x_{2,a}^2 + x_{2,b}^2)\n\\]\nThe kernel function can take many forms, including polynomial and radial functions. Different kernel functions allow for different levels of model flexibility. However, note that as with many other models we’ve seen this semester, the more flexible we make the kernel, the more likely we are to fit the training data well (low bias) but risk overfitting the training data (high variance). Therefore, in the support vector machine, a very flexible kernel function leads to low bias but high variance."
  },
  {
    "objectID": "api 222 files/section 9/section 9.html#extensions-to-multiclass-problems",
    "href": "api 222 files/section 9/section 9.html#extensions-to-multiclass-problems",
    "title": "Section 9 - Support Vector Machines",
    "section": "",
    "text": "The idea of support vector machines does not generalize easily to the multiclass setting, but two options have been proposed.\nOne is called the one-versus-one approach, where a collection of models is built that each evaluate the question of whether the observation belongs to class \\(a\\) or class \\(b\\). This is repeated with all possible pairs of classes in the data. For a given test point, you tally how many times the observation is assigned to each of the \\(K\\) classes and assign whichever class was assigned most often.\nThe other is called the one-versus-all approach, where \\(K\\) models are built and each model compares the class at hand to a collection of the other \\(K-1\\) classes, coded collectively as -1. For a given test point, you determine which \\(f_k(x)\\) is largest (most confidence) and assign that class."
  },
  {
    "objectID": "JSI@HKS/Lectures/Lecture 2/L2.html#todays-class",
    "href": "JSI@HKS/Lectures/Lecture 2/L2.html#todays-class",
    "title": "Consumers: preferences and constraints",
    "section": "Today’s Class",
    "text": "Today’s Class\n\nDiscuss announcements; student feedback; reflection question discussion\nProvide and overview of the consumer choice and public policy unit\nIntroduce a utility-based framework to model consume preferences\n\n\nUtility function\nIndifference curves; properties of the indifference curves"
  },
  {
    "objectID": "JSI@HKS/Lectures/Lecture 2/L2.html#consumer-choice-and-public-policy",
    "href": "JSI@HKS/Lectures/Lecture 2/L2.html#consumer-choice-and-public-policy",
    "title": "Consumers: preferences and constraints",
    "section": "Consumer Choice and Public Policy",
    "text": "Consumer Choice and Public Policy\nConsumers make choices by trying to select the best feasible option, given available information\nMany of the choices consumers make can be modeled through optimization (i.e. rational, self-interested “homo economicus”)\nAvailable information may be limited when there is some uncertainty. In such cases, people’s risk preferences will matter\nBehavioral economics identifies specific situations in which people fail to optimize i.e., behavior inconsistent with optimization"
  },
  {
    "objectID": "JSI@HKS/syllabus/syllabus.html#learning-community-and-norms",
    "href": "JSI@HKS/syllabus/syllabus.html#learning-community-and-norms",
    "title": "Jacob Jameson",
    "section": "Learning Community and Norms",
    "text": "Learning Community and Norms\n\nAccessibility and Accommodations for Student Learning\nHarvard Kennedy School is committed to the full inclusion of students with disabilities (learning, mental-health related, physical, chronic illness, temporary injury, etc.). The School provides accommodations and support to students with documented disabilities on an individual, case-by-case basis. If students have a disability or think they may have a disability and would like to receive accommodations for their learning, they must disclose and provide medical documentation about their disability to Melissa Wojciechowski St. John. Melissa is the Senior Director of Student Services —and serves as the local disability coordinator— in the HKS Office of Student Services. She can talk to you about your needs and assist you in the process for requesting and implementing accommodations. Because accommodations may require early planning and generally are not provided retroactively, we recommend that you contact her as soon as possible.\n\n\nAcademic Honesty and Integrity\nAll students are expected to abide by the University policies on academic honesty and integrity as given in the Student Handbook. Violations of these policies will not be tolerated and are subject to severe sanctions up to and including expulsion from the university.\n\n\nHonor code\nStudents will have the flexibility to complete two quizzes during a window of 24 hours. Since students are allowed to access the questions at the time of their choice during that window, each student is expected to sign an honor code as they start their quiz.\n\n\nUse of electronics\nOur past experience suggests that student use of electronic devices can be very disruptive to the flow of the class. As a result, no mobile phones, tablets, PDAs, or laptops may be used in class. When an exercise in class (such as polling) requires the use of an electronic device, students will take out their devices for the exercise only and will stow the devices directly after the exercise. Please email your instructor to seek their written permission if you must use electronic devices due to special learning needs.\n\n\nAttendance and Participation\nTBA\n\n\nCommunication\nAll students are required to have email delivery for Canvas announcements enabled. TBA"
  },
  {
    "objectID": "JSI@HKS/syllabus/syllabus.html#course-material",
    "href": "JSI@HKS/syllabus/syllabus.html#course-material",
    "title": "Jacob Jameson",
    "section": "Course Material",
    "text": "Course Material\n\nReadings\nSeveral chapters from two textbooks (listed below) will be assigned as either required readings or substitute readings to the pre-class videos. Links to those chapters will be provided on the Canvas page.\nTBA\nStudents are not required to purchase a textbook for this course. Other (optional) readings will also be posted on the course Canvas page."
  },
  {
    "objectID": "JSI@HKS/syllabus/syllabus.html#overview-of-the-course-schedule",
    "href": "JSI@HKS/syllabus/syllabus.html#overview-of-the-course-schedule",
    "title": "Jacob Jameson",
    "section": "Overview of the Course Schedule",
    "text": "Overview of the Course Schedule"
  },
  {
    "objectID": "api 222 files/section 6/section 6.html#principal-components-analysis-pca-and-partial-least-squares-pls",
    "href": "api 222 files/section 6/section 6.html#principal-components-analysis-pca-and-partial-least-squares-pls",
    "title": "Section 6 - Regularization and Dimension Reduction",
    "section": "Principal Components Analysis (PCA) and Partial Least Squares (PLS)",
    "text": "Principal Components Analysis (PCA) and Partial Least Squares (PLS)\nWe will use the Wage data set that is included in the package ISLR. This data set contains information on people’s wages.\n\nwage_data &lt;- Wage\n\nLet’s learn a little more about the data\n\nsummary(wage_data)\n\nThe data set contains 3000 observations on 11 variables. The variables are:\n\nstr(wage_data)\n\n'data.frame':   3000 obs. of  11 variables:\n $ year      : int  2006 2004 2003 2003 2005 2008 2009 2008 2006 2004 ...\n $ age       : int  18 24 45 43 50 54 44 30 41 52 ...\n $ maritl    : Factor w/ 5 levels \"1. Never Married\",..: 1 1 2 2 4 2 2 1 1 2 ...\n $ race      : Factor w/ 4 levels \"1. White\",\"2. Black\",..: 1 1 1 3 1 1 4 3 2 1 ...\n $ education : Factor w/ 5 levels \"1. &lt; HS Grad\",..: 1 4 3 4 2 4 3 3 3 2 ...\n $ region    : Factor w/ 9 levels \"1. New England\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ jobclass  : Factor w/ 2 levels \"1. Industrial\",..: 1 2 1 2 2 2 1 2 2 2 ...\n $ health    : Factor w/ 2 levels \"1. &lt;=Good\",\"2. &gt;=Very Good\": 1 2 1 2 1 2 2 1 2 2 ...\n $ health_ins: Factor w/ 2 levels \"1. Yes\",\"2. No\": 2 2 1 1 1 1 1 1 1 1 ...\n $ logwage   : num  4.32 4.26 4.88 5.04 4.32 ...\n $ wage      : num  75 70.5 131 154.7 75 ...\n\n\nTo the view the levels of a particular factor variable, we can use:\n\nlevels(wage_data$maritl)\n\n[1] \"1. Never Married\" \"2. Married\"       \"3. Widowed\"       \"4. Divorced\"     \n[5] \"5. Separated\"    \n\nlevels(wage_data$region)\n\n[1] \"1. New England\"        \"2. Middle Atlantic\"    \"3. East North Central\"\n[4] \"4. West North Central\" \"5. South Atlantic\"     \"6. East South Central\"\n[7] \"7. West South Central\" \"8. Mountain\"           \"9. Pacific\"           \n\n\nNotice there is a variable called “wage” and a variable called “logwage”. We just need one of these two. Let’s drop “wage”.\n\nwage_data &lt;- wage_data[, -11]\n\nLooking at the data, we see there are integer, factor, and numeric variable types. Let’s convert everything to numeric variables, which includes converting factor variables to a series of indicators.\n\nfor(i in 10:1){\n  if(is.factor(wage_data[, i])){\n    for(j in unique(wage_data[, i])){\n      new_col &lt;- paste(colnames(wage_data)[i], j, sep = \"_\")\n      wage_data[, new_col] &lt;- as.numeric(wage_data[, i] == j) \n    }\n    wage_data &lt;- wage_data[, -i]     \n  } else if(typeof(wage_data[, i]) == \"integer\") {\n    wage_data[, i] &lt;- as.numeric(as.character(wage_data[, i]))\n  } \n}\n\nCheck your code worked\n\n#View(wage_data)\nsummary(wage_data)\n\n      year           age           logwage      health_ins_2. No\n Min.   :2003   Min.   :18.00   Min.   :3.000   Min.   :0.0000  \n 1st Qu.:2004   1st Qu.:33.75   1st Qu.:4.447   1st Qu.:0.0000  \n Median :2006   Median :42.00   Median :4.653   Median :0.0000  \n Mean   :2006   Mean   :42.41   Mean   :4.654   Mean   :0.3057  \n 3rd Qu.:2008   3rd Qu.:51.00   3rd Qu.:4.857   3rd Qu.:1.0000  \n Max.   :2009   Max.   :80.00   Max.   :5.763   Max.   :1.0000  \n health_ins_1. Yes health_1. &lt;=Good health_2. &gt;=Very Good\n Min.   :0.0000    Min.   :0.000    Min.   :0.000        \n 1st Qu.:0.0000    1st Qu.:0.000    1st Qu.:0.000        \n Median :1.0000    Median :0.000    Median :1.000        \n Mean   :0.6943    Mean   :0.286    Mean   :0.714        \n 3rd Qu.:1.0000    3rd Qu.:1.000    3rd Qu.:1.000        \n Max.   :1.0000    Max.   :1.000    Max.   :1.000        \n jobclass_1. Industrial jobclass_2. Information region_2. Middle Atlantic\n Min.   :0.0000         Min.   :0.0000          Min.   :1                \n 1st Qu.:0.0000         1st Qu.:0.0000          1st Qu.:1                \n Median :1.0000         Median :0.0000          Median :1                \n Mean   :0.5147         Mean   :0.4853          Mean   :1                \n 3rd Qu.:1.0000         3rd Qu.:1.0000          3rd Qu.:1                \n Max.   :1.0000         Max.   :1.0000          Max.   :1                \n education_1. &lt; HS Grad education_4. College Grad education_3. Some College\n Min.   :0.00000        Min.   :0.0000            Min.   :0.0000           \n 1st Qu.:0.00000        1st Qu.:0.0000            1st Qu.:0.0000           \n Median :0.00000        Median :0.0000            Median :0.0000           \n Mean   :0.08933        Mean   :0.2283            Mean   :0.2167           \n 3rd Qu.:0.00000        3rd Qu.:0.0000            3rd Qu.:0.0000           \n Max.   :1.00000        Max.   :1.0000            Max.   :1.0000           \n education_2. HS Grad education_5. Advanced Degree race_1. White   \n Min.   :0.0000       Min.   :0.000                Min.   :0.0000  \n 1st Qu.:0.0000       1st Qu.:0.000                1st Qu.:1.0000  \n Median :0.0000       Median :0.000                Median :1.0000  \n Mean   :0.3237       Mean   :0.142                Mean   :0.8267  \n 3rd Qu.:1.0000       3rd Qu.:0.000                3rd Qu.:1.0000  \n Max.   :1.0000       Max.   :1.000                Max.   :1.0000  \n race_3. Asian     race_4. Other     race_2. Black     maritl_1. Never Married\n Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.000          \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.000          \n Median :0.00000   Median :0.00000   Median :0.00000   Median :0.000          \n Mean   :0.06333   Mean   :0.01233   Mean   :0.09767   Mean   :0.216          \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.000          \n Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :1.000          \n maritl_2. Married maritl_4. Divorced maritl_3. Widowed  maritl_5. Separated\n Min.   :0.0000    Min.   :0.000      Min.   :0.000000   Min.   :0.00000    \n 1st Qu.:0.0000    1st Qu.:0.000      1st Qu.:0.000000   1st Qu.:0.00000    \n Median :1.0000    Median :0.000      Median :0.000000   Median :0.00000    \n Mean   :0.6913    Mean   :0.068      Mean   :0.006333   Mean   :0.01833    \n 3rd Qu.:1.0000    3rd Qu.:0.000      3rd Qu.:0.000000   3rd Qu.:0.00000    \n Max.   :1.0000    Max.   :1.000      Max.   :1.000000   Max.   :1.00000    \n\nstr(wage_data)\n\n'data.frame':   3000 obs. of  24 variables:\n $ year                        : num  2006 2004 2003 2003 2005 ...\n $ age                         : num  18 24 45 43 50 54 44 30 41 52 ...\n $ logwage                     : num  4.32 4.26 4.88 5.04 4.32 ...\n $ health_ins_2. No            : num  1 1 0 0 0 0 0 0 0 0 ...\n $ health_ins_1. Yes           : num  0 0 1 1 1 1 1 1 1 1 ...\n $ health_1. &lt;=Good            : num  1 0 1 0 1 0 0 1 0 0 ...\n $ health_2. &gt;=Very Good       : num  0 1 0 1 0 1 1 0 1 1 ...\n $ jobclass_1. Industrial      : num  1 0 1 0 0 0 1 0 0 0 ...\n $ jobclass_2. Information     : num  0 1 0 1 1 1 0 1 1 1 ...\n $ region_2. Middle Atlantic   : num  1 1 1 1 1 1 1 1 1 1 ...\n $ education_1. &lt; HS Grad      : num  1 0 0 0 0 0 0 0 0 0 ...\n $ education_4. College Grad   : num  0 1 0 1 0 1 0 0 0 0 ...\n $ education_3. Some College   : num  0 0 1 0 0 0 1 1 1 0 ...\n $ education_2. HS Grad        : num  0 0 0 0 1 0 0 0 0 1 ...\n $ education_5. Advanced Degree: num  0 0 0 0 0 0 0 0 0 0 ...\n $ race_1. White               : num  1 1 1 0 1 1 0 0 0 1 ...\n $ race_3. Asian               : num  0 0 0 1 0 0 0 1 0 0 ...\n $ race_4. Other               : num  0 0 0 0 0 0 1 0 0 0 ...\n $ race_2. Black               : num  0 0 0 0 0 0 0 0 1 0 ...\n $ maritl_1. Never Married     : num  1 1 0 0 0 0 0 1 1 0 ...\n $ maritl_2. Married           : num  0 0 1 1 0 1 1 0 0 1 ...\n $ maritl_4. Divorced          : num  0 0 0 0 1 0 0 0 0 0 ...\n $ maritl_3. Widowed           : num  0 0 0 0 0 0 0 0 0 0 ...\n $ maritl_5. Separated         : num  0 0 0 0 0 0 0 0 0 0 ...\n\n\nLet’s split our data into a training and a test set\n\nset.seed(222)\ntrain &lt;- sample(seq(nrow(wage_data)),\n                floor(nrow(wage_data)*0.8))\n\ntrain &lt;- sort(train)\n\ntest &lt;- which(!(seq(nrow(wage_data)) %in% train))\n\nWe are interested in predicting log wage. First, we will use principle components regression. Principle components regression does a linear regression but instead of using the X-variables as predictors, it uses principle components as predictors. The optimal number of principle components to use for PCR is usually found through cross-validation. To run PCR, we will use the package pls.\n\nlibrary(pls)\n\n## Try running PCR\npcr_fit  &lt;- pcr(logwage ~ ., data = wage_data[train,],          \n                scale = TRUE, validation = \"CV\")\n\nError in La.svd(X) : infinite or missing values in 'x'\nSometime you can get an error message. This error is because some of our variables have almost zero variance. Usually, variables with near-zero variance are indicator variables we generated for a rare event. Think about what happens to these predictors when the data are split into cross-validation/bootstrap sub-samples: if a few uncommon unique values are removed from one sample, the predictors could become zero-variance predictors which would cause many models to not run. We can figure out which variables have such low variance to determine how we want to handle them. The simplest approach to identify them is to set a manual threshold (which can be adjusted: 0.05 is a common choice). Our options are then to drop them from the analysis or not to scale the data.\n\n## to drop them from the analysis or not to scale the data. \nfor(col_num in 1:ncol(wage_data)){\n  if(var(wage_data[, col_num]) &lt; 0.05){\n    print(colnames(wage_data)[col_num])\n    print(var(wage_data[, col_num]))\n  }\n}\n\n[1] \"region_2. Middle Atlantic\"\n[1] 0\n[1] \"race_4. Other\"\n[1] 0.01218528\n[1] \"maritl_3. Widowed\"\n[1] 0.006295321\n[1] \"maritl_5. Separated\"\n[1] 0.01800322\n\n## Let's drop these low variance columns\nfor(col_num in ncol(wage_data):1){\n  if(var(wage_data[, col_num]) &lt; 0.05) {\n    wage_data &lt;- wage_data[, -col_num]\n  }\n}\n\nWe can now try again to run PCR\n\nset.seed(222)\n\npcr_fit &lt;- pcr(logwage ~ ., data = wage_data[train,], \n               scale = TRUE, validation = \"CV\")\n\nsummary(pcr_fit)\n\nData:   X dimension: 2400 19 \n    Y dimension: 2400 1\nFit method: svdpc\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV          0.3484   0.2979   0.2936   0.2926   0.2926   0.2921   0.2911\nadjCV       0.3484   0.2978   0.2936   0.2925   0.2925   0.2920   0.2909\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV      0.2913   0.2904   0.2898    0.2874    0.2863    0.2794    0.2794\nadjCV   0.2912   0.2901   0.2898    0.2873    0.2862    0.2793    0.2793\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV       0.2794    0.2795    0.2795    0.2796    0.2796    0.2797\nadjCV    0.2793    0.2794    0.2794    0.2794    0.2795    0.2795\n\nTRAINING: % variance explained\n         1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX          14.50    25.94    36.75    46.22    54.83    61.89    68.87    75.11\nlogwage    27.05    29.19    29.67    29.73    30.11    30.62    30.62    31.24\n         9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX          81.28     86.91     92.14     96.43     99.52     99.79    100.00\nlogwage    31.35     32.46     33.11     36.40     36.45     36.45     36.47\n         16 comps  17 comps  18 comps  19 comps\nX          100.00    100.00    100.00    100.00\nlogwage     36.47     36.47     36.47     36.48\n\n\nWe are interested in finding which number of principle components should be included in the regression to lead to the lowest cross-validation error.\n\npcr_msep &lt;- MSEP(pcr_fit)\npcr_min_indx &lt;- which.min(pcr_msep$val[1, 1,])\nprint(pcr_min_indx)\n\n12 comps \n      13 \n\n\nHow could you get the RMSEP?\n\nprint(pcr_msep$val[1, 1, pcr_min_indx])\n\n[1] 0.07803666\n\n\nIt can also be helpful to plot the RMSEP as a function of the number of components. The black line is the CV, the red dashed line is the adjusted CV.\n\nvalidationplot(pcr_fit)\n\n\n\n\nWhy does the plot look the way it does? Do you expect the PLS plot to look similar? Why or why not?\nLet’s predict logwage for our test observations\n\npcr_pred &lt;- predict(pcr_fit, wage_data[test, ], ncomp = 12)\n\nWe can measure test MSE\n\npcr_test_MSE &lt;- mean((pcr_pred - wage_data[test, \"logwage\"])^2)\nprint(pcr_test_MSE)\n\n[1] 0.07887506\n\n\nWe can convert this to RMSE\n\nprint(sqrt(pcr_test_MSE))\n\n[1] 0.280847\n\n\nLet’s repeat this exercise for PLS. Use plsr() instead of pcr().\n\n## Step 1: Fit the model\npls_fit &lt;- plsr(logwage ~ ., data = wage_data[train, ], \n                scale = TRUE, validation = \"CV\")\nsummary(pls_fit)\n\nData:   X dimension: 2400 19 \n    Y dimension: 2400 1\nFit method: kernelpls\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV          0.3484   0.2843   0.2795   0.2789   0.2789   0.2789   0.2790\nadjCV       0.3484   0.2843   0.2794   0.2789   0.2788   0.2788   0.2789\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV      0.2791   0.2791   0.2791    0.2791    0.2791    0.2791    0.2791\nadjCV   0.2790   0.2790   0.2790    0.2790    0.2790    0.2790    0.2790\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV       0.2791    0.2791    0.2792    0.2792    0.2792    0.2792\nadjCV    0.2790    0.2790    0.2807    0.2807    0.2807    0.2806\n\nTRAINING: % variance explained\n         1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX          14.05    20.93    30.07    37.01    44.43    50.26    55.58    60.76\nlogwage    33.67    36.19    36.43    36.45    36.45    36.46    36.47    36.47\n         9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX          66.04     73.72     80.25     81.54     86.85     93.02    100.00\nlogwage    36.47     36.47     36.47     36.47     36.47     36.47     36.47\n         16 comps  17 comps  18 comps  19 comps\nX          104.22    108.64    113.19    117.49\nlogwage     35.69     35.68     35.68     35.72\n\n## Step 2: Which ncomp value had the lowest CV MSE?\npls_msep &lt;- MSEP(pls_fit)\npls_min_indx &lt;- which.min(pls_msep$val[1, 1,])\nprint(pls_min_indx)\n\n4 comps \n      5 \n\n## Step 3: Plot validation error as a function of # of components\nvalidationplot(pls_fit, val.type = c(\"RMSEP\"))\n\n\n\n## Step 4: Identify the CV RMSE for the number of components with\n## the lowest CV RMSE\npls_rmsep &lt;- RMSEP(pls_fit)\nprint(pls_rmsep$val[1, 1, as.numeric(pls_min_indx)])\n\n[1] 0.2788531\n\n## Step 5: Predict test set logwage values\npls_pred &lt;- predict(pls_fit, wage_data[test,],\n                    ncomp = (as.numeric(pls_min_indx) -1))\n\n## Step 6: Measure test MSE and RMSE\npls_test_MSE &lt;- mean((pls_pred - wage_data[test, \"logwage\"])^2)\nprint(pls_test_MSE)\n\n[1] 0.07881288\n\nprint(sqrt(pls_test_MSE))\n\n[1] 0.2807363"
  },
  {
    "objectID": "api 222 files/section 10/section 10.html",
    "href": "api 222 files/section 10/section 10.html",
    "title": "Section 10 - Neural Networks, Deep Learning, and Reinforcement Learning",
    "section": "",
    "text": "In this section we will briefly cover Neural Networks (NN), Deep Learning (DL), and Reinforcement Learning (RL). For a more in-depth treatment of these topics, please consult the syllabus for suggested readings. The material in these notes draws on past notes by TFs Laura Morris, Emily Mower, and Amy Wickett.\n\n\nNNs are a subset of machine learning which is inspired by the human brain. They mimic how biological neurons communicate with one another to come up with a decision. The main idea behind neural networks is to extract linear combinations of features as new features and model the response variable as a nonlinear function of these new features.\nA NN consists of an input layer, a hidden layer, and an output layer. The first layer receives raw input, it is processed by multiple hidden layers, and the last layer produces the result.\nDeep learning algorithms or deep NNs consist of multiple hidden layers and nodes. The word “deep” refers the depth of neural networks. They are generally used for solving complex problems such as Image classification, Speech recognition, and Text generation.\nWhen developing a NN model, we need to decide upon design, training, and level of learning.\n\n\nMultiple types of NNs are used for advanced machine-learning applications. In this section, we will cover the 2 popular types of NNs.\n\nFeedforward NNs:\n\nFeedforward NNs consist of an input layer, hidden layers, and an output layer. It is called feedforward because the data flow in the forward direction. It is mostly used in Classification, Face recognition, and Pattern recognition. Convolutional NNs are examples of feedforward NNs.\n\nRecurrent NNs:\n\nRecurrent NNs are commonly used for sequential data such as texts, sequences of images, and time series. They are similar to feed-forward networks, except they get inputs from previous sequences using a feedback loop. Recurrent NNs are used in natural language processing (NLP), sales predictions, and weather forecasting. Hopfield NNs are examples of Recurrent NNs.\n\n\n\nWe use training data sets to train NNs. The rule of Thumb is that the number of training examples should be at least five to ten times the number of weights of the network. The most common training algorithm is called Backpropagation. The main steps of Backpropagation:\n\nRandomization: Start with small random weights\nForward Pass: Calculate the output and measure the errors\nBackward Pass: Use errors to adjust weights (supervised learning)\nLoop: Repeat this process until a stopping criterion is met\n\nUsing the example below, can you briefly explain how backpropagation works?\n\n\n\n\nThis step consists of deciding whether to use a NN with one hidden layer (shallow learning) or a NN with multiple hidden layers (deep learning).\n\n\n\n\nFor a more thorough treatment of reinforcement learning, I recommend Reinforcement Learning: An Introduction by Richard Sutton and Andrew Barto, which is on the syllabus. In their book, they describe reinforcement learning as follows:\nReinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two characteristics—trial-and-error search and delayed reward—are the two most important distinguishing features of reinforcement learning.}\nRL is a fascinating field, where agents learn policies (that is, rules regarding which action to take given the current state ) based on interacting with the environment and receiving rewards . Therefore, RL problems are often described by the tuple \\(\\{s, a, r\\}\\), which correspond to state, action, and reward. The idea is that the agent is in a current state \\(s\\) and can take any number of actions. For each action, the agent will transition to a new state \\(s'\\) and receive a reward \\(r'\\), which will both depend on which action the agent takes. Often, the reward will be zero for a very long time before a non-zero reward is achieved. Throughout the learning process, the agent faces a tradeoff between exploitation and exploration.\nThere are many forms of RL. Some are based on statistical models. These often involve updating the models using new observations and then sampling subsequent actions based on their estimated probability of being optimal.\nIn this class, we focused on examples of Markov Decision Processes (MDPs). MDPs assume the Markov property holds. The Markov Property says that if I know the agent’s current state, then learning about the agent’s previous actions and states gives me no relevant information. In other words, I can ignore the agent’s history, since everything important from the past is captured in the agent’s present state and our current estimates of the environment. This is not always a reasonable assumption, but it is reasonable in the gridworld example that is commonly used to introduce people to MDPs.\nBelow is an example of gridworld. In gridworld, we assume that the agent is free to move between all the white boxes and will receive a reward of zero whenever he/she is in a white box. The agent cannot leave the grid or enter the black box. Any effort to move out of the grid or into the black box will result in the agent staying put. If the agent makes it to one of the boxes with +1 or -1, then the agent will receive the corresponding reward and will remain put forever (but will only collect the reward once – not each period). Often, the agent discounts future rewards using a discount factor \\(\\gamma\\), and sometimes the agent’s intended actions are perturbed by noise so that they only go in the intended direction with some probability \\(p\\).\nSuppose we want to figure out what the agent’s optimal policy would be, that is, for each white box, which action should the agent take? One way to figure this out is value iteration. Value iteration calculates the value of each action for each cell and then selects the action with the highest value. It repeats this process, updating its estimates of the values of the actions as it gets more information. Assume the discounting factor (\\(\\gamma\\)) is 0.8. In the gridworld example below, under the optimal policy, what is the value of being in each square?\n\nHint 1 : start from the square that is adjacent to the one with a reward of +1.\nHint 2: note that the value at each state is given by the equation:\n\\[\n    V^{*}(s) = \\max_{a}[r(s, a) + \\gamma V^{*}(\\delta (s, a))]\n\\]\nAs the example shows, value iteration is straightforward when you have information about your environment, such as the transition probabilities and rewards. There is an extension called Q-learning that can be used when you don’t have information about your environment, or at least don’t have full information about your environment, but you are still operating in a finite MDP. In Q-learning the agent follows a policy with some randomness to explore the space and updates its beliefs about optimal actions based on realized rewards. We will see an example of this problem in R."
  },
  {
    "objectID": "api 222 files/section 10/section 10.html#nns-and-dl",
    "href": "api 222 files/section 10/section 10.html#nns-and-dl",
    "title": "Section 10 - Neural Networks, Deep Learning, and Reinforcement Learning",
    "section": "",
    "text": "NNs are a subset of machine learning which is inspired by the human brain. They mimic how biological neurons communicate with one another to come up with a decision. The main idea behind neural networks is to extract linear combinations of features as new features and model the response variable as a nonlinear function of these new features.\nA NN consists of an input layer, a hidden layer, and an output layer. The first layer receives raw input, it is processed by multiple hidden layers, and the last layer produces the result.\nDeep learning algorithms or deep NNs consist of multiple hidden layers and nodes. The word “deep” refers the depth of neural networks. They are generally used for solving complex problems such as Image classification, Speech recognition, and Text generation.\nWhen developing a NN model, we need to decide upon design, training, and level of learning.\n\n\nMultiple types of NNs are used for advanced machine-learning applications. In this section, we will cover the 2 popular types of NNs.\n\nFeedforward NNs:\n\nFeedforward NNs consist of an input layer, hidden layers, and an output layer. It is called feedforward because the data flow in the forward direction. It is mostly used in Classification, Face recognition, and Pattern recognition. Convolutional NNs are examples of feedforward NNs.\n\nRecurrent NNs:\n\nRecurrent NNs are commonly used for sequential data such as texts, sequences of images, and time series. They are similar to feed-forward networks, except they get inputs from previous sequences using a feedback loop. Recurrent NNs are used in natural language processing (NLP), sales predictions, and weather forecasting. Hopfield NNs are examples of Recurrent NNs.\n\n\n\nWe use training data sets to train NNs. The rule of Thumb is that the number of training examples should be at least five to ten times the number of weights of the network. The most common training algorithm is called Backpropagation. The main steps of Backpropagation:\n\nRandomization: Start with small random weights\nForward Pass: Calculate the output and measure the errors\nBackward Pass: Use errors to adjust weights (supervised learning)\nLoop: Repeat this process until a stopping criterion is met\n\nUsing the example below, can you briefly explain how backpropagation works?\n\n\n\n\nThis step consists of deciding whether to use a NN with one hidden layer (shallow learning) or a NN with multiple hidden layers (deep learning)."
  },
  {
    "objectID": "api 222 files/section 10/section 10.html#rl",
    "href": "api 222 files/section 10/section 10.html#rl",
    "title": "Section 10 - Neural Networks, Deep Learning, and Reinforcement Learning",
    "section": "",
    "text": "For a more thorough treatment of reinforcement learning, I recommend Reinforcement Learning: An Introduction by Richard Sutton and Andrew Barto, which is on the syllabus. In their book, they describe reinforcement learning as follows:\nReinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two characteristics—trial-and-error search and delayed reward—are the two most important distinguishing features of reinforcement learning.}\nRL is a fascinating field, where agents learn policies (that is, rules regarding which action to take given the current state ) based on interacting with the environment and receiving rewards . Therefore, RL problems are often described by the tuple \\(\\{s, a, r\\}\\), which correspond to state, action, and reward. The idea is that the agent is in a current state \\(s\\) and can take any number of actions. For each action, the agent will transition to a new state \\(s'\\) and receive a reward \\(r'\\), which will both depend on which action the agent takes. Often, the reward will be zero for a very long time before a non-zero reward is achieved. Throughout the learning process, the agent faces a tradeoff between exploitation and exploration.\nThere are many forms of RL. Some are based on statistical models. These often involve updating the models using new observations and then sampling subsequent actions based on their estimated probability of being optimal.\nIn this class, we focused on examples of Markov Decision Processes (MDPs). MDPs assume the Markov property holds. The Markov Property says that if I know the agent’s current state, then learning about the agent’s previous actions and states gives me no relevant information. In other words, I can ignore the agent’s history, since everything important from the past is captured in the agent’s present state and our current estimates of the environment. This is not always a reasonable assumption, but it is reasonable in the gridworld example that is commonly used to introduce people to MDPs.\nBelow is an example of gridworld. In gridworld, we assume that the agent is free to move between all the white boxes and will receive a reward of zero whenever he/she is in a white box. The agent cannot leave the grid or enter the black box. Any effort to move out of the grid or into the black box will result in the agent staying put. If the agent makes it to one of the boxes with +1 or -1, then the agent will receive the corresponding reward and will remain put forever (but will only collect the reward once – not each period). Often, the agent discounts future rewards using a discount factor \\(\\gamma\\), and sometimes the agent’s intended actions are perturbed by noise so that they only go in the intended direction with some probability \\(p\\).\nSuppose we want to figure out what the agent’s optimal policy would be, that is, for each white box, which action should the agent take? One way to figure this out is value iteration. Value iteration calculates the value of each action for each cell and then selects the action with the highest value. It repeats this process, updating its estimates of the values of the actions as it gets more information. Assume the discounting factor (\\(\\gamma\\)) is 0.8. In the gridworld example below, under the optimal policy, what is the value of being in each square?\n\nHint 1 : start from the square that is adjacent to the one with a reward of +1.\nHint 2: note that the value at each state is given by the equation:\n\\[\n    V^{*}(s) = \\max_{a}[r(s, a) + \\gamma V^{*}(\\delta (s, a))]\n\\]\nAs the example shows, value iteration is straightforward when you have information about your environment, such as the transition probabilities and rewards. There is an extension called Q-learning that can be used when you don’t have information about your environment, or at least don’t have full information about your environment, but you are still operating in a finite MDP. In Q-learning the agent follows a policy with some randomness to explore the space and updates its beliefs about optimal actions based on realized rewards. We will see an example of this problem in R."
  },
  {
    "objectID": "api 222 files/section 10/section 10.html#neural-networks-and-deep-learning",
    "href": "api 222 files/section 10/section 10.html#neural-networks-and-deep-learning",
    "title": "Section 10 - Neural Networks, Deep Learning, and Reinforcement Learning",
    "section": "Neural Networks and Deep Learning",
    "text": "Neural Networks and Deep Learning\nThis section draws heavily from DataCamp’s tutorial on neural networks. The tutorial can be found here.\nIn this session, we will learn to create a simple neural network (NN) with the R packages neuralnet. The required packages are:\n\n# install.packages(\"neuralnet\")\nlibrary(neuralnet)\n\nWe will also use the tidyverse for data manipulation:\n\nlibrary(tidyverse)\n\nWe will use the built-in R data set “iris” and use NNs to classify observations to different species. The “Species” variable is thus our outcome.\nYou can access data by typing iris and running it in the R console. We start by converting character column types into factors.\n\niris &lt;- iris %&gt;% mutate_if(is.character, as.factor)\n\nUse the summary function to assess data distribution.\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nAs we can see, we have balanced data. All three target classes have 50 samples.\nTrain and Test Split\nWe will split the data into train and test datasets for model training and evaluation. We will be splitting it into 80:20.\n\nset.seed(222)\ndata_rows &lt;- floor(0.80 * nrow(iris))\ntrain_indices &lt;- sample(c(1:nrow(iris)), data_rows)\ntrain_data &lt;- iris[train_indices,]\ntest_data &lt;- iris[-train_indices,]\n\nThe neuralnet package allows a simple implementation of NNs. For packages that allow for more sophisticated implementation, check out “keras” and “tensorflow”.\nHere is an example NN with two hidden layers: the first layer with four neurons and the second with two neurons.\n\nmodel = neuralnet(\n  Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n  data = train_data,\n  hidden = c(4, 1),\n  linear.output = FALSE)\n\nTo view our model architecture, we will use the plot function. It requires a model object and rep argument.\n\nplot(model, rep = \"best\")\n\n\n\n\nTo predict the outcome, we will use the predict function. We will use the test data to predict the outcome.\n\npred &lt;- predict(model, test_data, response = \"Species\")\n\nModel evaluation can be done using confusion matrices. We will convert the predicted values into labels and calculate the error rate.\n\nlabels &lt;- levels(train_data$Species)\n\nprediction_label &lt;- data.frame(max.col(pred)) %&gt;% \n  mutate(pred = labels[max.col.pred.]) %&gt;%\n  select(2) %&gt;%\n  unlist()\n\nerrors = as.numeric(test_data$Species) != max.col(pred)\nerror_rate = (sum(errors)/nrow(test_data))*100\n\nprint(paste(\"Error Rate: \", round(error_rate,2), \"%\"))\n\n[1] \"Error Rate:  3.33 %\"\n\ntable(test_data$Species, prediction_label)\n\n            prediction_label\n             setosa versicolor virginica\n  setosa          9          0         0\n  versicolor      0         10         0\n  virginica       0          1        10\n\n\nThe error rate is 3.33%, which is quite good. The confusion matrix shows that the model is performing well."
  },
  {
    "objectID": "api 222 files/section 10/section 10.html#reinforcement-learning",
    "href": "api 222 files/section 10/section 10.html#reinforcement-learning",
    "title": "Section 10 - Neural Networks, Deep Learning, and Reinforcement Learning",
    "section": "Reinforcement Learning",
    "text": "Reinforcement Learning\nThis part draws heavily from the “ReinforcementLearning” package vignette. The package can be found here.\n\n#install.packages(\"ReinforcementLearning\")\nlibrary(ReinforcementLearning)\n\nThis section demonstrates the capabilities of the ReinforcementLearning package with the help of a practical example, a Gridworld. The goal is to teach optimal movements to a robot in a grid-shaped maze (adapted from Sutton (1998)). Here the agent must navigate from a random starting position to a final position on a simulated 2×2 grid (see figure below). The reward structures are as follows: each movement leads to a negative reward of -1 to penalize routes that are not the shortest path. If the agent reaches the goal position, it earns a reward of 10.\nThe gridworld is defined as follows:\nmake a table\n|———–| | s1 | s4 | | s2 s3 | |———–| Define the state and action sets, and load the built-in environment function for the 2×2 gridworld.\n\nstates &lt;- c(\"s1\", \"s2\", \"s3\", \"s4\")\nactions &lt;- c(\"up\", \"down\", \"left\", \"right\")\n\nLoad built-in environment function for 2x2 gridworld\n\nenv &lt;- gridworldEnvironment\nprint(env)\n\nfunction (state, action) \n{\n    next_state &lt;- state\n    if (state == state(\"s1\") && action == \"down\") \n        next_state &lt;- state(\"s2\")\n    if (state == state(\"s2\") && action == \"up\") \n        next_state &lt;- state(\"s1\")\n    if (state == state(\"s2\") && action == \"right\") \n        next_state &lt;- state(\"s3\")\n    if (state == state(\"s3\") && action == \"left\") \n        next_state &lt;- state(\"s2\")\n    if (state == state(\"s3\") && action == \"up\") \n        next_state &lt;- state(\"s4\")\n    if (next_state == state(\"s4\") && state != state(\"s4\")) {\n        reward &lt;- 10\n    }\n    else {\n        reward &lt;- -1\n    }\n    out &lt;- list(NextState = next_state, Reward = reward)\n    return(out)\n}\n&lt;bytecode: 0x11c382a58&gt;\n&lt;environment: namespace:ReinforcementLearning&gt;\n\n\nAfter having specified the environment function, we can use the built-in sampleExperience() function to sample observation sequences from the environment. The following code snippet generates a data frame data containing 1000 random state-transition tuples.\n\n?sampleExperience\n\ndata &lt;- sampleExperience(N = 1000, \n                         env = env, \n                         states = states, \n                         actions = actions)\nhead(data)\n\n  State Action Reward NextState\n1    s1  right     -1        s1\n2    s4  right     -1        s4\n3    s3     up     10        s4\n4    s1     up     -1        s1\n5    s4  right     -1        s4\n6    s2   left     -1        s2\n\n\nWe can now use the observation sequence in data in order to learn the optimal behavior of the agent. For this purpose, we first customize the learning behavior of the agent by defining a control object. We follow the default parameter choices and set the learning rate alpha to 0.1, the discount factor gamma to 0.5, and the exploration greediness epsilon to 0.1. Subsequently, we use the ReinforcementLearning() function to learn the best possible policy for the input data.\n\n# Define reinforcement learning parameters\ncontrol &lt;- list(alpha = 0.1, gamma = 0.5, epsilon = 0.1)\n\n# Perform reinforcement learning\n?ReinforcementLearning\nmodel &lt;- ReinforcementLearning(data, \n                               s = \"State\", \n                               a = \"Action\", \n                               r = \"Reward\", \n                               s_new = \"NextState\", \n                               control = control)\n\nEvaluating policy learning\n\n# Print policy\ncomputePolicy(model)\n\n     s1      s2      s3      s4 \n \"down\" \"right\"    \"up\"  \"left\" \n\n\nPrint state-action function\n\nprint(model)\n\nState-Action function Q\n        right        up       down       left\ns1 -0.6769943 -0.727481  0.7211750 -0.6641956\ns2  3.5459967 -0.711599  0.7227171  0.7305628\ns3  3.5167406  9.112867  3.5403134  0.7324369\ns4 -1.9264142 -1.906775 -1.9032334 -1.8981189\n\nPolicy\n     s1      s2      s3      s4 \n \"down\" \"right\"    \"up\"  \"left\" \n\nReward (last iteration)\n[1] -494\n\n\nPrint summary statistics\n\nsummary(model)\n\n\nModel details\nLearning rule:           experienceReplay\nLearning iterations:     1\nNumber of states:        4\nNumber of actions:       4\nTotal Reward:            -494\n\nReward details (per iteration)\nMin:                     -494\nMax:                     -494\nAverage:                 -494\nMedian:                  -494\nStandard deviation:      NA"
  },
  {
    "objectID": "JSI@HKS/syllabus/syllabus.html#course-requirements-and-grading",
    "href": "JSI@HKS/syllabus/syllabus.html#course-requirements-and-grading",
    "title": "Office Hours",
    "section": "Course Requirements and Grading",
    "text": "Course Requirements and Grading\n\nParticipation 20%\nPractice questions 20%\nEnd-of-Unit Assessments (4) 20%\nIn-Class Final Exam 40%\n\n\nParticipation\nAttendance and participation in class discussions are essential. Students are expected to read the assigned material or watch the assigned videos before class. Students are expected to participate in class discussions, however, we recognize that some students may be more comfortable participating in class discussions in writing. We will provide opportunities for students to participate in class discussions via other modalities, such as Canvas discussion boards.\nParticipation will be self-assessed by students at the end of each unit. Students will be asked to reflect on their participation in class discussions and other activities.\n\n\nPractice Questions\nOur teaching fellow will hold bi-weekly “Guided problem solving sessions” that will be offer a chance for students to walk through key problem for the week. These sessions will be recorded and posted on Canvas. We will post a short set of questions after each class (on Canvas). Students are encouraged to work on these questions and bring them to the guided problem solving sessions.\nStudents will have access to a set of practice questions at the start of each week. They will be due at the end of each week. Each practice question set has an equal weight.\n\n\nEnd-of-Unit Assessments (EUA)\nStudents will have access to a set of assessment questions at the end of each unit. Questions will be available during a 24-hour window. Students will have 1 hour to complete the assessment on their own once they start it during this window. Each EUA has an equal weight. No additional preparation is required for EUAs.\n\n\nIn-Class Final Exam\nThe final exam will be held during the last class session. The final exam will be a closed-book exam. The final exam will be cumulative, covering all material from the course. The final exam will be held in class and will be proctored by the teaching team. The final exam will be 2 hours long."
  },
  {
    "objectID": "JSI@HKS/syllabus/syllabus.html#accessibility-and-accommodations-for-student-learning",
    "href": "JSI@HKS/syllabus/syllabus.html#accessibility-and-accommodations-for-student-learning",
    "title": "Office Hours",
    "section": "Accessibility and Accommodations for Student Learning",
    "text": "Accessibility and Accommodations for Student Learning\nHarvard Kennedy School is committed to the full inclusion of students with disabilities (learning, mental-health related, physical, chronic illness, temporary injury, etc.). The School provides accommodations and support to students with documented disabilities on an individual, case-by-case basis. If students have a disability or think they may have a disability and would like to receive accommodations for their learning, they must disclose and provide medical documentation about their disability to Melissa Wojciechowski St. John. Melissa is the Senior Director of Student Services —and serves as the local disability coordinator— in the HKS Office of Student Services. She can talk to you about your needs and assist you in the process for requesting and implementing accommodations. Because accommodations may require early planning and generally are not provided retroactively, we recommend that you contact her as soon as possible."
  },
  {
    "objectID": "JSI@HKS/syllabus/syllabus.html#academic-honesty-and-integrity",
    "href": "JSI@HKS/syllabus/syllabus.html#academic-honesty-and-integrity",
    "title": "Office Hours",
    "section": "Academic Honesty and Integrity",
    "text": "Academic Honesty and Integrity\nAll students are expected to abide by the University policies on academic honesty and integrity as given in the Student Handbook. Violations of these policies will not be tolerated and are subject to severe sanctions up to and including expulsion from the university."
  },
  {
    "objectID": "JSI@HKS/syllabus/syllabus.html#honor-code",
    "href": "JSI@HKS/syllabus/syllabus.html#honor-code",
    "title": "Office Hours",
    "section": "Honor code",
    "text": "Honor code\nStudents will have the flexibility to complete two quizzes during a window of 24 hours. Since students are allowed to access the questions at the time of their choice during that window, each student is expected to sign an honor code as they start their quiz."
  },
  {
    "objectID": "JSI@HKS/syllabus/syllabus.html#use-of-electronics",
    "href": "JSI@HKS/syllabus/syllabus.html#use-of-electronics",
    "title": "Office Hours",
    "section": "Use of electronics",
    "text": "Use of electronics\nOur past experience suggests that student use of electronic devices can be very disruptive to the flow of the class. As a result, no mobile phones, tablets, PDAs, or laptops may be used in class. When an exercise in class (such as polling) requires the use of an electronic device, students will take out their devices for the exercise only and will stow the devices directly after the exercise. Please email your instructor to seek their written permission if you must use electronic devices due to special learning needs."
  },
  {
    "objectID": "JSI@HKS/syllabus/syllabus.html#readings",
    "href": "JSI@HKS/syllabus/syllabus.html#readings",
    "title": "Office Hours",
    "section": "Readings",
    "text": "Readings\n\nTextbook readings\nStudents are not required to purchase a textbook for this course. Several chapters from two textbooks (listed below) will be assigned as required readings. Links to those chapters will be provided on the Canvas page.\n\nRobert S. Pindyck and Daniel L. Rubinfeld, Microeconomics, 9th Edition, Pearson, 2018. (Hereafter referred to as P&R.)\nOpenStax (Rice University), Principles of Economics 2e, 2017 (updated Jan 2020) You can download it for free (PDF, Kindle, iBooks) at https://openstax.org/details/books/principles-economics (Hereafter referred to as OpenStax)\n\n\n\nOther readings\nOther (optional) readings are listed under “Optional Resources” and will also be posted on the course Canvas page."
  },
  {
    "objectID": "Math Camp/section 1/section1.html",
    "href": "Math Camp/section 1/section1.html",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "",
    "text": "Notes build on previous TFs (Ibou Dieye, Laura Morris, Amy Wickett & Emily Mower)\nThe following code is meant as a first introduction to R. It is therefore helpful to run it one line at a time and see what happens. To run one line of code in RStudio, you can highlight the code you want to run and hit “Run” at the top of the script.\nOn a mac, you can highlight the code you want to run and hit Command + Enter. On a PC, you can highlight the code you want to run and hit Ctrl + Enter. If you ever forget how a function works, you can type ? followed immediately (e.g. with no space) by the function name to get the help file."
  },
  {
    "objectID": "Math Camp/section 1/section1.html#sets",
    "href": "Math Camp/section 1/section1.html#sets",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "",
    "text": "Set: In mathematics, a set is a collection of elements. The elements in a set can be any mathematical objects, such as numbers, variables, or symbols. We can even have a set of sets.\n\n\n\n\n\\(A = \\{2, 3, 5, 7, 11, 13, 17, 19, 23, 29\\}\\)\n\\(B = \\{\\text{Lily, Yunan, Annabelle}\\}\\)\n\\(C = \\{(42^\\circ 21' 0'' N, 71^\\circ 30' 49'' W), (40^\\circ 42' 46'' N, 74^\\circ 0' 22'' W)\\}\\)\n\\(D = \\{\\{1, 2\\}, \\{3, 4\\}\\}\\)\n\\(E = \\{\\emptyset\\}\\)\n\nSome special sets in mathematics: - \\(\\emptyset\\): the “empty set” - \\(\\mathbb{N}\\): the set of natural numbers = \\(\\{1, 2, 3, \\dots\\}\\) - \\(\\mathbb{Z}\\): the set of integers = \\(\\{\\dots, -2, -1, 0, 1, 2, \\dots\\}\\) - \\(\\mathbb{Q}\\): the set of rational numbers - \\(\\mathbb{R}\\): the set of real numbers = \\(\\mathbb{Q}\\) [irrational numbers]\n\n\n\n\n\nMembership: If \\(x\\) is an element of \\(A\\), denoted \\(x \\in A\\). If \\(x\\) is not an element of \\(A\\), denoted \\(x \\notin A\\).\n\n\n\n\n\nOperations:\n\n\\(A \\cup B\\): union (the set of all elements in \\(A\\), \\(B\\), or both \\(A\\) and \\(B\\))\n\\(A \\cap B\\): intersection (the set of all elements in both \\(A\\) and \\(B\\))\n\\(A \\setminus B\\): set difference (the set of all elements in \\(A\\) that are not in \\(B\\))\n\n\n\n\n\nLet \\(A = \\{a, b\\}\\) and \\(B = \\{b, d\\}\\), then\n\n\\(A \\cup B = \\{a, b, d\\}\\)\n\\(A \\cap B = \\{b\\}\\)\n\\(A \\setminus B = \\{a\\}\\)"
  },
  {
    "objectID": "Math Camp/section 1/section1.html#functions",
    "href": "Math Camp/section 1/section1.html#functions",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "",
    "text": "Function: A function is a mapping that relates members of one set to members of another set, usually denoted as \\(f: X \\to Y\\).\n\n\n\n\n\nMonomials and polynomials:\n\nMonomials: \\(f(x) = ax^n\\)\nPolynomials: \\(f(x) = \\sum_{k=0}^n a_k x^k\\)"
  },
  {
    "objectID": "Math Camp/section 1/section1.html#limit-of-a-function",
    "href": "Math Camp/section 1/section1.html#limit-of-a-function",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "2.1 Limit of A Function",
    "text": "2.1 Limit of A Function\n\nLimit: Solving the limit of a function implies finding the value that a function “approaches” as the input “approaches” some value.\n\n\nExample 2.1.1:\n\nConsider \\(f(x) = x + 2\\). The limit of \\(f(x)\\) at \\(x = 3\\) is 5."
  },
  {
    "objectID": "Math Camp/section 1/section1.html#evaluating-limits",
    "href": "Math Camp/section 1/section1.html#evaluating-limits",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "2.2 Evaluating Limits",
    "text": "2.2 Evaluating Limits\n\n2.2.1 Single Function\n\nProperties:\n\nIf \\(f(x) = ax + b\\) then \\(\\lim_{x \\to N} f(x) = aN + b\\)\n\n\n\n\n2.2.2 Two Functions\n\nSum-Difference Limit Theorem:\n\nIf \\(\\lim_{x \\to N} f(x) = L_1\\) and \\(\\lim_{x \\to N} g(x) = L_2\\), then:\n\n\\(\\lim_{x \\to N} [f(x) \\pm g(x)] = L_1 \\pm L_2\\)\n\n\n\n\n\n2.2.3 Tricky Cases\n\nApproach: If a function can be factored to eliminate a zero in the denominator, then the limit can be evaluated."
  },
  {
    "objectID": "Math Camp/section 1/section1.html#continuity-and-differentiability",
    "href": "Math Camp/section 1/section1.html#continuity-and-differentiability",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "3.1 Continuity and Differentiability",
    "text": "3.1 Continuity and Differentiability\n\nContinuity: A function is continuous at \\(x = c\\) if \\(\\lim_{x \\to c} f(x) = f(c)\\)."
  },
  {
    "objectID": "Math Camp/section 1/section1.html#rules-of-differentiation",
    "href": "Math Camp/section 1/section1.html#rules-of-differentiation",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "3.2 Rules of Differentiation",
    "text": "3.2 Rules of Differentiation\n\n3.2.1 Single Function/ Single Variable Differentiation\n\nConstant function rule: If \\(f(x) = k\\), then \\(f'(x) = 0\\)."
  },
  {
    "objectID": "Math Camp/section 1/section1.html#exercise-1-basic-operations-and-functions",
    "href": "Math Camp/section 1/section1.html#exercise-1-basic-operations-and-functions",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 1: Basic Operations and Functions",
    "text": "Exercise 1: Basic Operations and Functions\nCreate a new vector: Create a vector my.vector containing any five numbers. Print the vector. Basic calculations: Find the sum, product, and average of the numbers in my.vector.\n\n\nSample Solution\n# Create a vector with five numbers\nmy.vector &lt;- c(1, 3, 5, 7, 9)\nprint(my.vector)\n\n# Perform basic calculations\nsum(my.vector)\nprod(my.vector)\nmean(my.vector)\n\n\nUse a built-in function: Use the length() function to find the length of my.vector.\n\n\nSample Solution\n# Use a built-in function\nlength(my.vector)"
  },
  {
    "objectID": "Math Camp/section 1/section1.html#exercise-2-vector-manipulation",
    "href": "Math Camp/section 1/section1.html#exercise-2-vector-manipulation",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 2: Vector Manipulation",
    "text": "Exercise 2: Vector Manipulation\nCreate and modify a vector: Create a numeric vector numbers from 1 to 20. Then, extract and print the first 5 elements.\n\n\nSample Solution\n# Create and modify a vector\nnumbers &lt;- 1:20\nprint(numbers[1:5])\n\n\nLogical indexing: From numbers, create a new vector even.numbers that contains only the even numbers. Print even.numbers.\n\n\nSample Solution\n# Logical indexing for even numbers\neven.numbers &lt;- numbers[numbers %% 2 == 0]\nprint(even.numbers)\n\n\nVector arithmetic: Create a new vector that is the square of each element in numbers.\n\n\nSample Solution\nsquared.numbers &lt;- numbers^2\nprint(squared.numbers)"
  },
  {
    "objectID": "Math Camp/section 1/section1.html#exercise-3-matrices",
    "href": "Math Camp/section 1/section1.html#exercise-3-matrices",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 3: Matrices",
    "text": "Exercise 3: Matrices\nCreate a matrix: Convert numbers into a \\(4 \\times 5\\) matrix matrix.1. Print matrix.1.\n\n\nSample Solution\nmatrix.1 &lt;- matrix(numbers, nrow = 4, ncol = 5)\nprint(matrix.1)\n\n\nMatrix transposition: Print the transpose of matrix.1.\n\n\nSample Solution\n# Matrix transposition\nprint(t(matrix.1))\n\n\nMatrix indexing: Extract and print the element in the 2nd row and 3rd column of matrix.1.\n\n\nSample Solution\n# Matrix indexing\nprint(matrix.1[2, 3])"
  },
  {
    "objectID": "Math Camp/section 1/section1.html#exercise-4-logical-statements",
    "href": "Math Camp/section 1/section1.html#exercise-4-logical-statements",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 4: Logical Statements",
    "text": "Exercise 4: Logical Statements\nSimple if-else: Write an if-else statement that prints “Big” if the average of numbers is greater than 10, and “Small” otherwise.\n\n\nSample Solution\n# Simple if-else\nif (mean(numbers) &gt; 10) {\n  print(\"Big\")\n} else {\n  print(\"Small\")\n}\n\n\nNested if-else: Modify the above to include a check if the average is exactly 10, printing “Exactly 10”.\n\n\nSample Solution\n# Nested if-else\nif (mean(numbers) == 10) {\n  print(\"Exactly 10\")\n} else if (mean(numbers) &gt; 10) {\n  print(\"Big\")\n} else {\n  print(\"Small\")\n}"
  },
  {
    "objectID": "Math Camp/section 1/section1.html#exercise-5-random-numbers",
    "href": "Math Camp/section 1/section1.html#exercise-5-random-numbers",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 5: Random Numbers",
    "text": "Exercise 5: Random Numbers\nGenerate random numbers: Generate a vector of 5 random numbers drawn from a normal distribution with mean 0 and standard deviation 1. Print the vector.\n\n\nSample Solution\n# Generate random numbers\nrandom.numbers &lt;- rnorm(5)\nprint(random.numbers)\n\n\nReproducibility: Set a seed of your choice and generate the same vector of random numbers as above.\n\n\nSample Solution\n# Generate random numbers\nset.seed(222) # Set seed for reproducibility\nrandom.numbers &lt;- rnorm(5)\nprint(random.numbers)"
  },
  {
    "objectID": "Math Camp/section 1/section1.html#exercise-6-data-frames",
    "href": "Math Camp/section 1/section1.html#exercise-6-data-frames",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 6: Data Frames",
    "text": "Exercise 6: Data Frames\nExplore college.data: Print the first 6 rows of college.data.\n\n\nSample Solution\n# Explore `college.data`\nhead(college.data)\n\n\nColumn operations: Calculate the mean of the PhD column in college.data.\n\n\nSample Solution\n# Column operations\nmean(college.data$PhD)\n\n\nSubsetting: Create a new data frame small.college that only includes colleges with less than 5000 students (use college.data$Enroll for enrollment numbers).\n\n\nSample Solution\n# Subsetting data frame\nsmall.college &lt;- college.data[college.data$Enroll &lt; 5000, ]\nprint(small.college)"
  },
  {
    "objectID": "posts/2024-10-08 Batching/2024-10-08 Batching.html",
    "href": "posts/2024-10-08 Batching/2024-10-08 Batching.html",
    "title": "My Paper Summary: A Scrollytelling Experience",
    "section": "",
    "text": "Introduction\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n\n\n\n\nMethodology\n\n\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n\n\n\n\nResults\n\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo.\n\n\n\n\nConclusion\n\n\nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit.\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "posts/2024-10-08 Batching/2024-10-08 Batching.html#introduction",
    "href": "posts/2024-10-08 Batching/2024-10-08 Batching.html#introduction",
    "title": "Scrollytelling Summary of My Paper",
    "section": "",
    "text": "Introduce the main theme of your paper."
  },
  {
    "objectID": "posts/2024-10-08 Batching/2024-10-08 Batching.html#introduction-1",
    "href": "posts/2024-10-08 Batching/2024-10-08 Batching.html#introduction-1",
    "title": "Scrollytelling Summary of My Paper",
    "section": "Introduction",
    "text": "Introduction\nIntroduce the main theme of your paper.\n\n\nKey Concept 1\nExplain the first key concept.\n\n\n\n\nKey Concept 2\nExplain the second key concept.\n\n\n\n\nConclusion\nSummarize your findings."
  },
  {
    "objectID": "api 222 files/section 7/section 7.html#regression-splines",
    "href": "api 222 files/section 7/section 7.html#regression-splines",
    "title": "Section 7 - Non-linear Models",
    "section": "Regression Splines",
    "text": "Regression Splines\n\nSplines\nWe’re going to start by making an age grid, so that we test our predictions on a grid of evenly spaced ages so that you capture the functional form well.\nA quick aside, na.rm = TRUE means exlucde missing observations. Then we are going to run a spline with knots at ages 25, 40, and 60.\n\nlibrary(splines)\n\nCreate the age grids\n\nage_grid &lt;- seq(from = min(wage_data$age, na.rm = TRUE),\n                to = max(wage_data$age, na.rm = TRUE))\n\nNow we use a basis function\n\nspline_age &lt;- lm(wage ~ bs(age, knots = c(25, 40, 60)), \n                 data = wage_data[train,])\n\n?bs # to check what the basis function does\n\nGet the predictions at the grid points we defined earlier\n\nspline_age_grid_pred  &lt;- predict(spline_age, \n                                 newdata = list(age = age_grid), \n                                 se = TRUE)\n\nPlot age on the x-axis and wage on the y-axis for the test data. Then add the predictions and the confidence intervals\n\nplot(wage_data[test, \"age\"], wage_data[test, \"wage\"],\ncex = 0.5, col = \"darkgrey\",\nxlab = \"age\", ylab = \"wage\")\nlines(age_grid, spline_age_grid_pred$fit, lwd = 2)\nlines(age_grid, spline_age_grid_pred$fit + \n        2 * spline_age_grid_pred$se, lty =\"dashed\")\nlines(age_grid, spline_age_grid_pred$fit - \n        2 * spline_age_grid_pred$se, lty =\"dashed\")\n\n\n\n\n\n\nNatural Splines\nIf we instead wanted to fit a natural spline, we use ns() instead of knots, we can specify degrees of freedom. In this case we are going to use 4 degrees of freedom.\n\nns_age_poly &lt;- lm(wage ~ ns(age, df = 4), data = wage_data[train,])\nns_age_grid_poly_pred &lt;- predict(ns_age_poly,\n                                 newdata = list(age = age_grid),\n                                 se = TRUE)\n\nplot(wage_data[test, \"age\"], wage_data[test, \"wage\"], \n     cex = 0.5, col = \"darkgrey\",\n     xlab = \"age\", ylab = \"wage\")\nlines(age_grid, ns_age_grid_poly_pred$fit, lwd = 2)\nlines(age_grid, ns_age_grid_poly_pred$fit + \n        2 * ns_age_grid_poly_pred$se, lty = \"dashed\")\nlines(age_grid, ns_age_grid_poly_pred$fit - \n        2 * ns_age_grid_poly_pred$se, lty = \"dashed\")\n\n\n\n\n\n\nSmoothing Splines\nTo fit a smoothing spline, we use smooth.spline(). We can specify our own df\n\nsmooth_age &lt;- smooth.spline(wage_data[train,\"age\"], \n                            wage_data[train, \"wage\"], \n                            df = 16)\n\nOr we can use cross Validation to get optimal df and penalty\n\nsmoothCV_age &lt;- smooth.spline(wage_data[train, \"age\"], \n                              wage_data[train, \"wage\"], \n                              cv = TRUE) # specify we want to use CV\nsmoothCV_age$df\n\n[1] 6.786486\n\n\nPlot the results as before\n\nplot(wage_data[test, \"age\"], wage_data[test, \"wage\"],\n     cex =.5, col = \"darkgrey\",\n     xlab = \"age\", ylab = \"wage\")\ntitle(\" Smoothing Spline \")\nlines(smooth_age, col =\"red\", lwd = 2)\nlines(smoothCV_age, col =\"blue\", lwd =2)\n\n\n\n\n\n\nLocal Regression\nLocal Regression use loess()\nNote: span = 0.2 makes neighborhoods with 20% of observations. Span = 0.5 creates neighborhoods with 50% of observations. So the larger the span, the smoother the fit\n\n# plot age and wage and add geom_smooth\nlibrary(ggplot2)\nggplot(wage_data[test,], aes(x = age, y = wage, color = education)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", span = 0.2) +\n  geom_smooth(method = \"loess\", span = 0.5) \n\n\n\n\n\nlocal2_age &lt;- loess(wage ~ age, span = 0.2, data = wage_data)\nlocal5_age &lt;- loess(wage ~ age, span = 0.5, data = wage_data)\n\nGet the predictions\n\npred_local2_age &lt;- predict(local2_age, newdata = data.frame(age = age_grid))\npred_local5_age &lt;- predict(local5_age, newdata = data.frame(age = age_grid))\n\nPlot the results\n\nplot(wage_data[test, \"age\"], wage_data[test, \"wage\"],\n     cex =.5, col = \"darkgrey\",\n     xlab = \"age\", ylab = \"wage\")\nlines(age_grid, pred_local2_age, col = \"red\", lwd = 2)\nlines(age_grid, pred_local5_age, col = \"blue\", lwd = 2)\n\n\n\n\n\n\nGAMs\nGeneralized Additive Models (GAMs)\nStart with using natural spline functions of year and age and treating education as a qualitative variable does not require any special packages\n\nExample 1: GAMs with splines\n\n\ngam_yae &lt;- lm(wage ~ ns(year, 4) + ns(age, 4) + education, \n              data = wage_data[train,])\n\n\nExample 2: GAMs with smoothing splines\n\n\nlibrary(gam)\n\nIn the gam library s() indicates that we want to use a smoothing spline\n\ngam_smooth &lt;- gam(wage ~ s(year, 4, spar = 0.5) + s(age, 5) + education,\n                  data = wage_data[train,])\n\nPlot the model results\n\npar(mfrow = c(1, 3))\nplot(gam_smooth, se = TRUE, col =\"blue \")\n\n\n\n\nTo plot the GAM we created just using lm(), we can use plot.Gam()\n\nplot.Gam(gam_yae, se = TRUE, col = \"red\") # Note the capitalization\n\n\n\n\n\n\n\n\n\n\nMake predictions\n\ngam_yae_pred &lt;- predict(gam_yae, newdata = wage_data[test,])\ngam_smooth_pred &lt;- predict(gam_smooth, newdata = wage_data[test,])\n\nPrint out the MSEP for these two GAMs using the function we created at the start of class\n\nprint(msep_func(gam_yae_pred, wage_data[test, \"wage\"]))\n\n[1] 1304.807\n\nprint(msep_func(gam_smooth_pred, wage_data[test, \"wage\"]))\n\n[1] 1300.582\n\n\n\nExample 3: GAMs with local regression\n\nTo use local regression in GAMs, use lo()\n\ngam_lo &lt;- gam(wage ~ s(year, df = 4) + lo(age, span = 0.7) + education, \n              data = wage_data[train,])\n\nPlot the results\n\nplot.Gam(gam_lo, se = TRUE, col = \"green\")\n\n\n\n\n\n\n\n\n\n\nIf you want to do a local regression in two variables:\n\ngam_2lo &lt;- gam(wage ~ lo(year, age, span =0.5) + education,\n               data = wage_data[train,])\n\nWarning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, bf.maxit, :\nliv too small.  (Discovered by lowesd)\n\n\nWarning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, bf.maxit, : lv\ntoo small.  (Discovered by lowesd)\n\n\nWarning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, bf.maxit, :\nliv too small.  (Discovered by lowesd)\n\n\nWarning in lo.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, bf.maxit, : lv\ntoo small.  (Discovered by lowesd)\n\n?s\n\nHelp on topic 's' was found in the following packages:\n\n  Package               Library\n  gam                   /opt/homebrew/Cellar/r/4.4.2_2/lib/R/library\n  mgcv                  /opt/homebrew/Cellar/r/4.4.2_2/lib/R/library\n\n\nUsing the first match ..."
  },
  {
    "objectID": "git files/section 1/Instructions.html",
    "href": "git files/section 1/Instructions.html",
    "title": "Git Installation Instructions",
    "section": "",
    "text": "Install homebrew following the instructions at https://brew.sh/.\nOpen Terminal and check if git is already installed.\ngit --version\nIf not, install git using brew install git. Then verify, its installed by running git --version.\n\n\n\n\n\nGo to gitforwindows.org. Make sure you include Git Bash in your installation!"
  },
  {
    "objectID": "git files/section 1/Instructions.html#part-i-installing-git",
    "href": "git files/section 1/Instructions.html#part-i-installing-git",
    "title": "Git Installation Instructions",
    "section": "",
    "text": "Install homebrew following the instructions at https://brew.sh/.\nOpen Terminal and check if git is already installed.\ngit --version\nIf not, install git using brew install git. Then verify, its installed by running git --version.\n\n\n\n\n\nGo to gitforwindows.org. Make sure you include Git Bash in your installation!"
  },
  {
    "objectID": "git files/section 1/Instructions.html#part-ii-creating-a-github-account",
    "href": "git files/section 1/Instructions.html#part-ii-creating-a-github-account",
    "title": "Git Installation Instructions",
    "section": "Part II: Creating a GitHub Account",
    "text": "Part II: Creating a GitHub Account\n\nGo to GitHub.\nClick the “Sign Up” button.\nFollow the on-screen instructions to create your account."
  },
  {
    "objectID": "git files/section 1/Instructions.html#part-iii-git-setup",
    "href": "git files/section 1/Instructions.html#part-iii-git-setup",
    "title": "Git Installation Instructions",
    "section": "Part III: Git Setup",
    "text": "Part III: Git Setup\n\nConfigure git with your name and email address. Be sure to use the same email associated with your Github account.\ngit config --global user.name \"YOUR NAME\"\ngit config --global user.email \"YOUR EMAIL ADDRESS\""
  },
  {
    "objectID": "git files/section 1/Instructions.html#part-iv-ssh",
    "href": "git files/section 1/Instructions.html#part-iv-ssh",
    "title": "Git Installation Instructions",
    "section": "Part IV: SSH",
    "text": "Part IV: SSH\nIn order to write code locally on our computer and be able to push to GitHub (or pull from GitHub) daily without constantly having to enter a username and password each time, we’re going to set up SSH keys.\n\nSSH keys come in pairs, a public key that gets shared with services like GitHub, and a private key that is stored only on your computer. If the keys match, you’re granted access.\nThe cryptography behind SSH keys ensures that no one can reverse engineer your private key from the public one.\nsource: https://jdblischak.github.io/2014-09-18-chicago/novice/git/05-sshkeys.html\n\nThe following steps are a simplification of the steps found in GitHub’s documentation. If you prefer, feel free to follow the steps at that link. Otherwise, for a simplified experience continue on below!\nSimplified Setup Steps\n\nStep 1: Check to see if you already have keys.\nRun the following command.\nls -al ~/.ssh/\nIf you see any output, that probably means you already have a public and private SSH key. If you have keys, you will most likely you will have two files, one named id_rsa (that contains your private key) and id_rsa.pub (that contains your public key).\nsidenote: Those files may also be named something like: id_ecdsa.pub or id_ed25519.pub. That just means you’re using a different encryption algorithm to generate your keys. You can learn more about that here if you chose to. Or, don’t worry about it and power on!\nIf you already have keys, continue to step 3. Otherwise, read on!\nStep 2: Create new SSH keys.\nRun the following comamnd, but makes sure to replace your_email@example.com with your own email address. Use the same email address you used to sign up to GitHub with.\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\nYou may then see a prompt like the one below. Just hit enter to save the key in the default location.\nEnter file in which to save the key (/Users/jacob/.ssh/id_rsa):\nAfter that, the system will prompt you to enter a passphrase. We’re not going to use a passphrase here, so just go ahead and leave that blank and hit enter twice.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nFinally you should see some randomart that looks like this\nYour identification has been saved in /Users/jacob/.ssh/id_rsa.\nYour public key has been saved in /Users/jacob/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:2AazdvCBP8d1li9tF8cszM2KbtjPe7iwfCK8gUgzIGY your_email@example.com\nThe key's randomart image is:\n+---[RSA 4096]----+\n|                 |\n|       .     o * |\n|  E . = .   . B.*|\n| o . . X o . + =o|\n|      B S o . o =|\n|     o * + +   o.|\n|      . ..o =  . |\n|          o+.=o .|\n|          .ooo=+ |\n+----[SHA256]-----+\nStep 3: Add your key to GitHub\nRun the following command to view your public key\ncat ~/.ssh/id_rsa.pub\nNavigate to https://github.com/settings/keys and hit “New SSH key”. Paste the SSH key from the last command into the text box as shown below and then hit “Add SSH key”. Make sure you copy paste exactly. The key will likely start with ssh_rsa and end with your email address. You can give the key a title like “My Macbook Pro” so you know which computer this key comes from.\n\nStep 4: Verify that it worked!\nRun the following command to test your computer’s SSH connection to GitHub\nssh -T git@github.com\nIf the connection is successful, you will see a message like this\n&gt; Hi username! You've successfully authenticated, but GitHub does not\n&gt; provide shell access.\n\n\nRecap: What did we just do?\nWe just created a public/private SSH Key pair. There is now a folder on your computer called .ssh (it is a hidden folder, hidden folders have names that start with .). You can run this command to see the files in that folder.\nls -al ~/.ssh/\nid_rsa.pub contains your public key, you can see what that looks like by running:\ncat ~/.ssh/id_rsa.pub\nid_rsa contains your private key, you can see what that looks like by running:\ncat ~/.ssh/id_rsa\nThis public and private key pair are mathematically linked. As the name suggests, you can share your public key far and wide, but must keep your private key safe and secure. Since you have shared your public key with GitHub, your computer can encrypt files with your private key and send them to GitHub. Since GitHub has your public key, it can match that file and verify that it is coming from you. Your computer can now securely communicate with GitHub without needing a username and password every time.\nTo get started with Git, you need to install it on your computer. You can download the installer from the Git website. Once you have Git installed, you can use the git command in your terminal to interact with Git."
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Research",
    "section": "",
    "text": "Authors: Jacob Jameson, Soroush Saghafian, Robert S. Huckman, Nathaniel Hodgson\n\n\nJournal: Health Services Research, 2024, Forthcoming\n\n\n\n\n\nAuthors: Nathan J. Glasser, Jacob C. Jameson, Elbert S. Huang, et al.\n\n\nJournal: JAMA Network Open, 2024, 7(10): e2441281"
  },
  {
    "objectID": "posts/2025-02-09-BL/BL.html",
    "href": "posts/2025-02-09-BL/BL.html",
    "title": "A Gentle Introduction to Bayesian Linear Regression in R",
    "section": "",
    "text": "Linear regression is a fundamental tool for modeling relationships between an outcome variable and one or more predictors. Traditionally, we often do this via ordinary least squares (OLS). In the frequentist approach, we estimate regression coefficients by finding values that best explain the observed data. The uncertainty in these estimates is represented by confidence intervals (CIs), which describe the expected variation if we were to repeat the entire data-collection process many times.\nHowever, confidence intervals are often misinterpreted. Many people assume a 95% confidence interval means, “There is a 95% probability that the true parameter lies within this interval.” In reality, this is incorrect—CIs do not assign probabilities to parameters. Instead, they describe how often the interval would contain the true parameter across many repeated samples, which is not the way we typically think about uncertainty.\nIn contrast, Bayesian credible intervals behave the way we actually want these things to work. A 95% credible interval does mean that, given our observed data and model, “There is a 95% probability that the true parameter lies within this range.” This makes Bayesian inference particularly appealing when we want intuitive probability statements about our parameters.\nBayesian linear regression treats model parameters (e.g., intercept, slope) as random variables with their own probability distributions. Instead of producing a single “best” estimate for each coefficient, we derive a posterior distribution, which represents a range of possible values for each parameter, given both:\n\nOur prior beliefs (before seeing the data)\nThe likelihood of the observed data under the model assumptions\n\nThe result is a credible interval, which gives us a direct probability statement about our parameters, making it a powerful tool in statistical modeling.\nIn this post, we’ll work through an example in R to see:\n\nHow the posterior distribution updates from the prior distribution.\nHow to visualize priors versus posteriors.\nHow to interpret Bayesian predictions (including posterior predictive distributions)."
  },
  {
    "objectID": "posts/2025-02-09-BL/BL.html#summaries-of-the-bayesian-posterior",
    "href": "posts/2025-02-09-BL/BL.html#summaries-of-the-bayesian-posterior",
    "title": "A Gentle Introduction to Bayesian Linear Regression",
    "section": "Summaries of the Bayesian Posterior",
    "text": "Summaries of the Bayesian Posterior\nWe can extract draws from the posterior and summarize them, or we can rely on the built-in print methods. Another useful tool is the posterior_interval() function:\n\nposterior_interval(bayes_model, prob = 0.95)\n\n                 2.5%    97.5%\n(Intercept) 0.6029273 3.131793\nx           3.2470775 4.107433\nsigma       2.8206029 3.710507\n\n\nThis provides 95% credible intervals for each parameter.\nWe can visualize the parameter distributions with the bayesplot package or rstanarm’s built-in plotting:\n\nplot(bayes_model, plotfun = \"hist\", pars = c(\"(Intercept)\", \"x\"), \n     include = TRUE, prob = 0.95)\n\n\n\n\nEach histogram shows the distribution of plausible values of the intercept and slope according to the posterior, along with a 95% credible interval band.\n\n# Posterior means for intercept and slope\nbeta0_post &lt;- coef(bayes_model)[1]  # intercept\nbeta1_post &lt;- coef(bayes_model)[2]  # slope\n\n# Predictions from Bayesian fit\nbayes_pred &lt;- beta0_post + beta1_post * x\n\nsim_data$bayes_pred &lt;- bayes_pred\n\nggplot(sim_data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_line(aes(y = ols_pred), color = \"blue\", size = 1) +\n  geom_line(aes(y = bayes_pred), color = \"green\", linetype = \"dashed\", size = 1) +\n  geom_abline(intercept = beta0_true, slope = beta1_true, color = \"red\", linetype = \"dotdash\") +\n  labs(title = \"OLS vs. Bayesian Fit\",\n       subtitle = \"Blue = OLS fit; Green dashed = Bayesian posterior mean fit; Red dotdash = True line\")"
  },
  {
    "objectID": "posts/2025-02-09-BL/BL.html#introduction-1",
    "href": "posts/2025-02-09-BL/BL.html#introduction-1",
    "title": "A Gentle Introduction to Bayesian Linear Regression",
    "section": "Introduction",
    "text": "Introduction\nLinear regression is a fundamental tool for modeling relationships between an outcome variable and one or more predictors. Traditionally, we often do this via ordinary least squares (OLS). In the frequentist mindset, we estimate regression coefficients and produce confidence intervals that (roughly speaking) capture the expected variability of those estimates if we were to repeat the entire data-collection process many times.\nIn Bayesian linear regression, parameters (intercept, slope, variance, etc.) are viewed as random variables, and we combine:\n\nA prior distribution over what values we think are plausible for the parameters (before seeing data),\nThe likelihood of the observed data,\nBayes’ rule,\n\nto yield a posterior distribution over the parameters. This posterior distribution directly shows which parameter values are plausible (or implausible) after seeing the data. We can then summarize the posterior using credible intervals, which have the straightforward interpretation: “Given our model and data, there is a 95% probability that the true parameter is in this interval.”\nIn this post, we’ll do a simple example in R to see:\n\nHow the posterior distribution updates from the prior distribution.\nHow to visualize priors versus posteriors.\nHow to interpret Bayesian predictions (including posterior predictive distributions)."
  },
  {
    "objectID": "posts/2025-02-09-BL/BL.html#simulate-some-data",
    "href": "posts/2025-02-09-BL/BL.html#simulate-some-data",
    "title": "A Gentle Introduction to Bayesian Linear Regression in R",
    "section": "1. Simulate Some Data",
    "text": "1. Simulate Some Data\nLet’s simulate a small dataset with a linear relationship. Suppose the “true” model is:\n\\[\ny = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\nwhere:\n\n\\(\\beta_0 = 2.0\\) (intercept),\n\\(\\beta_1 = 3.5\\) (slope),\n\\(\\varepsilon \\sim \\text{Normal}(0,1)\\) (noise).\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(rstanarm)  # for Bayesian regression\n\nset.seed(42)\n\nn &lt;- 50\nx &lt;- seq(0, 5, length.out = n)\nbeta0_true &lt;- 2.0\nbeta1_true &lt;- 3.5\nsigma_true &lt;- 1.0\n\n# Generate y\nnoise &lt;- rnorm(n, mean = 0, sd = sigma_true)\ny &lt;- beta0_true + beta1_true * x + noise\n\ndf &lt;- data.frame(x, y)\nhead(df)\n\n          x        y\n1 0.0000000 3.370958\n2 0.1020408 1.792445\n3 0.2040816 3.077414\n4 0.3061224 3.704291\n5 0.4081633 3.832840\n6 0.5102041 3.679590\n\n\n\nQuick Plot of the Data\n\nggplot(df, aes(x=x, y=y)) +\n  geom_point(color = \"#008Fd5\", alpha = 0.9, size=2) +\n  geom_abline(intercept=beta0_true, slope=beta1_true, \n              color=\"red\", linetype=\"dashed\") +\n  theme_minimal(base_size = 15) +\n  labs(title=\"Simulated Data with True Regression Line\",\n       subtitle=\"Red dashed line = true relationship\")"
  },
  {
    "objectID": "posts/2025-02-09-BL/BL.html#frequentist-ols-for-reference",
    "href": "posts/2025-02-09-BL/BL.html#frequentist-ols-for-reference",
    "title": "A Gentle Introduction to Bayesian Linear Regression in R",
    "section": "2. Frequentist OLS for Reference",
    "text": "2. Frequentist OLS for Reference\nFirst, we fit a simple OLS model:\n\nols_model &lt;- lm(y ~ x, data = df)\nsummary(ols_model)\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7403 -0.4366 -0.1193  0.8319  2.1072 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.3548     0.3175   7.416  1.7e-09 ***\nx             3.3438     0.1094  30.555  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.139 on 48 degrees of freedom\nMultiple R-squared:  0.9511,    Adjusted R-squared:  0.9501 \nF-statistic: 933.6 on 1 and 48 DF,  p-value: &lt; 2.2e-16\n\n\nWe get a point estimate for the intercept and slope plus confidence intervals."
  },
  {
    "objectID": "posts/2025-02-09-BL/BL.html#bayesian-linear-regression",
    "href": "posts/2025-02-09-BL/BL.html#bayesian-linear-regression",
    "title": "A Gentle Introduction to Bayesian Linear Regression in R",
    "section": "3. Bayesian Linear Regression",
    "text": "3. Bayesian Linear Regression\nNow let’s do a Bayesian version via the rstanarm package. Under the hood, stan_glm uses weakly informative priors by default (you can customize these).\n\nbayes_model &lt;- stan_glm(y ~ x, data=df, \n                        chains=2, iter=2000, seed=42)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.008109 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 81.09 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.042 seconds (Warm-up)\nChain 1:                0.039 seconds (Sampling)\nChain 1:                0.081 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3.1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.057 seconds (Warm-up)\nChain 2:                0.03 seconds (Sampling)\nChain 2:                0.087 seconds (Total)\nChain 2: \n\nprint(bayes_model, digits=3)\n\nstan_glm\n family:       gaussian [identity]\n formula:      y ~ x\n observations: 50\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 2.372  0.334 \nx           3.337  0.115 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.156  0.117 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nThe summary output gives us:\n\nPosterior mean: The most likely value for each coefficient given the data.\nStandard deviation: How uncertain we are about each estimate.\n95% credible interval: The range where the true coefficient likely falls with 95% probability.\n\nUnlike frequentist confidence intervals, which describe the long-run variability across repeated samples, Bayesian credible intervals provide a direct probability statement about our parameters.\n\n3.1 What Are Priors Here?\nBy default, stan_glm(..., family = gaussian()) uses something akin to a weakly informative prior on the slope and intercept. This means the prior allows a wide range of possible values for the coefficients but discourages extremely large magnitudes. You can supply arguments like prior, prior_intercept, etc., or switch to brms for more flexible syntax.\n\n\n3.2 Posterior Summaries\nThe printed output typically gives us:\n\nmean: Posterior mean of the parameter.\nsd: Posterior standard deviation (akin to “uncertainty”).\n2.5% / 97.5%: Bounds of the 95% credible interval.\n\nWe can visualize these distributions:\n\nplot(bayes_model, \n     plotfun = \"areas\",  \n     pars = c(\"(Intercept)\", \"x\"),  \n     include = TRUE, \n     prob = 0.95,  border = \"black\") + \n  theme_minimal(base_size = 15) + \n  labs(title = \"Posterior Distributions with 95% Credible Interval\",\n       x = \"Parameter Value\",\n       y = \"Density\")\n\n\n\n\nEach density plot is the posterior distribution for one parameter, showing the entire spread of plausible values given the model and data. The shaded areas give the 95% credible interval."
  },
  {
    "objectID": "posts/2025-02-09-BL/BL.html#explaining-the-posterior-more-explicitly",
    "href": "posts/2025-02-09-BL/BL.html#explaining-the-posterior-more-explicitly",
    "title": "A Gentle Introduction to Bayesian Linear Regression in R",
    "section": "4. Explaining the Posterior More Explicitly",
    "text": "4. Explaining the Posterior More Explicitly\n\n4.1 Combining Prior and Likelihood\nFrom a conceptual standpoint, Bayesian regression says:\n\\[\n\\text{Posterior}(\\beta_0, \\beta_1 \\mid \\text{data})\n\\;\\propto\\;\n\\text{Prior}(\\beta_0, \\beta_1) \\;\\times\\;\n\\text{Likelihood}(\\text{data} \\mid \\beta_0, \\beta_1).\n\\]\n\nPrior: What values of \\(\\beta_0, \\beta_1\\) are plausible before seeing any data?\nLikelihood: Given a candidate pair \\((\\beta_0, \\beta_1)\\), how well does it explain our observed \\(y\\) values?\nPosterior: The result of multiplying these together and renormalizing into a probability distribution.\n\nThe best way to see this in action is to do a “prior vs. posterior” plot for (say) the slope. Let’s do a quick example of customizing a prior so we can illustrate how it gets updated.\n\n4.1.1 A Very Simple Custom Prior Example\nSuppose we suspect the slope is likely around 1.0, with a standard deviation of 2. That means slopes near 1 are more plausible, but we allow for a broad range. Likewise, for the intercept, maybe we suspect a prior mean of 0 with a standard deviation of 10.\n\ncustom_bayes_model &lt;- stan_glm(\n  y ~ x,\n  data = df,\n  prior = normal(location = 1, scale = 2, autoscale=FALSE),       # slope prior\n  prior_intercept = normal(location = 0, scale = 10, autoscale=FALSE),\n  chains=2, iter=2000, seed=123\n)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.02 seconds (Warm-up)\nChain 1:                0.026 seconds (Sampling)\nChain 1:                0.046 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.039 seconds (Warm-up)\nChain 2:                0.036 seconds (Sampling)\nChain 2:                0.075 seconds (Total)\nChain 2: \n\nprint(custom_bayes_model, digits=3)\n\nstan_glm\n family:       gaussian [identity]\n formula:      y ~ x\n observations: 50\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 2.378  0.344 \nx           3.335  0.117 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.156  0.116 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nNow, let’s extract the prior and posterior draws and plot them. We can sample from the prior if we specify prior_PD = TRUE (prior predictive distribution).\n\n# Step 1: sample from prior only (no data used)\nprior_only_model &lt;- stan_glm(\n  y ~ x,\n  data = df,\n  prior = normal(location = 1, scale = 2, autoscale=FALSE),\n  prior_intercept = normal(location = 0, scale = 10, autoscale=FALSE),\n  chains=2, iter=2000, seed=123,\n  prior_PD = TRUE   # &lt;--- This means: ignore the likelihood of data\n)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.026 seconds (Warm-up)\nChain 1:                0.038 seconds (Sampling)\nChain 1:                0.064 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 4e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.041 seconds (Warm-up)\nChain 2:                0.043 seconds (Sampling)\nChain 2:                0.084 seconds (Total)\nChain 2: \n\n# Extract draws\nprior_draws &lt;- as.matrix(prior_only_model, pars=c(\"(Intercept)\",\"x\"))\npost_draws &lt;- as.matrix(custom_bayes_model, pars=c(\"(Intercept)\",\"x\"))\n\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\ndf_draws &lt;- data.frame(\n  prior_slope  = prior_draws[,\"x\"],\n  post_slope   = post_draws[,\"x\"]\n)\n\ndf_long &lt;- df_draws %&gt;% \n  pivot_longer(cols = everything(), \n               names_to = \"distribution\", values_to=\"slope_value\") %&gt;%\n  mutate(distribution = ifelse(\n    distribution == \"prior_slope\", \"Prior Distribution\", \n    \"Posterior Distribution\"))\n\nggplot(df_long, aes(x=slope_value, fill=distribution)) +\n  geom_density(alpha=0.4) +\n  scale_fill_manual(values=c(\"Prior Distribution\" = \"#D55E00\", \n                             \"Posterior Distribution\" = \"#0072B2\")) +\n  labs(title=\"Slope: Prior vs Posterior\",\n       x=\"Slope Value\", \n       fill = \"Distribution\") +\n  theme_minimal(base_size = 14)\n\n\n\n\n\nThe red or orange curve will show the prior distribution for the slope (centered near 1, quite wide).\nThe blue curve will show the posterior after seeing the data. Because our true slope is actually 3.5, you’ll see that the posterior is pulled far to the right of the prior’s center of 1. The data “pushes” the posterior to align with the actual effect in the data, reducing uncertainty."
  },
  {
    "objectID": "posts/2025-02-09-BL/BL.html#summary-key-takeaways",
    "href": "posts/2025-02-09-BL/BL.html#summary-key-takeaways",
    "title": "A Gentle Introduction to Bayesian Linear Regression",
    "section": "5. Summary & Key Takeaways",
    "text": "5. Summary & Key Takeaways\n\nFrequentist OLS\n\nYields point estimates and confidence intervals with a repeated-sampling interpretation.\n\nDoesn’t easily let you incorporate prior information.\n\nBayesian Regression\n\nUses priors to encode what values are plausible or likely before seeing data.\n\nData “update” the priors, yielding a posterior distribution of plausible parameter values.\n\nCredible intervals have a direct interpretation about parameter uncertainty.\n\nPosterior predictive distributions enable probability-based questions, such as “What is the probability that \\(y_* &gt; c\\)?”\n\n\nIn simple linear models, if you have plenty of data, the OLS estimates and Bayesian posterior means often match closely. But in Bayesian analysis, you have more flexible ways to incorporate domain expertise and make direct statements about probabilities of parameters or predictions. This can be especially valuable in health and policy decision-making contexts, where prior evidence and direct probability statements are often crucial.\nFurther Reading:\n\nIntroduction to Bayesian Statistics by William Bolstad.\nBayesian Data Analysis by Gelman et al. \nThe rstanarm, brms, or bayesplot packages in R for specifying, fitting, and visualizing Bayesian models.\n\nThat’s it for this walkthrough! With this framework, you can build an intuition for how Bayesian linear regression works, how priors influence the model, and how you can interpret posterior distributions in a more direct, probability-based way."
  },
  {
    "objectID": "posts/2025-02-09-BL/BL.html#introduction",
    "href": "posts/2025-02-09-BL/BL.html#introduction",
    "title": "A Gentle Introduction to Bayesian Linear Regression in R",
    "section": "",
    "text": "Linear regression is a fundamental tool for modeling relationships between an outcome variable and one or more predictors. Traditionally, we often do this via ordinary least squares (OLS). In the frequentist approach, we estimate regression coefficients by finding values that best explain the observed data. The uncertainty in these estimates is represented by confidence intervals (CIs), which describe the expected variation if we were to repeat the entire data-collection process many times.\nHowever, confidence intervals are often misinterpreted. Many people assume a 95% confidence interval means, “There is a 95% probability that the true parameter lies within this interval.” In reality, this is incorrect—CIs do not assign probabilities to parameters. Instead, they describe how often the interval would contain the true parameter across many repeated samples, which is not the way we typically think about uncertainty.\nIn contrast, Bayesian credible intervals behave the way we actually want these things to work. A 95% credible interval does mean that, given our observed data and model, “There is a 95% probability that the true parameter lies within this range.” This makes Bayesian inference particularly appealing when we want intuitive probability statements about our parameters.\nBayesian linear regression treats model parameters (e.g., intercept, slope) as random variables with their own probability distributions. Instead of producing a single “best” estimate for each coefficient, we derive a posterior distribution, which represents a range of possible values for each parameter, given both:\n\nOur prior beliefs (before seeing the data)\nThe likelihood of the observed data under the model assumptions\n\nThe result is a credible interval, which gives us a direct probability statement about our parameters, making it a powerful tool in statistical modeling.\nIn this post, we’ll work through an example in R to see:\n\nHow the posterior distribution updates from the prior distribution.\nHow to visualize priors versus posteriors.\nHow to interpret Bayesian predictions (including posterior predictive distributions)."
  },
  {
    "objectID": "posts/2025-02-09-BL/BL.html#posterior-predictive-distribution-why-it-matters",
    "href": "posts/2025-02-09-BL/BL.html#posterior-predictive-distribution-why-it-matters",
    "title": "A Gentle Introduction to Bayesian Linear Regression in R",
    "section": "Posterior Predictive Distribution: Why It Matters",
    "text": "Posterior Predictive Distribution: Why It Matters\nSo far, we’ve focused on estimating the posterior distribution of our regression parameters. However, in most real-world applications, we’re not just interested in estimating coefficients—we want to use our model to make predictions about future observations.\nIn a frequentist regression, we typically obtain a point prediction:\n\\[\n\\hat{y}_* = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_*\n\\]\nalong with a prediction interval that accounts for both residual variability and parameter uncertainty.\n\nThe Bayesian Approach: Uncertainty in Predictions\nIn Bayesian regression, we don’t just compute a single best guess for \\(y_*\\)—we generate an entire posterior predictive distribution, which tells us:\n\nThe most likely values of \\(y_*\\) based on our model.\nThe full range of plausible values, incorporating both parameter uncertainty and residual variability.\nThe probability of exceeding a critical threshold, which is extremely useful for decision-making.\n\nExample Use Case:\n\nSuppose \\(x_* = 2.5\\) represents the duration of a hospital stay, and \\(y_*\\) represents the total cost of treatment (in thousands).\nA hospital administrator might ask:\n\n“What is the probability that this patient’s costs will exceed $10,000?”\n“How much uncertainty is there in our cost estimate?”\n\nInstead of a single prediction, the Bayesian posterior predictive distribution provides a full range of possible outcomes, making it much more informative.\n\n\n\nSimulating from the Posterior Predictive Distribution\nWe generate predictions for a new observation \\(x_*\\) by sampling from the posterior:\n\\[\ny_* \\sim \\text{Normal}(\\beta_0 + \\beta_1 x_*, \\sigma^2)\n\\]\nwhere \\(\\beta_0, \\beta_1, \\sigma\\) are drawn from their posterior distributions.\nLet’s compute this for \\(x_* = 2.5\\) and visualize the range of plausible values for \\(y_*\\).\n\n# Posterior draws for intercept, slope, and sigma\npost_draws_full &lt;- as.matrix(custom_bayes_model)\n\n# Function to simulate predictions from posterior\nsimulate_posterior_y &lt;- function(x_star, n_sims=4000) {\n  idx_int &lt;- which(colnames(post_draws_full)==\"(Intercept)\")\n  idx_slope &lt;- which(colnames(post_draws_full)==\"x\")\n  idx_sigma &lt;- which(colnames(post_draws_full)==\"sigma\")\n\n  intercept_samples &lt;- post_draws_full[, idx_int]\n  slope_samples     &lt;- post_draws_full[, idx_slope]\n  sigma_samples     &lt;- post_draws_full[, idx_sigma]\n\n  # Compute predicted means, then sample from Normal(mean, sigma)\n  mu_star &lt;- intercept_samples + slope_samples * x_star\n  y_sim &lt;- rnorm(n_sims, mean=mu_star, sd=sigma_samples)\n  y_sim\n}\n\nx_star &lt;- 2.5\ny_sim &lt;- simulate_posterior_y(x_star)\n\n# Probability that y_* &gt; 10\nmean(y_sim &gt; 10)\n\n[1] 0.72675\n\n\nA histogram of simulated outcomes shows the full range of possible \\(y_*\\) values, allowing us to make probability-based decisions.\n\nlibrary(ggplot2)\n\nggplot(data.frame(y_sim), aes(y_sim)) +\n  geom_histogram(bins=30, fill=\"#D55E00\", color=\"white\") +\n  theme_minimal(base_size = 14) +\n  geom_vline(xintercept=10, color=\"black\", linetype=\"solid\", size=1.5) +\n  labs(title=\"Posterior Predictive Distribution at x=2.5\",\n       subtitle=\"Black line = threshold of 10\",\n       x=\"Possible y* values\", y=\"Frequency\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nIn this case, the probability that \\(y_*\\) exceeds $10,000 is approximately 73%. This information is crucial for making informed decisions about resource allocation, risk management, or policy interventions."
  },
  {
    "objectID": "posts/2025-02-09-BL/BL.html#key-takeaways-why-this-matters",
    "href": "posts/2025-02-09-BL/BL.html#key-takeaways-why-this-matters",
    "title": "A Gentle Introduction to Bayesian Linear Regression in R",
    "section": "Key Takeaways: Why This Matters",
    "text": "Key Takeaways: Why This Matters\nUnlike traditional frequentist regression, Bayesian posterior predictive distributions allow us to answer probabilistic questions about new data points:\n\nPoint Estimate + Full Uncertainty: Instead of a single predicted \\(y_*\\), we get a distribution over plausible values.\nProbability-Based Decisions: We can compute the probability that an outcome exceeds (or falls below) a critical threshold.\nMore Robust Uncertainty Quantification: Since we account for both parameter uncertainty and residual variance, our predictions are more realistic.\n\nFurther Reading:\n\nIntroduction to Bayesian Statistics by William Bolstad.\nBayesian Data Analysis by Gelman et al. \nThe rstanarm, brms, or bayesplot packages in R for specifying, fitting, and visualizing Bayesian models."
  },
  {
    "objectID": "posts/2025-02-09-BL/2025-02-09-BL.html",
    "href": "posts/2025-02-09-BL/2025-02-09-BL.html",
    "title": "A Gentle Introduction to Bayesian Linear Regression in R",
    "section": "",
    "text": "Linear regression is a fundamental tool for modeling relationships between an outcome variable and one or more predictors. Traditionally, we often do this via ordinary least squares (OLS). In the frequentist approach, we estimate regression coefficients by finding values that best explain the observed data. The uncertainty in these estimates is represented by confidence intervals (CIs), which describe the expected variation if we were to repeat the entire data-collection process many times.\nHowever, confidence intervals are often misinterpreted. Many people assume a 95% confidence interval means, “There is a 95% probability that the true parameter lies within this interval.” In reality, this is incorrect—CIs do not assign probabilities to parameters. Instead, they describe how often the interval would contain the true parameter across many repeated samples, which is not the way we typically think about uncertainty.\nIn contrast, Bayesian credible intervals behave the way we actually want these things to work. A 95% credible interval does mean that, given our observed data and model, “There is a 95% probability that the true parameter lies within this range.” This makes Bayesian inference particularly appealing when we want intuitive probability statements about our parameters.\nBayesian linear regression treats model parameters (e.g., intercept, slope) as random variables with their own probability distributions. Instead of producing a single “best” estimate for each coefficient, we derive a posterior distribution, which represents a range of possible values for each parameter, given both:\n\nOur prior beliefs (before seeing the data)\nThe likelihood of the observed data under the model assumptions\n\nThe result is a credible interval, which gives us a direct probability statement about our parameters, making it a powerful tool in statistical modeling.\nIn this post, we’ll work through an example in R to see:\n\nHow the posterior distribution updates from the prior distribution.\nHow to visualize priors versus posteriors.\nHow to interpret Bayesian predictions (including posterior predictive distributions)."
  },
  {
    "objectID": "posts/2025-02-09-BL/2025-02-09-BL.html#introduction",
    "href": "posts/2025-02-09-BL/2025-02-09-BL.html#introduction",
    "title": "A Gentle Introduction to Bayesian Linear Regression in R",
    "section": "",
    "text": "Linear regression is a fundamental tool for modeling relationships between an outcome variable and one or more predictors. Traditionally, we often do this via ordinary least squares (OLS). In the frequentist approach, we estimate regression coefficients by finding values that best explain the observed data. The uncertainty in these estimates is represented by confidence intervals (CIs), which describe the expected variation if we were to repeat the entire data-collection process many times.\nHowever, confidence intervals are often misinterpreted. Many people assume a 95% confidence interval means, “There is a 95% probability that the true parameter lies within this interval.” In reality, this is incorrect—CIs do not assign probabilities to parameters. Instead, they describe how often the interval would contain the true parameter across many repeated samples, which is not the way we typically think about uncertainty.\nIn contrast, Bayesian credible intervals behave the way we actually want these things to work. A 95% credible interval does mean that, given our observed data and model, “There is a 95% probability that the true parameter lies within this range.” This makes Bayesian inference particularly appealing when we want intuitive probability statements about our parameters.\nBayesian linear regression treats model parameters (e.g., intercept, slope) as random variables with their own probability distributions. Instead of producing a single “best” estimate for each coefficient, we derive a posterior distribution, which represents a range of possible values for each parameter, given both:\n\nOur prior beliefs (before seeing the data)\nThe likelihood of the observed data under the model assumptions\n\nThe result is a credible interval, which gives us a direct probability statement about our parameters, making it a powerful tool in statistical modeling.\nIn this post, we’ll work through an example in R to see:\n\nHow the posterior distribution updates from the prior distribution.\nHow to visualize priors versus posteriors.\nHow to interpret Bayesian predictions (including posterior predictive distributions)."
  },
  {
    "objectID": "posts/2025-02-09-BL/2025-02-09-BL.html#simulate-some-data",
    "href": "posts/2025-02-09-BL/2025-02-09-BL.html#simulate-some-data",
    "title": "A Gentle Introduction to Bayesian Linear Regression in R",
    "section": "1. Simulate Some Data",
    "text": "1. Simulate Some Data\nLet’s simulate a small dataset with a linear relationship. Suppose the “true” model is:\n\\[\ny = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\nwhere:\n\n\\(\\beta_0 = 2.0\\) (intercept),\n\\(\\beta_1 = 3.5\\) (slope),\n\\(\\varepsilon \\sim \\text{Normal}(0,1)\\) (noise).\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(rstanarm)  # for Bayesian regression\n\nset.seed(42)\n\nn &lt;- 50\nx &lt;- seq(0, 5, length.out = n)\nbeta0_true &lt;- 2.0\nbeta1_true &lt;- 3.5\nsigma_true &lt;- 1.0\n\n# Generate y\nnoise &lt;- rnorm(n, mean = 0, sd = sigma_true)\ny &lt;- beta0_true + beta1_true * x + noise\n\ndf &lt;- data.frame(x, y)\nhead(df)\n\n          x        y\n1 0.0000000 3.370958\n2 0.1020408 1.792445\n3 0.2040816 3.077414\n4 0.3061224 3.704291\n5 0.4081633 3.832840\n6 0.5102041 3.679590\n\n\n\nQuick Plot of the Data\n\nggplot(df, aes(x=x, y=y)) +\n  geom_point(color = \"#008Fd5\", alpha = 0.9, size=2) +\n  geom_abline(intercept=beta0_true, slope=beta1_true, \n              color=\"red\", linetype=\"dashed\") +\n  theme_minimal(base_size = 15) +\n  labs(title=\"Simulated Data with True Regression Line\",\n       subtitle=\"Red dashed line = true relationship\")"
  },
  {
    "objectID": "posts/2025-02-09-BL/2025-02-09-BL.html#frequentist-ols-for-reference",
    "href": "posts/2025-02-09-BL/2025-02-09-BL.html#frequentist-ols-for-reference",
    "title": "A Gentle Introduction to Bayesian Linear Regression in R",
    "section": "2. Frequentist OLS for Reference",
    "text": "2. Frequentist OLS for Reference\nFirst, we fit a simple OLS model:\n\nols_model &lt;- lm(y ~ x, data = df)\nsummary(ols_model)\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7403 -0.4366 -0.1193  0.8319  2.1072 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.3548     0.3175   7.416  1.7e-09 ***\nx             3.3438     0.1094  30.555  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.139 on 48 degrees of freedom\nMultiple R-squared:  0.9511,    Adjusted R-squared:  0.9501 \nF-statistic: 933.6 on 1 and 48 DF,  p-value: &lt; 2.2e-16\n\n\nWe get a point estimate for the intercept and slope plus confidence intervals."
  },
  {
    "objectID": "posts/2025-02-09-BL/2025-02-09-BL.html#bayesian-linear-regression",
    "href": "posts/2025-02-09-BL/2025-02-09-BL.html#bayesian-linear-regression",
    "title": "A Gentle Introduction to Bayesian Linear Regression in R",
    "section": "3. Bayesian Linear Regression",
    "text": "3. Bayesian Linear Regression\nNow let’s do a Bayesian version via the rstanarm package. Under the hood, stan_glm uses weakly informative priors by default (you can customize these).\n\nbayes_model &lt;- stan_glm(y ~ x, data=df, \n                        chains=2, iter=2000, seed=42)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.024376 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 243.76 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.026 seconds (Warm-up)\nChain 1:                0.032 seconds (Sampling)\nChain 1:                0.058 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.04 seconds (Warm-up)\nChain 2:                0.023 seconds (Sampling)\nChain 2:                0.063 seconds (Total)\nChain 2: \n\nprint(bayes_model, digits=3)\n\nstan_glm\n family:       gaussian [identity]\n formula:      y ~ x\n observations: 50\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 2.372  0.334 \nx           3.337  0.115 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.156  0.117 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nThe summary output gives us:\n\nPosterior mean: The most likely value for each coefficient given the data.\nStandard deviation: How uncertain we are about each estimate.\n95% credible interval: The range where the true coefficient likely falls with 95% probability.\n\nUnlike frequentist confidence intervals, which describe the long-run variability across repeated samples, Bayesian credible intervals provide a direct probability statement about our parameters.\n\n3.1 What Are Priors Here?\nBy default, stan_glm(..., family = gaussian()) uses something akin to a weakly informative prior on the slope and intercept. This means the prior allows a wide range of possible values for the coefficients but discourages extremely large magnitudes. You can supply arguments like prior, prior_intercept, etc., or switch to brms for more flexible syntax.\n\n\n3.2 Posterior Summaries\nThe printed output typically gives us:\n\nmean: Posterior mean of the parameter.\nsd: Posterior standard deviation (akin to “uncertainty”).\n2.5% / 97.5%: Bounds of the 95% credible interval.\n\nWe can visualize these distributions:\n\nplot(bayes_model, \n     plotfun = \"areas\",  \n     pars = c(\"(Intercept)\", \"x\"),  \n     include = TRUE, \n     prob = 0.95,  border = \"black\") + \n  theme_minimal(base_size = 15) + \n  labs(title = \"Posterior Distributions with 95% Credible Interval\",\n       x = \"Parameter Value\",\n       y = \"Density\")\n\n\n\n\nEach density plot is the posterior distribution for one parameter, showing the entire spread of plausible values given the model and data. The shaded areas give the 95% credible interval."
  },
  {
    "objectID": "posts/2025-02-09-BL/2025-02-09-BL.html#explaining-the-posterior-more-explicitly",
    "href": "posts/2025-02-09-BL/2025-02-09-BL.html#explaining-the-posterior-more-explicitly",
    "title": "A Gentle Introduction to Bayesian Linear Regression in R",
    "section": "4. Explaining the Posterior More Explicitly",
    "text": "4. Explaining the Posterior More Explicitly\n\n4.1 Combining Prior and Likelihood\nFrom a conceptual standpoint, Bayesian regression says:\n\\[\n\\text{Posterior}(\\beta_0, \\beta_1 \\mid \\text{data})\n\\;\\propto\\;\n\\text{Prior}(\\beta_0, \\beta_1) \\;\\times\\;\n\\text{Likelihood}(\\text{data} \\mid \\beta_0, \\beta_1).\n\\]\n\nPrior: What values of \\(\\beta_0, \\beta_1\\) are plausible before seeing any data?\nLikelihood: Given a candidate pair \\((\\beta_0, \\beta_1)\\), how well does it explain our observed \\(y\\) values?\nPosterior: The result of multiplying these together and renormalizing into a probability distribution.\n\nThe best way to see this in action is to do a “prior vs. posterior” plot for (say) the slope. Let’s do a quick example of customizing a prior so we can illustrate how it gets updated.\n\n4.1.1 A Very Simple Custom Prior Example\nSuppose we suspect the slope is likely around 1.0, with a standard deviation of 2. That means slopes near 1 are more plausible, but we allow for a broad range. Likewise, for the intercept, maybe we suspect a prior mean of 0 with a standard deviation of 10.\n\ncustom_bayes_model &lt;- stan_glm(\n  y ~ x,\n  data = df,\n  prior = normal(location = 1, scale = 2, autoscale=FALSE),       # slope prior\n  prior_intercept = normal(location = 0, scale = 10, autoscale=FALSE),\n  chains=2, iter=2000, seed=123\n)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.023 seconds (Warm-up)\nChain 1:                0.036 seconds (Sampling)\nChain 1:                0.059 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.022 seconds (Warm-up)\nChain 2:                0.037 seconds (Sampling)\nChain 2:                0.059 seconds (Total)\nChain 2: \n\nprint(custom_bayes_model, digits=3)\n\nstan_glm\n family:       gaussian [identity]\n formula:      y ~ x\n observations: 50\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 2.378  0.344 \nx           3.335  0.117 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.156  0.116 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nNow, let’s extract the prior and posterior draws and plot them. We can sample from the prior if we specify prior_PD = TRUE (prior predictive distribution).\n\n# Step 1: sample from prior only (no data used)\nprior_only_model &lt;- stan_glm(\n  y ~ x,\n  data = df,\n  prior = normal(location = 1, scale = 2, autoscale=FALSE),\n  prior_intercept = normal(location = 0, scale = 10, autoscale=FALSE),\n  chains=2, iter=2000, seed=123,\n  prior_PD = TRUE   # &lt;--- This means: ignore the likelihood of data\n)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 8e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.024 seconds (Warm-up)\nChain 1:                0.025 seconds (Sampling)\nChain 1:                0.049 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 4e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 2:                0.041 seconds (Sampling)\nChain 2:                0.054 seconds (Total)\nChain 2: \n\n# Extract draws\nprior_draws &lt;- as.matrix(prior_only_model, pars=c(\"(Intercept)\",\"x\"))\npost_draws &lt;- as.matrix(custom_bayes_model, pars=c(\"(Intercept)\",\"x\"))\n\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\ndf_draws &lt;- data.frame(\n  prior_slope  = prior_draws[,\"x\"],\n  post_slope   = post_draws[,\"x\"]\n)\n\ndf_long &lt;- df_draws %&gt;% \n  pivot_longer(cols = everything(), \n               names_to = \"distribution\", values_to=\"slope_value\") %&gt;%\n  mutate(distribution = ifelse(\n    distribution == \"prior_slope\", \"Prior Distribution\", \n    \"Posterior Distribution\"))\n\nggplot(df_long, aes(x=slope_value, fill=distribution)) +\n  geom_density(alpha=0.4) +\n  scale_fill_manual(values=c(\"Prior Distribution\" = \"#D55E00\", \n                             \"Posterior Distribution\" = \"#0072B2\")) +\n  labs(title=\"Slope: Prior vs Posterior\",\n       x=\"Slope Value\", \n       fill = \"Distribution\") +\n  theme_minimal(base_size = 14)\n\n\n\n\n\nThe red or orange curve will show the prior distribution for the slope (centered near 1, quite wide).\nThe blue curve will show the posterior after seeing the data. Because our true slope is actually 3.5, you’ll see that the posterior is pulled far to the right of the prior’s center of 1. The data “pushes” the posterior to align with the actual effect in the data, reducing uncertainty."
  },
  {
    "objectID": "posts/2025-02-09-BL/2025-02-09-BL.html#posterior-predictive-distribution-why-it-matters",
    "href": "posts/2025-02-09-BL/2025-02-09-BL.html#posterior-predictive-distribution-why-it-matters",
    "title": "A Gentle Introduction to Bayesian Linear Regression in R",
    "section": "Posterior Predictive Distribution: Why It Matters",
    "text": "Posterior Predictive Distribution: Why It Matters\nSo far, we’ve focused on estimating the posterior distribution of our regression parameters. However, in most real-world applications, we’re not just interested in estimating coefficients—we want to use our model to make predictions about future observations.\nIn a frequentist regression, we typically obtain a point prediction:\n\\[\n\\hat{y}_* = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_*\n\\]\nalong with a prediction interval that accounts for both residual variability and parameter uncertainty.\n\nThe Bayesian Approach: Uncertainty in Predictions\nIn Bayesian regression, we don’t just compute a single best guess for \\(y_*\\)—we generate an entire posterior predictive distribution, which tells us:\n\nThe most likely values of \\(y_*\\) based on our model.\nThe full range of plausible values, incorporating both parameter uncertainty and residual variability.\nThe probability of exceeding a critical threshold, which is extremely useful for decision-making.\n\nExample Use Case:\n\nSuppose \\(x_* = 2.5\\) represents the duration of a hospital stay, and \\(y_*\\) represents the total cost of treatment (in thousands).\nA hospital administrator might ask:\n\n“What is the probability that this patient’s costs will exceed $10,000?”\n“How much uncertainty is there in our cost estimate?”\n\nInstead of a single prediction, the Bayesian posterior predictive distribution provides a full range of possible outcomes, making it much more informative.\n\n\n\nSimulating from the Posterior Predictive Distribution\nWe generate predictions for a new observation \\(x_*\\) by sampling from the posterior:\n\\[\ny_* \\sim \\text{Normal}(\\beta_0 + \\beta_1 x_*, \\sigma^2)\n\\]\nwhere \\(\\beta_0, \\beta_1, \\sigma\\) are drawn from their posterior distributions.\nLet’s compute this for \\(x_* = 2.5\\) and visualize the range of plausible values for \\(y_*\\).\n\n# Posterior draws for intercept, slope, and sigma\npost_draws_full &lt;- as.matrix(custom_bayes_model)\n\n# Function to simulate predictions from posterior\nsimulate_posterior_y &lt;- function(x_star, n_sims=4000) {\n  idx_int &lt;- which(colnames(post_draws_full)==\"(Intercept)\")\n  idx_slope &lt;- which(colnames(post_draws_full)==\"x\")\n  idx_sigma &lt;- which(colnames(post_draws_full)==\"sigma\")\n\n  intercept_samples &lt;- post_draws_full[, idx_int]\n  slope_samples     &lt;- post_draws_full[, idx_slope]\n  sigma_samples     &lt;- post_draws_full[, idx_sigma]\n\n  # Compute predicted means, then sample from Normal(mean, sigma)\n  mu_star &lt;- intercept_samples + slope_samples * x_star\n  y_sim &lt;- rnorm(n_sims, mean=mu_star, sd=sigma_samples)\n  y_sim\n}\n\nx_star &lt;- 2.5\ny_sim &lt;- simulate_posterior_y(x_star)\n\n# Probability that y_* &gt; 10\nmean(y_sim &gt; 10)\n\n[1] 0.72675\n\n\nA histogram of simulated outcomes shows the full range of possible \\(y_*\\) values, allowing us to make probability-based decisions.\n\nlibrary(ggplot2)\n\nggplot(data.frame(y_sim), aes(y_sim)) +\n  geom_histogram(bins=30, fill=\"#D55E00\", color=\"white\") +\n  theme_minimal(base_size = 14) +\n  geom_vline(xintercept=10, color=\"black\", linetype=\"solid\", size=1.5) +\n  labs(title=\"Posterior Predictive Distribution at x=2.5\",\n       subtitle=\"Black line = threshold of 10\",\n       x=\"Possible y* values\", y=\"Frequency\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nIn this case, the probability that \\(y_*\\) exceeds $10,000 is approximately 73%. This information is crucial for making informed decisions about resource allocation, risk management, or policy interventions."
  },
  {
    "objectID": "posts/2025-02-09-BL/2025-02-09-BL.html#key-takeaways-why-this-matters",
    "href": "posts/2025-02-09-BL/2025-02-09-BL.html#key-takeaways-why-this-matters",
    "title": "A Gentle Introduction to Bayesian Linear Regression in R",
    "section": "Key Takeaways: Why This Matters",
    "text": "Key Takeaways: Why This Matters\nUnlike traditional frequentist regression, Bayesian posterior predictive distributions allow us to answer probabilistic questions about new data points:\n\nPoint Estimate + Full Uncertainty: Instead of a single predicted \\(y_*\\), we get a distribution over plausible values.\nProbability-Based Decisions: We can compute the probability that an outcome exceeds (or falls below) a critical threshold.\nMore Robust Uncertainty Quantification: Since we account for both parameter uncertainty and residual variance, our predictions are more realistic.\n\nFurther Reading:\n\nIntroduction to Bayesian Statistics by William Bolstad.\nBayesian Data Analysis by Gelman et al. \nThe rstanarm, brms, or bayesplot packages in R for specifying, fitting, and visualizing Bayesian models."
  },
  {
    "objectID": "posts/2025-02-23-Sankey/2025-02-23-Sankey.html",
    "href": "posts/2025-02-23-Sankey/2025-02-23-Sankey.html",
    "title": "A nice Sankey diagram",
    "section": "",
    "text": "The Gender Paradox: Depression, Treatment, and Suicide in America\n\n\n\n\n\n\n\n\nGender, Depression, and Suicide in America\n\n\nBy Jacob Jameson\n\n\nWhile women report higher rates of depression and are more likely to seek mental health care, men die by suicide at significantly higher rates — particularly in states with permissive gun laws.\n\n\n\n\n\nThe Depression Gender Gap\n\n\nDepression affects Americans differently across gender lines. A nine‑item depression screening questionnaire, called the Patient Health Questionnaire (PHQ‑9), was administered to determine the frequency of depression symptoms over the past 2 weeks.\n\n\n\nAmong 2021-2023 NHANES adult survey respondents, women were more than two times as likely as men to have PHQ-9 scores indicating severe depression."
  },
  {
    "objectID": "posts/2025-04-29-AR/2025-04-29-AR.html",
    "href": "posts/2025-04-29-AR/2025-04-29-AR.html",
    "title": "Beyond DiD: Autoregressive Models for Policy Evaluation When Parallel Trends Fail",
    "section": "",
    "text": "The difference-in-differences (DiD) framework has become a cornerstone of policy evaluation in health services research and beyond. When a policy change affects some regions or groups but not others, DiD offers a compelling approach to estimate causal effects. However, the validity of DiD hinges critically on the parallel trends assumption: that treatment and control groups would have followed parallel trajectories in the absence of intervention.\nWhat happens when this assumption fails? In this post, we explore autoregressive models as a robust alternative for policy evaluation when parallel trends are violated. We’ll walk through:\n\nWhy parallel trends matter for DiD\nWhen and how they fail in real-world applications\nHow autoregressive models can help\nA simulation comparing both approaches\nPractical implementation in R"
  },
  {
    "objectID": "posts/2025-04-29-AR/2025-04-29-AR.html#introduction",
    "href": "posts/2025-04-29-AR/2025-04-29-AR.html#introduction",
    "title": "Beyond DiD: Autoregressive Models for Policy Evaluation When Parallel Trends Fail",
    "section": "",
    "text": "The difference-in-differences (DiD) framework has become a cornerstone of policy evaluation in health services research and beyond. When a policy change affects some regions or groups but not others, DiD offers a compelling approach to estimate causal effects. However, the validity of DiD hinges critically on the parallel trends assumption: that treatment and control groups would have followed parallel trajectories in the absence of intervention.\nWhat happens when this assumption fails? In this post, we explore autoregressive models as a robust alternative for policy evaluation when parallel trends are violated. We’ll walk through:\n\nWhy parallel trends matter for DiD\nWhen and how they fail in real-world applications\nHow autoregressive models can help\nA simulation comparing both approaches\nPractical implementation in R"
  },
  {
    "objectID": "posts/2025-04-29-AR/2025-04-29-AR.html#the-problem-with-parallel-trends",
    "href": "posts/2025-04-29-AR/2025-04-29-AR.html#the-problem-with-parallel-trends",
    "title": "Beyond DiD: Autoregressive Models for Policy Evaluation When Parallel Trends Fail",
    "section": "The Problem with Parallel Trends",
    "text": "The Problem with Parallel Trends\nThe parallel trends assumption is the lynchpin of DiD methodology. It states that in the absence of treatment, the difference between treatment and control groups would have remained constant over time. This allows us to attribute any deviation from this pattern to the causal effect of the intervention.\nHowever, in health policy research, this assumption often fails for several reasons:\n\nHealth outcomes frequently have complex, non-linear trajectories\nTreatment assignment may be related to pre-existing trends (e.g., policies targeting areas with worsening health metrics)\nAnticipation effects may cause behavior changes before policy implementation\nDifferential shocks to treatment and control groups before the policy\n\nWhen parallel trends are violated, traditional DiD estimates become biased, potentially leading to incorrect policy conclusions."
  },
  {
    "objectID": "posts/2025-04-29-AR/2025-04-29-AR.html#autoregressive-models-as-an-alternative",
    "href": "posts/2025-04-29-AR/2025-04-29-AR.html#autoregressive-models-as-an-alternative",
    "title": "Beyond DiD: Autoregressive Models for Policy Evaluation When Parallel Trends Fail",
    "section": "Autoregressive Models as an Alternative",
    "text": "Autoregressive Models as an Alternative\nAutoregressive models offer a compelling alternative by explicitly modeling the relationship between past and current outcomes. Rather than assuming parallel trends, they directly account for the dynamic evolution of the outcome variable over time.\nThe basic form of an autoregressive model for policy evaluation can be expressed as:\n\\[Y_{it} = \\alpha + \\rho Y_{i,t-1} + \\beta D_{it} + \\gamma X_{it} + \\delta_i + \\lambda_t + \\epsilon_{it}\\]\nWhere: - \\(Y_{it}\\) is the outcome for unit \\(i\\) at time \\(t\\) - \\(Y_{i,t-1}\\) is the lagged outcome (the autoregressive component) - \\(D_{it}\\) is the treatment indicator - \\(X_{it}\\) represents time-varying controls - \\(\\delta_i\\) are unit fixed effects - \\(\\lambda_t\\) are time fixed effects - \\(\\epsilon_{it}\\) is the error term\nThe coefficient \\(\\beta\\) represents the treatment effect.\nBy including the lagged dependent variable, these models can accommodate different pre-treatment trajectories, making them particularly valuable when parallel trends are suspect."
  },
  {
    "objectID": "posts/2025-04-29-AR/2025-04-29-AR.html#simulation-did-vs.-autoregressive-models",
    "href": "posts/2025-04-29-AR/2025-04-29-AR.html#simulation-did-vs.-autoregressive-models",
    "title": "Beyond DiD: Autoregressive Models for Policy Evaluation When Parallel Trends Fail",
    "section": "Simulation: DiD vs. Autoregressive Models",
    "text": "Simulation: DiD vs. Autoregressive Models\nLet’s compare these approaches through simulation. We’ll generate data where the parallel trends assumption is violated and compare the performance of DiD and autoregressive models.\n\nlibrary(tidyverse)\nlibrary(lfe)       # For fixed effects models\nlibrary(ggplot2)\nlibrary(kableExtra)\nlibrary(fixest)  \n\n# Set seed for reproducibility\nset.seed(123)\n\n# Simulation parameters\nn_states &lt;- 50        # 25 treatment, 25 control\nn_periods &lt;- 20       # 10 pre-treatment, 10 post-treatment\ntreatment_period &lt;- 11  # Treatment starts at period 11\ntrue_effect &lt;- 2      # True policy effect\n\n# Generate panel data\nsimulate_data &lt;- function() {\n  # Create empty dataframe\n  df &lt;- expand.grid(\n    state = 1:n_states,\n    time = 1:n_periods\n  )\n  \n  # Assign treatment (first half of states)\n  df$treated &lt;- ifelse(df$state &lt;= n_states/2, 1, 0)\n  \n  # Generate treatment indicator (post-treatment for treated states)\n  df$treatment &lt;- ifelse(df$treated == 1 & df$time &gt;= treatment_period, 1, 0)\n  \n  # Different trends for treatment and control groups\n  df$state_trend &lt;- ifelse(df$treated == 1, 0.5, 0.2)  # Non-parallel trends\n  \n  # Generate outcomes\n  # First, create a starting value for each state\n  state_initial &lt;- tibble(\n    state = 1:n_states,\n    initial_value = 10 + rnorm(n_states, 0, 2)\n  )\n  \n  df &lt;- left_join(df, state_initial, by = \"state\")\n  \n  # Generate outcomes with AR(1) process and non-parallel trends\n  # This requires multiple passes\n  df$outcome &lt;- df$initial_value  # Initialize\n  \n  for (t in 2:n_periods) {\n    # Get previous period data\n    prev_data &lt;- df %&gt;% \n      filter(time == t-1) %&gt;% \n      select(state, prev_outcome = outcome)\n    \n    # Update current period\n    df &lt;- df %&gt;%\n      left_join(prev_data, by = \"state\") %&gt;%\n      mutate(\n        outcome = ifelse(\n          time == t,\n          0.7 * prev_outcome + state_trend * time + treatment * true_effect + rnorm(n(), 0, 1),\n          outcome\n        )\n      ) %&gt;%\n      select(-prev_outcome)\n  }\n  \n  return(df)\n}\n\n# Generate data\npolicy_data &lt;- simulate_data()\n\nNow let’s visualize the data to confirm we have non-parallel trends:\n\n# Calculate group means by time\ngroup_means &lt;- policy_data %&gt;%\n  group_by(time, treated) %&gt;%\n  summarize(mean_outcome = mean(outcome), .groups = \"drop\") %&gt;%\n  mutate(group = ifelse(treated == 1, \"Treatment Group\", \"Control Group\"))\n\n# Plot trends with intervention line\nggplot(group_means, aes(x = time, y = mean_outcome, color = group, group = group)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = treatment_period, linetype = \"dashed\", color = \"red\") +\n  annotate(\"text\", x = treatment_period + 0.5, y = max(group_means$mean_outcome), \n           label = \"Policy Implementation\", hjust = 0, color = \"red\") +\n  labs(\n    title = \"Outcome Trends by Group\",\n    subtitle = \"Note the non-parallel pre-treatment trends\",\n    x = \"Time Period\",\n    y = \"Outcome\",\n    color = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    text = element_text(size = 12),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\nAs we can see, the treatment and control groups follow different trajectories even before the policy implementation (vertical dashed line), violating the parallel trends assumption.\nNow let’s estimate the treatment effect using both methods:\n\n# 1. Standard DiD model\ndid_model &lt;- felm(outcome ~ treated + time + treatment | 0 | 0 | state, data = policy_data)\n\n# 2. Autoregressive model\n# First, create lagged outcome\npolicy_data_lagged &lt;- policy_data %&gt;%\n  arrange(state, time) %&gt;%\n  group_by(state) %&gt;%\n  mutate(lag_outcome = lag(outcome)) %&gt;%\n  ungroup() %&gt;%\n  filter(!is.na(lag_outcome))  # Drop first period for each state\n\n# Run autoregressive model\nar_model &lt;- felm(outcome ~ lag_outcome + treated + time + treatment | 0 | 0 | state, \n                data = policy_data_lagged)\n\n# Compare results\nresults_df &lt;- tibble(\n  Model = c(\"True Effect\", \"DiD Estimate\", \"Autoregressive\"),\n  Estimate = c(true_effect, coef(did_model)[\"treatment\"], coef(ar_model)[\"treatment\"]),\n  Bias = c(0, coef(did_model)[\"treatment\"] - true_effect, \n           coef(ar_model)[\"treatment\"] - true_effect)\n)\n\n# Format table\nresults_df %&gt;%\n  mutate(\n    Estimate = round(Estimate, 3),\n    Bias = round(Bias, 3),\n    `Percent Bias` = ifelse(Model == \"True Effect\", \"-\", \n                           paste0(round(Bias / true_effect * 100, 1), \"%\"))\n  ) %&gt;%\n  kable(align = \"lrrr\", caption = \"Comparison of Model Estimates\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\nComparison of Model Estimates\n\n\nModel\nEstimate\nBias\nPercent Bias\n\n\n\n\nTrue Effect\n2.000\n0.000\n-\n\n\nDiD Estimate\n13.371\n11.371\n568.6%\n\n\nAutoregressive\n3.070\n1.070\n53.5%"
  },
  {
    "objectID": "posts/2025-04-29-AR/2025-04-29-AR.html#why-autoregressive-models-work",
    "href": "posts/2025-04-29-AR/2025-04-29-AR.html#why-autoregressive-models-work",
    "title": "Beyond DiD: Autoregressive Models for Policy Evaluation When Parallel Trends Fail",
    "section": "Why Autoregressive Models Work",
    "text": "Why Autoregressive Models Work\nThe results demonstrate why autoregressive models often outperform DiD when parallel trends are violated. Here are the key advantages:\n\nDynamic relationships: By including the lagged dependent variable, autoregressive models directly account for the relationship between past and current outcomes, capturing pre-existing dynamics.\nDifferent pre-treatment trajectories: Instead of assuming parallel trends, autoregressive models can accommodate different trajectories between treatment and control groups.\nControlling for omitted variables: The lagged outcome can serve as a proxy for unobserved time-varying confounders that affect the trajectory of the outcome.\nAnticipation effects: These models can better handle situations where units change behavior in anticipation of policy changes."
  },
  {
    "objectID": "posts/2025-04-29-AR/2025-04-29-AR.html#model-assumptions-and-limitations",
    "href": "posts/2025-04-29-AR/2025-04-29-AR.html#model-assumptions-and-limitations",
    "title": "Beyond DiD: Autoregressive Models for Policy Evaluation When Parallel Trends Fail",
    "section": "Model Assumptions and Limitations",
    "text": "Model Assumptions and Limitations\nWhile autoregressive models address the parallel trends issue, they come with their own assumptions:\n\nNo contemporaneous reverse causality: The policy implementation cannot be simultaneously determined by current outcomes.\nDynamic completeness: The model must include sufficient lags to capture the full dynamics of the process.\nCorrect functional form: The relationship between past and current values must be correctly specified.\nExogeneity of treatment: The policy implementation should be exogenous after controlling for lagged outcomes.\nNickell bias: In short panels with unit fixed effects, coefficients on the lagged dependent variable can be biased. This may require specialized estimators like Arellano-Bond for correction."
  },
  {
    "objectID": "posts/2025-04-29-AR/2025-04-29-AR.html#application-to-health-policy-research",
    "href": "posts/2025-04-29-AR/2025-04-29-AR.html#application-to-health-policy-research",
    "title": "Beyond DiD: Autoregressive Models for Policy Evaluation When Parallel Trends Fail",
    "section": "Application to Health Policy Research",
    "text": "Application to Health Policy Research\nThe flexibility of autoregressive models makes them particularly valuable for health policy research, where outcomes often have complex dynamics:\n\nHospital readmission policies: When evaluating policies to reduce readmissions, hospitals often have different pre-existing trends\nMedicaid expansion: States that expanded Medicaid may have had different health trajectories before expansion\nPrescription drug monitoring programs: States implementing these programs often do so in response to worsening opioid trends"
  },
  {
    "objectID": "posts/2025-04-29-AR/2025-04-29-AR.html#practical-implementation",
    "href": "posts/2025-04-29-AR/2025-04-29-AR.html#practical-implementation",
    "title": "Beyond DiD: Autoregressive Models for Policy Evaluation When Parallel Trends Fail",
    "section": "Practical Implementation",
    "text": "Practical Implementation\nTo implement autoregressive models for policy evaluation in R:\n\n# Basic autoregressive model\nar_basic &lt;- lm(outcome ~ lag_outcome + treatment, data = policy_data_lagged)\n\n# With fixed effects and time trends\nar_fe &lt;- felm(outcome ~ lag_outcome + treatment | state + time | 0 | state, \n             data = policy_data_lagged)\n\n# With clustered standard errors\nar_cluster &lt;- felm(outcome ~ lag_outcome + treatment | state + time | 0 | state, \n                  data = policy_data_lagged)\n\n# Custom function to create leads and lags\ncreate_leads_lags &lt;- function(data, id_var = \"state\", time_var = \"time\", \n                             treatment_var = \"treatment\", leads = 3, lags = 5) {\n  # Ensure data is arranged correctly\n  data &lt;- data %&gt;% arrange(.data[[id_var]], .data[[time_var]])\n  \n  # Create lag variables\n  for (i in 1:lags) {\n    lag_name &lt;- paste0(\"lag\", i, \"_\", treatment_var)\n    data &lt;- data %&gt;%\n      group_by(.data[[id_var]]) %&gt;%\n      mutate(!!lag_name := lag(.data[[treatment_var]], n = i)) %&gt;%\n      ungroup()\n  }\n  \n  # Create lead variables\n  for (i in 1:leads) {\n    lead_name &lt;- paste0(\"lead\", i, \"_\", treatment_var)\n    data &lt;- data %&gt;%\n      group_by(.data[[id_var]]) %&gt;%\n      mutate(!!lead_name := lead(.data[[treatment_var]], n = i)) %&gt;%\n      ungroup()\n  }\n  \n  # Create lagged outcome variable for autoregressive component\n  data &lt;- data %&gt;%\n    group_by(.data[[id_var]]) %&gt;%\n    mutate(lag_outcome = lag(.data[[\"outcome\"]], n = 1)) %&gt;%\n    ungroup()\n  \n  return(data)\n}"
  },
  {
    "objectID": "posts/2025-04-29-AR/2025-04-29-AR.html#testing-model-suitability",
    "href": "posts/2025-04-29-AR/2025-04-29-AR.html#testing-model-suitability",
    "title": "Beyond DiD: Autoregressive Models for Policy Evaluation When Parallel Trends Fail",
    "section": "Testing Model Suitability",
    "text": "Testing Model Suitability\nBefore choosing an autoregressive approach, it’s important to assess whether parallel trends are indeed violated:\n\n# When evaluating policy interventions with dynamic effects, we need to look at both\n# pre-treatment effects (to check for anticipation) and post-treatment effects\n# (to assess how impacts evolve over time).\n\n# This creates both the leads/lags AND the required lag_outcome variable\npolicy_data_dynamic &lt;- create_leads_lags(policy_data_lagged, \n                                      id_var = \"state\", \n                                      time_var = \"time\", \n                                      treatment_var = \"treatment\", \n                                      leads = 3, \n                                      lags = 3)\n\n# Keep only complete cases for this analysis\npolicy_data_dynamic &lt;- policy_data_dynamic %&gt;%\n  filter(!is.na(lag_outcome))\n\n# For dynamic effects (leads and lags of treatment)\nar_dynamic &lt;- tryCatch({\n  felm(outcome ~ lag_outcome + lag1_treatment + lag2_treatment + lag3_treatment + \n      treatment + lead1_treatment + lead2_treatment + lead3_treatment | \n      state + time | 0 | state, data = policy_data_dynamic)\n}, error = function(e) {\n  message(\"Error in dynamic model: \", e$message)\n  # Simpler fallback model\n  felm(outcome ~ lag_outcome + treatment | state + time | 0 | state, \n      data = policy_data_dynamic)\n})\n\n# Print summary\nsummary(ar_dynamic)\n\n\nCall:\n   felm(formula = outcome ~ lag_outcome + lag1_treatment + lag2_treatment +      lag3_treatment + treatment + lead1_treatment + lead2_treatment +      lead3_treatment | state + time | 0 | state, data = policy_data_dynamic) \n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.93016 -0.65451  0.01728  0.62855  3.12924 \n\nCoefficients:\n                Estimate Cluster s.e. t value Pr(&gt;|t|)    \nlag_outcome      0.58767      0.03071  19.136  &lt; 2e-16 ***\nlag1_treatment   0.62796      0.34698   1.810 0.076466 .  \nlag2_treatment   0.34763      0.44517   0.781 0.438612    \nlag3_treatment   1.37402      0.32756   4.195 0.000114 ***\ntreatment        1.94893      0.37674   5.173 4.26e-06 ***\nlead1_treatment  1.18728      0.41469   2.863 0.006158 ** \nlead2_treatment  0.39214      0.35710   1.098 0.277510    \nlead3_treatment  0.56319      0.37265   1.511 0.137134    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.015 on 580 degrees of freedom\n  (250 observations deleted due to missingness)\nMultiple R-squared(full model): 0.9867   Adjusted R-squared: 0.9851 \nMultiple R-squared(proj model): 0.9152   Adjusted R-squared: 0.9051 \nF-statistic(full model, *iid*):624.1 on 69 and 580 DF, p-value: &lt; 2.2e-16 \nF-statistic(proj model): 625.9 on 8 and 49 DF, p-value: &lt; 2.2e-16 \n\n\n\n# Load necessary packages\nlibrary(fixest)  # For event study plots\n\n# 1. Visual inspection with leads and lags plot\n# Create relative time variable (time to treatment)\npolicy_data &lt;- policy_data %&gt;%\n  mutate(rel_time = ifelse(treated == 1, time - treatment_period, NA))\n\n# Use fixest for event study\nevent_study &lt;- feols(outcome ~ i(rel_time, ref = -1) + i(time) | state, \n                    data = policy_data %&gt;% filter(treated == 1, !is.na(rel_time)))\n\n# Plot the event study coefficients\niplot(event_study, main = \"Event Study: Effect Relative to Treatment Time\",\n     xlab = \"Time Relative to Treatment\", ylab = \"Estimated Effect\")\n\n\n\n# Plot coefficients for visual inspection of pre-trends\n\n# 2. Statistical test for pre-trends\npre_data &lt;- policy_data %&gt;% filter(time &lt; treatment_period)\nsummary(felm(outcome ~ treated * time | state | 0 | state, data = pre_data))\n\n\nCall:\n   felm(formula = outcome ~ treated * time | state | 0 | state,      data = pre_data) \n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7575 -1.3608 -0.2419  1.1661  6.3767 \n\nCoefficients:\n             Estimate Cluster s.e. t value Pr(&gt;|t|)    \ntreated           NaN      0.00000     NaN      NaN    \ntime         -0.43327      0.05084  -8.523 3.06e-11 ***\ntreated:time  0.95150      0.06717  14.167  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.91 on 448 degrees of freedom\nMultiple R-squared(full model): 0.6632   Adjusted R-squared: 0.6249 \nMultiple R-squared(proj model): 0.3654   Adjusted R-squared: 0.2932 \nF-statistic(full model, *iid*): 17.3 on 51 and 448 DF, p-value: &lt; 2.2e-16 \nF-statistic(proj model): 70.67 on 3 and 49 DF, p-value: &lt; 2.2e-16 \n\n# If interaction is significant, parallel trends is likely violated"
  },
  {
    "objectID": "posts/2025-04-29-AR/2025-04-29-AR.html#conclusion",
    "href": "posts/2025-04-29-AR/2025-04-29-AR.html#conclusion",
    "title": "Beyond DiD: Autoregressive Models for Policy Evaluation When Parallel Trends Fail",
    "section": "Conclusion",
    "text": "Conclusion\nWhen evaluating policy interventions, the reliability of our estimates hinges on the validity of model assumptions. While DiD remains a powerful tool when its assumptions hold, autoregressive models offer a valuable alternative when parallel trends are violated.\nBy explicitly modeling the dynamic relationship between past and current outcomes, these models can deliver more accurate causal estimates in complex policy environments. Rather than viewing them as competitors, researchers should understand the strengths and limitations of both approaches, selecting the appropriate method based on the specific context and data characteristics.\nIn health policy research, where outcomes often follow complex trajectories and policy implementations are frequently endogenous to pre-existing trends, the autoregressive approach offers a flexible framework that can accommodate these realities while still enabling credible causal inference."
  },
  {
    "objectID": "index.html#research-pillars",
    "href": "index.html#research-pillars",
    "title": "Jacob Jameson",
    "section": "Research Pillars",
    "text": "Research Pillars\n\n\n\n\nCausal Inference\n\n\nFrom IVs to double machine learning, I apply modern causal tools to evaluate real-world interventions.\n\n\n\n\n\nDynamic Treatment Regimes\n\n\nI develop DTRs to recommend optimal care pathways using longitudinal patient data and reinforcement learning.\n\n\n\n\n\nBayesian Modeling\n\n\nI use Bayesian models—hierarchical, decision-theoretic, and autoregressive—to handle uncertainty and inform policy."
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Jacob Jameson",
    "section": "News",
    "text": "News\n\nSeptember 15, 2025\nNamed as one of four finalists for the 2025 INFORMS Health Applications Society Best Student Paper Competition.\nThe Impact of Batching Advanced Imaging Tests in Emergency Departments (with Soroush Saghafian, Robert Huckman, Nicole Hodgson, and Joshua Baugh).\n\n\nJune 10, 2025\nAwarded the Distinction in Student Teaching at the Harvard Kennedy School.\n\n\nMay 23, 2024\nAwarded the Distinction in Student Teaching at the Harvard Kennedy School.\n\n\nMay 22, 2024\nAwarded the Dean’s Award for Excellence in Student Teaching at the Harvard Kennedy School.\nJameson Wins Student Teaching Awards\n\n\nMay 18, 2024\nAwarded National Institute of Mental Health-Harvard T.H. Chan School of Public Health T32 Predoctoral Fellowship in Comparative Effectiveness Research for Suicide Risk Prevention.\n\n\nFebruary 15, 2024\nSelected to be a Center for Health Decision Science Educational Innovation Scholar.\n\n\nFebruary 8, 2024\nAwarded the 2024 Howard Raiffa Award by the Harvard Center for Risk Analysis at the Harvard T.H. Chan School of Public Health.\n\n\nSeptember 9, 2023\nNamed as one of four finalists for the INFORMS Public Sector Operations Research Best Video Award.\nImproving Emergency Department Flow (with Soroush Saghafian, and Nicole Hodgson).\n\n\nJanuary 9, 2023\nAwarded the 2023 Howard Raiffa Award by the Harvard Center for Risk Analysis at the Harvard T.H. Chan School of Public Health."
  }
]