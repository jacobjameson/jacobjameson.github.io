[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jacob Jameson",
    "section": "",
    "text": "I am a PhD student at Harvard University studying Health Policy and Decision Sciences. I am currently advised by Dr. Soroush Saghafian and work in The Public Impact Analytics Science Lab (PIAS-Lab) at Harvard. I also hold a research position in Boston Children’s Hospital General Pediatric Unit.\nI use tools from operations research, engineering, computer science, and economics to inform decision being made under conditions of uncertainty."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "My teaching experiences are quite diverse. The summer after completing my undergraduate degree, I began teaching in a highschool Algebra II credit recovery program in The School District of Philadelphia. I then transitioned to a full-time middle school mathematics position in New Haven, CT, where I spent the next two years falling in love with the profession. During my second year, I held the Grade-level Chair position for the 8th grade and consulted for the Achievement First Charter Network on middle school mathematics curriculum. I am also passionate about computer science education, and have experience running Girls Who Code clubs and participating in the Code.org Middle and High School Computer Science Professional Learning Program for educators in computer science. Since teaching at the middle school level, I have been fortunate to hold a variety of teaching positions at The University of Chicago and Harvard University. I hope to pursue a teaching career after completing my PhD.\n\nHarvard T.H. Chan School of Public Health\n\nDecision Science for Public Health. Teaching Fellow (2023)\n\n\n\nHarvard Kennedy School\n\nMath Camp. Instructor (2023)\nBig Data and Machine Learning. Teaching Fellow (2024)\nGame Theory. Teaching Fellow (2023, 2024)\nData and Programming for Policymakers. Course Assistant (2023)\nResources, Incentives, and Choices I: Markets and Market Failures. Teaching Fellow (2022, 2023)\n\n\n\nThe University of Chicago, Center for Translational Science\n\nData, Quantitative Methods, and Applications in HSR. Teaching Assistant and Instructor (2021,2022,2023)\nIntroduction to Health Services Research. Teaching Assistant (2021)\n\n\n\nThe University of Chicago, Department of Computer Science\n\nMachine Learning for Public Policy. Teaching Assistant (2022)\nMathematics for Data Analysis and Computer Science. Teaching Assistant (2022)\n\n\n\nThe University of Chicago Booth School of Business\n\nIntroductory Finance. Teaching Assistant (2020-2022)\nData Analysis in R and Python. Teaching Assistant (2021)\n\n\n\nThe University of Chicago Harris School of Public Policy\n\nCoding Lab for Public Policy. Instructor and Curriculum Developer (2021)\n\n\n\nTeach For America, Achievement First Amistad Academy Middle School\n\n7th/8th Grade Mathematics. Mathematics Teacher (2018-2020)"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "",
    "text": "“There is certainly no unanimity on exactly what centrality is or on its conceptual foundations, and there is little agreement on the proper procedure for its measurement.” - Linton Freeman (1977)\nSocial network analysis can be used to measure the importance of a person as a function of the social structure of a community or organization. This post uses visualization as a tool to explain how different measures of centrality may be used to analyze different questions in a network analysis. In these examples we will be specifically looking at directed graphs to compare the following centrality measures and their use-cases:\n\nDegree Centrality\nBetweenness Centrality\nEigenvector Centrality\nKatz Centrality\nHITS Hubs and Authorities\n\nAn example of a directed graph would be one in which people nominate their top 2 friends. In this graph, nodes (people) would connect to others nodes through directed edges (nominations). It is possible for Jacob to nominate Jenna without Jenna nominating him back. You can imagine why centrality in a friendship network might take into account the direction of these nominations. If I list 100 people as my friends and none of them list me back, do we think I am a popular person?"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#simulate-our-data",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#simulate-our-data",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Simulate Our Data",
    "text": "Simulate Our Data\nFor our simulated data we are going to be looking at a classroom that contains 14 male and 14 female students. Suppose that each student was asked to name their top 2 male and top 2 female friends in the class. We are interested in analyzing a slew of different research questions where the centrality of student in the class may be of importance.\nLet’s create the data:\n\nlibrary(tidyverse)\n\nWe begin with a 4 vectors: all males in the classroom, all females in the classroom, a probability distribution for selecting friends of the same sex, a probability distribution for selecting friends of the opposite sex.\n\nmales &lt;- c('Jacob', 'Louis', 'Chris', 'Wyatt', 'Nolan', 'Robert', \n           'Zach', 'John','Bob', 'David', 'Avery', 'Ronald', \n           'Dallas', 'Dylan')\n\nfemales &lt;- c('Bohan', 'Jenna', 'Katarina', 'Hassina', 'Towo', \n             'Becca', 'Meredith', 'Gracie', 'Kayla', 'Marlene', \n             'Jade', 'Allyssa', 'Reigne', 'Wendy')\n\nprobs.diff.sex = c(0.15,0.15,0.05,0.05,0.1,0.1,0.05,0.1,0.01,0.09,0.05,0.05,0.025,0.025)\nprobs.same.sex = c(0.15,0.15,0.05,0.05,0.1,0.1,0.05,0.1,0.01,0.09,0.05,0.05,0.05)\n\nThe function below, simulate.top.friends, will produce a dataframe that will contain each student’s picks for their top 2 male and top 2 female friends in the classroom.\n\nset.seed(1997)\n\nsimulate.top.friends &lt;- function(males, females, probs.diff.sex, probs.same.sex) {\n\n  dat &lt;- setNames(data.frame(matrix(ncol = 6, nrow = 0)), \n                  c(\"Ego\", \"Ego Sex\", \"MF1\", \"MF2\", \"FF1\", \"FF2\"))\n\n  for (ego in males) {\n    temp.males &lt;- males[! males %in% ego]\n    \n    male.friends.i &lt;- sample.int(13, 2, replace = FALSE, prob = probs.same.sex)\n    female.friends.i &lt;- sample.int(14, 2, replace = FALSE, prob = probs.diff.sex)\n    \n    male.friend.1 &lt;- temp.males[male.friends.i[1]]\n    male.friend.2 &lt;- temp.males[male.friends.i[2]]\n    \n    female.friend.1 &lt;- females[female.friends.i[1]]\n    female.friend.2 &lt;- females[female.friends.i[2]]\n    \n    dat[nrow(dat) + 1,] = c(ego, 'Male', male.friend.1, male.friend.2, \n                           female.friend.1, female.friend.2)\n    \n  }\n    for (ego in females) {\n    temp.females &lt;- females[! females %in% ego]\n    \n    male.friends.i &lt;- sample.int(14, 2, replace = FALSE, prob = probs.diff.sex)\n    female.friends.i &lt;- sample.int(13, 2, replace = FALSE, prob = probs.same.sex)\n    \n    male.friend.1 &lt;- males[male.friends.i[1]]\n    male.friend.2 &lt;- males[male.friends.i[2]]\n    \n    female.friend.1 &lt;- temp.females[female.friends.i[1]]\n    female.friend.2 &lt;- temp.females[female.friends.i[2]]\n    \n    dat[nrow(dat) + 1,] = c(ego, 'Female', male.friend.1, male.friend.2, \n                           female.friend.1, female.friend.2)\n    \n  }\n  return(dat)\n}\n\nLet’s take a look at our friendship data that we will be working with!\n\nsimulate.top.friends(males,females, probs.diff.sex, probs.same.sex)\n\n\n\n\n\n\n\nEgo\nEgo Sex\nMF1\nMF2\nFF1\nFF2\n\n\n\n\nJacob\nMale\nChris\nAvery\nKatarina\nMeredith\n\n\nLouis\nMale\nJacob\nAvery\nTowo\nHassina\n\n\nChris\nMale\nJacob\nRobert\nGracie\nBecca\n\n\nWyatt\nMale\nNolan\nJacob\nMarlene\nJenna\n\n\nNolan\nMale\nJacob\nBob\nBohan\nTowo\n\n\nRobert\nMale\nNolan\nLouis\nJenna\nBohan\n\n\nZach\nMale\nJacob\nAvery\nMeredith\nTowo\n\n\nJohn\nMale\nBob\nLouis\nBohan\nWendy\n\n\nBob\nMale\nAvery\nJacob\nJenna\nWendy\n\n\nDavid\nMale\nWyatt\nLouis\nBecca\nJenna\n\n\nAvery\nMale\nLouis\nJacob\nJenna\nBecca\n\n\nRonald\nMale\nJacob\nRobert\nBecca\nWendy\n\n\nDallas\nMale\nRobert\nJacob\nBohan\nJenna\n\n\nDylan\nMale\nJacob\nDavid\nTowo\nMarlene\n\n\nBohan\nFemale\nRobert\nNolan\nAllyssa\nKayla\n\n\nJenna\nFemale\nDavid\nBob\nKatarina\nMeredith\n\n\nKatarina\nFemale\nDavid\nNolan\nGracie\nReigne\n\n\nHassina\nFemale\nRobert\nDavid\nBecca\nJade\n\n\nTowo\nFemale\nJohn\nNolan\nJade\nJenna\n\n\nBecca\nFemale\nNolan\nDallas\nKayla\nTowo\n\n\nMeredith\nFemale\nLouis\nJacob\nTowo\nBohan\n\n\nGracie\nFemale\nJacob\nRonald\nBohan\nHassina\n\n\nKayla\nFemale\nRonald\nLouis\nBohan\nGracie\n\n\nMarlene\nFemale\nJohn\nRobert\nHassina\nTowo\n\n\nJade\nFemale\nRonald\nLouis\nHassina\nBohan\n\n\nAllyssa\nFemale\nAvery\nZach\nGracie\nBohan\n\n\nReigne\nFemale\nJohn\nZach\nBohan\nHassina\n\n\nWendy\nFemale\nNolan\nJacob\nTowo\nBohan"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#creating-a-graphing-object",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#creating-a-graphing-object",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Creating a Graphing Object",
    "text": "Creating a Graphing Object\nThere are many available centrality measures that have been developed for network analysis. At this time, there are no packages that are so comprehensive that it includes all of the measures. I will, therefore, limit this discussion to a subset of the measures that are included in igraph.\nigraph has a really great function that allows us to turn a dataframe into an igraph object. However, the function requires our data to be in “long” form so we will need to do some reshaping. Let’s restructure our data such that each row represents one directed friend nomination. We are going to call this “Source-Target Form”.\n\nlibrary(reshape) \n\nfriendships &lt;- melt(simulate.top.friends(males,females,probs.diff.sex, probs.same.sex), \n                    id=c(\"Ego\", \"Ego Sex\")) %&gt;%\n  select(source=Ego, source_sex =`Ego Sex`, target=value) %&gt;% \n  arrange(source)\n\nLet’s look at the first 10 observations so we can understand the format needed to turn this data into an igraph object.\n\nhead(friendships, 10)\n\n\n\n\n\n\n\nsource\nsource_sex\ntarget\n\n\n\n\nAllyssa\nFemale\nDavid\n\n\nAllyssa\nFemale\nNolan\n\n\nAllyssa\nFemale\nJenna\n\n\nAllyssa\nFemale\nTowo\n\n\nAvery\nMale\nJohn\n\n\nAvery\nMale\nChris\n\n\nAvery\nMale\nAllyssa\n\n\nAvery\nMale\nMeredith\n\n\nBecca\nFemale\nChris\n\n\nBecca\nFemale\nWyatt\n\n\n\n\n\n\n\n\nWe will use the graph_from_data_frame function to create the igraph object.\n\nlibrary(igraph)\nnetwork &lt;- graph_from_data_frame(friendships[,c('source','target','source_sex')],\n                                 directed = TRUE)\n\nnetwork\n\nIGRAPH 006a09e DN-- 28 112 -- \n+ attr: name (v/c), source_sex (e/c)\n+ edges from 006a09e (vertex names):\n [1] Allyssa-&gt;David    Allyssa-&gt;Nolan    Allyssa-&gt;Jenna    Allyssa-&gt;Towo    \n [5] Avery  -&gt;John     Avery  -&gt;Chris    Avery  -&gt;Allyssa  Avery  -&gt;Meredith\n [9] Becca  -&gt;Chris    Becca  -&gt;Wyatt    Becca  -&gt;Jade     Becca  -&gt;Jenna   \n[13] Bob    -&gt;Dylan    Bob    -&gt;David    Bob    -&gt;Marlene  Bob    -&gt;Wendy   \n[17] Bohan  -&gt;Nolan    Bohan  -&gt;John     Bohan  -&gt;Katarina Bohan  -&gt;Reigne  \n[21] Chris  -&gt;Jacob    Chris  -&gt;Avery    Chris  -&gt;Bohan    Chris  -&gt;Allyssa \n[25] Dallas -&gt;Louis    Dallas -&gt;Robert   Dallas -&gt;Jenna    Dallas -&gt;Towo    \n[29] David  -&gt;Zach     David  -&gt;Dallas   David  -&gt;Bohan    David  -&gt;Jenna   \n+ ... omitted several edges\n\n\nLet’s better understand the information contained in an igraph object:\n\nIGRAPH simply annotates network as an igraph object\nWhatever random six digit alphanumeric string follows IGRAPH is simply how igraph identifies the graph for itself, it’s not important for our purposes.\nD would tell us that it is directed graph\nN indicates that network is a named graph, in that the vertices have a name attribute\n– refers to attributes not applicable to network, but we will see them in the future:\n28 refers to the number of vertices in network\n112 refers to the number of edges in network\nattr: is a list of attributes within the graph.\n(v/c), which will appear following name, tells us that it is a vertex attribute of a character data type.\n(e/c) or (e/n) referring to edge attributes that are of character or numeric data types\n\nedges from arbitrary igraph name (vertex names): lists a sample of network’s edges using the names of the vertices which they connect.\n\n\nNow let’s create a rough plot to look at our network!\n\nlay &lt;- layout_with_kk(network)\n\npar(bg=\"grey98\")\nplot(network, layout = lay, edge.color=\"grey80\",\n     vertex.color=\"lightblue\", vertex.label.color = \"black\")\n\n\n\n\n\n\n\n\nFor the rest of this post, we are going to talk about a few different measures of centrality, what they capture mathematically and intuitively, and we will look at plots where the size of the node corresponds to the relative centrality score."
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#measures-of-centrality",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#measures-of-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Measures of Centrality",
    "text": "Measures of Centrality\n\nDegree Centrality\nFor directed graphs, in-degree, or number of incoming points, is one way we can determine the importance factor for nodes. The Degree of a node is the number of edges that it has. The basic intuition is that, nodes with more connections are more influential and important in a network. In other words, the people with more friend nominations in our simulated social network are the ones with greater importance according to this metric.\n\nDegree.Directed &lt;- degree(network)\nIndegree &lt;- degree(network, mode=\"in\")\nOutdegree &lt;- degree(network, mode=\"out\")\n\nCompareDegree &lt;- cbind(Degree.Directed, Indegree, Outdegree)\n\n\nhead(CompareDegree, 10)\n\n\n\n\n\n\n\n\nDegree.Directed\nIndegree\nOutdegree\n\n\n\n\nAllyssa\n7\n3\n4\n\n\nAvery\n7\n3\n4\n\n\nBecca\n7\n3\n4\n\n\nBob\n7\n3\n4\n\n\nBohan\n12\n8\n4\n\n\nChris\n7\n3\n4\n\n\nDallas\n7\n3\n4\n\n\nDavid\n11\n7\n4\n\n\nDylan\n7\n3\n4\n\n\nGracie\n5\n1\n4\n\n\n\n\n\n\n\n\nThis is a very reasonable way to measure importance within a network. If we are trying to determine who in our classroom is the most popular, we might define that as the greatest number of friendship nominations.\n\nlay &lt;- layout_with_kk(network)\n\npar(bg=\"grey98\")\nplot(network, layout = lay, edge.color=\"grey80\",\n     vertex.size=degree(network, mode=\"in\")*2,  # Rescaled by multiplying by 2\n     main=\"In-Degree\", vertex.label.dist=1.5,\n     vertex.color=\"lightblue\", vertex.label.color = \"black\")"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#betweenness-centrality",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#betweenness-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Betweenness Centrality",
    "text": "Betweenness Centrality\nBetweenness Centrality is another centrality that is based on shortest path between nodes. It is determined as number of the shortest path passing by the given node. For starting node \\(s\\), destination node \\(t\\) and the input node \\(i\\) that holds \\(s \\ne t \\ne i\\), let \\(n_{st}^i\\) be 1 if node \\(i\\) lies on the shortest path between \\(s\\) and \\(t\\); and \\(0\\) if not. So the betweenness centrality is defined as:\n\\[x_i = \\sum_{st} n_{st}^i\\] However, there can be more than one shortest path between \\(s\\) and \\(t\\) and that will count for centrality measure more than once. Thus, we need to divide the contribution to \\(g\\_{st}\\), total number of shortest paths between \\(s\\) and \\(t\\).\n\\[x_i = \\sum_{st} \\frac{n_{st}^i}{g_{st}}\\]\nEssentially the size of the node here represents the frequency with which that node lies on the shortest path between other nodes.\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=betweenness(network)*0.25,  # Rescaled by multiplying by 0.25\n     main=\"Betweenness Centrality\", vertex.color=\"lightblue\")"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#eigenvector-centrality",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#eigenvector-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Eigenvector Centrality",
    "text": "Eigenvector Centrality\nEigenvector centrality is a basic extension of degree centrality, which defines centrality of a node as proportional to its neighbors’ importance. When we sum up all connections of a node, not all neighbors are equally important. This is a very interesting way to measure popularity in our classroom where we say the popularity of your friends matters more than the number of friends. Let’s consider two nodes in a friend network with same degree, the one who is connected to more central nodes should be more central.\nFirst, we define an initial guess for the centrality of nodes in a graph as \\(x_i=1\\). Now we are going to iterate for the new centrality value \\(x_i'\\) for node \\(i\\) as following:\n\\[x_i' = \\sum_{j} A_{ij}x_j\\]\nHere \\(A_{ij}\\) is an element of the adjacency matrix, where it gives 1 or 0 for whether an edge exists between nodes \\(i\\) and \\(j\\). it can also be written in matrix notation as \\(\\mathbf{x'} = \\mathbf{Ax}\\).\nWe iterate over t steps to find the vector \\(\\mathbf{x}(t)\\) as:\n\\[\\mathbf{x}(t) = \\mathbf{A^t x}(0)\\]\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=evcent(network)$vector*15,   # Rescaled by multiplying by 15\n     main=\"Eigenvector Centrality\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\nThe plot shows, the students which have the same number of friend nominations are not necessarily in the same size. The one that is connected to more central, or “popular” nodes are larger in this visualization.\nHowever, as we can see from the definition, this can be a problematic measure for directed graphs. Let’s say that a student who received no friend nominations themselves nominates another student as a friend. Because that person has 0 friend nominations themselves, they would not contribute any importance to the person they nominated. In other words, eigenvector centrality would not take zero in-degree nodes into account in directed graphs.\nHowever there is a solution to this!"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#katz-centrality",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#katz-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Katz Centrality",
    "text": "Katz Centrality\nKatz centrality introduces two positive constants \\(\\alpha\\) and \\(\\beta\\) to tackle the problem of eigenvector centrality with zero in-degree nodes:\n\\[x_i = \\alpha \\sum_{j} A_{ij} x_j + \\beta\\],\nagain \\(A_{ij}\\) is an element of the adjacency matrix, and it can also be written in matrix notation as \\(\\mathbf{x} = \\alpha \\mathbf{Ax} + \\beta \\mathbf{1}\\). This \\(\\beta\\) constant gives a free centrality contribution for all nodes even though they don’t get any contribution from other nodes. The existence of a node alone would provide it some importance. \\(\\alpha\\) constant determines the balances between the contribution from other nodes and the free constant.\nUnfortunately, igraph does not have a function to compute Katz centrality, so we will need to do it the old fashioned way.\n\nkatz.centrality = function(g, alpha, beta, t) {\n  n = vcount(g);\n  A = get.adjacency(g);\n  x0 = rep(0, n);\n  x1 = rep(1/n, n);\n  eps = 1/10^t;\n  iter = 0;\n  while (sum(abs(x0 - x1)) &gt; eps) {\n    x0 = x1;\n    x1 = as.vector(alpha * x1 %*% A) + beta;\n    iter = iter + 1;\n  } \n  return(list(aid = x0, vector = x1, iter = iter))\n}\n\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=katz.centrality(network, 0.1, 1, 0.01)$vector*5,   # Rescaled by multiplying by 15\n     main=\"Katz Centrality\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\nAlthough this method is introduced as a solution for directed graphs, it can be useful for some applications of undirected graphs as well."
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#hits-hubs-and-authorities",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#hits-hubs-and-authorities",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "HITS Hubs and Authorities",
    "text": "HITS Hubs and Authorities\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=hub.score(network)$vector*15,   # Rescaled by multiplying by 15\n     main=\"HITS Hubs\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5 , vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=authority.score(network)$vector*15,   # Rescaled by multiplying by 15\n     main=\"HITS Authorities\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\nUp until this point, we have discussed the measures that captures high node centrality, however, there can be nodes in the network which are important for the network, but they are not central. In order to find out such nodes, the HITS algorithm introduces two types of central nodes: Hubs and Authorities. For Hubs, we might consider a node important if it links to many highly nominated nodes (i.e. the person nominates many popular people as their friends). For Authorities, we might consider a node to be of importance if many highly nominated nodes link to it (i.e. nominated by many popular people).\nAuthority Centrality is defined as the sum of the hub centralities which point to the node (i):\n\\[x_i = \\alpha \\sum_{j} A_{ij} y_j,\\]\nwhere \\(\\alpha\\) is constant. Likewise, Hub Centrality is the sum of the authorities which are pointed by the node \\(i\\):\n\\[y_i = \\beta \\sum_{j} A_{ji} x_j,\\]\nwith constant \\(\\beta\\). Here notice that the element of the adjacency matrix are swapped for Hub Centrality because we are concerned with outgoing edges for hubs. So in matrix notation:\n\\[\\mathbf{x} = \\alpha \\mathbf{Ay}, \\quad\\]\n\\[\\mathbf{y} = \\beta \\mathbf{A^Tx}.\\] As it can be seen from the drawing, HITS Algorithm also tackles the problem with zero in-degree nodes of Eigenvector Centrality. These zero in-degree nodes become central hubs and contribute to other nodes. Yet we can still use a free centrality contribution constant like in Katz Centrality or other variants.\nAlthough these measures are generally used in a citation network or the internet, this can be pretty interesting in the context of our classroom frienship network. Maybe we are interested in knowing who are the non-popular students that nominate many popular students as friends, but receive no reciprocity in terms of nominations."
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#other-packages-that-i-like-for-visualization",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#other-packages-that-i-like-for-visualization",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Other Packages that I Like for Visualization",
    "text": "Other Packages that I Like for Visualization\nI want to conclude this post with some sample code to produce a nice network plot using the ggnetwork package. igraph is only one package that exists for network visualization and centrality calculations, I encourage you to check out additional packages which may be stronger than igraph for some purposes.\n\nlibrary(GGally)\nlibrary(network)\nlibrary(ggnetwork)\n\nnet &lt;- list(nodes=friendships[c('source', 'target', 'source_sex')], \n            edges=friendships[c('source', 'target', 'source_sex')])\n\n# create node attribute data\nnet.cet &lt;- as.character(net$nodes$source_sex)\nnames(net.cet) = net$nodes$source\nedges &lt;- net$edges\n\n# create network\nnet.net &lt;- edges[, c(\"source\", \"target\") ]\nnet.net &lt;- network::network(net.net, directed = TRUE)\n\n# create sourc sex node attribute\nnet.net %v% \"source_sex\" &lt;- net.cet[ network.vertex.names(net.net) ]\n\n\nset.seed(1)\nggnet2(net.net, color = \"source_sex\",\n       palette = c(\"Female\" = \"purple\", \"Male\" = \"maroon\"), size = 'indegree',\n       arrow.size = 3, arrow.gap = 0.04, alpha = 1,  label = TRUE, vjust = 2.5, label.size = 3.5,\n       edge.alpha = 0.5, mode = \"kamadakawai\",edge.color = 'grey50',\n       color.legend = \"Student Sex\") + theme_bw() + theme_blank()  + \n  theme(legend.position = \"bottom\", text = element_text(size = 15),\n        plot.caption = element_text(hjust = 0)) + guides(size=F) + \n  labs(title='Social Network Mapping of Friendship Nominations for Simulated Data', \n       caption = str_wrap(\"The size of the dots represent the number of \n                          friendship nominations by others. The maroon dots represent males \n                          while the purple dots represent females. Directed edges are used \n                          to indicate the direction of friendship nominations. \n                          Edges that are bi-directional indicated friendship reciprocity. \n                          \\n\\n This particular network represents the frienship nominations \n                          of 28 students from our simulated data.\", 128))\n\n\n\n\n\n\n\n\nThank you!\nJacob"
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Coding",
    "section": "",
    "text": "An article about computational science in a scientific publication is not the scholarship itself, it is merely advertising of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures. - J. Buckheit and D. Donoho\nI am committed to open science principles and research reproducibility. You can find the code repositories for my academic papers on my Github."
  },
  {
    "objectID": "code.html#introduction-to-r-programming",
    "href": "code.html#introduction-to-r-programming",
    "title": "Coding",
    "section": "Introduction to R Programming",
    "text": "Introduction to R Programming\nThis page contains the content for the “Introduction to Data Analysis in R” mini-course. The goal is to introduce R programming concepts with a focus on preparing and analyzing data, and conducting statistical simulations. We will cover how to read data into R, manipulate data with a suite of tools from the tidyverse package dplyr. We will also discuss some basic programming concepts including data types and operators. This mini-course is aimed at people with no programming background that are looking for a gentle and practical introduction to data analysis in R."
  },
  {
    "objectID": "horror.html",
    "href": "horror.html",
    "title": "Horror Plots",
    "section": "",
    "text": "Most Haunted Places in America\n\n\n\n\n\n\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n\n\nHorrorPlots Slasher Kill Count\n\n\n\n\n\n\n\n\n\n\n\n\njacob Jameson\n\n\n\n\n\n\n\n\nTidyTuesday Horror Movies\n\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\njacob Jameson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data/horrorplots/ridgeplot/horror_movies.html",
    "href": "Data/horrorplots/ridgeplot/horror_movies.html",
    "title": "#TidyTuesday Horror Movies",
    "section": "",
    "text": "horror_movies <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-01/horror_movies.csv')\n\n\nhorror_movies <- horror_movies %>%\n  mutate(year = year(release_date)) \n\nhorror_comedies <- filter(horror_movies, grepl(\"Comedy\",genre_names)) \nhorror_comedies <- filter(horror_comedies, year > 2011)\n\n\n# Plot\nggplot(horror_comedies, aes(x = `vote_average`, y = factor(year), fill = ..x..)) +\n  geom_density_ridges_gradient(scale = 6, rel_min_height = 0.01) +\n  scale_fill_gradient(low = \"#eb0e34\",high = \"#910019\") +\n  theme_void() + \n    theme(\n      legend.position=\"none\",\n      text = element_text(family = \"creepster\", color = \"#eb0e34\"),\n      axis.line = element_blank(),\n      axis.text.x = element_text(family = \"creepster\", color = \"#eb0e34\", size = 20),\n      axis.text.y = element_text(family = \"creepster\", color = \"#eb0e34\", size = 15),\n      panel.grid.major = element_blank(),\n      axis.title.x = element_text(family = \"creepster\", color = \"#eb0e34\", size = 10),\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"black\", color = NA), \n      panel.background = element_rect(fill = \"black\", color = NA), \n      legend.background = element_blank()) +\n  xlab(\"\\nAverage Voter Score\") +\n  annotation_custom(xmin=-Inf, ymin=-Inf, xmax=Inf, ymax=Inf, rasterGrob(w)) +\n   theme(plot.title = element_text(size = 25, family = \"creepster\", \n                                  face=\"bold\", hjust=.5, color = 'orange'),\n         plot.subtitle = element_text(family = \"creepster\", size = 10,  color='orange',\n                                      hjust=.5, margin=margin(2, 0, 5, 0)),\n         plot.caption = element_text(size = 6, family = \"creepster\", \n                                    color='orange', hjust=1, margin=margin(2, 0, 5, 0)))+\n  labs(title = \"THE PUBLIC IS SPLIT ON THE HORROR COMEDY\", \n       subtitle = 'DISTRIBUTION OF AVERAGE VOTER SCORES FOR HORROR\n       COMEDIES RELEASED OVER THE LAST 10 YEARS',\n       caption = str_wrap(\"\\nData source : Horror movies dataset extracted by\\n\n                          Tanya Shapiro from The Movie Database (TMDB)\", 70))"
  },
  {
    "objectID": "Data/horrorplots/kill count/kill count.html",
    "href": "Data/horrorplots/kill count/kill count.html",
    "title": "#HorrorPlots Slasher Kill Count",
    "section": "",
    "text": "plt <- ggplot(df) +\n  geom_hline(\n    aes(yintercept = y), \n    data.frame(y = c(0:2) * 100),\n    color = \"lightgrey\"\n  ) + \n  geom_col(\n    aes(\n      x = reorder(str_wrap(Slasher, 5), Kill.Count),\n      y = Kill.Count,\n      fill = Movies,\n    ),\n    position = \"dodge2\",\n    width = 0.75,\n    show.legend = TRUE,\n    alpha = .9\n  ) +\n  # Lollipop shaft for mean gain per region\n  geom_segment(\n    aes(\n      x = reorder(str_wrap(Slasher, 5), Kill.Count),\n      y = 0,\n      xend = reorder(str_wrap(Slasher, 5), Kill.Count),\n      yend = 190\n    ),\n    linetype = \"dashed\",\n    color = \"black\"\n  ) + \n  \n  # Make it circular!\n  coord_polar()\n\nplt\n\n\n\n\n\nplt <- plt +\n  annotate(\n    x = 10, \n    y = 170,\n    label = \"Number of Kills\",\n    geom = \"text\",\n    color = \"#FC3205\",\n    size = 5.5,\n    family = \"creepster\"\n  ) +\n  # Annotate custom scale inside plot\n  annotate(\n    x = 11.7, \n    y = 60, \n    label = \"50\", \n    geom = \"text\", \n    color = \"gray12\", \n    family = \"creepster\"\n  ) +\n  annotate(\n    x = 11.7, \n    y = 110, \n    label = \"100\", \n    geom = \"text\", \n    color = \"gray12\", \n    family = \"creepster\"\n  ) +\n  annotate(\n    x = 11.7, \n    y = 160, \n    label = \"150\", \n    geom = \"text\", \n    color = \"gray12\", \n    family = \"creepster\"\n  ) +\n  # Scale y axis so bars don't start in the center\n  scale_y_continuous(\n    limits = c(0, 180),\n    expand = c(0, 0),\n  ) + \n  # New fill and legend title for number of tracks per region\n  scale_fill_gradientn(\n    \"Number of Movies\",\n     colours = c(\"#621605\",\"#91210C\",\"#D92E08\",\"#FC3205\")\n  ) + \n  # Make the guide for the fill discrete\n  guides(\n    fill = guide_colorsteps(\n      barwidth = 15, barheight = .5, title.position = \"top\", title.hjust = .5\n    )\n  ) +\n  theme(\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(color = \"black\", \n                               size = 11, family = 'creepster'),\n    legend.position = \"top\",\n  )\n\nplt\n\n\n\n\n\nplt <- plt + \n  labs(\n    title = \"\\nNumber of Kills\\nOver Slasher Franchise\",\n    subtitle  = \"\\n\\nData Visualization by @JacobCJameson\\nSource: Dead Meat Wiki\\nLink to Data: https://the-dead-meat.fandom.com/wiki\") +\n  # Customize general theme\n  theme(\n    # Set default color and font family for the text\n    text = element_text(color = \"gray12\", family = \"techmono\"),\n    # Customize the text in the title, subtitle, and caption\n    plot.title = element_text(face = \"bold\", size = 20, hjust = 0.5, family = 'nosifer'),\n    plot.subtitle = element_text(size = 10, hjust = .5),\n    # Make the background white and remove extra grid lines\n    panel.background = element_rect(fill = \"#f5f5f2\", color = \"#f5f5f2\"),\n    plot.background = element_rect(fill = \"#f5f5f2\", color = NA),\n    legend.background = element_rect(fill = \"#f5f5f2\", color = NA)\n    #panel.grid = element_blank(),\n    #panel.grid.major.x = element_blank()\n  )\n\nplt"
  },
  {
    "objectID": "Data/Intro_R_Course/Intro R.html",
    "href": "Data/Intro_R_Course/Intro R.html",
    "title": "Introduction to Programming in R",
    "section": "",
    "text": "This page contains the content for the “Introduction to Data Analysis in R” mini-course. The goal is to introduce R programming concepts with a focus on preparing and analyzing data, and conducting statistical simulations. We will cover how to read data into R, manipulate data with a suite of tools from the tidyverse package dplyr. We will also discuss some basic programming concepts including data types and operators.\nThis mini-course is aimed at people with no programming background that are looking for a gentle and practical introduction to data analysis in R.\nEach Module involves:\n\na pre-recorded lecture with guided practice exercises\na corresponding lab to practice the skills taught in the video and stretch your thinking\n\nThe labs are an essential part of the learning process as they walkthrough the use of many key functions and topics that are not explicitly covered in the videos."
  },
  {
    "objectID": "Data/Intro_R_Course/Intro R.html#table-of-contents",
    "href": "Data/Intro_R_Course/Intro R.html#table-of-contents",
    "title": "Introduction to Programming in R",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nModule 0: R and RStudio Installation Guide\nModule 1: An Introduction and Motivation for R Programming\nModule 2: Installing Packages and Reading Data\nModule 3: Vectors and Lists\nModule 4: Data Manipulation\nModule 5: Data Manipulation and Analysis II\nModule 6: Data Visualization as a Tool for Analysis\nModule 7: Grouped Analysis\nModule 8: Iteration\nModule 9: Writing Functions\n\n\nAdditional Resources\n\n\n\ntidyverse cheetsheets start with dplyr and ggplot\n\n\nR for Data Science: free online book with clear explanations of many tidyverse functions, the book to read on data analysis with R"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Jacob Jameson",
    "section": "",
    "text": "Welcome to my blog!\n\n\n\n\n\n\n\n\n  \n\n\n\n\nThe ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research\n\n\n\n\n\n\n\nmarginal effects\n\n\n\n\nMarginal Effects &gt;&gt;&gt; Odds Ratios\n\n\n\n\n\n\nSep 1, 2023\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nBasic Network Analysis and Visualization for Directed Graphs in R\n\n\n\n\n\n\n\nnetworks\n\n\ncentrality\n\n\n\n\nChoosing the Right Centrality Measure.\n\n\n\n\n\n\nNov 1, 2022\n\n\nJacob Jameson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data/horrorplots/haunted/haunted.html",
    "href": "Data/horrorplots/haunted/haunted.html",
    "title": "Most Haunted Places in America",
    "section": "",
    "text": "usa <- st_as_sf(maps::map(\"state\", fill=TRUE, plot =FALSE))\nhaunted <- read_csv('haunted_places.csv')\n\nhaunted$ID = tolower(haunted$state)\n\nhaunted <- haunted %>%\n  group_by(ID) %>% summarize(`Number of Haunted Locations` = n()) \n\nhaunts <- merge(usa, haunted, by='ID')\n\ngg_nc = ggplot(haunts) +\n  geom_sf(aes(fill = `Number of Haunted Locations`)) +\n  scale_fill_viridis(option = \"F\") + theme_bw() +labs(\n     title = str_to_upper(\"Most Haunted U.S. States\\n\"),\n     caption = str_wrap(\"Data was extracted from Shadow Lands which has a great \n                        index of haunted places. Locations where “ghosts and hauntings” \n                        have been witnessed are included in the list and people can \n                        report new sightings through the website — which is surely \n                        an extremely accurate and scientific method.· @JacobCJameson\"), 50) +\n   theme(\n     plot.title = element_text(family = \"creepster\", size = 40),\n     plot.caption = element_text(color = \"#111111\", size = 7),\n     axis.line=element_blank(), \n        axis.text.x=element_blank(), \n     axis.title.x=element_blank(),\n        axis.text.y=element_blank(), \n     axis.title.y=element_blank(),\n        axis.ticks=element_blank(), panel.background = element_blank())\n\ngg_nc\n\n\n\n     plot_gg(gg_nc\n            , width=6\n            , height = 6\n            , multicore = TRUE\n            , windowsize = c(1400,866)\n            , sunangle=225\n            , zoom = 0.60\n            , phi = 30\n            , theta = 45\n            )\n     \nrender_depth(focallength=50)"
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Tidy Tuesday, 2023 Week 7 🎬\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 7, 2023: Hollywood Age Gaps\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 6 📈\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 6, 2023: Big Tech Stock Prices\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 5 🐱\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 5, 2023: Pet Cats UK\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 4 🐻\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 4, 2023: ALONE\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 3 🎨\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 3, 2023: Art History\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 2 🐦\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 2, 2023: Project FeederWatch\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 1 🏡\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 1, 2023: Bring your own data from 2022!\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHealthcare Area Chart\n\n\n\n\n\n\n\narea chart\n\n\nD3\n\n\n\n\nIn 2020, a greater percent of previously uninsured people in the U.S. with low incomes were enrolled in Medicare or Medicaid.\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data/viz/healthcare area chart/healthcare_areachart.html",
    "href": "Data/viz/healthcare area chart/healthcare_areachart.html",
    "title": "Healthcare Area Chart",
    "section": "",
    "text": "Code here"
  },
  {
    "objectID": "Data/viz/lollipop timeseries/multiple-line-chart.html",
    "href": "Data/viz/lollipop timeseries/multiple-line-chart.html",
    "title": "Multiple Line Chart",
    "section": "",
    "text": "Tom Brady’s Greatness\n\n\nTom Brady has retired from football after a 22-year career of consistent success and unmatched achievement. But his stature as the N.F.L.’s greatest quarterback may be best understood by seeing his achievements stacked up against those of hundreds of others who played the same position.\n\n\nPlayoff Wins\n\n\nBy Age"
  },
  {
    "objectID": "posts/2022-12-24-ORs/2022-12-24-ORs.html",
    "href": "posts/2022-12-24-ORs/2022-12-24-ORs.html",
    "title": "Odds Ratio Schmodds Ratio: Report the Relative Risk",
    "section": "",
    "text": "library(tidyverse)\nlibrary(RNHANES)\nlibrary(pROC)\nlibrary(sjPlot)\nlibrary(sjmisc)\nlibrary(sjlabelled)\n\n\nd07 = nhanes_load_data(\"DEMO_E\", \"2007-2008\") %>%\n  select(SEQN, cycle, RIAGENDR, RIDAGEYR) %>%\n  transmute(SEQN=SEQN, wave=cycle, RIAGENDR, RIDAGEYR) %>% \n  left_join(nhanes_load_data(\"VID_E\", \"2007-2008\"), by=\"SEQN\") %>%\n  select(SEQN, wave, RIAGENDR, RIDAGEYR, LBXVIDMS) %>% \n  transmute(SEQN, wave, RIAGENDR, RIDAGEYR, vitD=LBXVIDMS) %>% \n  left_join(nhanes_load_data(\"BIOPRO_E\", \"2007-2008\"), by=\"SEQN\") %>%\n  select(SEQN, wave, RIAGENDR, RIDAGEYR, vitD, LBXSCA) %>% \n  transmute(SEQN, wave, RIAGENDR, RIDAGEYR, vitD, Calcium = LBXSCA) %>% \n  left_join(nhanes_load_data(\"OSQ_E\", \"2007-2008\"), by=\"SEQN\") %>%\n  select(SEQN, wave, RIAGENDR, RIDAGEYR, vitD, Calcium, OSQ060) %>% \n  transmute(SEQN, wave, RIAGENDR, RIDAGEYR, vitD, Calcium, Osteop = OSQ060)\n\nd09 = nhanes_load_data(\"DEMO_F\", \"2009-2010\") %>%\n  select(SEQN, cycle, RIAGENDR, RIDAGEYR) %>%\n  transmute(SEQN=SEQN, wave=cycle, RIAGENDR, RIDAGEYR) %>% \n  left_join(nhanes_load_data(\"VID_F\", \"2009-2010\"), by=\"SEQN\") %>%\n  select(SEQN, wave, RIAGENDR, RIDAGEYR, LBXVIDMS) %>% \n  transmute(SEQN, wave, RIAGENDR, RIDAGEYR, vitD=LBXVIDMS) %>% \n  left_join(nhanes_load_data(\"BIOPRO_F\", \"2009-2010\"), by=\"SEQN\") %>%\n  select(SEQN, wave, RIAGENDR, RIDAGEYR, vitD,  LBXSCA) %>% \n  transmute(SEQN, wave, RIAGENDR, RIDAGEYR, vitD, Calcium = LBXSCA) %>% \n  left_join(nhanes_load_data(\"OSQ_F\", \"2009-2010\"), by=\"SEQN\") %>%\n  select(SEQN, wave, RIAGENDR, RIDAGEYR, vitD, Calcium, OSQ060) %>% \n  transmute(SEQN, wave, RIAGENDR, RIDAGEYR, vitD, Calcium, Osteop = OSQ060)\n\ndat = bind_rows(d07, d09) %>% as.data.frame()\n\nInstitute of Medicine cutoffs for Vitamin D\nVitamin D deficiency: Serum 25OHD less than 30 nmol/L (12 ng/mL)\n\ndat <- dat %>% \n  mutate(vitD_deficient = ifelse(vitD < 30, 1, 0))\n\n#exclude missing observations\ndat <- dat %>% \n  filter(!is.na(vitD_deficient), !is.na(Calcium), !is.na(Osteop), Osteop!=9) %>% \n  mutate(Gender = recode_factor(RIAGENDR, \n                           `1` = \"Man\", \n                           `2` = \"Woman\"),\n         Osteop = recode_factor(Osteop, \n                           `1` = 1, \n                           `2` = 0))\n\nhead(dat)\n\n   SEQN      wave RIAGENDR RIDAGEYR vitD Calcium Osteop vitD_deficient Gender\n1 41475 2007-2008        2       62 58.8     9.5      0              0  Woman\n2 41477 2007-2008        1       71 81.8    10.0      0              0    Man\n3 41479 2007-2008        1       52 78.4     9.0      0              0    Man\n4 41482 2007-2008        1       64 61.9     9.1      0              0    Man\n5 41483 2007-2008        1       66 53.3     8.9      0              0    Man\n6 41485 2007-2008        2       30 39.1     9.3      0              0  Woman\n\n\n\nfit <- glm(Osteop ~ vitD_deficient + Calcium + Gender + RIDAGEYR, \n           data = dat, family = \"binomial\"(link = \"logit\"))\n\ntab_model(fit)\n\n\n\n\n \nOsteop\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n1879.73\n239.24 – 14670.09\n<0.001\n\n\nvitD deficient\n1.58\n1.13 – 2.28\n0.011\n\n\nCalcium\n1.09\n0.88 – 1.35\n0.443\n\n\nGender [Woman]\n0.13\n0.10 – 0.16\n<0.001\n\n\nRIDAGEYR\n0.93\n0.92 – 0.94\n<0.001\n\n\nObservations\n10244\n\n\nR2 Tjur\n0.136\n\n\n\n\n\n\n\nplot_model(fit, vline.color = \"red\") + theme_bw()"
  },
  {
    "objectID": "Data/Intro_R_Course/mod0.html",
    "href": "Data/Intro_R_Course/mod0.html",
    "title": "Module 0",
    "section": "",
    "text": "Mac Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Mac version, save the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder.\n\n\nWindows Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Windows version, and save the executable file. Run the .exe file and follow the installation instructions."
  },
  {
    "objectID": "Data/Intro_R_Course/mod0.html#r-and-rstudio-installation-guide",
    "href": "Data/Intro_R_Course/mod0.html#r-and-rstudio-installation-guide",
    "title": "Module 0",
    "section": "R and RStudio Installation Guide",
    "text": "R and RStudio Installation Guide\n\nMac Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Mac version, save the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder.\n\n\nWindows Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Windows version, and save the executable file. Run the .exe file and follow the installation instructions."
  },
  {
    "objectID": "R files/mod0.html",
    "href": "R files/mod0.html",
    "title": "Module 0",
    "section": "",
    "text": "Mac Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Mac version, save the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder.\n\n\nWindows Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Windows version, and save the executable file. Run the .exe file and follow the installation instructions."
  },
  {
    "objectID": "Intro R.html",
    "href": "Intro R.html",
    "title": "Introduction to Programming in R",
    "section": "",
    "text": "This page contains the content for the “Introduction to Data Analysis in R” mini-course. The goal is to introduce R programming concepts with a focus on preparing and analyzing data, and conducting statistical simulations. We will cover how to read data into R, manipulate data with a suite of tools from the tidyverse package dplyr. We will also discuss some basic programming concepts including data types and operators.\nThis mini-course is aimed at people with no programming background that are looking for a gentle and practical introduction to data analysis in R.\nEach Module involves:\n\na pre-recorded lecture with guided practice exercises\na corresponding lab to practice the skills taught in the video and stretch your thinking\n\nThe labs are an essential part of the learning process as they walkthrough the use of many key functions and topics that are not explicitly covered in the videos.\n\nAdditional Resources\n\n\n\ntidyverse cheetsheets start with dplyr and ggplot\n\n\nR for Data Science: free online book with clear explanations of many tidyverse functions, the book to read on data analysis with R"
  },
  {
    "objectID": "Intro R.html#table-of-contents",
    "href": "Intro R.html#table-of-contents",
    "title": "Introduction to Programming in R",
    "section": "Table of Contents",
    "text": "Table of Contents"
  },
  {
    "objectID": "R files/0 Module/mod0.html",
    "href": "R files/0 Module/mod0.html",
    "title": "Module 0",
    "section": "",
    "text": "Mac Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Mac version, save the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder.\n\n\nWindows Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Windows version, and save the executable file. Run the .exe file and follow the installation instructions."
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html",
    "href": "R files/0 Module/2022-11-01-SNA.html",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "",
    "text": "“There is certainly no unanimity on exactly what centrality is or on its conceptual foundations, and there is little agreement on the proper procedure for its measurement.” - Linton Freeman (1977)\nSocial network analysis can be used to measure the importance of a person as a function of the social structure of a community or organization. This post uses visualization as a tool to explain how different measures of centrality may be used to analyze different questions in a network analysis. In these examples we will be specifically looking at directed graphs to compare the following centrality measures and their use-cases:\n\nDegree Centrality\nBetweenness Centrality\nEigenvector Centrality\nKatz Centrality\nHITS Hubs and Authorities\n\nAn example of a directed graph would be one in which people nominate their top 2 friends. In this graph, nodes (people) would connect to others nodes through directed edges (nominations). It is possible for Jacob to nominate Jenna without Jenna nominating him back. You can imagine why centrality in a friendship network might take into account the direction of these nominations. If I list 100 people as my friends and none of them list me back, do we think I am a popular person?"
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html#simulate-our-data",
    "href": "R files/0 Module/2022-11-01-SNA.html#simulate-our-data",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Simulate Our Data",
    "text": "Simulate Our Data\nFor our simulated data we are going to be looking at a classroom that contains 14 male and 14 female students. Suppose that each student was asked to name their top 2 male and top 2 female friends in the class. We are interested in analyzing a slew of different research questions where the centrality of student in the class may be of importance.\nLet’s create the data:\n\nlibrary(tidyverse)\n\nWe begin with a 4 vectors: all males in the classroom, all females in the classroom, a probability distribution for selecting friends of the same sex, a probability distribution for selecting friends of the opposite sex.\n\nmales <- c('Jacob', 'Louis', 'Chris', 'Wyatt', 'Nolan', 'Robert', \n           'Zach', 'John','Bob', 'David', 'Avery', 'Ronald', \n           'Dallas', 'Dylan')\n\nfemales <- c('Bohan', 'Jenna', 'Katarina', 'Hassina', 'Towo', \n             'Becca', 'Meredith', 'Gracie', 'Kayla', 'Marlene', \n             'Jade', 'Allyssa', 'Reigne', 'Wendy')\n\nprobs.diff.sex = c(0.15,0.15,0.05,0.05,0.1,0.1,0.05,0.1,0.01,0.09,0.05,0.05,0.025,0.025)\nprobs.same.sex = c(0.15,0.15,0.05,0.05,0.1,0.1,0.05,0.1,0.01,0.09,0.05,0.05,0.05)\n\nThe function below, simulate.top.friends, will produce a dataframe that will contain each student’s picks for their top 2 male and top 2 female friends in the classroom.\n\nset.seed(1997)\n\nsimulate.top.friends <- function(males, females, probs.diff.sex, probs.same.sex) {\n\n  dat <- setNames(data.frame(matrix(ncol = 6, nrow = 0)), \n                  c(\"Ego\", \"Ego Sex\", \"MF1\", \"MF2\", \"FF1\", \"FF2\"))\n\n  for (ego in males) {\n    temp.males <- males[! males %in% ego]\n    \n    male.friends.i <- sample.int(13, 2, replace = FALSE, prob = probs.same.sex)\n    female.friends.i <- sample.int(14, 2, replace = FALSE, prob = probs.diff.sex)\n    \n    male.friend.1 <- temp.males[male.friends.i[1]]\n    male.friend.2 <- temp.males[male.friends.i[2]]\n    \n    female.friend.1 <- females[female.friends.i[1]]\n    female.friend.2 <- females[female.friends.i[2]]\n    \n    dat[nrow(dat) + 1,] = c(ego, 'Male', male.friend.1, male.friend.2, \n                           female.friend.1, female.friend.2)\n    \n  }\n    for (ego in females) {\n    temp.females <- females[! females %in% ego]\n    \n    male.friends.i <- sample.int(14, 2, replace = FALSE, prob = probs.diff.sex)\n    female.friends.i <- sample.int(13, 2, replace = FALSE, prob = probs.same.sex)\n    \n    male.friend.1 <- males[male.friends.i[1]]\n    male.friend.2 <- males[male.friends.i[2]]\n    \n    female.friend.1 <- temp.females[female.friends.i[1]]\n    female.friend.2 <- temp.females[female.friends.i[2]]\n    \n    dat[nrow(dat) + 1,] = c(ego, 'Female', male.friend.1, male.friend.2, \n                           female.friend.1, female.friend.2)\n    \n  }\n  return(dat)\n}\n\nLet’s take a look at our friendship data that we will be working with!\n\nsimulate.top.friends(males,females, probs.diff.sex, probs.same.sex)\n\n\n\n\n\n \n  \n    Ego \n    Ego Sex \n    MF1 \n    MF2 \n    FF1 \n    FF2 \n  \n \n\n  \n    Jacob \n    Male \n    Chris \n    Avery \n    Katarina \n    Meredith \n  \n  \n    Louis \n    Male \n    Jacob \n    Avery \n    Towo \n    Hassina \n  \n  \n    Chris \n    Male \n    Jacob \n    Robert \n    Gracie \n    Becca \n  \n  \n    Wyatt \n    Male \n    Nolan \n    Jacob \n    Marlene \n    Jenna \n  \n  \n    Nolan \n    Male \n    Jacob \n    Bob \n    Bohan \n    Towo \n  \n  \n    Robert \n    Male \n    Nolan \n    Louis \n    Jenna \n    Bohan \n  \n  \n    Zach \n    Male \n    Jacob \n    Avery \n    Meredith \n    Towo \n  \n  \n    John \n    Male \n    Bob \n    Louis \n    Bohan \n    Wendy \n  \n  \n    Bob \n    Male \n    Avery \n    Jacob \n    Jenna \n    Wendy \n  \n  \n    David \n    Male \n    Wyatt \n    Louis \n    Becca \n    Jenna \n  \n  \n    Avery \n    Male \n    Louis \n    Jacob \n    Jenna \n    Becca \n  \n  \n    Ronald \n    Male \n    Jacob \n    Robert \n    Becca \n    Wendy \n  \n  \n    Dallas \n    Male \n    Robert \n    Jacob \n    Bohan \n    Jenna \n  \n  \n    Dylan \n    Male \n    Jacob \n    David \n    Towo \n    Marlene \n  \n  \n    Bohan \n    Female \n    Robert \n    Nolan \n    Allyssa \n    Kayla \n  \n  \n    Jenna \n    Female \n    David \n    Bob \n    Katarina \n    Meredith \n  \n  \n    Katarina \n    Female \n    David \n    Nolan \n    Gracie \n    Reigne \n  \n  \n    Hassina \n    Female \n    Robert \n    David \n    Becca \n    Jade \n  \n  \n    Towo \n    Female \n    John \n    Nolan \n    Jade \n    Jenna \n  \n  \n    Becca \n    Female \n    Nolan \n    Dallas \n    Kayla \n    Towo \n  \n  \n    Meredith \n    Female \n    Louis \n    Jacob \n    Towo \n    Bohan \n  \n  \n    Gracie \n    Female \n    Jacob \n    Ronald \n    Bohan \n    Hassina \n  \n  \n    Kayla \n    Female \n    Ronald \n    Louis \n    Bohan \n    Gracie \n  \n  \n    Marlene \n    Female \n    John \n    Robert \n    Hassina \n    Towo \n  \n  \n    Jade \n    Female \n    Ronald \n    Louis \n    Hassina \n    Bohan \n  \n  \n    Allyssa \n    Female \n    Avery \n    Zach \n    Gracie \n    Bohan \n  \n  \n    Reigne \n    Female \n    John \n    Zach \n    Bohan \n    Hassina \n  \n  \n    Wendy \n    Female \n    Nolan \n    Jacob \n    Towo \n    Bohan"
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html#creating-a-graphing-object",
    "href": "R files/0 Module/2022-11-01-SNA.html#creating-a-graphing-object",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Creating a Graphing Object",
    "text": "Creating a Graphing Object\nThere are many available centrality measures that have been developed for network analysis. At this time, there are no packages that are so comprehensive that it includes all of the measures. I will, therefore, limit this discussion to a subset of the measures that are included in igraph.\nigraph has a really great function that allows us to turn a dataframe into an igraph object. However, the function requires our data to be in “long” form so we will need to do some reshaping. Let’s restructure our data such that each row represents one directed friend nomination. We are going to call this “Source-Target Form”.\n\nlibrary(reshape) \n\nfriendships <- melt(simulate.top.friends(males,females,probs.diff.sex, probs.same.sex), \n                    id=c(\"Ego\", \"Ego Sex\")) %>%\n  select(source=Ego, source_sex =`Ego Sex`, target=value) %>% \n  arrange(source)\n\nLet’s look at the first 10 observations so we can understand the format needed to turn this data into an igraph object.\n\nhead(friendships, 10)\n\n\n\n\n\n \n  \n    source \n    source_sex \n    target \n  \n \n\n  \n    Allyssa \n    Female \n    David \n  \n  \n    Allyssa \n    Female \n    Nolan \n  \n  \n    Allyssa \n    Female \n    Jenna \n  \n  \n    Allyssa \n    Female \n    Towo \n  \n  \n    Avery \n    Male \n    John \n  \n  \n    Avery \n    Male \n    Chris \n  \n  \n    Avery \n    Male \n    Allyssa \n  \n  \n    Avery \n    Male \n    Meredith \n  \n  \n    Becca \n    Female \n    Chris \n  \n  \n    Becca \n    Female \n    Wyatt \n  \n\n\n\n\n\nWe will use the graph_from_data_frame function to create the igraph object.\n\nlibrary(igraph)\nnetwork <- graph_from_data_frame(friendships[,c('source','target','source_sex')],\n                                 directed = TRUE)\n\nnetwork\n\nIGRAPH bedd467 DN-- 28 112 -- \n+ attr: name (v/c), source_sex (e/c)\n+ edges from bedd467 (vertex names):\n [1] Allyssa->David    Allyssa->Nolan    Allyssa->Jenna    Allyssa->Towo    \n [5] Avery  ->John     Avery  ->Chris    Avery  ->Allyssa  Avery  ->Meredith\n [9] Becca  ->Chris    Becca  ->Wyatt    Becca  ->Jade     Becca  ->Jenna   \n[13] Bob    ->Dylan    Bob    ->David    Bob    ->Marlene  Bob    ->Wendy   \n[17] Bohan  ->Nolan    Bohan  ->John     Bohan  ->Katarina Bohan  ->Reigne  \n[21] Chris  ->Jacob    Chris  ->Avery    Chris  ->Bohan    Chris  ->Allyssa \n[25] Dallas ->Louis    Dallas ->Robert   Dallas ->Jenna    Dallas ->Towo    \n[29] David  ->Zach     David  ->Dallas   David  ->Bohan    David  ->Jenna   \n+ ... omitted several edges\n\n\nLet’s better understand the information contained in an igraph object:\n\nIGRAPH simply annotates network as an igraph object\nWhatever random six digit alphanumeric string follows IGRAPH is simply how igraph identifies the graph for itself, it’s not important for our purposes.\nD would tell us that it is directed graph\nN indicates that network is a named graph, in that the vertices have a name attribute\n– refers to attributes not applicable to network, but we will see them in the future:\n28 refers to the number of vertices in network\n112 refers to the number of edges in network\nattr: is a list of attributes within the graph.\n(v/c), which will appear following name, tells us that it is a vertex attribute of a character data type.\n(e/c) or (e/n) referring to edge attributes that are of character or numeric data types\n\nedges from arbitrary igraph name (vertex names): lists a sample of network’s edges using the names of the vertices which they connect.\n\n\nNow let’s create a rough plot to look at our network!\n\nlay <- layout_with_kk(network)\n\npar(bg=\"grey98\")\nplot(network, layout = lay, edge.color=\"grey80\",\n     vertex.color=\"lightblue\", vertex.label.color = \"black\")\n\n\n\n\n\n\n\n\nFor the rest of this post, we are going to talk about a few different measures of centrality, what they capture mathematically and intuitively, and we will look at plots where the size of the node corresponds to the relative centrality score."
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html#measures-of-centrality",
    "href": "R files/0 Module/2022-11-01-SNA.html#measures-of-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Measures of Centrality",
    "text": "Measures of Centrality\n\nDegree Centrality\nFor directed graphs, in-degree, or number of incoming points, is one way we can determine the importance factor for nodes. The Degree of a node is the number of edges that it has. The basic intuition is that, nodes with more connections are more influential and important in a network. In other words, the people with more friend nominations in our simulated social network are the ones with greater importance according to this metric.\n\nDegree.Directed <- degree(network)\nIndegree <- degree(network, mode=\"in\")\nOutdegree <- degree(network, mode=\"out\")\n\nCompareDegree <- cbind(Degree.Directed, Indegree, Outdegree)\n\n\nhead(CompareDegree, 10)\n\n\n\n\n\n \n  \n      \n    Degree.Directed \n    Indegree \n    Outdegree \n  \n \n\n  \n    Allyssa \n    7 \n    3 \n    4 \n  \n  \n    Avery \n    7 \n    3 \n    4 \n  \n  \n    Becca \n    7 \n    3 \n    4 \n  \n  \n    Bob \n    7 \n    3 \n    4 \n  \n  \n    Bohan \n    12 \n    8 \n    4 \n  \n  \n    Chris \n    7 \n    3 \n    4 \n  \n  \n    Dallas \n    7 \n    3 \n    4 \n  \n  \n    David \n    11 \n    7 \n    4 \n  \n  \n    Dylan \n    7 \n    3 \n    4 \n  \n  \n    Gracie \n    5 \n    1 \n    4 \n  \n\n\n\n\n\nThis is a very reasonable way to measure importance within a network. If we are trying to determine who in our classroom is the most popular, we might define that as the greatest number of friendship nominations.\n\nlay <- layout_with_kk(network)\n\npar(bg=\"grey98\")\nplot(network, layout = lay, edge.color=\"grey80\",\n     vertex.size=degree(network, mode=\"in\")*2,  # Rescaled by multiplying by 2\n     main=\"In-Degree\", vertex.label.dist=1.5,\n     vertex.color=\"lightblue\", vertex.label.color = \"black\")"
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html#betweenness-centrality",
    "href": "R files/0 Module/2022-11-01-SNA.html#betweenness-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Betweenness Centrality",
    "text": "Betweenness Centrality\nBetweenness Centrality is another centrality that is based on shortest path between nodes. It is determined as number of the shortest path passing by the given node. For starting node \\(s\\), destination node \\(t\\) and the input node \\(i\\) that holds \\(s \\ne t \\ne i\\), let \\(n_{st}^i\\) be 1 if node \\(i\\) lies on the shortest path between \\(s\\) and \\(t\\); and \\(0\\) if not. So the betweenness centrality is defined as:\n\\[x_i = \\sum_{st} n_{st}^i\\] However, there can be more than one shortest path between \\(s\\) and \\(t\\) and that will count for centrality measure more than once. Thus, we need to divide the contribution to \\(g\\_{st}\\), total number of shortest paths between \\(s\\) and \\(t\\).\n\\[x_i = \\sum_{st} \\frac{n_{st}^i}{g_{st}}\\]\nEssentially the size of the node here represents the frequency with which that node lies on the shortest path between other nodes.\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=betweenness(network)*0.25,  # Rescaled by multiplying by 0.25\n     main=\"Betweenness Centrality\", vertex.color=\"lightblue\")"
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html#eigenvector-centrality",
    "href": "R files/0 Module/2022-11-01-SNA.html#eigenvector-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Eigenvector Centrality",
    "text": "Eigenvector Centrality\nEigenvector centrality is a basic extension of degree centrality, which defines centrality of a node as proportional to its neighbors’ importance. When we sum up all connections of a node, not all neighbors are equally important. This is a very interesting way to measure popularity in our classroom where we say the popularity of your friends matters more than the number of friends. Let’s consider two nodes in a friend network with same degree, the one who is connected to more central nodes should be more central.\nFirst, we define an initial guess for the centrality of nodes in a graph as \\(x_i=1\\). Now we are going to iterate for the new centrality value \\(x_i'\\) for node \\(i\\) as following:\n\\[x_i' = \\sum_{j} A_{ij}x_j\\]\nHere \\(A_{ij}\\) is an element of the adjacency matrix, where it gives 1 or 0 for whether an edge exists between nodes \\(i\\) and \\(j\\). it can also be written in matrix notation as \\(\\mathbf{x'} = \\mathbf{Ax}\\).\nWe iterate over t steps to find the vector \\(\\mathbf{x}(t)\\) as:\n\\[\\mathbf{x}(t) = \\mathbf{A^t x}(0)\\]\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=evcent(network)$vector*15,   # Rescaled by multiplying by 15\n     main=\"Eigenvector Centrality\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\nThe plot shows, the students which have the same number of friend nominations are not necessarily in the same size. The one that is connected to more central, or “popular” nodes are larger in this visualization.\nHowever, as we can see from the definition, this can be a problematic measure for directed graphs. Let’s say that a student who received no friend nominations themselves nominates another student as a friend. Because that person has 0 friend nominations themselves, they would not contribute any importance to the person they nominated. In other words, eigenvector centrality would not take zero in-degree nodes into account in directed graphs.\nHowever there is a solution to this!"
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html#katz-centrality",
    "href": "R files/0 Module/2022-11-01-SNA.html#katz-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Katz Centrality",
    "text": "Katz Centrality\nKatz centrality introduces two positive constants \\(\\alpha\\) and \\(\\beta\\) to tackle the problem of eigenvector centrality with zero in-degree nodes:\n\\[x_i = \\alpha \\sum_{j} A_{ij} x_j + \\beta\\],\nagain \\(A_{ij}\\) is an element of the adjacency matrix, and it can also be written in matrix notation as \\(\\mathbf{x} = \\alpha \\mathbf{Ax} + \\beta \\mathbf{1}\\). This \\(\\beta\\) constant gives a free centrality contribution for all nodes even though they don’t get any contribution from other nodes. The existence of a node alone would provide it some importance. \\(\\alpha\\) constant determines the balances between the contribution from other nodes and the free constant.\nUnfortunately, igraph does not have a function to compute Katz centrality, so we will need to do it the old fashioned way.\n\nkatz.centrality = function(g, alpha, beta, t) {\n  n = vcount(g);\n  A = get.adjacency(g);\n  x0 = rep(0, n);\n  x1 = rep(1/n, n);\n  eps = 1/10^t;\n  iter = 0;\n  while (sum(abs(x0 - x1)) > eps) {\n    x0 = x1;\n    x1 = as.vector(alpha * x1 %*% A) + beta;\n    iter = iter + 1;\n  } \n  return(list(aid = x0, vector = x1, iter = iter))\n}\n\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=katz.centrality(network, 0.1, 1, 0.01)$vector*5,   # Rescaled by multiplying by 15\n     main=\"Katz Centrality\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\nAlthough this method is introduced as a solution for directed graphs, it can be useful for some applications of undirected graphs as well."
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html#hits-hubs-and-authorities",
    "href": "R files/0 Module/2022-11-01-SNA.html#hits-hubs-and-authorities",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "HITS Hubs and Authorities",
    "text": "HITS Hubs and Authorities\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=hub.score(network)$vector*15,   # Rescaled by multiplying by 15\n     main=\"HITS Hubs\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5 , vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=authority.score(network)$vector*15,   # Rescaled by multiplying by 15\n     main=\"HITS Authorities\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\nUp until this point, we have discussed the measures that captures high node centrality, however, there can be nodes in the network which are important for the network, but they are not central. In order to find out such nodes, the HITS algorithm introduces two types of central nodes: Hubs and Authorities. For Hubs, we might consider a node important if it links to many highly nominated nodes (i.e. the person nominates many popular people as their friends). For Authorities, we might consider a node to be of importance if many highly nominated nodes link to it (i.e. nominated by many popular people).\nAuthority Centrality is defined as the sum of the hub centralities which point to the node (i):\n\\[x_i = \\alpha \\sum_{j} A_{ij} y_j,\\]\nwhere \\(\\alpha\\) is constant. Likewise, Hub Centrality is the sum of the authorities which are pointed by the node \\(i\\):\n\\[y_i = \\beta \\sum_{j} A_{ji} x_j,\\]\nwith constant \\(\\beta\\). Here notice that the element of the adjacency matrix are swapped for Hub Centrality because we are concerned with outgoing edges for hubs. So in matrix notation:\n\\[\\mathbf{x} = \\alpha \\mathbf{Ay}, \\quad\\]\n\\[\\mathbf{y} = \\beta \\mathbf{A^Tx}.\\] As it can be seen from the drawing, HITS Algorithm also tackles the problem with zero in-degree nodes of Eigenvector Centrality. These zero in-degree nodes become central hubs and contribute to other nodes. Yet we can still use a free centrality contribution constant like in Katz Centrality or other variants.\nAlthough these measures are generally used in a citation network or the internet, this can be pretty interesting in the context of our classroom frienship network. Maybe we are interested in knowing who are the non-popular students that nominate many popular students as friends, but receive no reciprocity in terms of nominations."
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html#other-packages-that-i-like-for-visualization",
    "href": "R files/0 Module/2022-11-01-SNA.html#other-packages-that-i-like-for-visualization",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Other Packages that I Like for Visualization",
    "text": "Other Packages that I Like for Visualization\nI want to conclude this post with some sample code to produce a nice network plot using the ggnetwork package. igraph is only one package that exists for network visualization and centrality calculations, I encourage you to check out additional packages which may be stronger than igraph for some purposes.\n\nlibrary(GGally)\nlibrary(network)\nlibrary(ggnetwork)\n\nnet <- list(nodes=friendships[c('source', 'target', 'source_sex')], \n            edges=friendships[c('source', 'target', 'source_sex')])\n\n# create node attribute data\nnet.cet <- as.character(net$nodes$source_sex)\nnames(net.cet) = net$nodes$source\nedges <- net$edges\n\n# create network\nnet.net <- edges[, c(\"source\", \"target\") ]\nnet.net <- network::network(net.net, directed = TRUE)\n\n# create sourc sex node attribute\nnet.net %v% \"source_sex\" <- net.cet[ network.vertex.names(net.net) ]\n\n\nset.seed(1)\nggnet2(net.net, color = \"source_sex\",\n       palette = c(\"Female\" = \"purple\", \"Male\" = \"maroon\"), size = 'indegree',\n       arrow.size = 3, arrow.gap = 0.04, alpha = 1,  label = TRUE, vjust = 2.5, label.size = 3.5,\n       edge.alpha = 0.5, mode = \"kamadakawai\",edge.color = 'grey50',\n       color.legend = \"Student Sex\") + theme_bw() + theme_blank()  + \n  theme(legend.position = \"bottom\", text = element_text(size = 15),\n        plot.caption = element_text(hjust = 0)) + guides(size=F) + \n  labs(title='Social Network Mapping of Friendship Nominations for Simulated Data', \n       caption = str_wrap(\"The size of the dots represent the number of \n                          friendship nominations by others. The maroon dots represent males \n                          while the purple dots represent females. Directed edges are used \n                          to indicate the direction of friendship nominations. \n                          Edges that are bi-directional indicated friendship reciprocity. \n                          \\n\\n This particular network represents the frienship nominations \n                          of 28 students from our simulated data.\", 128))\n\n\n\n\n\n\n\n\nThank you!\nJacob"
  },
  {
    "objectID": "R files/5 Module/mod5.html",
    "href": "R files/5 Module/mod5.html",
    "title": "Module 5: Data Manipulation and Analysis II",
    "section": "",
    "text": "Download a copy of Module 5 slides\nDownload data for Module 5 lab and tutorial"
  },
  {
    "objectID": "R files/9 Module/mod9.html",
    "href": "R files/9 Module/mod9.html",
    "title": "Module 9: Writing Functions",
    "section": "",
    "text": "Download a copy of Module 9 slides"
  },
  {
    "objectID": "R files/7 Module/mod0.html",
    "href": "R files/7 Module/mod0.html",
    "title": "Module 0",
    "section": "",
    "text": "Mac Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Mac version, save the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder.\n\n\nWindows Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Windows version, and save the executable file. Run the .exe file and follow the installation instructions."
  },
  {
    "objectID": "R files/3 Module/mod3.html",
    "href": "R files/3 Module/mod3.html",
    "title": "Module 3: Vectors and Lists",
    "section": "",
    "text": "Download a copy of Module 3 slides\nDownload data for Module 3 lab and tutorial"
  },
  {
    "objectID": "R files/1 Module/mod1.html",
    "href": "R files/1 Module/mod1.html",
    "title": "Module 1: An Introduction and Motivation for R Programming",
    "section": "",
    "text": "Download a copy of Module 1 slides"
  },
  {
    "objectID": "R files/4 Module/mod4.html",
    "href": "R files/4 Module/mod4.html",
    "title": "Module 4: Data Manipulation",
    "section": "",
    "text": "Download a copy of Module 4 slides\nDownload data for Module 4 lab and tutorial"
  },
  {
    "objectID": "R files/8 Module/mod8.html",
    "href": "R files/8 Module/mod8.html",
    "title": "Module 8: Iteration",
    "section": "",
    "text": "Download a copy of Module 8 slides"
  },
  {
    "objectID": "R files/6 Module/mod7.html",
    "href": "R files/6 Module/mod7.html",
    "title": "Module 0",
    "section": "",
    "text": "Mac Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Mac version, save the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder.\n\n\nWindows Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Windows version, and save the executable file. Run the .exe file and follow the installation instructions."
  },
  {
    "objectID": "R files/2 Module/mod2.html",
    "href": "R files/2 Module/mod2.html",
    "title": "Module 2: Installing Packages and Reading Data",
    "section": "",
    "text": "Download a copy of Module 2 slides\nDownload data for Module 2 lab and tutorial"
  },
  {
    "objectID": "R files/7 Module/mod7.html",
    "href": "R files/7 Module/mod7.html",
    "title": "Module 7: Grouped Analysis",
    "section": "",
    "text": "Download a copy of Module 7 slides\nDownload data for Module 7 lab and tutorial"
  },
  {
    "objectID": "R files/6 Module/mod6.html",
    "href": "R files/6 Module/mod6.html",
    "title": "Module 6: Data Visualization as a Tool for Analysis",
    "section": "",
    "text": "Download a copy of Module 6 slides\nDownload data for Module 6 lab and tutorial"
  },
  {
    "objectID": "R files/1 Module/mod1.html#lab-1",
    "href": "R files/1 Module/mod1.html#lab-1",
    "title": "Module 1: An Introduction and Motivation for R Programming",
    "section": "Lab 1",
    "text": "Lab 1\nWe expect you to watch the Module 1 material prior to lab.\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nWarm-up\n\nThe most important warm up question: do you have R and RStudio installed?\nWhich of these allow you to pull up the documentation for a command in R?\n\n\n*\n?\nhelp()\ndocumentation()\n\n\nIn the code block below, run code that will pull up documentation for the function paste0().\n\n\n?paste0()\n\nWhat does this function do?\n\nWhat are the two ways that you can assign a value to a variable?\nGuess the Output: Algebra\n\nGuess the output of the following code:\n\n\na <- 3\nb <- a^2 + 1\n\nb\n\nNow, run the code block to check your answer.\n\nGuess the output of the following code:\n\n\na <- 10\nb <-3 %% a\n\nb + 5\n\nHint: If you are not sure what %% does you can try running ?'%%' to better understand.\n\nGuess the output of the following code:\n\n\na <- c(1,2,3)\nb <- a^2 + 1\n\nb\n\nGuess the Output: Boolean\n\nGuess the output of the following code:\n\n\n25 >= 14\n\n\nGuess the output of the following code:\n\n\n10 != 100\n\n\nGuess the output of the following code:\n\n\n7%%5 == 2\n\n\nGuess the output of the following code:\n\n\n(5 > 7) & (7 * 7 == 49)\n\n\nOk, let’s try some logic! Try to figure out each one before running the code!\n\n\n\n\n\nTRUE & FALSE\n\n\n\n\n\nFALSE & FALSE\n\n\n\n\n\nTRUE | (FALSE & TRUE)\n\n\n\n\n\nFALSE | (TRUE | FALSE)\n\n\n\n\n\n(TRUE & (TRUE | FALSE)) | FALSE\n\nData Types\n\nRun these lines to create these variables in your environment.\n\n\nitem_1 <- \"Hi, my name is item 1!\"\nitem_2 <- 7\nitem_3 <- FALSE\n\nWhat are the type (or mode) of each of these items?\nHint: If you are not sure, you could apply the mode() function to each item and check the output. If you are unsure about how to apply the mode() function, you can always run ?mode().\n\nGuess the output of the following code:\n\n\n(item_2 + 19 <= 25) == item_3\n\n\nDo you remember earlier when you ran ?paste0()? We are now going to try to use this function. In the code block below, initialize two variables that are of mode “character”. The output when you apply paste0() to these variables should be “Hello, world!”.\n\n\n#v1 <-\n#v2 <- \n\nWell done! You’ve learned how to work with R to perform simple variable assignment and operations!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/1 Module/mod1.html#general-guidelines",
    "href": "R files/1 Module/mod1.html#general-guidelines",
    "title": "Module 1",
    "section": "General Guidelines:",
    "text": "General Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps"
  },
  {
    "objectID": "R files/1 Module/mod1.html#warm-up",
    "href": "R files/1 Module/mod1.html#warm-up",
    "title": "Module 1",
    "section": "Warm-up",
    "text": "Warm-up\n\nThe most important warm up question: do you have R and RStudio installed?\nWhich of these allow you to pull up the documentation for a command in R?\n\n\n*\n?\nhelp()\ndocumentation()\n\n\nIn the code block below, run code that will pull up documentation for the function paste0().\n\n\n?paste0()\n\nWhat does this function do?\n\nWhat are the two ways that you can assign a value to a variable?\n\n\nGuess the Output: Algebra\n\nGuess the output of the following code:\n\n\na <- 3\nb <- a^2 + 1\n\nb\n\nNow, run the code block to check your answer.\n\nGuess the output of the following code:\n\n\na <- 10\nb <-3 %% a\n\nb + 5\n\nHint: If you are not sure what %% does you can try running ?'%%' to better understand.\n\nGuess the output of the following code:\n\n\na <- c(1,2,3)\nb <- a^2 + 1\n\nb\n\n\n\nGuess the Output: Boolean\n\nGuess the output of the following code:\n\n\n25 >= 14\n\n\nGuess the output of the following code:\n\n\n10 != 100\n\n\nGuess the output of the following code:\n\n\n7%%5 == 2\n\n\nGuess the output of the following code:\n\n\n(5 > 7) & (7 * 7 == 49)\n\n\nOk, let’s try some logic! Try to figure out each one before running the code!\n\n\n\n\n\nTRUE & FALSE\n\n\n\n\n\nFALSE & FALSE\n\n\n\n\n\nTRUE | (FALSE & TRUE)\n\n\n\n\n\nFALSE | (TRUE | FALSE)\n\n\n\n\n\n(TRUE & (TRUE | FALSE)) | FALSE\n\n\n\nData Types\n\nRun these lines to create these variables in your environment.\n\n\nitem_1 <- \"Hi, my name is item 1!\"\nitem_2 <- 7\nitem_3 <- FALSE\n\nWhat are the type (or mode) of each of these items?\nHint: If you are not sure, you could apply the mode() function to each item and check the output. If you are unsure about how to apply the mode() function, you can always run ?mode().\n\nGuess the output of the following code:\n\n\n(item_2 + 19 <= 25) == item_3\n\n\nDo you remember earlier when you ran ?paste0()? We are now going to try to use this function. In the code block below, initialize two variables that are of mode “character”. The output when you apply paste0() to these variables should be “Hello, world!”.\n\n\n#v1 <-\n#v2 <- \n\nWell done! You’ve learned how to work with R to perform simple variable assignment and operations!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/1 Module/mod1.html#guess-the-output-algebra",
    "href": "R files/1 Module/mod1.html#guess-the-output-algebra",
    "title": "Module 1",
    "section": "Guess the Output: Algebra",
    "text": "Guess the Output: Algebra\n\nGuess the output of the following code:\n\n\na <- 3\nb <- a^2 + 1\n\nb\n\nNow, run the code block to check your answer.\n\nGuess the output of the following code:\n\n\na <- 10\nb <-3 %% a\n\nb + 5\n\nHint: If you are not sure what %% does you can try running ?'%%' to better understand.\n\nGuess the output of the following code:\n\n\na <- c(1,2,3)\nb <- a^2 + 1\n\nb"
  },
  {
    "objectID": "R files/1 Module/mod1.html#guess-the-output-boolean",
    "href": "R files/1 Module/mod1.html#guess-the-output-boolean",
    "title": "Module 1",
    "section": "Guess the Output: Boolean",
    "text": "Guess the Output: Boolean\n\nGuess the output of the following code:\n\n\n25 >= 14\n\n\nGuess the output of the following code:\n\n\n10 != 100\n\n\nGuess the output of the following code:\n\n\n7%%5 == 2\n\n\nGuess the output of the following code:\n\n\n(5 > 7) & (7 * 7 == 49)\n\n\nOk, let’s try some logic! Try to figure out each one before running the code!\n\n\n\n\n\nTRUE & FALSE\n\n\n\n\n\nFALSE & FALSE\n\n\n\n\n\nTRUE | (FALSE & TRUE)\n\n\n\n\n\nFALSE | (TRUE | FALSE)\n\n\n\n\n\n(TRUE & (TRUE | FALSE)) | FALSE"
  },
  {
    "objectID": "R files/1 Module/mod1.html#data-types",
    "href": "R files/1 Module/mod1.html#data-types",
    "title": "Module 1",
    "section": "Data Types",
    "text": "Data Types\n\nRun these lines to create these variables in your environment.\n\n\nitem_1 <- \"Hi, my name is item 1!\"\nitem_2 <- 7\nitem_3 <- FALSE\n\nWhat are the type (or mode) of each of these items?\nHint: If you are not sure, you could apply the mode() function to each item and check the output. If you are unsure about how to apply the mode() function, you can always run ?mode().\n\nGuess the output of the following code:\n\n\n(item_2 + 19 <= 25) == item_3\n\n\nDo you remember earlier when you ran ?paste0()? We are now going to try to use this function. In the code block below, initialize two variables that are of mode “character”. The output when you apply paste0() to these variables should be “Hello, world!”.\n\n\n#v1 <-\n#v2 <- \n\nWell done! You’ve learned how to work with R to perform simple variable assignment and operations!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/2 Module/mod2.html#general-guidelines",
    "href": "R files/2 Module/mod2.html#general-guidelines",
    "title": "Module 2: Installing Packages and Reading Data",
    "section": "General Guidelines:",
    "text": "General Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps"
  },
  {
    "objectID": "R files/2 Module/mod2.html#warm-up",
    "href": "R files/2 Module/mod2.html#warm-up",
    "title": "Module 2: Installing Packages and Reading Data",
    "section": "Warm-up",
    "text": "Warm-up\n\nCreate a new Rmd and add code to load the tidyverse package.\nYour classmate comes to you and says they can’t get data to load after restarting their R session. You see the code:\n\n\ninstall.packages(\"haven\")\nawesome_data <- read_dta(\"awesome_data.dta\")\n\nError in read_dta(\"awesome_data.dta\") : could not find function \"read_dta\"\n\nDiagnose the problem.\nNote: If they say the code worked before, it’s likely they had loaded haven in the console or perhaps in an earlier script. R packages will stay attached as long as the R session is live.\n\nIn general, once you have successfully used install.packages(pkg) for a “pkg”, you won’t need to do it again. Install haven and readxl using the console.\nIn your script, load haven and readxl. Notice that if you had to restart R right now. You could reproduce the entire warm-up by running the script. We strive for reproducibility by keeping the code we want organized in scripts or Rmds.\nIt’s good practice when starting a new project to clear your R environment. This helps you make sure you are not relying on data or functions you wrote in another project. After you library() statements add the following code rm(list = ls()).\nrm() is short for remove. Find the examples in ?rm and run them in the console."
  },
  {
    "objectID": "R files/2 Module/mod2.html#islr-chapter-2-q8",
    "href": "R files/2 Module/mod2.html#islr-chapter-2-q8",
    "title": "Module 2: Installing Packages and Reading Data",
    "section": "ISLR Chapter 2 Q8",
    "text": "ISLR Chapter 2 Q8\nThis exercise relates to the College data set, which can be found in the file College.csv. It contains a number of variables for 777 different universities and colleges in the US. The variables are\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nPrivate\nPublic/private indicator\n\n\nApps\nNumber of applications received\n\n\nAccept\nNumber of applicants accepted\n\n\nEnroll\nNumber of new students enrolled\n\n\nTop10perc\nNew students from top 10 % of high school class\n\n\nTop25perc\nNew students from top 25 % of high school class\n\n\nF.Undergrad\nNumber of full-time undergraduates\n\n\nP.Undergrad\nNumber of part-time undergraduates\n\n\nOutstate\nOut-of-state tuition\n\n\nRoom.Board\nRoom and board costs\n\n\nBooks\nEstimated book costs\n\n\nPersonal\nEstimated personal spending\n\n\nPhD\nPercent of faculty with Ph.D.’s\n\n\nTerminal\nPercent of faculty with terminal degree\n\n\nS.F.Ratio\nStudent/faculty ratio\n\n\nperc.alumni\nPercent of alumni who donate\n\n\nExpend\nInstructional expenditure per student\n\n\nGrad.Rate\nGraduation rate\n\n\n\nBefore reading the data into R, it can be viewed in Excel or a text editor. Make sure that you have the directory set to the correct location for the data.\n\nUse the base R read.csv() function to read the data into R with option stringsAsFactors=T (this is needed later on for plotting figures). Call the loaded data college.\nLook at the data using the View() function. You should notice that the first column is just the name of each university. Load your data and then try the following commands:\n\n\n#set your working directory ,fill in your code after this line\n\n#read in the file College.csv using read.csv() with option `stringsAsFactors=T`\ncollege <- read.csv('College.csv', stringsAsFactors = T)\n\n#Give data frame college rownames\nrownames(college) <- college[,1] \n\n# Please comment out View function after using it. Otherwise you'll see some error when knit.\n# View(college)\n\n\nYou should see that there is now a row.names column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. Next, we will remove the first column in the data where the names are stored. Try\n\n\n#Use a negative number to generate a subset with all but one column\n# college[, -c(1, 2, 3)]  will generate a subset with all but the first three columns\ncollege <- college[,-1]\n# as.factor can turn a character column to a factor column so that we can use it to plot later on\ncollege$Private <- as.factor(college$Private)\n#View(college)\n\nNow you should see that the first data column is Private. Note that another column labeled row.names now appears before the Private column. However, this is not a data column but rather the name that R is giving to each row.\n\nUse the summary() function to produce a numerical summary of the variables in the data set. Hint: summary() takes in an object such as data.frame and return the summery results\nUse the pairs() function to produce a scatterplot matrix of the first five columns or variables of the data. Recall that you can reference the first five columns of a data frame dat using dat[,1:5]\nUse the plot() function to produce side-by-side boxplots of Outstate versus Private. Hint: plot() takes two arguments one vector for x axis and one vector for y axis. Try plot(dat$col_name, dat$col_name).\n\n\n# replicate \"No\" for the same times as the number of colleges using rep()\nElite <- rep(\"No\",nrow(college))\n# change the values in Elite for colleges with proportion of students \n# coming from the top 10% of their high school classes \n# exceeds 50 % to \"Yes\"\nElite[college$Top10perc >50] <- \"Yes\"\n# as.factor change ELite, a character vector to a factor vector\n# (we will touch on factors later in class) \nElite <- as.factor(Elite)\n# add the newly created vector to the college data frame\ncollege <- data.frame(college ,Elite)\n\n\nUse the summary() function to see how many elite universities there are. Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite.\n\nContinue exploring the data, and provide a brief summary of what you discover."
  },
  {
    "objectID": "R files/2 Module/mod2.html#islr-chapter-2-q9",
    "href": "R files/2 Module/mod2.html#islr-chapter-2-q9",
    "title": "Module 2: Installing Packages and Reading Data",
    "section": "ISLR Chapter 2 Q9",
    "text": "ISLR Chapter 2 Q9\nThis exercise involves the Auto data set. na.omit() removes the missing values from the data and returns a new data frame.\n\n#load the Auto.csv into a variable called auto using read_csv()\n\n\n# remove all rows with missing values using na.omit()\nauto <- na.omit(auto)\n\nWe can use class() to check which of the columns are quantitative (numeric or integer), and which are qualitative( logical or character). And sapply() function takes in a data frame and a function (in this case class()), apply the class function to each column. Try the following commands:\n\n#apply the class() function to each column of auto data frame\nsapply(auto, class)\n\n\nWhat is the range of each quantitative columns? You can answer this using the range() function. Hint: You can call range() function individually on each column. You can also subset the quantitative columns by creating a variable quant_cols equal to all columns with a numeric mode, then use sapply the function range() with the data frame with only quantitative columns. This is not required.\nUsing the functions mean() and sd(). Find out what is the mean and standard deviation of each quantitative columns?\nNow remove the 10th through 85th observations (rows). What is the range, mean, and standard deviation of each column in the subset of the data that remains? Hint: We’ve seen removing columns in question 8. To remove the rows, we can use the negative sign - again. For example, auto[-c(1,3),] removes the first and third row\nUsing the full data set, investigate the columns graphically, using scatterplots (pairs or plot) or other tools of your choice. Create some plots highlighting the relationships among the columns Comment on your findings.\nSuppose that we wish to predict gas mileage (mpg) on the basis of the other numerical variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.\n\n\nISLR Chapter 2 Q10\nThis exercise involves the Boston housing data set.\nTo begin, load in the Boston data set. The Boston data set is part of the MASS library in R. You may need to install the package using install.packages() function if you haven’t done so.\n\n# install.packages(MASS)\nlibrary(MASS)\n\nNow the data set is contained in the object Boston.\n\nBoston\n\nRead about the data set:\n\n?Boston\n\n\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\nMake some pairwise scatterplots of the columns in this data set. Describe your findings. Hint: Use function pairs()\nHow many of the suburbs in this data set bound the Charles river? Hint: Subset the data using a logical vector to check if variable chas==1, then use nrow() to see the number of suburbs.\nUsing median(), find out what is the median pupil-teacher ratio among the towns in this data set?\n\nWell done! You’ve learned how to work with R to read in data and perform some simple analysis and exploration!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/2 Module/mod2.html#islr-chapter-2-q10",
    "href": "R files/2 Module/mod2.html#islr-chapter-2-q10",
    "title": "Module 2: Installing Packages and Reading Data",
    "section": "ISLR Chapter 2 Q10",
    "text": "ISLR Chapter 2 Q10\nThis exercise involves the Boston housing data set.\nTo begin, load in the Boston data set. The Boston data set is part of the MASS library in R. You may need to install the package using install.packages() function if you haven’t done so.\n\n# install.packages(MASS)\nlibrary(MASS)\n\nNow the data set is contained in the object Boston.\n\nBoston\n\nRead about the data set:\n\n?Boston\n\n\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\nMake some pairwise scatterplots of the columns in this data set. Describe your findings. Hint: Use function pairs()\nHow many of the suburbs in this data set bound the Charles river? Hint: Subset the data using a logical vector to check if variable chas==1, then use nrow() to see the number of suburbs.\nUsing median(), find out what is the median pupil-teacher ratio among the towns in this data set?\n\nWell done! You’ve learned how to work with R to read in data and perform some simple analysis and exploration!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/2 Module/mod2.html#lab-2",
    "href": "R files/2 Module/mod2.html#lab-2",
    "title": "Module 2: Installing Packages and Reading Data",
    "section": "Lab 2",
    "text": "Lab 2\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\n\n\n\nWarm-up\n\nCreate a new Rmd and add code to load the tidyverse package.\nYour classmate comes to you and says they can’t get data to load after restarting their R session. You see the code:\n\n\ninstall.packages(\"haven\")\nawesome_data <- read_dta(\"awesome_data.dta\")\n\nError in read_dta(\"awesome_data.dta\") : could not find function \"read_dta\"\n\nDiagnose the problem.\nNote: If they say the code worked before, it’s likely they had loaded haven in the console or perhaps in an earlier script. R packages will stay attached as long as the R session is live.\n\nIn general, once you have successfully used install.packages(pkg) for a “pkg”, you won’t need to do it again. Install haven and readxl using the console.\nIn your script, load haven and readxl. Notice that if you had to restart R right now. You could reproduce the entire warm-up by running the script. We strive for reproducibility by keeping the code we want organized in scripts or Rmds.\nIt’s good practice when starting a new project to clear your R environment. This helps you make sure you are not relying on data or functions you wrote in another project. After you library() statements add the following code rm(list = ls()).\nrm() is short for remove. Find the examples in ?rm and run them in the console.\n\n\n\nISLR Chapter 2 Q8\nThis exercise relates to the College data set, which can be found in the file College.csv. It contains a number of variables for 777 different universities and colleges in the US. The variables are\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nPrivate\nPublic/private indicator\n\n\nApps\nNumber of applications received\n\n\nAccept\nNumber of applicants accepted\n\n\nEnroll\nNumber of new students enrolled\n\n\nTop10perc\nNew students from top 10 % of high school class\n\n\nTop25perc\nNew students from top 25 % of high school class\n\n\nF.Undergrad\nNumber of full-time undergraduates\n\n\nP.Undergrad\nNumber of part-time undergraduates\n\n\nOutstate\nOut-of-state tuition\n\n\nRoom.Board\nRoom and board costs\n\n\nBooks\nEstimated book costs\n\n\nPersonal\nEstimated personal spending\n\n\nPhD\nPercent of faculty with Ph.D.’s\n\n\nTerminal\nPercent of faculty with terminal degree\n\n\nS.F.Ratio\nStudent/faculty ratio\n\n\nperc.alumni\nPercent of alumni who donate\n\n\nExpend\nInstructional expenditure per student\n\n\nGrad.Rate\nGraduation rate\n\n\n\nBefore reading the data into R, it can be viewed in Excel or a text editor. Make sure that you have the directory set to the correct location for the data.\n\nUse the base R read.csv() function to read the data into R with option stringsAsFactors=T (this is needed later on for plotting figures). Call the loaded data college.\nLook at the data using the View() function. You should notice that the first column is just the name of each university. Load your data and then try the following commands:\n\n\n#set your working directory ,fill in your code after this line\n\n#read in the file College.csv using read.csv() with option `stringsAsFactors=T`\ncollege <- read.csv('College.csv', stringsAsFactors = T)\n\n#Give data frame college rownames\nrownames(college) <- college[,1] \n\n# Please comment out View function after using it. Otherwise you'll see some error when knit.\n# View(college)\n\n\nYou should see that there is now a row.names column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. Next, we will remove the first column in the data where the names are stored. Try\n\n\n#Use a negative number to generate a subset with all but one column\n# college[, -c(1, 2, 3)]  will generate a subset with all but the first three columns\ncollege <- college[,-1]\n# as.factor can turn a character column to a factor column so that we can use it to plot later on\ncollege$Private <- as.factor(college$Private)\n#View(college)\n\nNow you should see that the first data column is Private. Note that another column labeled row.names now appears before the Private column. However, this is not a data column but rather the name that R is giving to each row.\n\nUse the summary() function to produce a numerical summary of the variables in the data set. Hint: summary() takes in an object such as data.frame and return the summery results\nUse the pairs() function to produce a scatterplot matrix of the first five columns or variables of the data. Recall that you can reference the first five columns of a data frame dat using dat[,1:5]\nUse the plot() function to produce side-by-side boxplots of Outstate versus Private. Hint: plot() takes two arguments one vector for x axis and one vector for y axis. Try plot(dat$col_name, dat$col_name).\n\n\n# replicate \"No\" for the same times as the number of colleges using rep()\nElite <- rep(\"No\",nrow(college))\n# change the values in Elite for colleges with proportion of students \n# coming from the top 10% of their high school classes \n# exceeds 50 % to \"Yes\"\nElite[college$Top10perc >50] <- \"Yes\"\n# as.factor change ELite, a character vector to a factor vector\n# (we will touch on factors later in class) \nElite <- as.factor(Elite)\n# add the newly created vector to the college data frame\ncollege <- data.frame(college ,Elite)\n\n\nUse the summary() function to see how many elite universities there are. Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite.\n\nContinue exploring the data, and provide a brief summary of what you discover.\n\n\nISLR Chapter 2 Q9\nThis exercise involves the Auto data set. na.omit() removes the missing values from the data and returns a new data frame.\n\n#load the Auto.csv into a variable called auto using read_csv()\n\n\n# remove all rows with missing values using na.omit()\nauto <- na.omit(auto)\n\nWe can use class() to check which of the columns are quantitative (numeric or integer), and which are qualitative( logical or character). And sapply() function takes in a data frame and a function (in this case class()), apply the class function to each column. Try the following commands:\n\n#apply the class() function to each column of auto data frame\nsapply(auto, class)\n\n\nWhat is the range of each quantitative columns? You can answer this using the range() function. Hint: You can call range() function individually on each column. You can also subset the quantitative columns by creating a variable quant_cols equal to all columns with a numeric mode, then use sapply the function range() with the data frame with only quantitative columns. This is not required.\nUsing the functions mean() and sd(). Find out what is the mean and standard deviation of each quantitative columns?\nNow remove the 10th through 85th observations (rows). What is the range, mean, and standard deviation of each column in the subset of the data that remains? Hint: We’ve seen removing columns in question 8. To remove the rows, we can use the negative sign - again. For example, auto[-c(1,3),] removes the first and third row\nUsing the full data set, investigate the columns graphically, using scatterplots (pairs or plot) or other tools of your choice. Create some plots highlighting the relationships among the columns Comment on your findings.\nSuppose that we wish to predict gas mileage (mpg) on the basis of the other numerical variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.\n\n\n\nISLR Chapter 2 Q10\nThis exercise involves the Boston housing data set.\nTo begin, load in the Boston data set. The Boston data set is part of the MASS library in R. You may need to install the package using install.packages() function if you haven’t done so.\n\n# install.packages(MASS)\nlibrary(MASS)\n\nNow the data set is contained in the object Boston.\n\nBoston\n\nRead about the data set:\n\n?Boston\n\n\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\nMake some pairwise scatterplots of the columns in this data set. Describe your findings. Hint: Use function pairs()\nHow many of the suburbs in this data set bound the Charles river? Hint: Subset the data using a logical vector to check if variable chas==1, then use nrow() to see the number of suburbs.\nUsing median(), find out what is the median pupil-teacher ratio among the towns in this data set?\n\nWell done! You’ve learned how to work with R to read in data and perform some simple analysis and exploration!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/3 Module/mod3.html#lab-3",
    "href": "R files/3 Module/mod3.html#lab-3",
    "title": "Module 3: Vectors and Lists",
    "section": "Lab 3",
    "text": "Lab 3\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\n\n\n\nWarm-up\n\nIn the lecture, we covered c(), :, rep(), seq() among other ways to create vectors.\n\n\ndolly = c(9, 10, 11, 12, 13, 14, 15, 16, 17) \nbees = c(\"b\", \"b\", \"b\", \"b\", \"b\")\n\n\nRecreate dolly using :.\nCreate the same vector using seq().\nRecreate bees using rep().\n\n\nWe are now going to use the functions rnorm() and runif() to initialize vectors.\n\n\nrandom_norm = rnorm(100) \nrandom_unif = runif(1000)\n\n\nHow long are the vectors random_norm and random_unif? Use length() to verify.\nWhat are the largest and smallest values in random_norm and random_unif? Use min() and max().\nUse mean() and sd() to calculate the mean and standard deviation of the two distributions.\nCreate a new vector with 10000 draws from the standard normal distribution.\nrnorm() by default sets mean = 0 (see ?rnorm). Create a vector of 10000 draws from the normal distribution with mean = 1. Use mean() to verify.\n\nNotice the functions min(), max(), mean() and sd() all take a vector with many values and summarize them as one value. These are good to use with summarize() when doing data analysis on simple dataframes.\n\nData Types\n\nUse typeof() to verify the data types of dolly, bees, random_unif\nCoerce dolly to a character vector. Recall we have functions as.<type>() for this kind of coercion.\nTry to coerce bees to type numeric. What does R do when you ask it to turn “b” into a number?\n\n\n\nVectorized Math\n\na and b are vectors of length 10. Look at them in the console.\n\n\na <- 1:10\nb <- rep(c(2, 4), 5)\n\n\nAdd a and b element by element.\nSubtract a and b element by element.\nDivide a by b element by element.\nMultiply a and b element by element.\nRaise the element of a to the power of b element by element.\nMultiply each element of a by 3 then subtract b\nRaise each element of b to the third power.\nTake the square root of each element of a.\n\n\n\nCalculating Mean and Standard Deviation\n\nCalculating the Mean\nIn this exercise, we will calculate the mean of a vector of random numbers. Wewill practice assigning new variables and using functions in R.\nWe can run the following code to create a vector of 1000 random numbers. The function set.seed() ensures that the process used to generate random numbers is the same across computers.\nNote: rf() is a R command we use to generate 1000 random numbers according to the F distribution, and 10 and 100 are parameters that specify how “peaked” the distribution is.\n\nset.seed(1)\nrandom_numbers = rf(1000, 10, 100)\n\nWrite code that gives you the sum of random_numbers and saves it to a new variable called numbers_sum:\nHint: To sum the numbers in a vector, use the sum() function.\nNote: You don’t automatically see the output of numbers_sum when you assign it to a variable. Type numbers_sum into the console and run it to see the value that you assigned it.\nWrite code that gives you the number of items in the random_numbers vector and saves it to a new variable called numbers_count:\nHint: To count the number of items in a vector, use the length() function.\nNow write code that uses the above two variables to calculate the average of random_numbers and assign it to a new variable called this_mean.\nWhat number did you get? It should have been 1.018. If it isn’t, double check your code!\nR actually has a built in function to calculate the mean for you, so you don’t have to remember how to build it from scratch each time! Check your above answer by using the mean() function on the random_numbers vector.\n\n\nCalculating the Standard Deviation\nNow that you’ve got that under your fingers, let’s move on to standard deviation.\nWe will be converting the following formula for calculating the sample standard deviation into code:\n\\(s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2} {n-1}}\\)\nFor this, we’ll review the concept of vectorization. This means that an operation like subtraction will act on all numbers in a vector at the same time.\nSubtract this_mean from the random_numbers vector. Did each number in random_numbers change?\nTry to write the formula for standard deviation in R code using the sqrt(), sum(), and length() functions, along with other operators (^, /, -). Assign it to a new variable called this_sd. Watch out for your parentheses!\nWhat number did you get for this_sd, or the standard deviation of random_numbers? If you didn’t get 0.489704, recheck your code!\nR also has a built in function for standard deviation. Check if you calculated the standard deviation correctly by using the sd() function on the random_numbers vector.\n\n\n\nMaking a Histogram of Our Numbers\nWhat do these random numbers look like, anyway? We can use base plotting in R to visualize the distribution of our random numbers.\nRun the following code to visualize the original distribution of random_numbers as a histogram.\n\nhist(random_numbers)\n\nNotice how most of the values are concentrated on the left-hand side of the graph, while there is a longer “tail” to the right? Counterintuitively, this is known as a right-skewed distribution. When we see a distribution like this, one common thing to do is to normalize it.\nThis is also known as calculating a z-score, which we will cover next.\n\n\nCalculating a Z-Score\nThe formula for calculating a z-score for a single value, or normalizing that value, is as follows:\n\\(z = \\frac{x - \\bar{x}}{s}\\)\nThis can be calculated for each value in random_numbers in context of the larger set of values.\nCan you translate this formula into code?\nUsing random_numbers, this_mean, and this_sd that are already in your environment, write a formula to transform all the values in random_numbers into z-scores, and assign it to the new variable normalized_data.\nHint: R is vectorized, so you can subtract the mean from each random number in random_numbers in a straightforward way.\nTake the mean of normalized_data and assign it to a variable called normalized_mean.\nNote: If you see something that ends in “e-16”, that means that it’s a very small decimal number (16 places to the right of the decimal point), and is essentially 0.\nTake the standard deviation of normalized_data and assign it to a variable called normalized_sd.\nWhat is the value of normalized_mean? What is the value of normalized_sd? You should get a vector that is mean zero and has a standard deviation of one, because the data has been normalized.\n\nMaking a Histogram of Z-scores\nLet’s plot the z-scores and see if our values are still skewed. How does this compare to the histogram of random_numbers? Run the following code:\n\nhist(normalized_data)\n\nIs the resulting data skewed?\n\n\n\n\nCalculating a T-Score\nT-tests are used to determine if two sample means are equal. The formula for calculating a t-score is as follows:\n\\(t = \\frac{\\overline{x}_1 - \\overline{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\)\nwhere \\(\\overline{x}_i\\) is the mean of the first or second set of data, \\(s_i\\) is the sample standard deviation of the first or second set of data, and \\(n_i\\) is the sample size of the \\(i\\)th set of data.\nWe’ll first create two data sets of random numbers following a normal distribution:\n\nset.seed(1)\ndata_1 <- rnorm(1000, 3)\ndata_2 <- rnorm(100, 2)\n\nHere’s how we’ll calculate the mean (x_1), standard deviation (s_1), and sample size (n_1) of the first data set:\n\nx_1 <- mean(data_1)\ns_1 <- sd(data_1)\nn_1 <- length(data_1)\n\nWhat numeric types do you get from doing this? Try running the typeof() function on each of x_1, s_1, and n_1. We have you started with x_1.\n\ntypeof(x_1)\n\n[1] \"double\"\n\n\nWhat object type is n_1?\nCan you calculate the same values for data_2, assigning mean, standard deviation, and length to the variables of x_2, s_2, and n_2, respectively?\nWhat values do you get for x_2 and s_2?\nNow, you should be able to translate the t-score formula (\\(\\frac{\\overline{x}_1 - \\overline{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\)) into code, based on the above calculated values.\nWhat did you get for the t-score? You should have gotten 9.243, if not, double check your code!\nThe t-score’s meaning depends on your sample size, but in general t-scores close to 0 imply that the means are not statistically distinguishable, and large t-scores (e.g. t > 3) imply the data have different means.\n\nPerforming a T-Test\nOnce again, R has a built in function that will perform a T-test for us, aptly named t.test(). Look up the arguments the function t.test() takes, and perform a T-test on data_1 and data_2.\nWhat are the sample means, and are they distinguishable from each other?\nWell done! You’ve learned how to work with R to calculate basic statistics. We’ve had you generate a few by hand, but be sure to use the built-in functions in R in the future.\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/4 Module/mod4.html#examining-wid_data",
    "href": "R files/4 Module/mod4.html#examining-wid_data",
    "title": "Module 4: Data Manipulation",
    "section": "Examining ’wid_data",
    "text": "Examining ’wid_data\n\nLook at the data. What is the main problem here?\nWe don’t have columns headers. The World Inequality Database says the “structure” of the download is as shown in the image below.\n\n\nSo we can create our own header in read_xlsx. Calling the read_xlsx function using readxl::read_xlsx() ensures that we use the read_xlsx() function from the readxl package.\n\nwid_data_raw <- readxl::read_xlsx(\"world_wealth_inequality.xlsx\",\n                                   col_names = c(\"country\", \"indicator\",\n                                                 \"percentile\", \"year\", \"value\"))\n\nNow when we look at the second column. It’s a mess. We can separate it based on where the \\n are and then deal with the data later. Don’t worry about this code right now.\n\nwid_data_raw <- readxl::read_xlsx(\"world_wealth_inequality.xlsx\",\n                                  col_names = c(\"country\", \"indicator\",\n                                                \"percentile\", \"year\", \n                                                \"value\")) %>%\n  separate(indicator, sep = \"\\\\n\", into = c(\"row_tag\", \"type\", \"notes\"))\n\nNote: We want a clean reproducible script so you should just have one block of code reading the data: that last one. The other code were building blocks. If you want to keep “extra” code temporarily in your script you can use # to comment out the code."
  },
  {
    "objectID": "R files/4 Module/mod4.html#manipulating-world-inequality-data-with-dplyr",
    "href": "R files/4 Module/mod4.html#manipulating-world-inequality-data-with-dplyr",
    "title": "Module 4: Data Manipulation",
    "section": "Manipulating World Inequality Data with dplyr",
    "text": "Manipulating World Inequality Data with dplyr\nNow we have some data and are ready to use select(), filter(), mutate(), summarize() and arrange() to explore it.\n\nThe data comes with some redundant columns that add clutter when we examine the data. What dplyr verb let’s you choose what columns to see? Remove the unwanted column row_tag and move notes to the last column position and assign the output to the name wid_data1\nLet’s start to dig into the data. We have two types of data: “Net personal wealth” and “National income”. Start by filter()ing the data so we only have “Net personal wealth” for France, name the resulting data french_data and then run the code below to visualize the data.\n\n1 Hint: You can type all the column names or use the slicker select(-notes, everything())\n# replace each ... with relevant code\nfrench_data <- wid_data %>% filter( ... , ...)\n\nNote: When refering to words in the data, make sure they are in quotes “France”, “Net personal wealth”. When referring to columns, do not use quotes.\n\nfrench_data %>% \n  ggplot(aes(y = value, x = year, color = percentile)) + \n  geom_line()\n\nNow we’re getting somewhere! The plot shows the proportion of national wealth owned by different segements of French society overtime. For example in 2000, the top 1 percent owned roughly 28 percent of the wealth, while the bottom 50 percent owned abouy 7 percent.\n\nExplain the gaps in the plot. Using filter(), look at french_data in the years between 1960 and 1970. Does what you see line up with what you guessed by looking at the graph?\nUsing mutate(), create a new column called perc_national_wealth that equals value multiplied by 100. Adjust the graph code so that the y axis shows perc_national_wealth instead of value.\nNow following the same steps, explore data from the “Russian Federation”.\nThe data for “Russian Federation” does not start in 1900, but our y-axis does. That’s because we have a bunch of NAs. Let’s filter out the NAs and remake the plot. You cannot test for NA using == (Try: NA == NA). Instead we have a function called is.na(). (Try: is.na(NA) and !is.na(NA)).\nUse two dplyr verbs to figure out what year the bottom 50 percent held the least wealth. First, choose the rows that cover the bottom 50 percent and then sort the data in descending order using arrange()2.\n\n2 Hint: Look at the examples in ?arrange\n# replace ... with relevant code\nrussian_data %>% \n  filter(...) %>% \n  arrange(...)\n\n\nFor both the Russian Federation and French data, calculate the average proportion of wealth owned by the top 10 percent over the period from 1995 to 2010. You’ll have to filter and then summarize with summarize().\n\n\n# replace ... with relevant code\nrussian_data %>% \n  filter(...) %>% \n  summarize(top10 = mean(...))"
  },
  {
    "objectID": "R files/4 Module/mod4.html#manipulating-midwest-demographic-data-with-dplyr",
    "href": "R files/4 Module/mod4.html#manipulating-midwest-demographic-data-with-dplyr",
    "title": "Module 4: Data Manipulation",
    "section": "Manipulating Midwest Demographic Data with dplyr",
    "text": "Manipulating Midwest Demographic Data with dplyr\n\nNow we’ll use midwestern demographic data which is at this link. The dataset includes county level data for a single year. We call data this type of data “cross-sectional” since it gives a point-in-time cross-section of the counties of the midwest. (The world inequality data is “timeseries” data).\nSave midwest.dta in your data folder and load it into R.\n\n\nmidwest <- read_dta('midwest.dta')\n\n\nRun the following code to get a sense of what the data looks like:\n\n\nglimpse(midwest)\n\n\nI wanted a tibble called midwest_pop that only had county identifiers and the 9 columns from midwest concerned with population counts. Replicate my work to create midwest_pop on your own3.\n\n3 Hint: notice that all the columns start with the same few letters.\nnames(midwest_pop)\n\n [1] \"county\"          \"state\"           \"poptotal\"        \"popdensity\"     \n [5] \"popwhite\"        \"popblack\"        \"popamerindian\"   \"popasian\"       \n [9] \"popother\"        \"popadults\"       \"poppovertyknown\"\n\n\nHint: I went to ?select and found a selection helper that allowed me to select those 9 columns without typing all their names\n\n# replace ... with relevant code\nmidwest_pop <- midwest %>% select(county, state, ...)\n\n\nFrom midwest_pop calculate the area of each county4. What’s the largest county in the midwest? How about in Illinois?\nFrom midwest_pop calculate percentage adults for each county. What county in the midwest has the highest proportion of adults? What’s county in the midwest has the lowest proportion of adults?\nHow many people live in Michigan?\nNote that together population density and population can give you information about the area (geographic size) of a location. What’s the total area of Illinois? You probably have no idea what the units are though. If you google, you’ll find that it doesn’t align perfectly with online sources. Given the units don’t align with other sources, can this data still be useful?\n\n4 Notice that \\(popdensity = \\frac{poptotal}{area}\\)Well done! You’ve learned how to work with R to perform simple data manipulation and analysis!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/4 Module/mod4.html#lab-4",
    "href": "R files/4 Module/mod4.html#lab-4",
    "title": "Module 4: Data Manipulation",
    "section": "Lab 4",
    "text": "Lab 4\nIn this lab, you will work with 2 data sets (world_wealth_inequality.xlsx and midwest.dta).\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\nWarm-up\n\nWhich of these are commands from dplyr?\n\n\nmutate()\nfilter()\nmean()\n\n\nIn the videos, you learned about head(). What if you wanted to get the tail end of your data instead?\nImagine you have a data set, df with 4 variables, county, year, income, and employment. You only need the year and employment status of people whose income is below $5000. Which two dplyr commands do you need to do this? Can you write the code for this?\nRemember the mean() function? What dplyr commands would we need if we want the average income in counties for the year 2003? Can you write the code for this?\nLoad tidyverse, haven, and readxl in your Rmd. If you haven’t yet, download the data from this page and put the data in your data folder and set your working directory. The data source is the World Inequality Database where you can find data about the distribution of income and wealth in several contries over time. Outside of lab time, check out wid.world for more information.\nIf you followed the set-up from above, you should be able to run the following code with no error.\n\n\nwid_data <- read_xlsx(\"world_wealth_inequality.xlsx\")\n\nExamining ’wid_data\n\nLook at the data. What is the main problem here?\nWe don’t have columns headers. The World Inequality Database says the “structure” of the download is as shown in the image below.\n\n\nSo we can create our own header in read_xlsx. Calling the read_xlsx function using readxl::read_xlsx() ensures that we use the read_xlsx() function from the readxl package.\n\nwid_data_raw <- readxl::read_xlsx(\"world_wealth_inequality.xlsx\",\n                                   col_names = c(\"country\", \"indicator\",\n                                                 \"percentile\", \"year\", \"value\"))\n\nNow when we look at the second column. It’s a mess. We can separate it based on where the \\n are and then deal with the data later. Don’t worry about this code right now.\n\nwid_data_raw <- readxl::read_xlsx(\"world_wealth_inequality.xlsx\",\n                                  col_names = c(\"country\", \"indicator\",\n                                                \"percentile\", \"year\", \n                                                \"value\")) %>%\n  separate(indicator, sep = \"\\\\n\", into = c(\"row_tag\", \"type\", \"notes\"))\n\nNote: We want a clean reproducible script so you should just have one block of code reading the data: that last one. The other code were building blocks. If you want to keep “extra” code temporarily in your script you can use # to comment out the code.\nManipulating World Inequality Data with dplyr\n\nNow we have some data and are ready to use select(), filter(), mutate(), summarize() and arrange() to explore it.\n\nThe data comes with some redundant columns that add clutter when we examine the data. What dplyr verb let’s you choose what columns to see? Remove the unwanted column row_tag and move notes to the last column position and assign the output to the name wid_data1\nLet’s start to dig into the data. We have two types of data: “Net personal wealth” and “National income”. Start by filter()ing the data so we only have “Net personal wealth” for France, name the resulting data french_data and then run the code below to visualize the data.\n\n1 Hint: You can type all the column names or use the slicker select(-notes, everything())\n# replace each ... with relevant code\nfrench_data <- wid_data %>% filter( ... , ...)\n\nNote: When refering to words in the data, make sure they are in quotes “France”, “Net personal wealth”. When referring to columns, do not use quotes.\n\nfrench_data %>% \n  ggplot(aes(y = value, x = year, color = percentile)) + \n  geom_line()\n\nNow we’re getting somewhere! The plot shows the proportion of national wealth owned by different segements of French society overtime. For example in 2000, the top 1 percent owned roughly 28 percent of the wealth, while the bottom 50 percent owned abouy 7 percent.\n\nExplain the gaps in the plot. Using filter(), look at french_data in the years between 1960 and 1970. Does what you see line up with what you guessed by looking at the graph?\nUsing mutate(), create a new column called perc_national_wealth that equals value multiplied by 100. Adjust the graph code so that the y axis shows perc_national_wealth instead of value.\nNow following the same steps, explore data from the “Russian Federation”.\nThe data for “Russian Federation” does not start in 1900, but our y-axis does. That’s because we have a bunch of NAs. Let’s filter out the NAs and remake the plot. You cannot test for NA using == (Try: NA == NA). Instead we have a function called is.na(). (Try: is.na(NA) and !is.na(NA)).\nUse two dplyr verbs to figure out what year the bottom 50 percent held the least wealth. First, choose the rows that cover the bottom 50 percent and then sort the data in descending order using arrange()2.\n\n2 Hint: Look at the examples in ?arrange\n# replace ... with relevant code\nrussian_data %>% \n  filter(...) %>% \n  arrange(...)\n\n\nFor both the Russian Federation and French data, calculate the average proportion of wealth owned by the top 10 percent over the period from 1995 to 2010. You’ll have to filter and then summarize with summarize().\n\n\n# replace ... with relevant code\nrussian_data %>% \n  filter(...) %>% \n  summarize(top10 = mean(...))\n\nManipulating Midwest Demographic Data with dplyr\n\n\nNow we’ll use midwestern demographic data which is at this link. The dataset includes county level data for a single year. We call data this type of data “cross-sectional” since it gives a point-in-time cross-section of the counties of the midwest. (The world inequality data is “timeseries” data).\nSave midwest.dta in your data folder and load it into R.\n\n\nmidwest <- read_dta('midwest.dta')\n\n\nRun the following code to get a sense of what the data looks like:\n\n\nglimpse(midwest)\n\n\nI wanted a tibble called midwest_pop that only had county identifiers and the 9 columns from midwest concerned with population counts. Replicate my work to create midwest_pop on your own3.\n\n3 Hint: notice that all the columns start with the same few letters.\nnames(midwest_pop)\n\n [1] \"county\"          \"state\"           \"poptotal\"        \"popdensity\"     \n [5] \"popwhite\"        \"popblack\"        \"popamerindian\"   \"popasian\"       \n [9] \"popother\"        \"popadults\"       \"poppovertyknown\"\n\n\nHint: I went to ?select and found a selection helper that allowed me to select those 9 columns without typing all their names\n\n# replace ... with relevant code\nmidwest_pop <- midwest %>% select(county, state, ...)\n\n\nFrom midwest_pop calculate the area of each county4. What’s the largest county in the midwest? How about in Illinois?\nFrom midwest_pop calculate percentage adults for each county. What county in the midwest has the highest proportion of adults? What’s county in the midwest has the lowest proportion of adults?\nHow many people live in Michigan?\nNote that together population density and population can give you information about the area (geographic size) of a location. What’s the total area of Illinois? You probably have no idea what the units are though. If you google, you’ll find that it doesn’t align perfectly with online sources. Given the units don’t align with other sources, can this data still be useful?\n\n4 Notice that \\(popdensity = \\frac{poptotal}{area}\\)Well done! You’ve learned how to work with R to perform simple data manipulation and analysis!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/5 Module/mod5.html#lab-5",
    "href": "R files/5 Module/mod5.html#lab-5",
    "title": "Module 5: Data Manipulation and Analysis II",
    "section": "Lab 5",
    "text": "Lab 5\nIn this lab, you will work with data sets from recent_college_grads.dta.\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\n\n\n\nWarm up\n\nData wrangling and visualization with college data\n\nWe will explore data on college majors and earnings, specifically the data behind the FiveThirtyEight story “The Economic Guide To Picking A College Major”.\nWe read it in with the read_dta function, and save the result as a new data frame called college_recent_grads. Because read_dta is a function from haven, we will need to load that package.\n\nlibrary(tidyverse)\nlibrary(haven)\ncollege_recent_grads <- read_dta('recent_college_grads.dta')\n\ncollege_recent_grads is a tidy data frame, with each row representing an observation and each column representing a variable.\nTo view the data, you can take a quick peek at your data frame and view its dimensions with the glimpse function.\n\nglimpse(college_recent_grads)\n\nThe description of the variables, i.e. the codebook, is given below.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nrank\nRank by median earnings\n\n\nmajor_code\nMajor code, FO1DP in ACS PUMS\n\n\nmajor\nMajor description\n\n\nmajor_category\nCategory of major from Carnevale et al\n\n\ntotal\nTotal number of people with major\n\n\nsample_size\nSample size (unweighted) of full-time, year-round ONLY (used for earnings)\n\n\nmen\nMale graduates\n\n\nwomen\nFemale graduates\n\n\nsharewomen\nWomen as share of total\n\n\nemployed\nNumber employed (ESR == 1 or 2)\n\n\nemployed_full_time\nEmployed 35 hours or more\n\n\nemployed_part_time\nEmployed less than 35 hours\n\n\nemployed_full_time_yearround\nEmployed at least 50 weeks (WKW == 1) and at least 35 hours (WKHP >= 35)\n\n\nunemployed\nNumber unemployed (ESR == 3)\n\n\nunemployment_rate\nUnemployed / (Unemployed + Employed)\n\n\nmedian\nMedian earnings of full-time, year-round workers\n\n\np25th\n25th percentile of earnigns\n\n\np75th\n75th percentile of earnings\n\n\ncollege_jobs\nNumber with job requiring a college degree\n\n\nnon_college_jobs\nNumber with job not requiring a college degree\n\n\nlow_wage_jobs\nNumber in low-wage service jobs\n\n\n\n\n\nWhich major has the lowest unemployment rate?\nIn order to answer this question all we need to do is sort the data. We use the arrange function to do this, and sort it by the unemployment_rate variable. By default arrange sorts in ascending order, which is what we want here – we’re interested in the major with the lowest unemployment rate.\n\ncollege_recent_grads %>%\n  arrange(unemployment_rate)\n\n# A tibble: 173 × 21\n    rank major…¹ major major…² total sampl…³ men   women share…⁴ emplo…⁵ emplo…⁶\n   <dbl>   <dbl> <chr> <chr>   <chr>   <dbl> <chr> <chr> <chr>     <dbl>   <dbl>\n 1    53    4005 Math… Comput… 609         7 500   109   0.1789…     559     584\n 2    74    3801 Mili… Indust… 124         4 124   0     0             0     111\n 3    84    3602 Bota… Biolog… 1329        9 626   703   0.5289…    1010     946\n 4   113    1106 Soil… Agricu… 685         4 476   209   0.3051…     613     488\n 5   121    2301 Educ… Educat… 804         5 280   524   0.6517…     703     733\n 6    15    2409 Engi… Engine… 4321       30 3526  795   0.1839…    3608    2999\n 7    20    3201 Cour… Law & … 1148       14 877   271   0.2360…     930     808\n 8   120    2305 Math… Educat… 14237     123 3872  10365 0.7280…   13115   11259\n 9     1    2419 Petr… Engine… 2339       36 2057  282   0.1205…    1976    1849\n10    65    1100 Gene… Agricu… 10399     158 6053  4346  0.4179…    8884    7589\n# … with 163 more rows, 10 more variables: employed_parttime <dbl>,\n#   employed_fulltime_yearround <dbl>, unemployed <dbl>,\n#   unemployment_rate <dbl>, p25th <dbl>, median <dbl>, p75th <dbl>,\n#   college_jobs <dbl>, non_college_jobs <dbl>, low_wage_jobs <dbl>, and\n#   abbreviated variable names ¹​major_code, ²​major_category, ³​sample_size,\n#   ⁴​sharewomen, ⁵​employed, ⁶​employed_fulltime\n\n\nThis gives us what we wanted, but not in an ideal form. First, the name of the major barely fits on the page. Second, some of the variables are not that useful (e.g. major_code, major_category) and some we might want front and center are not easily viewed (e.g. unemployment_rate).\nWe can use the select function to choose which variables to display, and in which order:\n\ncollege_recent_grads %>%\n  arrange(unemployment_rate) %>%\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   <dbl> <chr>                                                  <dbl>\n 1    53 Mathematics And Computer Science                     0      \n 2    74 Military Technologies                                0      \n 3    84 Botany                                               0      \n 4   113 Soil Science                                         0      \n 5   121 Educational Administration And Supervision           0      \n 6    15 Engineering Mechanics Physics And Science            0.00633\n 7    20 Court Reporting                                      0.0117 \n 8   120 Mathematics Teacher Education                        0.0162 \n 9     1 Petroleum Engineering                                0.0184 \n10    65 General Agriculture                                  0.0196 \n# … with 163 more rows\n\n\nOk, this is looking better, but do we really need all those decimal places in the unemployment variable? Not really!\n\n1a. Round unemployment_rate: We create a new variable with the mutate function. In this case, we’re overwriting the existing unemployment_rate variable, by rounding it to 1 decimal places. Incomplete code is given below to guide you in the right direction, however you will need to fill in the blanks.\n\n\ncollege_recent_grads<- college_recent_grads %>%\n  arrange(unemployment_rate) %>%\n  select(rank, major, unemployment_rate) %>%\n  mutate(unemployment_rate = ___(___, 1))\n\nWhile were making some changes, let’s change sharewomen to numeric (it appears to be a string). Make sure to save your changes by overwriting the existing data frame!\n\ncollege_recent_grads <- college_recent_grads %>%\n  mutate(sharewomen = as.numeric(___))\n\n\n\nWhich major has the highest percentage of women?\nTo answer such a question we need to arrange the data in descending order. For example, if earlier we were interested in the major with the highest unemployment rate, we would use the following:\nThe desc function specifies that we want unemployment_rate in descending order.\n\ncollege_recent_grads %>%\n  arrange(desc(unemployment_rate)) %>%\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   <dbl> <chr>                                                  <dbl>\n 1     6 Nuclear Engineering                                    0.177\n 2    90 Public Administration                                  0.159\n 3    85 Computer Networking And Telecommunications             0.152\n 4   171 Clinical Psychology                                    0.149\n 5    30 Public Policy                                          0.128\n 6   106 Communication Technologies                             0.120\n 7     2 Mining And Mineral Engineering                         0.117\n 8    54 Computer Programming And Data Processing               0.114\n 9    80 Geography                                              0.113\n10    59 Architecture                                           0.113\n# … with 163 more rows\n\n\n\n1b. Using what you’ve learned so far, arrange the data in descending order with respect to proportion of women in a major, and display only the major, the total number of people with major, and proportion of women. Show only the top 3 majors by adding head(3) at the end of the pipeline.\n\n\n#code here\n\n\n\nHow do the distributions of median income compare across major categories?\nA percentile is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value below which 20% of the observations may be found. (Source: Wikipedia\nThere are three types of incomes reported in this data frame: p25th, median, and p75th. These correspond to the 25th, 50th, and 75th percentiles of the income distribution of sampled individuals for a given major.\nThe question we want to answer “How do the distributions of median income compare across major categories?”. We need to do a few things to answer this question: First, we need to group the data by major_category. Then, we need a way to summarize the distributions of median income within these groups. This decision will depend on the shapes of these distributions. So first, we need to visualize the data.\n\n1c.Let’s start simple and take a look at the distribution of all median incomes using geom_histogram, without considering the major categories.\n\n\nggplot(data = ____,\n       mapping = aes(x = median)) +\n  geom_histogram()\n\n\n1d. Try binwidths of \\(1000\\) and \\(5000\\) and choose one. Explain your reasoning for your choice.\n\n\nggplot(data = ___,\n       mapping = aes(x = median)) +\n  geom_histogram(binwidth = ___)\n\nWe can also calculate summary statistics for this distribution using the summarise function:\n\ncollege_recent_grads %>%\n  summarise(min = min(median), max = max(median),\n            mean = mean(median), med = median(median),\n            sd = sd(median), \n            q1 = quantile(median, probs = 0.25),\n            q3 = quantile(median, probs = 0.75))\n\n# A tibble: 1 × 7\n    min    max   mean   med     sd    q1    q3\n  <dbl>  <dbl>  <dbl> <dbl>  <dbl> <dbl> <dbl>\n1 22000 110000 40151. 36000 11470. 33000 45000\n\n\n\n1e. Based on the shape of the histogram you created in the previous 1e, determine which of these summary statistics above (min, max, mean, med, sd, q1, q3) is/are useful for describing the distribution. Write up your description and include the summary statistic output as well.You can pick single/multiple statistics and briefly explain why you pick it/them.\n1f. Next, we facet the plot by major category. Plot the distribution of median income using a histogram, faceted by major_category. Use the binwidth you chose in 1e.\n\n\nggplot(data = ___,\n       mapping = aes (x=median)) +\n  geom_histogram(bindwidth = 5000) +\n  facet_wrap(.~major_category)\n\n\n1g. Use filter to find out which major has the highest median income? lowest? Which major has the median() median income? Hint: refer to the statistics in 1d.\n\n\ncollege_recent_grads %>%\n  ____(median == ____) \n\n\n1h. Which major category is the most popular in this sample? To answer this question we use a new function called count, which first groups the data , then counts the number of observations in each category and store the counts into a column named n. Add to the pipeline appropriately to arrange the results so that the major with the highest observations is on top.\n\n\ncollege_recent_grads %>%\n  count(major_category) %>% \n  ___(___(n))\n\n\n\nWhat types of majors do women tend to major in?\nFirst, let’s create a new vector called stem_categories that lists the major categories that are considered STEM fields.\n\nstem_categories <- c(\"Biology & Life Science\",\n                     \"Computers & Mathematics\",\n                     \"Engineering\",\n                     \"Physical Sciences\")\n\nThen, we can use this to create a new variable in our data frame indicating whether a major is STEM or not.\n\ncollege_recent_grads <- college_recent_grads %>%\n  mutate(major_type = ifelse(major_category %in% \n                               stem_categories, \"stem\", \"not stem\"))\n\nLet’s unpack this: with mutate we create a new variable called major_type, which is defined as \"stem\" if the major_category is in the nector called stem_categories we created earlier, and as \"not stem\" otherwise.\n\n1i. Create a scatterplot of median income vs. proportion of women in that major, colored by whether the major is in a STEM field or not. Describe the association between these three variables.\n\n\nggplot(data = ___, \n       mapping = aes(x=median, \n           y= sharewomen, \n           color=major_type)) + \n  geom_point()\n\n\n1j.. We can use the logical operators to also filter our data for STEM majors whose median earnings is less than median for all majors’s median earnings, which we found to be $36,000 earlier. Your output should only show the major name and median, 25th percentile, and 75th percentile earning for that major and should be sorted such that the major with the lowest median earning is on top.\n\n\n#code here\n\nWell done! You’ve learned how to work with R to perform basic data analysis!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/6 Module/mod6.html#lab-6",
    "href": "R files/6 Module/mod6.html#lab-6",
    "title": "Module 6: Data Visualization as a Tool for Analysis",
    "section": "Lab 6",
    "text": "Lab 6\nIn this lab, you will work with midwest.dta.\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\n\n\n\nQuestions\nRecall ggplot works by mapping data to aesthetics and then telling ggplot how to visualize the aesthetic with geoms. Like so:\n\nmidwest %>%\n  ggplot(aes(x = percollege, \n             y = percbelowpoverty,\n             color = state,\n             size = poptotal,\n             alpha = percpovertyknown)) + \n  geom_point() + facet_wrap(vars(state))\n\n\n\n\n\nWhich is more highly correlated with poverty at the county level, college completion rates or high school completion rates? Is it consistent across states? Change one line of code in the above graph.\n\n\n\ngeoms\nFor the following, write code to reproduce each plot using midwest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice here inmetro is numeric, but I want it to behave like a discrete variable so I use x = as.character(inmetro). Use labs(title = \"Asian population by metro status\") to create the title.\n\n\n\n\n\n\n\nUse geom_boxplot() instead of geom_point() for “Asian population by metro status”\nUse geom_jitter() instead of geom_point() for “Asian population by metro status”\nUse geom_jitter() and geom_boxplot() at the same time for “Asian population by metro status”. Does order matter?\nHistograms are used to visualize distributions. What happens when you change the bins argument? What happens if you leave the bins argument off?\n\n\nmidwest %>%\n  ggplot(aes(x = perchsd)) +\n  geom_histogram(bins = 100) +\n  labs(title = \"distribution of county-level hs completion rate\")\n\n\n\n\n\nRemake “distribution of county-level hs completion rate” with geom_density() instead of geom_histogram().\nAdd a vertical line at the median perchsd using geom_vline. You can calculate the median directly in the ggplot code.\n\n\nAesthetics\nFor the following, write code to reproduce each plot using midwest\n\nUse x, y, color and size\n\n\n\n\n\n\n\nUse x, y, color and size\n\n\n\n\n\n\n\nWhen making bar graphs, color only changes the outline of the bar. Change the aestethic name to fill to get the desired result\n\n\nmidwest %>% \n  count(state) %>% \n  ggplot(aes(x = state,\n             y = n, \n             color = state)) + \n  geom_col()\n\n\n\n\n\nThere’s a geom called geom_bar that takes a dataset and calculates the count. Read the following code and compare it to the geom_col code above. Describe how geom_bar() is different than geom_col\n\n\nmidwest %>% \n  ggplot(aes(x = state,\n             color = state)) +\n  geom_bar()\n\n\n\n\nWell done! You’ve learned how to work with R to create some awesome looking visuals!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/7 Module/mod7.html#lab-7",
    "href": "R files/7 Module/mod7.html#lab-7",
    "title": "Module 7: Grouped Analysis",
    "section": "Lab 7",
    "text": "Lab 7\nIn this lab, you will work with data_traffic.csv.\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\n\n\n\nBackground and Data\nRead the background and data section before lab.\n\nFollow the tweet thread and you’ll see how Professor Damon Jones, of Harris, prepares and analyzes his data. In this lab, you’re going to follow his lead and dig into traffic stop data from the University of Chicago Police Department, one of the largest private police forces in the world.\n\n\nWarm-up\n\nOpen a new Rmd and save it in your coding lab folder; if you downloaded the data, move your data file to your preferred data location.\nIn your Rmd, write code to load your packages. If you load packages in the console, you will get an error when you knit because knitting starts a fresh R session.\nLoad data_traffic.csv and assign it to the name traffic_data. This data was scrapped from the UCPD website and partially cleaned by Prof. Jones.\nRecall that group_by() operates silently. Below I create a new data frame called grouped_data.\n\n\ngrouped_data <- traffic_data %>%\n  group_by(Race, Gender)\n\n\n\nHow can you tell grouped_data is different from traffic_data?\n\n\nHow many groups (Race-Gender pairs) are in the data? (This information should be available without writing additional code!)\n\n\nWithout running the code, predict the dimensions (number of rows by number of columns) of the tibbles created by traffic_data %>% summarize(n = n()) and grouped_data %>% summarize(n = n()).\n\n\nNow check you intuition by running the code.\n\n\n\nUse group_by() and summarize() to recreate the following table.\n\n\n\n# A tibble: 6 × 2\n  Race                                       n\n  <chr>                                  <int>\n1 African American                        3278\n2 American Indian/Alaskan Native            12\n3 Asian                                    226\n4 Caucasian                                741\n5 Hispanic                                 217\n6 Native Hawaiian/Other Pacific Islander     4\n\n\n\nUse count() to produce the same table.\n\n\n\nMoving beyond counts\n\nRaw counts are okay, but frequencies (or proportions) are easier to compare across data sets. Add a column with frequencies and assign the new tibble to the name traffic_stop_freq. The result should be identical to Prof. Jones’s analysis on twitter.\n\nTry on your own first. If you’re not sure how to add a frequency though, you could google “add a proportion to count with tidyverse” and find this stackoverflow post. Follow the advice of the number one answer. The green checkmark and large number of upvotes indicate the answer is likely reliable.\n\nThe frequencies out of context are not super insightful. What additional information do we need to argue the police are disproportionately stopping members of a certain group? (Hint: Prof. Jones shares the information in his tweets.)\nFor the problem above, your groupmate tried the following code. Explain why the frequencies are all 1.\n\n\ntraffic_stop_freq_bad <- traffic_data %>%\n  group_by(Race) %>% \n  summarize(n = n(),\n            freq = n / sum(n)) \n\ntraffic_stop_freq_bad\n\nNow we want to go a step further. Do outcomes differ by race? In the first code block below, I provide code so you can visualize disposition by race. “Disposition” is police jargon that means the current status or final outcome of a police interaction.\n\ncitation_strings <- c(\"citation issued\", \"citations issued\",\n                      \"citation issued\" )\n\narrest_strings <- c(\"citation issued, arrested on active warrant\",\n                    \"citation issued; arrested on warrant\",\n                    \"arrested by cpd\", \"arrested on warrant\",\n                    \"arrested\",\"arrest\")\n\ndisposition_by_race <- traffic_data %>%\n  mutate(Disposition = str_to_lower(Disposition),\n         Disposition = case_when(Disposition %in% citation_strings ~ \"citation\",\n                                 Disposition %in% arrest_strings ~ \"arrest\",\n                                 TRUE ~ Disposition)) %>%\n  count(Race, Disposition) %>% group_by(Race) %>%\n  mutate(freq = round(n / sum(n), 3))\n\ndisposition_by_race %>%\n  filter(n > 5, Disposition == \"citation\") %>%\n  ggplot(aes(y = freq, x = Race)) + geom_col() +\n  labs(y = \"Citation Rate Once Stopped\", x = \"\", title = \"Traffic Citation Rate\") +\n  theme_minimal()\n\n\n\n\nLet’s break down how we got to this code. First, I ran traffic_data %>% count(Race, Disposition) and noticed that we have a lot of variety in how officers enter information into the system. I knew I could deal with some of the issue by standardizing capitalization.\n\n\nIn the console, try out str_to_lower(...) by replacing the … with different strings. The name may be clear enough, but what does str_to_lower() do?\n\n\nAfter using mutate with str_to_lower(), I piped into count() again and looked for strings that represent the same Disposition. I stored terms in character vectors (e.g. citation_strings). The purpose is to make the case_when() easier to code and read. Once I got that right, I added frequencies to finalize disposition_by_race.\n\nTo make the graph, I first tried to get all the disposition data on the same plot.\n\n\n disposition_by_race %>%\n  ggplot(aes(y = freq, x = Race, fill = Disposition)) + \n  geom_col()\n\n\n\n\nBy default, the bar graph is stacked. Look at the resulting graph and discuss the pros and cons of this plot.\n\nI decided I would focus on citations only and added the filter(n > 5, Disposition == \"citation\") to the code. What is the impact of filtering based on n > 5? Would you make the same choice? This question doesn’t have a “right” answer. You should try different options and reflect.\nNow, you can create a similar plot based called “Search Rate” using the Search variable. Write code to re-produce this plot.\n\n\n\n\n\n\nWell done! You’ve learned how to conduct grouped analysis using real-world data!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/8 Module/mod8.html#lab-8",
    "href": "R files/8 Module/mod8.html#lab-8",
    "title": "Module 8: Iteration",
    "section": "Lab 8",
    "text": "Lab 8\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\nWarm-up\nRecall, for-loops are an iterator that help us repeat tasks while changing inputs. The most common structure for your code will look like the following code. This can be simplified if you are not storing results.\n\n# what are you iterating over? The vector from -10:10\nitems_to_iterate_over <- c(-10:10) \n\n# pre-allocate the results\nout <- rep(0, length(items_to_iterate_over))\n# write the iteration statement --\n# we'll use indices so we can store the output easily \nfor (i in seq_along(items_to_iterate_over)) {\n# do something\n# we capture the median of three random numbers\n# from normal distributions various means\n    out[[i]] <- median(rnorm(n = 3,mean = items_to_iterate_over[[i]]))\n    }\n\nWriting for-loops\n\nWrite a for-loop that prints the numbers 5, 10, 15, 20, 250000.\nWrite a for-loop that iterates over the indices of x and prints the ith value of x.\n\n\nx <- c(5, 10, 15, 20, 250000)\n# replace the ... with the relevant code\n\nfor (i in ... ){ \n  print(x[[...]])\n  }\n\n\nWrite a for-loop that simplifies the following code so that you don’t repeat yourself! Don’t worry about storing the output yet. Use print() so that you can see the output. What happens if you don’t use print()?\n\n\nsd(rnorm(5))\nsd(rnorm(10))\nsd(rnorm(15))\nsd(rnorm(20)) \nsd(rnorm(25000))\n\n\nadjust your for-loop to see how the sd changes when you use rnorm(n, mean = 4)\n\nadjust your for-loop to see how the sd changes when you use rnorm(n, sd = 4)\n\n\n\nNow store the results of your for-loop above in a vector. Pre-allocate a vector of length 5 to capture the standard deviations.\nvectorization vs for loops\nRecall, vectorized functions operate on a vector item by item. It’s like looping over the vector! The following for-loop is better written vectorized.\nCompare the loop version\n\nnames <- c(\"Marlene\", \"Jacob\", \"Buddy\")\nout <- character(length(names))\n\nfor (i in seq_along(names)) {\n  out[[i]] <- paste0(\"Welcome \", names[[i]])\n  }\n\nto the vectorized version\n\nnames <- c(\"Marlene\", \"Jacob\", \"Buddy\") \nout <- paste0(\"Welcome \", names)\n\nThe vectorized code is preferred because it is easier to write and read, and is possibly more efficient.\n\nRewrite your first for-loop, where you printed 5, 10, 15, 20, 250000 as vectorised code\nRewrite this for-loop as vectorized code:\n\n\nradii <- c(0:10)\n\narea <- double(length(radii)) \n\nfor (i in seq_along(radii)) { \n  area[[i]] <- pi * radii[[i]] ^ 2 \n  }\n\n\nRewrite this for-loop as vectorized code:\n\n\nradii <- c(-1:10)\n\narea <- double(length(radii))\n\nfor (i in seq_along(radii)) { \n  if (radii[[i]] < 0) { \n    area[[i]] <- NaN \n  } else {\n      area[[i]] <- pi * radii[[i]] ^ 2 }\n  }\n\nExtension\nSimulating the Law of Large Numbers\nThe Law of Large Numbers says that as sample sizes increase, the mean of the sample will approach the true mean of the distribution. We are going to simulate this phenomenon!\nWe’ll start by making a vector of sample sizes from 1 to 50, to represent increasing sample sizes.\nCreate a vector called sample_sizes that is made up of the numbers 1 through 50. (Hint: You can use seq() or : notation).\nWe’ll make an empty tibble to store the results of the for loop:\n\nestimates <- tibble(n = integer(), sample_mean = double())\n\nWrite a loop over the sample_sizes you specified above. In the loop, for each sample size you will:\n\nCalculate the mean of a sample from the random normal distribution with mean = 0 and sd = 5.\nMake an intermediate tibble to store the results\nAppend the intermediate tibble to your tibble using bind_rows().\n\n\nset.seed(60637) \nfor (___ in ___) {\n  # Calculate the mean of a sample from the random normal distribution with mean = 0 and s\n  ___ <- ___\n  # Make a tibble with your estimates\n  this_estimate <- tibble(n = ___, sample_mean = ___) \n  # Append the new rows to your tibble\n  ___ <- bind_rows(estimates, ___)\n  }\n\nWe can use ggplot2 to view the results. Fill in the correct information for the data and x and y variables, so that the n column of the estimates tibble is plotted on the x-axis, while the sample_mean column of the estimates tibble is plotted on the y-axis.\n\n# your data goes in the first position\n___ %>%\n  ggplot(aes(x = ___, y = ___)) + \n  geom_line()\n\n\nAs the sample size (n) increases, does the sample mean becomes closer to 0, or farther away from 0?\n\nRewrite the loop code without looking at your previous code and use a wider range of sample sizes. Try several different sample size combinations. What happens when you increase the sample size to 100? 500? 1000? Use the seq() function to generate a sensibly spaced sequence.\n\nset.seed(60637) \nsample_sizes <- ___ \nestimates_larger_n <- ___\n\nfor (___ in ___) {\n  ___ <- ___\n  ___ <- ___\n  ___ <- ___\n}\n\n___ %>%\n  ggplot(___(___ = ___, ___ = ___)) +\n  geom_line()\n\n\nHow does this compare to before?\nExtending Our Simulation\nLooking at your results, you might think a small sample size is sufficient for estimating a mean, but your data had a relatively small standard deviation compared to the mean. Let’s run the same simulation as before with different standard deviations.\nDo the following:\n\nCreate a vector called population_sd of length 4 with values 1, 5, 10, and 20 (you’re welcome to add larger numbers if you wish).\nMake an empty tibble to store the output. Compared to before, this has an extra column for the changing population standard deviations.\nWrite a loop inside a loop over population_sd and then sample_sizes.\nThen, make a ggplot graph where the x and y axes are the same, but we facet (aka we create small multiples of individual graphs) on population_sd.\n\n\nset.seed(60637)\npopulation_sd <- ___\n# use what every you came up with in the previous part \nsample_sizes <- ___\nestimates_adjust_sd <- ___\nfor (___ in ___){ \n  for (___ in ___) {\n    ___ <- ___\n    ___ <- ___\n    ___ <- ___\n  } \n}\n\n___ %>%\n  ggplot(___) +\n  geom_line() + \n  facet_wrap(~population_sd) + \n  theme_minimal()\n\nHow do these estimates differ as you increase the standard deviation?\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/9 Module/mod9.html#lab-9",
    "href": "R files/9 Module/mod9.html#lab-9",
    "title": "Module 9: Writing Functions",
    "section": "Lab 9",
    "text": "Lab 9\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\n\n\n\nQuestions\nRecall a function has the following form\n\nname <- function(args) { \n  # body\n  do something (probably with args) \n  }\n\n\nWrite a function called calc_quadratic that takes an input x and calculates \\(f(x) = x^2 + 2x + 1\\). For example:\n\n\ncalc_quadratic(5)\n\n[1] 36\n\n\n\nWhat are the arguments to your function? What is the body of the function?\nThis function is vectorized! (Since binary operators are vectorized). Show this is true by running calc_quadratic with an input vector that is -10 to 10.\n\n\nYou realize you want to be able to work with any quadratic. Update your functions so that it can work with any quadratic in standard form \\(f(x) = ax^2 + bx + c\\).\n\n\nYour new function will take arguments x, a, b and c.\nSet the default arguments to a=1, b=2 and c=1.\n\n\nWrite a function called solve_quadratic that takes arguments a, b and c and provides the two roots using the quadratic formula.\n\nIn our outline, we suggest you:\n\nCalculate the determinant \\(\\sqrt{b^2 − 4ac}\\) and store as an intermediate value.\nReturn two values by putting them in a vector. If you stored the roots as root_1 and root_2, then the final line of code in the function should be c(root_1, root_2) or, if you prefer, return(c(root_1, root_2)).\n\n\n# fill in the ... with appropriate code\nsolve_quadratic <- function(...){\n  determinant <- ...\n  root_1 <- ...\n  root_2 <- ...\n  c(root_1, root_2)\n}\n\nThe code should work as follows:\n\nsolve_quadratic(a = -4, b = 0, c = 1)\n\n[1] -0.5  0.5\n\n\n\nWe “normalize” a variable by subtracting the mean and dividing by the standard deviation \\(\\frac{x - \\mu}{\\sigma}\\) .\n\nWrite a function called normalize that takes a vector as input and normalizes it. You should get the following output.\n\nnormalize(1:5)\n\n[1] -1.2649111 -0.6324555  0.0000000  0.6324555  1.2649111\n\n\n\nWhat output do you get when the input vector is 0:4? How about -100:-96? Why?\nWhat happens when your input vector is c(1,2,3,4,5, NA)? Rewrite the function so the result is:\nThe txhousing data set comes with ggplot. Use your normalize function in mutate to create normalized_annual_volume to make the following graph.\n\n\n# replace the ... with the appropriate code.\ntxhousing %>%\n  group_by(year, city) %>%\n  summarize(annual_volume = sum(volume, na.rm = TRUE)) %>% \n  group_by(year) %>%\n  mutate(...) %>%\n  ggplot(aes(x = year, y = normalized_annual_volume)) + \n  geom_point() +\n  geom_line(aes(color = city))\n\n\n\n\n\n\n\n\nSimulating Data with Monte Carlo Simulations (Extension)\nYou will be asked to investigate statistical concepts using Monte Carlo simulations. We’ll try not to get too technical in the main body of the lab. There are some “technical notes” which you can ignore!\nIn a monte carlo simulation, you repeatedly:\n\nGenerate random samples of data using a known process.\nMake calculations based on the random sample.\nAggregate the results.\n\nFunctions and loops help us do these repetitious acts efficiently, without repeatedly writing similar code or copying and pasting.\nToday’s problem: Let us investigate how good the random number generator in R is.1 We hypothesize that rnorm(n, mean = true_mean) provides random sample of size n from the normal distribution with mean = true_mean and standard deviation = 1.\nThe lesson is organized as follows.\n\nWe do a single simulation.\nWe take the logic of the simulation, encapsulate it in functions and then run 1000s of simulations!\n\n\nA single simulation\nRecall our hypothesis is that rnorm() faithfully gives us random numbers from the normal distribution. If we test this with a single random draw, we might be misled. For example, let’s draw 30 numbers from a normal distribution with true mean of 0.5 and see if the observed mean appears statistically different from the true mean.\n\n# Setting a seed ensures replicability\nset.seed(4)\n# we set our parameters\ntrue_mean <- .5\nN <- 30\n# We simulate and observe outcomes\nsimulated_data <- rnorm(N, mean = true_mean) # the standard deviation is 1 by default!\nobs_mean <- mean(simulated_data)\nobs_mean\n\n[1] 0.9871873\n\n\nWow! The observed mean is twice what we expected given true_mean! Let’s calculate a z-score to put that in perspective. (Focus on the formulas, you’ll learn the intuition in stats class)\nA z-score is calculated \\(\\frac{\\bar{X}-\\mu}{\\frac{s_n}{\\sqrt{N}}}\\) where \\(\\bar{X}\\) is the sample mean, \\(\\mu\\) is the true mean, \\(s_n\\) is the sample standard deviation and \\(N\\) is the number of observations.\n\nobs_sd <- sd(simulated_data)\nzscore <- (obs_mean - true_mean) / (obs_sd / sqrt(N)) \nzscore\n\n[1] 3.303849\n\n\nWe expect the observed mean of this simulated data will be within 1.96 standard deviations of \\(\\mu\\) 95 out of 100 times.This observation is 3.3 standard deviations from Mu. The probability of that happening by chance is very small. To be more formal about this probability, we can calculate a p-value. Plug in the z-score below:\n\n(1 - pnorm(abs(zscore)))*2\n\n[1] 0.000953672\n\n\nThis says that the probability of getting this draw by chance is less than 0.1 percent or 1 in 1000.\nThat outcome seems surprising, but we could also just have made an unusual draw. In this workshop, we want to see how often we get such extreme results. We will repeat the steps above 1000 times each, but first we’ll write functions that will make this process smooth!\n\n\nWriting Helper Functions to Make Our Monte Carlo Simulation\nWe want to develop functions that automate repeated steps in our Monte Carlo. In that way, we can define a few important parameters and run the entire process without rewriting or copying and pasting code over and over again.\nAs you saw in the motivating example, we must do the following a 1000 times or B times if parameterize the number of iterations with B:\n\nSimulate data and calculate sample statistics.\nDetermine z-scores.\nTest whether the z-score is outside the threshold. Finally, we:\nMeasure to what extent our simulations match the theory.\n\nTo proceed, we’ll write the steps into their own functions, then call them in the right order in the function do_monte_carlo(). We are breaking a complicated process into smaller chunks and tackling them one by one!\nLet’s look at do_monte_carlo(). It takes a sample-size N, a true_mean, number of iterations B (1000 by default) and a significance level alpha (.05 by default). It returns the proportion of observed means that are significantly different from the true_mean with 95 percent confidence level\n\nBefore following our road map, think about how you would set up functions to automate this process. What would the inputs and outputs be of each step/function? Your processes will be different from ours, but that doesn’t mean ours is better.\n\nNow check out do_monte_carlo() below. It’s our road map.\n\ndo_monte_carlo <- function(N, true_mean, B= 1000, alpha = .05){\n# step 1: Simulate B random samples and calculate sample statistics\n  sample_statistics <- make_mc_sample(N, true_mean, B)\n# step 2: Determine z-scores\n  z_scores <- get_zscores(sample_statistics$mean, true_mean, sample_statistics$sd, N)\n# step 3: Test whether the z-scores are outside the threshold.\n  significance <- test_significance(z_scores, alpha)\n# step 4: Measure to what extent our simulations match the theory. (We expect a number close to alpha)\n  mean(significance)\n}\n\n\n\nDetermine z-scores\nWe’ll start with step 2 determine z-scores. Recall the formula for a zscore is\\(\\frac{\\bar{X}-\\mu}{\\frac{s_n}{\\sqrt{N}}}\\).\nWrite a function called get_zscores that takes the observed means and sds, the true mean and N as inputs and returns a z-score as an output. Name the arguments obs_mean, true_mean, obs_sd, and N.\n\n\n\nIf your functions works, it should return 4 for test.\n\ntest <- get_zscores(obs_mean = 4.4, true_mean = 4.3, obs_sd = 0.25, N = 100)\ntest\n\n[1] 4\n\n\nThe function you wrote should also work on vectorized functions. Run the following code which takes estimates of the mean and standard deviation from 5 random draws and returns their associated z-scores:\n\n# before running set eval = TRUE (and delete this comment)\nmade_up_means <- c(4.4, 4.1, 4.2, 4.4, 4.2)\nmade_up_sd <- c(.25, .5, .4, 1, .4)\nmade_up_zscores <- get_zscores(obs_mean = made_up_means,\n                               true_mean = 4.3,\n                               obs_sd = made_up_sd,\n                               N = 100)\nmade_up_zscores\n\n\nWhich observation from made_up_zscores is not statistically different from 4.3 with 95 percent confidence? In other words, which observed mean and standard deviation return \\(|z-score| < 1.96\\)?\n\n\n\n\nCheck for Significance\nNow we write code for step 3. Test whether the z-scores are outside the threshold.\nThe threshold depends on alpha and the formula is abs(qnorm(alpha/2)).\n\nFor example, for a two-tailed z-test at the 95% confidence level, the cutoff is set at 1.96. Verify this using the formula above.\nWrite a function test_significance() that takes zscores and a given alpha and determines if there is a significant difference at the given level.\n\nRun the following code, and check that your code matches the expected output:\n\n# before knitting set eval = TRUE (and delete this comment)\ntest_significance(zscores = 2, alpha = 0.05)\n\nShould return TRUE. And:\n\n# before knitting set eval = TRUE (and delete this comment)\ntest_significance(zscores = c(1.9, -0.3, -3), alpha = 0.05)\n\nShould return FALSE, FALSE, and TRUE.\n\nBuilding make_mc_sample()\nNow we do step 1: simulate B random samples and calculate sample statistics.\nOur goal is make_mc_sample(N, true_mean, B) a function that produces sample statistics from B random samples from the normal distribution with mean true_mean of size N. When you think of doing something B times it suggest we need a loop. Let’s start with the body of the loop. And because we’re in a lesson about functions, let’s write a function.\n\nWrite a function called calc_mean_and_sd_from_sample() that\n\n\nGenerates a random sample with rnorm() of size N centered around true_mean\nCalculate the mean() and sd() of the random sample.\nReturn the mean and sd in a tibble with column names mean and sd.\n\nIdea: To return two values from a function, we need to put those values into a data structure like a vector or tibble.\nHere’s a test! Verify your function works. Remember, what guarantees that you get the same numbers from a random number generator as we did is that we’re setting a seed to 5\n\n# before knitting set eval = TRUE (and delete this comment)\nset.seed(5)\ncalc_mean_and_sd_from_sample(N = 30, true_mean = 0.5)\n\nNow, this function only does what we need once, while we’ll need it to do it B times. This is an appropriate time for a loop!\n\nWrite the function make_mc_sample. The inputs are described above. The output is a tibble with B rows of means and standard deviations.\n\nHere’s a test.\n\nset.seed(24601)\nmake_mc_sample(N = 30, true_mean = 100, B = 3 )\n\n# A tibble: 3 × 2\n   mean    sd\n  <dbl> <dbl>\n1  99.8 0.920\n2  99.8 0.952\n3 100.  1.04 \n\n\n\n\nFunctions, Assemble\nNow you have all the helper functions that are critical for our simulation. We want to simulate 1000 sets of 30 data points drawn from a normal distribution with true mean 0.5 and then see how often our random sample mean is significantly different from the true mean at a significance level of 0.05. If everything is working as expected, we should see about 5% of the random means to be statistically different.\n\ndo_monte_carlo <- function(N, true_mean, B = 1000, alpha = .05){\n# step 1: Simulate B random samples and calculate sample statistics\nsample_statistics <- make_mc_sample(N, true_mean, B)\n# step 2: Determine z-scores\nz_scores <- get_zscores(sample_statistics$mean, true_mean, sample_statistics$sd, N)\n# step 3: Test whether the z-scores are outside the threshold.\nsignificance <- test_significance(z_scores, alpha)\n# step 5: Measure to what extent our simulations match the theory. (We expect a number close to alpha)\nmean(significance)\n}\n\n\nTest out your function with N equals 30 and true_mean equals 0.5. The resulting number should be close to .05 (alpha).\nTry again with a different alpha and verify that do_monte_carlo returns a number in the ball park of alpha.\n\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "Data/viz/2023-01-03 Tidy Tuesday/TT_wk1.html",
    "href": "Data/viz/2023-01-03 Tidy Tuesday/TT_wk1.html",
    "title": "Tidy Tuesday, 2023 Week 1 🏡",
    "section": "",
    "text": "The theme of this week’s #TidyTuesday is “Bring your own data from 2022!”\nI decided to use data from Tidy Tuesday 2022-07-05. Here is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-01-03 Tidy Tuesday/TT_wk1.html#my-plot",
    "href": "Data/viz/2023-01-03 Tidy Tuesday/TT_wk1.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 1 🏡",
    "section": "My plot",
    "text": "My plot\n\n🏡 SF Housing Prices by Kate Pennington’s Craigslist scrape of Bay Area housing posts\n\nCode here"
  },
  {
    "objectID": "Data/viz/2023-01-10 Tidy Tuesday/TT_wk2.html",
    "href": "Data/viz/2023-01-10 Tidy Tuesday/TT_wk2.html",
    "title": "Tidy Tuesday, 2023 Week 2 🐦",
    "section": "",
    "text": "Data for this week’s TidyTuesday comes from FeederWatch!\nHere is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-01-10 Tidy Tuesday/TT_wk2.html#my-plot",
    "href": "Data/viz/2023-01-10 Tidy Tuesday/TT_wk2.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 2 🐦",
    "section": "My plot",
    "text": "My plot\n\n🐦 FeederWatch\n\nCode here"
  },
  {
    "objectID": "Data/viz/2023-01-17 Tidy Tuesday/TT_wk3.html",
    "href": "Data/viz/2023-01-17 Tidy Tuesday/TT_wk3.html",
    "title": "Tidy Tuesday, 2023 Week 3 🎨",
    "section": "",
    "text": "Data for this week’s TidyTuesday comes from Art History!\nHere is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-01-17 Tidy Tuesday/TT_wk3.html#my-plot",
    "href": "Data/viz/2023-01-17 Tidy Tuesday/TT_wk3.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 3 🎨",
    "section": "My plot",
    "text": "My plot\n\n🎨 Art Histpry\n\nCode here"
  },
  {
    "objectID": "Data/viz/2023-01-24 Tidy Tuesday/TT_wk4.html",
    "href": "Data/viz/2023-01-24 Tidy Tuesday/TT_wk4.html",
    "title": "Tidy Tuesday, 2023 Week 4 🐻",
    "section": "",
    "text": "Data for this week’s TidyTuesday comes from the TV show ALONE!\nHere is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-01-24 Tidy Tuesday/TT_wk4.html#my-plot",
    "href": "Data/viz/2023-01-24 Tidy Tuesday/TT_wk4.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 4 🐻",
    "section": "My plot",
    "text": "My plot\n\n🐻 ALONE\n\nCode here"
  },
  {
    "objectID": "Data/viz/2023-01-31 Tidy Tuesday/TT_wk5.html",
    "href": "Data/viz/2023-01-31 Tidy Tuesday/TT_wk5.html",
    "title": "Tidy Tuesday, 2023 Week 5 🐱",
    "section": "",
    "text": "Week\nDate\nData\nSource\nArticle\n\n\n\n\n5\n2023-01-31\nPet Cats UK\nMovebank for Animal Tracking Data\nCats on the Move\nHere is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-01-31 Tidy Tuesday/TT_wk5.html#my-plot",
    "href": "Data/viz/2023-01-31 Tidy Tuesday/TT_wk5.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 5 🐱",
    "section": "My plot",
    "text": "My plot\n\n🐱 Pet Cats UK\n\nCode here"
  },
  {
    "objectID": "Data/viz/2023-02-07 Tidy Tuesday/TT_wk6.html",
    "href": "Data/viz/2023-02-07 Tidy Tuesday/TT_wk6.html",
    "title": "Tidy Tuesday, 2023 Week 6 📈",
    "section": "",
    "text": "Week\nDate\nData\nSource\nArticle\n\n\n\n\n6\n2023-02-07\nBig Tech Stock Prices\nBig Tech Stock Prices on Kaggle\n5 Charts on Big Tech Stocks’ Collapse\nHere is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-02-07 Tidy Tuesday/TT_wk6.html#my-plot",
    "href": "Data/viz/2023-02-07 Tidy Tuesday/TT_wk6.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 6 📈",
    "section": "My plot",
    "text": "My plot\n\n📈 Big Tech Stock Prices\n\nCode here"
  },
  {
    "objectID": "Data/viz/2023-02-14 Tidy Tuesday/TT_wk7.html",
    "href": "Data/viz/2023-02-14 Tidy Tuesday/TT_wk7.html",
    "title": "Tidy Tuesday, 2023 Week 7 🎬",
    "section": "",
    "text": "Week\nDate\nData\nSource\nArticle\n\n\n\n\n7\n2023-02-14\nHollywood Age Gaps\nHollywood Age Gap Download Data\nHollywood Age Gap\nHere is a little more information about the data: # Hollywood Age Gaps\nThe data this week comes from Hollywood Age Gap via Data Is Plural.\nThe data follows certain rules:\nWe previously provided a dataset about the Bechdel Test. It might be interesting to see whether there is any correlation between these datasets! The Bechdel Test dataset also included additional information about the films that were used in that dataset.\nNote: The age gaps dataset includes “gender” columns, which always contain the values “man” or “woman”. These values appear to indicate how the characters in each film identify. Some of these values do not match how the actor identifies. We apologize if any characters are misgendered in the data!"
  },
  {
    "objectID": "Data/viz/2023-02-14 Tidy Tuesday/TT_wk7.html#my-plot",
    "href": "Data/viz/2023-02-14 Tidy Tuesday/TT_wk7.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 7 🎬",
    "section": "My plot",
    "text": "My plot\n\n🎬 Hollywood Age Gaps\n\nCode here"
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "",
    "text": "Odds ratios have been a long-standing mainstay in health research. Their use can be found sprinkled throughout countless academic journals and research papers. But just because something is popular doesn’t mean it’s the best choice for all contexts. The purpose of this article is to shed light on the limitations of odds ratios, particularly when it comes to interpretation, and advocate for the more intuitive marginal effects and predicted probabilities."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#the-odd-issue-with-odds-ratios",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#the-odd-issue-with-odds-ratios",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "The Odd Issue with Odds Ratios",
    "text": "The Odd Issue with Odds Ratios\nImagine a study on the effect of a new drug on reducing heart attack risk. The findings say there’s an odds ratio of 2. Sounds impressive, doesn’t it? But what does that really mean? It’s tempting to think that patients on the drug are twice as likely to avoid heart attacks. But that’s not quite right. They actually have twice the odds, which isn’t the same as probability, especially when the event is common.\nHere’s the crux of the issue: while odds ratios can be a neat statistical tool, they aren’t always intuitive. They can be easily misinterpreted, leading to overstated or misunderstood results."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#the-math-behind-odds-ratios",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#the-math-behind-odds-ratios",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "The math behind odds ratios",
    "text": "The math behind odds ratios\nTo understand why odds ratios can be confusing, let’s briefly discuss what they represent.\nIn a logistic regression, the fundamental equation is:\n\\[log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ...\\]\nWhere \\(p\\) is the probability of an event occurring.\nThe odds ratio for a given predictor (let’s say \\(X_1\\)) is simply the exponentiated coefficient for that predictor:\nOdds Ratio for \\(X_1 = E^{\\beta_1}\\)\nIt represents the multiplicative change in the odds of the outcome for a one-unit increase in \\(X_1\\) , holding other variables constant. If this sounds confusing, you’re not alone."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#when-odds-ratios-mislead",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#when-odds-ratios-mislead",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "When Odds Ratios Mislead",
    "text": "When Odds Ratios Mislead\nThe challenge with odds ratios arises particularly when the outcome of interest is common. The reason being, odds ratios tend to exaggerate relative risks for common outcomes.\nLet’s say we find an odds ratio of 2 for a drug reducing heart attack risk. Many might (mistakenly) interpret this as “patients on the drug are twice as likely to avoid heart attacks.” In reality, they have twice the odds, which doesn’t translate directly to actual probabilities, especially when the event is common."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#marginal-effects-clarity-over-confusion",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#marginal-effects-clarity-over-confusion",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Marginal Effects: Clarity over Confusion",
    "text": "Marginal Effects: Clarity over Confusion\nMarginal effects shine where odds ratios falter. They convey the change in the probability of an outcome for a one-unit change in the predictor, holding other variables constant. It’s a direct measure that offers a tangible understanding of impact.\nMathematically, for a binary predictor (like drug use: yes or no):\n\\[\\text{Marginal Effect} (dy/dx) = P(Y = 1|X=1) - P(Y=1|X=0)\\]\nFor continuous predictors, the marginal effect represents the derivative of the probability with respect to the predictor, which essentially gives us the rate of change."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#predicted-probabilities-concrete-scenarios-over-abstract-odds",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#predicted-probabilities-concrete-scenarios-over-abstract-odds",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Predicted Probabilities: Concrete Scenarios Over Abstract Odds",
    "text": "Predicted Probabilities: Concrete Scenarios Over Abstract Odds\nPredicted probabilities go a step further, providing the likelihood of an outcome under specific conditions. Instead of leaving things in the realm of odds and abstract increases, we can state tangible scenarios."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#demonstrating-with-real-world-data",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#demonstrating-with-real-world-data",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Demonstrating with “Real-World” Data",
    "text": "Demonstrating with “Real-World” Data\nTo provide concrete evidence of the limitations of odds ratios, let’s explore a simulated dataset that mirrors a health study on heart attacks, age, and drug use for high-risk individuals.\nFirst, we’ll generate a dataset with age as a continuous predictor and drug_use as a binary predictor:\nSimulate data\n\n\nShow the code\n# Setting the seed for reproducibility\nset.seed(1234)\n\nn &lt;- 1000\n\n# Age distribution: Normally distributed between 40 and 70\nage &lt;- rnorm(n, 55, 7.5)\n\n# Drug use: 50% of the participants use the drug\ndrug_use &lt;- rbinom(n, 1, 0.5)\n\n# Log odds of heart attack: Base risk, then increasing slightly with age, \n# and decreasing with drug use (this is where we're setting the odds ratio to roughly 2 for drug use)\nlog_odds_heart_attack &lt;- -2 + 0.04 * age - log(2) * drug_use\n\n# Converting log odds to probability\nprob_heart_attack &lt;- exp(log_odds_heart_attack) / (1 + exp(log_odds_heart_attack))\n\n# Simulating whether a heart attack occurs or not\nheart_attack &lt;- rbinom(n, 1, prob_heart_attack)\n\n# Create a dataframe\ndata_high_risk &lt;- data.frame(age, drug_use, heart_attack)\n\n\n\nnrow(data_high_risk)\n\n[1] 1000\n\n\n\ndata_high_risk\n\n\n\n\n\n\n\nage\ndrug_use\nheart_attack\n\n\n\n\n45.94701\n0\n1\n\n\n57.08072\n0\n1\n\n\n63.13331\n1\n0\n\n\n37.40727\n1\n0\n\n\n58.21844\n0\n1\n\n\n58.79542\n1\n0\n\n\n50.68945\n1\n0\n\n\n50.90026\n0\n0\n\n\n50.76661\n1\n0\n\n\n48.32472\n0\n1\n\n\n\n\n\n\n\n\n\nModeling and Results\nUsing a logistic regression, we’ll model the probability of a heart attack based on age and drug use:\n\nmodel_high_risk &lt;- glm(heart_attack ~ age + drug_use, \n                       data=data_high_risk, family = binomial)\n\nsummary(model_high_risk)\n\n\nCall:\nglm(formula = heart_attack ~ age + drug_use, family = binomial, \n    data = data_high_risk)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.662  -1.049  -0.854   1.188   1.685  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.24031    0.49848  -4.494 6.98e-06 ***\nage          0.04281    0.00896   4.777 1.78e-06 ***\ndrug_use    -0.66254    0.13064  -5.072 3.94e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1373.7  on 999  degrees of freedom\nResidual deviance: 1324.4  on 997  degrees of freedom\nAIC: 1330.4\n\nNumber of Fisher Scoring iterations: 4\n\n\nUpon examining the output, you’ll observe an odds ratio close to 2 for drug use (exponentiate the coeffecient). A common misinterpretation might be: “Patients on the drug have twice the likelihood to avoid heart attacks.” This is a misconception. They have twice the odds, not twice the probability. This distinction becomes especially murky when the event (heart attack) is common.\n\n\nMarginal Effects and Interpretation\nThe marginal effect of the drug can be found by computing the difference in predicted probabilities between drug users and non-users, holding other variables constant. This gives a more direct measure of the drug’s impact on the probability of heart attack.\nUsing our model, you can compute the marginal effect for any given age. For example, for someone aged 55:\n\npredicted_prob_on_drug &lt;- predict(model_high_risk, \n                                  newdata = data.frame(age = 55, drug_use = 1),\n                                  type = \"response\")\n\npredicted_prob_off_drug &lt;- predict(model_high_risk, \n                                   newdata = data.frame(age = 55, drug_use = 0),\n                                   type = \"response\")\n\nmarginal_effect_55 &lt;- predicted_prob_on_drug - predicted_prob_off_drug\n\nmarginal_effect_55\n\n         1 \n-0.1622666 \n\n\nThe result is a clear percentage point decrease in the probability of a heart attack for drug users compared to non-users.\nThis means that, for individuals aged 55, using the drug results in a 16 percentage point decrease in the probability of experiencing a heart attack compared to those not using the drug. This direct interpretation offers a tangible sense of the drug’s impact and is far more intuitive than grappling with odds ratios."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#conclusion-a-call-to-transition",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#conclusion-a-call-to-transition",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Conclusion: A Call to Transition",
    "text": "Conclusion: A Call to Transition\nThe era of odds ratios has provided us with valuable insights, but as the field of health research progresses, so should our statistical tools and reporting practices. Marginal effects and predicted probabilities provide clearer, more intuitive insights, pushing us away from the ambiguous realm of odds and into the tangible world of real impact.\nAfter all, the end goal of our research is to provide clear, actionable insights that can be readily applied to improve health outcomes. Let’s choose clarity over tradition, and marginal effects over odds ratios.\nThank you!\nJacob"
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#average-marginal-effects-a-comprehensive-view",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#average-marginal-effects-a-comprehensive-view",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Average Marginal Effects: A Comprehensive View",
    "text": "Average Marginal Effects: A Comprehensive View\nBeyond just evaluating the marginal effect at specific values, we can compute the average marginal effect (AME) across our sample. AMEs provide a holistic view of the predictor’s impact by averaging its effects over all observations in our dataset. It’s particularly insightful when dealing with continuous predictors like age.\n\nComputing the Average Marginal Effects\nTo compute the AMEs in R, you can leverage the margins package:\n\n#install.packages(\"margins\")\nlibrary(margins)\n\name &lt;- margins(model_high_risk, variables = \"drug_use\")\nsummary(ame)\n\n   factor     AME     SE       z      p   lower   upper\n drug_use -0.1556 0.0291 -5.3429 0.0000 -0.2127 -0.0985\n\n\nThis will give you the average marginal effect of drug use on the probability of a heart attack across all ages in the dataset.\n\n\nInterpreting the AMEs\nSuppose the AME for drug use is -0.156. This means that, on average, across all ages in our sample, using the drug is associated with a 15.6 percentage point decrease in the probability of experiencing a heart attack relative to not using the drug.\nThis single number provides an overarching sense of the drug’s impact, making it immensely useful for policy and clinical decisions. For instance, in public health discussions or in communications with patients, being able to state the average effect of a treatment can be more practical than specifying its impact at particular ages or conditions.\nBy incorporating AMEs into your analysis, you’re providing an extra layer of depth to your findings. This approach shifts the discussion from “what might the effect be?” to “here’s the effect, on average, across our population.” When combined with specific marginal effects and predicted probabilities, this trio offers a robust, comprehensive, and intuitive understanding of your predictors’ impacts."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#visualizing-predicted-probabilities",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#visualizing-predicted-probabilities",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Visualizing Predicted Probabilities",
    "text": "Visualizing Predicted Probabilities\nTo provide a tangible representation, let’s plot the predicted probabilities of heart attacks by age from our model:\n\nlibrary(ggplot2)\n\ndata_high_risk$predicted_prob &lt;- predict(model_high_risk, type = \"response\")\npredictions &lt;- predict(model_high_risk, type = \"response\", se.fit = TRUE)\ndata_high_risk$lower_ci &lt;- predictions$fit - 1.96 * predictions$se.fit\ndata_high_risk$upper_ci &lt;- predictions$fit + 1.96 * predictions$se.fit\n\n\nggplot(data_high_risk, aes(x = age, \n                           y = predicted_prob, \n                           color = as.factor(drug_use))) +\n  geom_ribbon(aes(ymin = lower_ci, \n                  ymax = upper_ci, \n                  fill = as.factor(drug_use)), alpha = 0.3) +\n  geom_line() +\n  labs(title = \"Predicted Probability of Heart Attack by Age with 95% CI\",\n       y = \"Predicted Probability\",\n       x = \"Age\",\n       color = \"Drug Use\",\n       fill = \"Drug Use\") +\n  scale_color_discrete(labels = c(\"No Drug\", \"On Drug\")) +\n  scale_fill_discrete(labels = c(\"No Drug\", \"On Drug\")) +\n  theme_minimal()\n\n\n\n\nThe visual clearly illustrates the beneficial effect of the drug over time. It gives a tangible sense of how the risk of heart attacks evolves with age for both groups and the significant advantage of drug users. This can be especially useful for diagnostic purposes or to demonstrate the results of your model to stakeholders."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "An article about computational science in a scientific publication is not the scholarship itself, it is merely advertising of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures. - J. Buckheit and D. Donoho\nI am committed to open science principles and research reproducibility. You can find the code repositories for my academic papers on my Github.\n\n\n\n\n\n\nRecruiting patients for clinical research is challenging, especially for underrepresented populations, and may be influenced by patients’ relationships with their physicians, care experiences, and engagement with care. This study sought to understand predictors of enrollment in a research study among socioeconomically diverse participants in studies of care models that promote continuity in the doctor–patient relationship.\n\nCopy Citation\n\n\nPDF\n\n\nCode Repo\n\n\n\n\n\n\n\n\n\nCopy Citation\n\n\nPDF\n\n\nCode Repo\n\n\n\n\n\nCopy Citation\n\n\nPDF\n\n\nCode Repo"
  },
  {
    "objectID": "teaching.html#introduction-to-r-programming",
    "href": "teaching.html#introduction-to-r-programming",
    "title": "Teaching",
    "section": "Introduction to R Programming",
    "text": "Introduction to R Programming\nThis page contains the content for the “Introduction to Data Analysis in R” mini-course. The goal is to introduce R programming concepts with a focus on preparing and analyzing data, and conducting statistical simulations. We will cover how to read data into R, manipulate data with a suite of tools from the tidyverse package dplyr. We will also discuss some basic programming concepts including data types and operators. This mini-course is aimed at people with no programming background that are looking for a gentle and practical introduction to data analysis in R.\n\nHarvard T.H. Chan School of Public Health\n\nDecision Science for Public Health. Teaching Fellow (2023)\n\n\n\nHarvard Kennedy School\n\nMath Camp. Instructor (2023)\nBig Data and Machine Learning. Course Assistant (2023)\nGame Theory. Teaching Fellow (2023)\nData and Programming for Policymakers. Course Assistant (2023)\nResources, Incentives, and Choices I: Markets and Market Failures. Teaching Fellow (2022, 2023)\n\n\n\nThe University of Chicago, Center for Translational Science\n\nData, Quantitative Methods, and Applications in HSR. Teaching Assistant and Instructor (2021,2022,2023)\nIntroduction to Health Services Research. Teaching Assistant (2021)\n\n\n\nThe University of Chicago, Department of Computer Science\n\nMachine Learning for Public Policy. Teaching Assistant (2022)\nMathematics for Data Analysis and Computer Science. Teaching Assistant (2022)\n\n\n\nThe University of Chicago Booth School of Business\n\nIntroductory Finance. Teaching Assistant (2020-2022)\nData Analysis in R and Python. Teaching Assistant (2021)\n\n\n\nThe University of Chicago Harris School of Public Policy\n\nCoding Lab for Public Policy. Instructor and Curriculum Developer (2021)\n\n\n\nTeach For America, Achievement First Amistad Academy Middle School\n\n7th/8th Grade Mathematics. Mathematics Teacher (2018-2020)"
  },
  {
    "objectID": "posts/2023-09-23-docker/2023-09-23-docker.html",
    "href": "posts/2023-09-23-docker/2023-09-23-docker.html",
    "title": "Reproducibility in Health Research - The Power of Docker for Microsimulations",
    "section": "",
    "text": "In the domain of health research, particularly within the realm of medical decision-making, our ability to make accurate and impactful decisions is profoundly influenced by the robustness and reliability of our research. However, robustness is not just about good statistical practices or rigorous study design; it extends to the reproducibility of our research. Without reproducibility, even the most meticulously designed studies can lose their value and, in some cases, credibility.\n\nMedical decision-making often hinges on intricate computational models such as microsimulations. These models, while powerful, are composed of multifaceted components including various dependencies, data sources, and algorithms. A slight change or inconsistency in any of these components can lead to vastly different outcomes.\nImagine, for instance, a microsimulation that forecasts the spread of an infectious disease. Policy-makers could utilize this simulation to shape national health strategies. Now imagine if a slight variation in the software environment—say a minor version change in a data processing library—alters the projected number of cases. The implications could be enormous, ranging from resource misallocation to ineffective interventions.\nThis underlines a fundamental truth: For research to be trusted and actionable, it must be reproducible.\n\nSo, how can we ensure that our complex microsimulations run consistently, producing the same outcomes regardless of where and when they are executed? Enter Docker, a game-changer in the world of reproducible research.\n\nThink of a container as a standalone package, a box if you will, that encloses your entire software environment—your code, its dependencies, system libraries, system settings, and so forth. This container ensures that your software runs identically regardless of where the container is executed, be it your laptop, a colleague’s machine, or a cloud server on the other side of the world.\nWhy is this groundbreaking? Because it eradicates the notorious “but it works on my machine” problem. Every researcher knows the headache of trying to replicate an environment across multiple machines, battling inconsistent library versions, missing dependencies, and unexpected system behaviors. Docker containers eliminate this inconsistency.\n\nWith Docker, every element of your software environment is codified and encapsulated. You don’t just share your code; you share its entire ecosystem. Here’s what it means for reproducibility:\n\nIsolation: Docker ensures that your software’s environment is isolated from the host system, meaning external factors on a machine won’t affect your software’s execution.\nVersion Control for Environments: Just as Git allows you to version control your code, Docker lets you version control your software environment. This is crucial when using libraries and tools that are continuously updated.\nEase of Sharing: Docker containers can be easily shared with peers, ensuring that they don’t just get your code but the exact environment in which your code was designed to run.\n\nAs health researchers, our ultimate aim is to produce knowledge that can improve health outcomes. But this knowledge must be built on trust. Docker, by championing reproducibility, fosters this trust, ensuring our research remains both impactful and credible. In the next sections, we’ll delve deeper into how to leverage Docker for your microsimulations, but for now, it’s crucial to recognize its potential in safeguarding the integrity of our research."
  },
  {
    "objectID": "research.html#published-papers",
    "href": "research.html#published-papers",
    "title": "Research",
    "section": "",
    "text": "Recruiting patients for clinical research is challenging, especially for underrepresented populations, and may be influenced by patients’ relationships with their physicians, care experiences, and engagement with care. This study sought to understand predictors of enrollment in a research study among socioeconomically diverse participants in studies of care models that promote continuity in the doctor–patient relationship.\n\nCopy Citation\n\n\nPDF\n\n\nCode Repo"
  },
  {
    "objectID": "research.html#under-review",
    "href": "research.html#under-review",
    "title": "Research",
    "section": "",
    "text": "Copy Citation\n\n\nPDF\n\n\nCode Repo\n\n\n\n\n\nCopy Citation\n\n\nPDF\n\n\nCode Repo"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Jacob Jameson",
    "section": "",
    "text": "I am a PhD student at Harvard University studying Health Policy and Decision Sciences. I am currently advised by Dr. Soroush Saghafian and work in The Public Impact Analytics Science Lab (PIAS-Lab) at Harvard. I also hold a research position in Boston Children’s Hospital General Pediatric Unit.\nI use tools from operations research, engineering, computer science, and economics to inform decision being made under conditions of uncertainty."
  },
  {
    "objectID": "Intro R.html#about-this-course",
    "href": "Intro R.html#about-this-course",
    "title": "Introduction to Programming in R",
    "section": "",
    "text": "This page contains the content for the “Introduction to Data Analysis in R” mini-course. The goal is to introduce R programming concepts with a focus on preparing and analyzing data, and conducting statistical simulations. We will cover how to read data into R, manipulate data with a suite of tools from the tidyverse package dplyr. We will also discuss some basic programming concepts including data types and operators.\nThis mini-course is aimed at people with no programming background that are looking for a gentle and practical introduction to data analysis in R.\nEach Module involves:\n\na pre-recorded lecture with guided practice exercises\na corresponding lab to practice the skills taught in the video and stretch your thinking\n\nThe labs are an essential part of the learning process as they walkthrough the use of many key functions and topics that are not explicitly covered in the videos.\n\nAdditional Resources\n\n\n\ntidyverse cheetsheets start with dplyr and ggplot\n\n\nR for Data Science: free online book with clear explanations of many tidyverse functions, the book to read on data analysis with R"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#social-network-analysis",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#social-network-analysis",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "",
    "text": "“There is certainly no unanimity on exactly what centrality is or on its conceptual foundations, and there is little agreement on the proper procedure for its measurement.” - Linton Freeman (1977)\nSocial network analysis can be used to measure the importance of a person as a function of the social structure of a community or organization. This post uses visualization as a tool to explain how different measures of centrality may be used to analyze different questions in a network analysis. In these examples we will be specifically looking at directed graphs to compare the following centrality measures and their use-cases:\n\nDegree Centrality\nBetweenness Centrality\nEigenvector Centrality\nKatz Centrality\nHITS Hubs and Authorities\n\nAn example of a directed graph would be one in which people nominate their top 2 friends. In this graph, nodes (people) would connect to others nodes through directed edges (nominations). It is possible for Jacob to nominate Jenna without Jenna nominating him back. You can imagine why centrality in a friendship network might take into account the direction of these nominations. If I list 100 people as my friends and none of them list me back, do we think I am a popular person?"
  }
]