[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jacob Jameson",
    "section": "",
    "text": "|\n\n\nI am a PhD student at Harvard University studying Health Policy and Decision Sciences. I am currently advised by Dr. Soroush Saghafian and work in The Public Impact Analytics Science Lab (PIAS-Lab) at Harvard. I also hold a research position in Boston Children’s Hospital General Pediatric Unit and I am an Educational Innovation Scholar at the Harvard Center for Health Decision Science and Global Health Education and Learning Incubator.\nI use tools from operations research, engineering, computer science, and economics to inform decisions being made under conditions of uncertainty, with a focus on hospital operations.\n\n\n\nAside from research, I am passionate about teaching. I have extensive experience teaching students in a variety of settings, including as a teaching fellow at Harvard, instructor at UChicago Medicine, and as a Teach For America corps member in New Haven Public Schools."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "My teaching experiences are quite diverse. The summer after completing my undergraduate degree, I began teaching in a highschool Algebra II credit recovery program in The School District of Philadelphia. I then transitioned to a full-time middle school mathematics position in New Haven, CT, where I spent the next two years falling in love with the profession. During my second year, I held the Grade-level Chair position for the 8th grade and consulted for the Achievement First Charter Network on middle school mathematics curriculum. I am also passionate about computer science education, and have experience running Girls Who Code clubs and participating in the Code.org Middle and High School Computer Science Professional Learning Program for educators in computer science. Since teaching at the middle school level, I have been fortunate to hold a variety of teaching positions at The University of Chicago and Harvard University. I hope to pursue a teaching career after completing my PhD.\n\nHarvard T.H. Chan School of Public Health\n\nDecision Science for Public Health. Teaching Fellow (2023, 2024)\n\n\n\nHarvard Kennedy School\n\nMath Camp. Instructor (2023)\nBig Data and Machine Learning. Teaching Fellow (2024)\nGame Theory. Teaching Fellow (2023, 2024)\nData and Programming for Policymakers. Course Assistant (2023)\nResources, Incentives, and Choices I: Markets and Market Failures. Teaching Fellow (2022, 2023)\n\n\n\nThe University of Chicago, Center for Translational Science\n\nData, Quantitative Methods, and Applications in HSR. Teaching Assistant and Instructor (2021,2022,2023)\nIntroduction to Health Services Research. Teaching Assistant (2021)\n\n\n\nThe University of Chicago, Department of Computer Science\n\nMachine Learning for Public Policy. Teaching Assistant (2022)\nMathematics for Data Analysis and Computer Science. Teaching Assistant (2022)\n\n\n\nThe University of Chicago Booth School of Business\n\nIntroductory Finance. Teaching Assistant (2020-2022)\nData Analysis in R and Python. Teaching Assistant (2021)\n\n\n\nThe University of Chicago Harris School of Public Policy\n\nCoding Lab for Public Policy. Instructor and Curriculum Developer (2021)\n\n\n\nTeach For America, Achievement First Amistad Academy Middle School\n\n7th/8th Grade Mathematics. Mathematics Teacher (2018-2020)"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "",
    "text": "“There is certainly no unanimity on exactly what centrality is or on its conceptual foundations, and there is little agreement on the proper procedure for its measurement.” - Linton Freeman (1977)\nSocial network analysis can be used to measure the importance of a person as a function of the social structure of a community or organization. This post uses visualization as a tool to explain how different measures of centrality may be used to analyze different questions in a network analysis. In these examples we will be specifically looking at directed graphs to compare the following centrality measures and their use-cases:\n\nDegree Centrality\nBetweenness Centrality\nEigenvector Centrality\nKatz Centrality\nHITS Hubs and Authorities\n\nAn example of a directed graph would be one in which people nominate their top 2 friends. In this graph, nodes (people) would connect to others nodes through directed edges (nominations). It is possible for Jacob to nominate Jenna without Jenna nominating him back. You can imagine why centrality in a friendship network might take into account the direction of these nominations. If I list 100 people as my friends and none of them list me back, do we think I am a popular person?"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#simulate-our-data",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#simulate-our-data",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Simulate Our Data",
    "text": "Simulate Our Data\nFor our simulated data we are going to be looking at a classroom that contains 14 male and 14 female students. Suppose that each student was asked to name their top 2 male and top 2 female friends in the class. We are interested in analyzing a slew of different research questions where the centrality of student in the class may be of importance.\nLet’s create the data:\n\nlibrary(tidyverse)\n\nWe begin with a 4 vectors: all males in the classroom, all females in the classroom, a probability distribution for selecting friends of the same sex, a probability distribution for selecting friends of the opposite sex.\n\nmales &lt;- c('Jacob', 'Louis', 'Chris', 'Wyatt', 'Nolan', 'Robert', \n           'Zach', 'John','Bob', 'David', 'Avery', 'Ronald', \n           'Dallas', 'Dylan')\n\nfemales &lt;- c('Bohan', 'Jenna', 'Katarina', 'Hassina', 'Towo', \n             'Becca', 'Meredith', 'Gracie', 'Kayla', 'Marlene', \n             'Jade', 'Allyssa', 'Reigne', 'Wendy')\n\nprobs.diff.sex = c(0.15,0.15,0.05,0.05,0.1,0.1,0.05,0.1,0.01,0.09,0.05,0.05,0.025,0.025)\nprobs.same.sex = c(0.15,0.15,0.05,0.05,0.1,0.1,0.05,0.1,0.01,0.09,0.05,0.05,0.05)\n\nThe function below, simulate.top.friends, will produce a dataframe that will contain each student’s picks for their top 2 male and top 2 female friends in the classroom.\n\nset.seed(1997)\n\nsimulate.top.friends &lt;- function(males, females, probs.diff.sex, probs.same.sex) {\n\n  dat &lt;- setNames(data.frame(matrix(ncol = 6, nrow = 0)), \n                  c(\"Ego\", \"Ego Sex\", \"MF1\", \"MF2\", \"FF1\", \"FF2\"))\n\n  for (ego in males) {\n    temp.males &lt;- males[! males %in% ego]\n    \n    male.friends.i &lt;- sample.int(13, 2, replace = FALSE, prob = probs.same.sex)\n    female.friends.i &lt;- sample.int(14, 2, replace = FALSE, prob = probs.diff.sex)\n    \n    male.friend.1 &lt;- temp.males[male.friends.i[1]]\n    male.friend.2 &lt;- temp.males[male.friends.i[2]]\n    \n    female.friend.1 &lt;- females[female.friends.i[1]]\n    female.friend.2 &lt;- females[female.friends.i[2]]\n    \n    dat[nrow(dat) + 1,] = c(ego, 'Male', male.friend.1, male.friend.2, \n                           female.friend.1, female.friend.2)\n    \n  }\n    for (ego in females) {\n    temp.females &lt;- females[! females %in% ego]\n    \n    male.friends.i &lt;- sample.int(14, 2, replace = FALSE, prob = probs.diff.sex)\n    female.friends.i &lt;- sample.int(13, 2, replace = FALSE, prob = probs.same.sex)\n    \n    male.friend.1 &lt;- males[male.friends.i[1]]\n    male.friend.2 &lt;- males[male.friends.i[2]]\n    \n    female.friend.1 &lt;- temp.females[female.friends.i[1]]\n    female.friend.2 &lt;- temp.females[female.friends.i[2]]\n    \n    dat[nrow(dat) + 1,] = c(ego, 'Female', male.friend.1, male.friend.2, \n                           female.friend.1, female.friend.2)\n    \n  }\n  return(dat)\n}\n\nLet’s take a look at our friendship data that we will be working with!\n\nsimulate.top.friends(males,females, probs.diff.sex, probs.same.sex)\n\n\n\n\n\n\n\nEgo\nEgo Sex\nMF1\nMF2\nFF1\nFF2\n\n\n\n\nJacob\nMale\nChris\nAvery\nKatarina\nMeredith\n\n\nLouis\nMale\nJacob\nAvery\nTowo\nHassina\n\n\nChris\nMale\nJacob\nRobert\nGracie\nBecca\n\n\nWyatt\nMale\nNolan\nJacob\nMarlene\nJenna\n\n\nNolan\nMale\nJacob\nBob\nBohan\nTowo\n\n\nRobert\nMale\nNolan\nLouis\nJenna\nBohan\n\n\nZach\nMale\nJacob\nAvery\nMeredith\nTowo\n\n\nJohn\nMale\nBob\nLouis\nBohan\nWendy\n\n\nBob\nMale\nAvery\nJacob\nJenna\nWendy\n\n\nDavid\nMale\nWyatt\nLouis\nBecca\nJenna\n\n\nAvery\nMale\nLouis\nJacob\nJenna\nBecca\n\n\nRonald\nMale\nJacob\nRobert\nBecca\nWendy\n\n\nDallas\nMale\nRobert\nJacob\nBohan\nJenna\n\n\nDylan\nMale\nJacob\nDavid\nTowo\nMarlene\n\n\nBohan\nFemale\nRobert\nNolan\nAllyssa\nKayla\n\n\nJenna\nFemale\nDavid\nBob\nKatarina\nMeredith\n\n\nKatarina\nFemale\nDavid\nNolan\nGracie\nReigne\n\n\nHassina\nFemale\nRobert\nDavid\nBecca\nJade\n\n\nTowo\nFemale\nJohn\nNolan\nJade\nJenna\n\n\nBecca\nFemale\nNolan\nDallas\nKayla\nTowo\n\n\nMeredith\nFemale\nLouis\nJacob\nTowo\nBohan\n\n\nGracie\nFemale\nJacob\nRonald\nBohan\nHassina\n\n\nKayla\nFemale\nRonald\nLouis\nBohan\nGracie\n\n\nMarlene\nFemale\nJohn\nRobert\nHassina\nTowo\n\n\nJade\nFemale\nRonald\nLouis\nHassina\nBohan\n\n\nAllyssa\nFemale\nAvery\nZach\nGracie\nBohan\n\n\nReigne\nFemale\nJohn\nZach\nBohan\nHassina\n\n\nWendy\nFemale\nNolan\nJacob\nTowo\nBohan"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#creating-a-graphing-object",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#creating-a-graphing-object",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Creating a Graphing Object",
    "text": "Creating a Graphing Object\nThere are many available centrality measures that have been developed for network analysis. At this time, there are no packages that are so comprehensive that it includes all of the measures. I will, therefore, limit this discussion to a subset of the measures that are included in igraph.\nigraph has a really great function that allows us to turn a dataframe into an igraph object. However, the function requires our data to be in “long” form so we will need to do some reshaping. Let’s restructure our data such that each row represents one directed friend nomination. We are going to call this “Source-Target Form”.\n\nlibrary(reshape) \n\nfriendships &lt;- melt(simulate.top.friends(males,females,probs.diff.sex, probs.same.sex), \n                    id=c(\"Ego\", \"Ego Sex\")) %&gt;%\n  select(source=Ego, source_sex =`Ego Sex`, target=value) %&gt;% \n  arrange(source)\n\nLet’s look at the first 10 observations so we can understand the format needed to turn this data into an igraph object.\n\nhead(friendships, 10)\n\n\n\n\n\n\n\nsource\nsource_sex\ntarget\n\n\n\n\nAllyssa\nFemale\nDavid\n\n\nAllyssa\nFemale\nNolan\n\n\nAllyssa\nFemale\nJenna\n\n\nAllyssa\nFemale\nTowo\n\n\nAvery\nMale\nJohn\n\n\nAvery\nMale\nChris\n\n\nAvery\nMale\nAllyssa\n\n\nAvery\nMale\nMeredith\n\n\nBecca\nFemale\nChris\n\n\nBecca\nFemale\nWyatt\n\n\n\n\n\n\n\n\nWe will use the graph_from_data_frame function to create the igraph object.\n\nlibrary(igraph)\nnetwork &lt;- graph_from_data_frame(friendships[,c('source','target','source_sex')],\n                                 directed = TRUE)\n\nnetwork\n\nIGRAPH 006a09e DN-- 28 112 -- \n+ attr: name (v/c), source_sex (e/c)\n+ edges from 006a09e (vertex names):\n [1] Allyssa-&gt;David    Allyssa-&gt;Nolan    Allyssa-&gt;Jenna    Allyssa-&gt;Towo    \n [5] Avery  -&gt;John     Avery  -&gt;Chris    Avery  -&gt;Allyssa  Avery  -&gt;Meredith\n [9] Becca  -&gt;Chris    Becca  -&gt;Wyatt    Becca  -&gt;Jade     Becca  -&gt;Jenna   \n[13] Bob    -&gt;Dylan    Bob    -&gt;David    Bob    -&gt;Marlene  Bob    -&gt;Wendy   \n[17] Bohan  -&gt;Nolan    Bohan  -&gt;John     Bohan  -&gt;Katarina Bohan  -&gt;Reigne  \n[21] Chris  -&gt;Jacob    Chris  -&gt;Avery    Chris  -&gt;Bohan    Chris  -&gt;Allyssa \n[25] Dallas -&gt;Louis    Dallas -&gt;Robert   Dallas -&gt;Jenna    Dallas -&gt;Towo    \n[29] David  -&gt;Zach     David  -&gt;Dallas   David  -&gt;Bohan    David  -&gt;Jenna   \n+ ... omitted several edges\n\n\nLet’s better understand the information contained in an igraph object:\n\nIGRAPH simply annotates network as an igraph object\nWhatever random six digit alphanumeric string follows IGRAPH is simply how igraph identifies the graph for itself, it’s not important for our purposes.\nD would tell us that it is directed graph\nN indicates that network is a named graph, in that the vertices have a name attribute\n– refers to attributes not applicable to network, but we will see them in the future:\n28 refers to the number of vertices in network\n112 refers to the number of edges in network\nattr: is a list of attributes within the graph.\n(v/c), which will appear following name, tells us that it is a vertex attribute of a character data type.\n(e/c) or (e/n) referring to edge attributes that are of character or numeric data types\n\nedges from arbitrary igraph name (vertex names): lists a sample of network’s edges using the names of the vertices which they connect.\n\n\nNow let’s create a rough plot to look at our network!\n\nlay &lt;- layout_with_kk(network)\n\npar(bg=\"grey98\")\nplot(network, layout = lay, edge.color=\"grey80\",\n     vertex.color=\"lightblue\", vertex.label.color = \"black\")\n\n\n\n\n\n\n\n\nFor the rest of this post, we are going to talk about a few different measures of centrality, what they capture mathematically and intuitively, and we will look at plots where the size of the node corresponds to the relative centrality score."
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#measures-of-centrality",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#measures-of-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Measures of Centrality",
    "text": "Measures of Centrality\n\nDegree Centrality\nFor directed graphs, in-degree, or number of incoming points, is one way we can determine the importance factor for nodes. The Degree of a node is the number of edges that it has. The basic intuition is that, nodes with more connections are more influential and important in a network. In other words, the people with more friend nominations in our simulated social network are the ones with greater importance according to this metric.\n\nDegree.Directed &lt;- degree(network)\nIndegree &lt;- degree(network, mode=\"in\")\nOutdegree &lt;- degree(network, mode=\"out\")\n\nCompareDegree &lt;- cbind(Degree.Directed, Indegree, Outdegree)\n\n\nhead(CompareDegree, 10)\n\n\n\n\n\n\n\n\nDegree.Directed\nIndegree\nOutdegree\n\n\n\n\nAllyssa\n7\n3\n4\n\n\nAvery\n7\n3\n4\n\n\nBecca\n7\n3\n4\n\n\nBob\n7\n3\n4\n\n\nBohan\n12\n8\n4\n\n\nChris\n7\n3\n4\n\n\nDallas\n7\n3\n4\n\n\nDavid\n11\n7\n4\n\n\nDylan\n7\n3\n4\n\n\nGracie\n5\n1\n4\n\n\n\n\n\n\n\n\nThis is a very reasonable way to measure importance within a network. If we are trying to determine who in our classroom is the most popular, we might define that as the greatest number of friendship nominations.\n\nlay &lt;- layout_with_kk(network)\n\npar(bg=\"grey98\")\nplot(network, layout = lay, edge.color=\"grey80\",\n     vertex.size=degree(network, mode=\"in\")*2,  # Rescaled by multiplying by 2\n     main=\"In-Degree\", vertex.label.dist=1.5,\n     vertex.color=\"lightblue\", vertex.label.color = \"black\")"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#betweenness-centrality",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#betweenness-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Betweenness Centrality",
    "text": "Betweenness Centrality\nBetweenness Centrality is another centrality that is based on shortest path between nodes. It is determined as number of the shortest path passing by the given node. For starting node \\(s\\), destination node \\(t\\) and the input node \\(i\\) that holds \\(s \\ne t \\ne i\\), let \\(n_{st}^i\\) be 1 if node \\(i\\) lies on the shortest path between \\(s\\) and \\(t\\); and \\(0\\) if not. So the betweenness centrality is defined as:\n\\[x_i = \\sum_{st} n_{st}^i\\] However, there can be more than one shortest path between \\(s\\) and \\(t\\) and that will count for centrality measure more than once. Thus, we need to divide the contribution to \\(g\\_{st}\\), total number of shortest paths between \\(s\\) and \\(t\\).\n\\[x_i = \\sum_{st} \\frac{n_{st}^i}{g_{st}}\\]\nEssentially the size of the node here represents the frequency with which that node lies on the shortest path between other nodes.\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=betweenness(network)*0.25,  # Rescaled by multiplying by 0.25\n     main=\"Betweenness Centrality\", vertex.color=\"lightblue\")"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#eigenvector-centrality",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#eigenvector-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Eigenvector Centrality",
    "text": "Eigenvector Centrality\nEigenvector centrality is a basic extension of degree centrality, which defines centrality of a node as proportional to its neighbors’ importance. When we sum up all connections of a node, not all neighbors are equally important. This is a very interesting way to measure popularity in our classroom where we say the popularity of your friends matters more than the number of friends. Let’s consider two nodes in a friend network with same degree, the one who is connected to more central nodes should be more central.\nFirst, we define an initial guess for the centrality of nodes in a graph as \\(x_i=1\\). Now we are going to iterate for the new centrality value \\(x_i'\\) for node \\(i\\) as following:\n\\[x_i' = \\sum_{j} A_{ij}x_j\\]\nHere \\(A_{ij}\\) is an element of the adjacency matrix, where it gives 1 or 0 for whether an edge exists between nodes \\(i\\) and \\(j\\). it can also be written in matrix notation as \\(\\mathbf{x'} = \\mathbf{Ax}\\).\nWe iterate over t steps to find the vector \\(\\mathbf{x}(t)\\) as:\n\\[\\mathbf{x}(t) = \\mathbf{A^t x}(0)\\]\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=evcent(network)$vector*15,   # Rescaled by multiplying by 15\n     main=\"Eigenvector Centrality\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\nThe plot shows, the students which have the same number of friend nominations are not necessarily in the same size. The one that is connected to more central, or “popular” nodes are larger in this visualization.\nHowever, as we can see from the definition, this can be a problematic measure for directed graphs. Let’s say that a student who received no friend nominations themselves nominates another student as a friend. Because that person has 0 friend nominations themselves, they would not contribute any importance to the person they nominated. In other words, eigenvector centrality would not take zero in-degree nodes into account in directed graphs.\nHowever there is a solution to this!"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#katz-centrality",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#katz-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Katz Centrality",
    "text": "Katz Centrality\nKatz centrality introduces two positive constants \\(\\alpha\\) and \\(\\beta\\) to tackle the problem of eigenvector centrality with zero in-degree nodes:\n\\[x_i = \\alpha \\sum_{j} A_{ij} x_j + \\beta\\],\nagain \\(A_{ij}\\) is an element of the adjacency matrix, and it can also be written in matrix notation as \\(\\mathbf{x} = \\alpha \\mathbf{Ax} + \\beta \\mathbf{1}\\). This \\(\\beta\\) constant gives a free centrality contribution for all nodes even though they don’t get any contribution from other nodes. The existence of a node alone would provide it some importance. \\(\\alpha\\) constant determines the balances between the contribution from other nodes and the free constant.\nUnfortunately, igraph does not have a function to compute Katz centrality, so we will need to do it the old fashioned way.\n\nkatz.centrality = function(g, alpha, beta, t) {\n  n = vcount(g);\n  A = get.adjacency(g);\n  x0 = rep(0, n);\n  x1 = rep(1/n, n);\n  eps = 1/10^t;\n  iter = 0;\n  while (sum(abs(x0 - x1)) &gt; eps) {\n    x0 = x1;\n    x1 = as.vector(alpha * x1 %*% A) + beta;\n    iter = iter + 1;\n  } \n  return(list(aid = x0, vector = x1, iter = iter))\n}\n\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=katz.centrality(network, 0.1, 1, 0.01)$vector*5,   # Rescaled by multiplying by 15\n     main=\"Katz Centrality\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\nAlthough this method is introduced as a solution for directed graphs, it can be useful for some applications of undirected graphs as well."
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#hits-hubs-and-authorities",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#hits-hubs-and-authorities",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "HITS Hubs and Authorities",
    "text": "HITS Hubs and Authorities\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=hub.score(network)$vector*15,   # Rescaled by multiplying by 15\n     main=\"HITS Hubs\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5 , vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=authority.score(network)$vector*15,   # Rescaled by multiplying by 15\n     main=\"HITS Authorities\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\nUp until this point, we have discussed the measures that captures high node centrality, however, there can be nodes in the network which are important for the network, but they are not central. In order to find out such nodes, the HITS algorithm introduces two types of central nodes: Hubs and Authorities. For Hubs, we might consider a node important if it links to many highly nominated nodes (i.e. the person nominates many popular people as their friends). For Authorities, we might consider a node to be of importance if many highly nominated nodes link to it (i.e. nominated by many popular people).\nAuthority Centrality is defined as the sum of the hub centralities which point to the node (i):\n\\[x_i = \\alpha \\sum_{j} A_{ij} y_j,\\]\nwhere \\(\\alpha\\) is constant. Likewise, Hub Centrality is the sum of the authorities which are pointed by the node \\(i\\):\n\\[y_i = \\beta \\sum_{j} A_{ji} x_j,\\]\nwith constant \\(\\beta\\). Here notice that the element of the adjacency matrix are swapped for Hub Centrality because we are concerned with outgoing edges for hubs. So in matrix notation:\n\\[\\mathbf{x} = \\alpha \\mathbf{Ay}, \\quad\\]\n\\[\\mathbf{y} = \\beta \\mathbf{A^Tx}.\\] As it can be seen from the drawing, HITS Algorithm also tackles the problem with zero in-degree nodes of Eigenvector Centrality. These zero in-degree nodes become central hubs and contribute to other nodes. Yet we can still use a free centrality contribution constant like in Katz Centrality or other variants.\nAlthough these measures are generally used in a citation network or the internet, this can be pretty interesting in the context of our classroom frienship network. Maybe we are interested in knowing who are the non-popular students that nominate many popular students as friends, but receive no reciprocity in terms of nominations."
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#other-packages-that-i-like-for-visualization",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#other-packages-that-i-like-for-visualization",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Other Packages that I Like for Visualization",
    "text": "Other Packages that I Like for Visualization\nI want to conclude this post with some sample code to produce a nice network plot using the ggnetwork package. igraph is only one package that exists for network visualization and centrality calculations, I encourage you to check out additional packages which may be stronger than igraph for some purposes.\n\nlibrary(GGally)\nlibrary(network)\nlibrary(ggnetwork)\n\nnet &lt;- list(nodes=friendships[c('source', 'target', 'source_sex')], \n            edges=friendships[c('source', 'target', 'source_sex')])\n\n# create node attribute data\nnet.cet &lt;- as.character(net$nodes$source_sex)\nnames(net.cet) = net$nodes$source\nedges &lt;- net$edges\n\n# create network\nnet.net &lt;- edges[, c(\"source\", \"target\") ]\nnet.net &lt;- network::network(net.net, directed = TRUE)\n\n# create sourc sex node attribute\nnet.net %v% \"source_sex\" &lt;- net.cet[ network.vertex.names(net.net) ]\n\n\nset.seed(1)\nggnet2(net.net, color = \"source_sex\",\n       palette = c(\"Female\" = \"purple\", \"Male\" = \"maroon\"), size = 'indegree',\n       arrow.size = 3, arrow.gap = 0.04, alpha = 1,  label = TRUE, vjust = 2.5, label.size = 3.5,\n       edge.alpha = 0.5, mode = \"kamadakawai\",edge.color = 'grey50',\n       color.legend = \"Student Sex\") + theme_bw() + theme_blank()  + \n  theme(legend.position = \"bottom\", text = element_text(size = 15),\n        plot.caption = element_text(hjust = 0)) + guides(size=F) + \n  labs(title='Social Network Mapping of Friendship Nominations for Simulated Data', \n       caption = str_wrap(\"The size of the dots represent the number of \n                          friendship nominations by others. The maroon dots represent males \n                          while the purple dots represent females. Directed edges are used \n                          to indicate the direction of friendship nominations. \n                          Edges that are bi-directional indicated friendship reciprocity. \n                          \\n\\n This particular network represents the frienship nominations \n                          of 28 students from our simulated data.\", 128))\n\n\n\n\n\n\n\n\nThank you!\nJacob"
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Coding",
    "section": "",
    "text": "An article about computational science in a scientific publication is not the scholarship itself, it is merely advertising of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures. - J. Buckheit and D. Donoho\nI am committed to open science principles and research reproducibility. You can find the code repositories for my academic papers on my Github."
  },
  {
    "objectID": "code.html#introduction-to-r-programming",
    "href": "code.html#introduction-to-r-programming",
    "title": "Coding",
    "section": "Introduction to R Programming",
    "text": "Introduction to R Programming\nThis page contains the content for the “Introduction to Data Analysis in R” mini-course. The goal is to introduce R programming concepts with a focus on preparing and analyzing data, and conducting statistical simulations. We will cover how to read data into R, manipulate data with a suite of tools from the tidyverse package dplyr. We will also discuss some basic programming concepts including data types and operators. This mini-course is aimed at people with no programming background that are looking for a gentle and practical introduction to data analysis in R."
  },
  {
    "objectID": "horror.html",
    "href": "horror.html",
    "title": "Horror Plots",
    "section": "",
    "text": "Most Haunted Places in America\n\n\n\n\n\n\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n\n\nHorrorPlots Slasher Kill Count\n\n\n\n\n\n\n\n\n\n\n\n\njacob Jameson\n\n\n\n\n\n\n\n\nTidyTuesday Horror Movies\n\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\njacob Jameson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data/horrorplots/ridgeplot/horror_movies.html",
    "href": "Data/horrorplots/ridgeplot/horror_movies.html",
    "title": "#TidyTuesday Horror Movies",
    "section": "",
    "text": "horror_movies <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-01/horror_movies.csv')\n\n\nhorror_movies <- horror_movies %>%\n  mutate(year = year(release_date)) \n\nhorror_comedies <- filter(horror_movies, grepl(\"Comedy\",genre_names)) \nhorror_comedies <- filter(horror_comedies, year > 2011)\n\n\n# Plot\nggplot(horror_comedies, aes(x = `vote_average`, y = factor(year), fill = ..x..)) +\n  geom_density_ridges_gradient(scale = 6, rel_min_height = 0.01) +\n  scale_fill_gradient(low = \"#eb0e34\",high = \"#910019\") +\n  theme_void() + \n    theme(\n      legend.position=\"none\",\n      text = element_text(family = \"creepster\", color = \"#eb0e34\"),\n      axis.line = element_blank(),\n      axis.text.x = element_text(family = \"creepster\", color = \"#eb0e34\", size = 20),\n      axis.text.y = element_text(family = \"creepster\", color = \"#eb0e34\", size = 15),\n      panel.grid.major = element_blank(),\n      axis.title.x = element_text(family = \"creepster\", color = \"#eb0e34\", size = 10),\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"black\", color = NA), \n      panel.background = element_rect(fill = \"black\", color = NA), \n      legend.background = element_blank()) +\n  xlab(\"\\nAverage Voter Score\") +\n  annotation_custom(xmin=-Inf, ymin=-Inf, xmax=Inf, ymax=Inf, rasterGrob(w)) +\n   theme(plot.title = element_text(size = 25, family = \"creepster\", \n                                  face=\"bold\", hjust=.5, color = 'orange'),\n         plot.subtitle = element_text(family = \"creepster\", size = 10,  color='orange',\n                                      hjust=.5, margin=margin(2, 0, 5, 0)),\n         plot.caption = element_text(size = 6, family = \"creepster\", \n                                    color='orange', hjust=1, margin=margin(2, 0, 5, 0)))+\n  labs(title = \"THE PUBLIC IS SPLIT ON THE HORROR COMEDY\", \n       subtitle = 'DISTRIBUTION OF AVERAGE VOTER SCORES FOR HORROR\n       COMEDIES RELEASED OVER THE LAST 10 YEARS',\n       caption = str_wrap(\"\\nData source : Horror movies dataset extracted by\\n\n                          Tanya Shapiro from The Movie Database (TMDB)\", 70))"
  },
  {
    "objectID": "Data/horrorplots/kill count/kill count.html",
    "href": "Data/horrorplots/kill count/kill count.html",
    "title": "#HorrorPlots Slasher Kill Count",
    "section": "",
    "text": "plt <- ggplot(df) +\n  geom_hline(\n    aes(yintercept = y), \n    data.frame(y = c(0:2) * 100),\n    color = \"lightgrey\"\n  ) + \n  geom_col(\n    aes(\n      x = reorder(str_wrap(Slasher, 5), Kill.Count),\n      y = Kill.Count,\n      fill = Movies,\n    ),\n    position = \"dodge2\",\n    width = 0.75,\n    show.legend = TRUE,\n    alpha = .9\n  ) +\n  # Lollipop shaft for mean gain per region\n  geom_segment(\n    aes(\n      x = reorder(str_wrap(Slasher, 5), Kill.Count),\n      y = 0,\n      xend = reorder(str_wrap(Slasher, 5), Kill.Count),\n      yend = 190\n    ),\n    linetype = \"dashed\",\n    color = \"black\"\n  ) + \n  \n  # Make it circular!\n  coord_polar()\n\nplt\n\n\n\n\n\nplt <- plt +\n  annotate(\n    x = 10, \n    y = 170,\n    label = \"Number of Kills\",\n    geom = \"text\",\n    color = \"#FC3205\",\n    size = 5.5,\n    family = \"creepster\"\n  ) +\n  # Annotate custom scale inside plot\n  annotate(\n    x = 11.7, \n    y = 60, \n    label = \"50\", \n    geom = \"text\", \n    color = \"gray12\", \n    family = \"creepster\"\n  ) +\n  annotate(\n    x = 11.7, \n    y = 110, \n    label = \"100\", \n    geom = \"text\", \n    color = \"gray12\", \n    family = \"creepster\"\n  ) +\n  annotate(\n    x = 11.7, \n    y = 160, \n    label = \"150\", \n    geom = \"text\", \n    color = \"gray12\", \n    family = \"creepster\"\n  ) +\n  # Scale y axis so bars don't start in the center\n  scale_y_continuous(\n    limits = c(0, 180),\n    expand = c(0, 0),\n  ) + \n  # New fill and legend title for number of tracks per region\n  scale_fill_gradientn(\n    \"Number of Movies\",\n     colours = c(\"#621605\",\"#91210C\",\"#D92E08\",\"#FC3205\")\n  ) + \n  # Make the guide for the fill discrete\n  guides(\n    fill = guide_colorsteps(\n      barwidth = 15, barheight = .5, title.position = \"top\", title.hjust = .5\n    )\n  ) +\n  theme(\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(color = \"black\", \n                               size = 11, family = 'creepster'),\n    legend.position = \"top\",\n  )\n\nplt\n\n\n\n\n\nplt <- plt + \n  labs(\n    title = \"\\nNumber of Kills\\nOver Slasher Franchise\",\n    subtitle  = \"\\n\\nData Visualization by @JacobCJameson\\nSource: Dead Meat Wiki\\nLink to Data: https://the-dead-meat.fandom.com/wiki\") +\n  # Customize general theme\n  theme(\n    # Set default color and font family for the text\n    text = element_text(color = \"gray12\", family = \"techmono\"),\n    # Customize the text in the title, subtitle, and caption\n    plot.title = element_text(face = \"bold\", size = 20, hjust = 0.5, family = 'nosifer'),\n    plot.subtitle = element_text(size = 10, hjust = .5),\n    # Make the background white and remove extra grid lines\n    panel.background = element_rect(fill = \"#f5f5f2\", color = \"#f5f5f2\"),\n    plot.background = element_rect(fill = \"#f5f5f2\", color = NA),\n    legend.background = element_rect(fill = \"#f5f5f2\", color = NA)\n    #panel.grid = element_blank(),\n    #panel.grid.major.x = element_blank()\n  )\n\nplt"
  },
  {
    "objectID": "Data/Intro_R_Course/Intro R.html",
    "href": "Data/Intro_R_Course/Intro R.html",
    "title": "Introduction to Programming in R",
    "section": "",
    "text": "This page contains the content for the “Introduction to Data Analysis in R” mini-course. The goal is to introduce R programming concepts with a focus on preparing and analyzing data, and conducting statistical simulations. We will cover how to read data into R, manipulate data with a suite of tools from the tidyverse package dplyr. We will also discuss some basic programming concepts including data types and operators.\nThis mini-course is aimed at people with no programming background that are looking for a gentle and practical introduction to data analysis in R.\nEach Module involves:\n\na pre-recorded lecture with guided practice exercises\na corresponding lab to practice the skills taught in the video and stretch your thinking\n\nThe labs are an essential part of the learning process as they walkthrough the use of many key functions and topics that are not explicitly covered in the videos."
  },
  {
    "objectID": "Data/Intro_R_Course/Intro R.html#table-of-contents",
    "href": "Data/Intro_R_Course/Intro R.html#table-of-contents",
    "title": "Introduction to Programming in R",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nModule 0: R and RStudio Installation Guide\nModule 1: An Introduction and Motivation for R Programming\nModule 2: Installing Packages and Reading Data\nModule 3: Vectors and Lists\nModule 4: Data Manipulation\nModule 5: Data Manipulation and Analysis II\nModule 6: Data Visualization as a Tool for Analysis\nModule 7: Grouped Analysis\nModule 8: Iteration\nModule 9: Writing Functions\n\n\nAdditional Resources\n\n\n\ntidyverse cheetsheets start with dplyr and ggplot\n\n\nR for Data Science: free online book with clear explanations of many tidyverse functions, the book to read on data analysis with R"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Jacob Jameson",
    "section": "",
    "text": "Welcome to my blog!\n\n\n\n\n\n\n\n\n  \n\n\n\n\nDemystifying KNN\n\n\n\n\n\n\n\nknn\n\n\n\n\nA deep dive into the K-Nearest Neighbors (KNN) algorithm, exploring its mathematical foundations and practical applications.\n\n\n\n\n\n\nFeb 1, 2024\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nThe ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research\n\n\n\n\n\n\n\nmarginal effects\n\n\n\n\nMarginal Effects &gt;&gt;&gt; Odds Ratios\n\n\n\n\n\n\nSep 1, 2023\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nBasic Network Analysis and Visualization for Directed Graphs in R\n\n\n\n\n\n\n\nnetworks\n\n\ncentrality\n\n\n\n\nChoosing the Right Centrality Measure.\n\n\n\n\n\n\nNov 1, 2022\n\n\nJacob Jameson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data/horrorplots/haunted/haunted.html",
    "href": "Data/horrorplots/haunted/haunted.html",
    "title": "Most Haunted Places in America",
    "section": "",
    "text": "usa <- st_as_sf(maps::map(\"state\", fill=TRUE, plot =FALSE))\nhaunted <- read_csv('haunted_places.csv')\n\nhaunted$ID = tolower(haunted$state)\n\nhaunted <- haunted %>%\n  group_by(ID) %>% summarize(`Number of Haunted Locations` = n()) \n\nhaunts <- merge(usa, haunted, by='ID')\n\ngg_nc = ggplot(haunts) +\n  geom_sf(aes(fill = `Number of Haunted Locations`)) +\n  scale_fill_viridis(option = \"F\") + theme_bw() +labs(\n     title = str_to_upper(\"Most Haunted U.S. States\\n\"),\n     caption = str_wrap(\"Data was extracted from Shadow Lands which has a great \n                        index of haunted places. Locations where “ghosts and hauntings” \n                        have been witnessed are included in the list and people can \n                        report new sightings through the website — which is surely \n                        an extremely accurate and scientific method.· @JacobCJameson\"), 50) +\n   theme(\n     plot.title = element_text(family = \"creepster\", size = 40),\n     plot.caption = element_text(color = \"#111111\", size = 7),\n     axis.line=element_blank(), \n        axis.text.x=element_blank(), \n     axis.title.x=element_blank(),\n        axis.text.y=element_blank(), \n     axis.title.y=element_blank(),\n        axis.ticks=element_blank(), panel.background = element_blank())\n\ngg_nc\n\n\n\n     plot_gg(gg_nc\n            , width=6\n            , height = 6\n            , multicore = TRUE\n            , windowsize = c(1400,866)\n            , sunangle=225\n            , zoom = 0.60\n            , phi = 30\n            , theta = 45\n            )\n     \nrender_depth(focallength=50)"
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Tidy Tuesday, 2023 Week 7 🎬\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 7, 2023: Hollywood Age Gaps\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 6 📈\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 6, 2023: Big Tech Stock Prices\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 5 🐱\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 5, 2023: Pet Cats UK\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 4 🐻\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 4, 2023: ALONE\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 3 🎨\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 3, 2023: Art History\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 2 🐦\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 2, 2023: Project FeederWatch\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, 2023 Week 1 🏡\n\n\n\n\n\n\n\nTidy Tuesday\n\n\nR\n\n\n\n\n#TidyTuesday Week 1, 2023: Bring your own data from 2022!\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHealthcare Area Chart\n\n\n\n\n\n\n\narea chart\n\n\nD3\n\n\n\n\nIn 2020, a greater percent of previously uninsured people in the U.S. with low incomes were enrolled in Medicare or Medicaid.\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data/viz/healthcare area chart/healthcare_areachart.html",
    "href": "Data/viz/healthcare area chart/healthcare_areachart.html",
    "title": "Healthcare Area Chart",
    "section": "",
    "text": "Code here"
  },
  {
    "objectID": "Data/viz/lollipop timeseries/multiple-line-chart.html",
    "href": "Data/viz/lollipop timeseries/multiple-line-chart.html",
    "title": "Multiple Line Chart",
    "section": "",
    "text": "Tom Brady’s Greatness\n\n\nTom Brady has retired from football after a 22-year career of consistent success and unmatched achievement. But his stature as the N.F.L.’s greatest quarterback may be best understood by seeing his achievements stacked up against those of hundreds of others who played the same position.\n\n\nPlayoff Wins\n\n\nBy Age"
  },
  {
    "objectID": "posts/2022-12-24-ORs/2022-12-24-ORs.html",
    "href": "posts/2022-12-24-ORs/2022-12-24-ORs.html",
    "title": "Odds Ratio Schmodds Ratio: Report the Relative Risk",
    "section": "",
    "text": "library(tidyverse)\nlibrary(RNHANES)\nlibrary(pROC)\nlibrary(sjPlot)\nlibrary(sjmisc)\nlibrary(sjlabelled)\n\n\nd07 = nhanes_load_data(\"DEMO_E\", \"2007-2008\") %>%\n  select(SEQN, cycle, RIAGENDR, RIDAGEYR) %>%\n  transmute(SEQN=SEQN, wave=cycle, RIAGENDR, RIDAGEYR) %>% \n  left_join(nhanes_load_data(\"VID_E\", \"2007-2008\"), by=\"SEQN\") %>%\n  select(SEQN, wave, RIAGENDR, RIDAGEYR, LBXVIDMS) %>% \n  transmute(SEQN, wave, RIAGENDR, RIDAGEYR, vitD=LBXVIDMS) %>% \n  left_join(nhanes_load_data(\"BIOPRO_E\", \"2007-2008\"), by=\"SEQN\") %>%\n  select(SEQN, wave, RIAGENDR, RIDAGEYR, vitD, LBXSCA) %>% \n  transmute(SEQN, wave, RIAGENDR, RIDAGEYR, vitD, Calcium = LBXSCA) %>% \n  left_join(nhanes_load_data(\"OSQ_E\", \"2007-2008\"), by=\"SEQN\") %>%\n  select(SEQN, wave, RIAGENDR, RIDAGEYR, vitD, Calcium, OSQ060) %>% \n  transmute(SEQN, wave, RIAGENDR, RIDAGEYR, vitD, Calcium, Osteop = OSQ060)\n\nd09 = nhanes_load_data(\"DEMO_F\", \"2009-2010\") %>%\n  select(SEQN, cycle, RIAGENDR, RIDAGEYR) %>%\n  transmute(SEQN=SEQN, wave=cycle, RIAGENDR, RIDAGEYR) %>% \n  left_join(nhanes_load_data(\"VID_F\", \"2009-2010\"), by=\"SEQN\") %>%\n  select(SEQN, wave, RIAGENDR, RIDAGEYR, LBXVIDMS) %>% \n  transmute(SEQN, wave, RIAGENDR, RIDAGEYR, vitD=LBXVIDMS) %>% \n  left_join(nhanes_load_data(\"BIOPRO_F\", \"2009-2010\"), by=\"SEQN\") %>%\n  select(SEQN, wave, RIAGENDR, RIDAGEYR, vitD,  LBXSCA) %>% \n  transmute(SEQN, wave, RIAGENDR, RIDAGEYR, vitD, Calcium = LBXSCA) %>% \n  left_join(nhanes_load_data(\"OSQ_F\", \"2009-2010\"), by=\"SEQN\") %>%\n  select(SEQN, wave, RIAGENDR, RIDAGEYR, vitD, Calcium, OSQ060) %>% \n  transmute(SEQN, wave, RIAGENDR, RIDAGEYR, vitD, Calcium, Osteop = OSQ060)\n\ndat = bind_rows(d07, d09) %>% as.data.frame()\n\nInstitute of Medicine cutoffs for Vitamin D\nVitamin D deficiency: Serum 25OHD less than 30 nmol/L (12 ng/mL)\n\ndat <- dat %>% \n  mutate(vitD_deficient = ifelse(vitD < 30, 1, 0))\n\n#exclude missing observations\ndat <- dat %>% \n  filter(!is.na(vitD_deficient), !is.na(Calcium), !is.na(Osteop), Osteop!=9) %>% \n  mutate(Gender = recode_factor(RIAGENDR, \n                           `1` = \"Man\", \n                           `2` = \"Woman\"),\n         Osteop = recode_factor(Osteop, \n                           `1` = 1, \n                           `2` = 0))\n\nhead(dat)\n\n   SEQN      wave RIAGENDR RIDAGEYR vitD Calcium Osteop vitD_deficient Gender\n1 41475 2007-2008        2       62 58.8     9.5      0              0  Woman\n2 41477 2007-2008        1       71 81.8    10.0      0              0    Man\n3 41479 2007-2008        1       52 78.4     9.0      0              0    Man\n4 41482 2007-2008        1       64 61.9     9.1      0              0    Man\n5 41483 2007-2008        1       66 53.3     8.9      0              0    Man\n6 41485 2007-2008        2       30 39.1     9.3      0              0  Woman\n\n\n\nfit <- glm(Osteop ~ vitD_deficient + Calcium + Gender + RIDAGEYR, \n           data = dat, family = \"binomial\"(link = \"logit\"))\n\ntab_model(fit)\n\n\n\n\n \nOsteop\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n1879.73\n239.24 – 14670.09\n<0.001\n\n\nvitD deficient\n1.58\n1.13 – 2.28\n0.011\n\n\nCalcium\n1.09\n0.88 – 1.35\n0.443\n\n\nGender [Woman]\n0.13\n0.10 – 0.16\n<0.001\n\n\nRIDAGEYR\n0.93\n0.92 – 0.94\n<0.001\n\n\nObservations\n10244\n\n\nR2 Tjur\n0.136\n\n\n\n\n\n\n\nplot_model(fit, vline.color = \"red\") + theme_bw()"
  },
  {
    "objectID": "Data/Intro_R_Course/mod0.html",
    "href": "Data/Intro_R_Course/mod0.html",
    "title": "Module 0",
    "section": "",
    "text": "Mac Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Mac version, save the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder.\n\n\nWindows Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Windows version, and save the executable file. Run the .exe file and follow the installation instructions."
  },
  {
    "objectID": "Data/Intro_R_Course/mod0.html#r-and-rstudio-installation-guide",
    "href": "Data/Intro_R_Course/mod0.html#r-and-rstudio-installation-guide",
    "title": "Module 0",
    "section": "R and RStudio Installation Guide",
    "text": "R and RStudio Installation Guide\n\nMac Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Mac version, save the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder.\n\n\nWindows Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Windows version, and save the executable file. Run the .exe file and follow the installation instructions."
  },
  {
    "objectID": "R files/mod0.html",
    "href": "R files/mod0.html",
    "title": "Module 0",
    "section": "",
    "text": "Mac Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Mac version, save the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder.\n\n\nWindows Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Windows version, and save the executable file. Run the .exe file and follow the installation instructions."
  },
  {
    "objectID": "Intro R.html",
    "href": "Intro R.html",
    "title": "Introduction to Programming in R",
    "section": "",
    "text": "This page contains the content for the “Introduction to Data Analysis in R” mini-course. The goal is to introduce R programming concepts with a focus on preparing and analyzing data, and conducting statistical simulations. We will cover how to read data into R, manipulate data with a suite of tools from the tidyverse package dplyr. We will also discuss some basic programming concepts including data types and operators.\nThis mini-course is aimed at people with no programming background that are looking for a gentle and practical introduction to data analysis in R.\nEach Module involves:\n\na pre-recorded lecture with guided practice exercises\na corresponding lab to practice the skills taught in the video and stretch your thinking\n\nThe labs are an essential part of the learning process as they walkthrough the use of many key functions and topics that are not explicitly covered in the videos.\n\nAdditional Resources\n\n\n\ntidyverse cheetsheets start with dplyr and ggplot\n\n\nR for Data Science: free online book with clear explanations of many tidyverse functions, the book to read on data analysis with R"
  },
  {
    "objectID": "Intro R.html#table-of-contents",
    "href": "Intro R.html#table-of-contents",
    "title": "Introduction to Programming in R",
    "section": "Table of Contents",
    "text": "Table of Contents"
  },
  {
    "objectID": "R files/0 Module/mod0.html",
    "href": "R files/0 Module/mod0.html",
    "title": "Module 0",
    "section": "",
    "text": "Mac Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Mac version, save the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder.\n\n\nWindows Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Windows version, and save the executable file. Run the .exe file and follow the installation instructions."
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html",
    "href": "R files/0 Module/2022-11-01-SNA.html",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "",
    "text": "“There is certainly no unanimity on exactly what centrality is or on its conceptual foundations, and there is little agreement on the proper procedure for its measurement.” - Linton Freeman (1977)\nSocial network analysis can be used to measure the importance of a person as a function of the social structure of a community or organization. This post uses visualization as a tool to explain how different measures of centrality may be used to analyze different questions in a network analysis. In these examples we will be specifically looking at directed graphs to compare the following centrality measures and their use-cases:\n\nDegree Centrality\nBetweenness Centrality\nEigenvector Centrality\nKatz Centrality\nHITS Hubs and Authorities\n\nAn example of a directed graph would be one in which people nominate their top 2 friends. In this graph, nodes (people) would connect to others nodes through directed edges (nominations). It is possible for Jacob to nominate Jenna without Jenna nominating him back. You can imagine why centrality in a friendship network might take into account the direction of these nominations. If I list 100 people as my friends and none of them list me back, do we think I am a popular person?"
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html#simulate-our-data",
    "href": "R files/0 Module/2022-11-01-SNA.html#simulate-our-data",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Simulate Our Data",
    "text": "Simulate Our Data\nFor our simulated data we are going to be looking at a classroom that contains 14 male and 14 female students. Suppose that each student was asked to name their top 2 male and top 2 female friends in the class. We are interested in analyzing a slew of different research questions where the centrality of student in the class may be of importance.\nLet’s create the data:\n\nlibrary(tidyverse)\n\nWe begin with a 4 vectors: all males in the classroom, all females in the classroom, a probability distribution for selecting friends of the same sex, a probability distribution for selecting friends of the opposite sex.\n\nmales <- c('Jacob', 'Louis', 'Chris', 'Wyatt', 'Nolan', 'Robert', \n           'Zach', 'John','Bob', 'David', 'Avery', 'Ronald', \n           'Dallas', 'Dylan')\n\nfemales <- c('Bohan', 'Jenna', 'Katarina', 'Hassina', 'Towo', \n             'Becca', 'Meredith', 'Gracie', 'Kayla', 'Marlene', \n             'Jade', 'Allyssa', 'Reigne', 'Wendy')\n\nprobs.diff.sex = c(0.15,0.15,0.05,0.05,0.1,0.1,0.05,0.1,0.01,0.09,0.05,0.05,0.025,0.025)\nprobs.same.sex = c(0.15,0.15,0.05,0.05,0.1,0.1,0.05,0.1,0.01,0.09,0.05,0.05,0.05)\n\nThe function below, simulate.top.friends, will produce a dataframe that will contain each student’s picks for their top 2 male and top 2 female friends in the classroom.\n\nset.seed(1997)\n\nsimulate.top.friends <- function(males, females, probs.diff.sex, probs.same.sex) {\n\n  dat <- setNames(data.frame(matrix(ncol = 6, nrow = 0)), \n                  c(\"Ego\", \"Ego Sex\", \"MF1\", \"MF2\", \"FF1\", \"FF2\"))\n\n  for (ego in males) {\n    temp.males <- males[! males %in% ego]\n    \n    male.friends.i <- sample.int(13, 2, replace = FALSE, prob = probs.same.sex)\n    female.friends.i <- sample.int(14, 2, replace = FALSE, prob = probs.diff.sex)\n    \n    male.friend.1 <- temp.males[male.friends.i[1]]\n    male.friend.2 <- temp.males[male.friends.i[2]]\n    \n    female.friend.1 <- females[female.friends.i[1]]\n    female.friend.2 <- females[female.friends.i[2]]\n    \n    dat[nrow(dat) + 1,] = c(ego, 'Male', male.friend.1, male.friend.2, \n                           female.friend.1, female.friend.2)\n    \n  }\n    for (ego in females) {\n    temp.females <- females[! females %in% ego]\n    \n    male.friends.i <- sample.int(14, 2, replace = FALSE, prob = probs.diff.sex)\n    female.friends.i <- sample.int(13, 2, replace = FALSE, prob = probs.same.sex)\n    \n    male.friend.1 <- males[male.friends.i[1]]\n    male.friend.2 <- males[male.friends.i[2]]\n    \n    female.friend.1 <- temp.females[female.friends.i[1]]\n    female.friend.2 <- temp.females[female.friends.i[2]]\n    \n    dat[nrow(dat) + 1,] = c(ego, 'Female', male.friend.1, male.friend.2, \n                           female.friend.1, female.friend.2)\n    \n  }\n  return(dat)\n}\n\nLet’s take a look at our friendship data that we will be working with!\n\nsimulate.top.friends(males,females, probs.diff.sex, probs.same.sex)\n\n\n\n\n\n \n  \n    Ego \n    Ego Sex \n    MF1 \n    MF2 \n    FF1 \n    FF2 \n  \n \n\n  \n    Jacob \n    Male \n    Chris \n    Avery \n    Katarina \n    Meredith \n  \n  \n    Louis \n    Male \n    Jacob \n    Avery \n    Towo \n    Hassina \n  \n  \n    Chris \n    Male \n    Jacob \n    Robert \n    Gracie \n    Becca \n  \n  \n    Wyatt \n    Male \n    Nolan \n    Jacob \n    Marlene \n    Jenna \n  \n  \n    Nolan \n    Male \n    Jacob \n    Bob \n    Bohan \n    Towo \n  \n  \n    Robert \n    Male \n    Nolan \n    Louis \n    Jenna \n    Bohan \n  \n  \n    Zach \n    Male \n    Jacob \n    Avery \n    Meredith \n    Towo \n  \n  \n    John \n    Male \n    Bob \n    Louis \n    Bohan \n    Wendy \n  \n  \n    Bob \n    Male \n    Avery \n    Jacob \n    Jenna \n    Wendy \n  \n  \n    David \n    Male \n    Wyatt \n    Louis \n    Becca \n    Jenna \n  \n  \n    Avery \n    Male \n    Louis \n    Jacob \n    Jenna \n    Becca \n  \n  \n    Ronald \n    Male \n    Jacob \n    Robert \n    Becca \n    Wendy \n  \n  \n    Dallas \n    Male \n    Robert \n    Jacob \n    Bohan \n    Jenna \n  \n  \n    Dylan \n    Male \n    Jacob \n    David \n    Towo \n    Marlene \n  \n  \n    Bohan \n    Female \n    Robert \n    Nolan \n    Allyssa \n    Kayla \n  \n  \n    Jenna \n    Female \n    David \n    Bob \n    Katarina \n    Meredith \n  \n  \n    Katarina \n    Female \n    David \n    Nolan \n    Gracie \n    Reigne \n  \n  \n    Hassina \n    Female \n    Robert \n    David \n    Becca \n    Jade \n  \n  \n    Towo \n    Female \n    John \n    Nolan \n    Jade \n    Jenna \n  \n  \n    Becca \n    Female \n    Nolan \n    Dallas \n    Kayla \n    Towo \n  \n  \n    Meredith \n    Female \n    Louis \n    Jacob \n    Towo \n    Bohan \n  \n  \n    Gracie \n    Female \n    Jacob \n    Ronald \n    Bohan \n    Hassina \n  \n  \n    Kayla \n    Female \n    Ronald \n    Louis \n    Bohan \n    Gracie \n  \n  \n    Marlene \n    Female \n    John \n    Robert \n    Hassina \n    Towo \n  \n  \n    Jade \n    Female \n    Ronald \n    Louis \n    Hassina \n    Bohan \n  \n  \n    Allyssa \n    Female \n    Avery \n    Zach \n    Gracie \n    Bohan \n  \n  \n    Reigne \n    Female \n    John \n    Zach \n    Bohan \n    Hassina \n  \n  \n    Wendy \n    Female \n    Nolan \n    Jacob \n    Towo \n    Bohan"
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html#creating-a-graphing-object",
    "href": "R files/0 Module/2022-11-01-SNA.html#creating-a-graphing-object",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Creating a Graphing Object",
    "text": "Creating a Graphing Object\nThere are many available centrality measures that have been developed for network analysis. At this time, there are no packages that are so comprehensive that it includes all of the measures. I will, therefore, limit this discussion to a subset of the measures that are included in igraph.\nigraph has a really great function that allows us to turn a dataframe into an igraph object. However, the function requires our data to be in “long” form so we will need to do some reshaping. Let’s restructure our data such that each row represents one directed friend nomination. We are going to call this “Source-Target Form”.\n\nlibrary(reshape) \n\nfriendships <- melt(simulate.top.friends(males,females,probs.diff.sex, probs.same.sex), \n                    id=c(\"Ego\", \"Ego Sex\")) %>%\n  select(source=Ego, source_sex =`Ego Sex`, target=value) %>% \n  arrange(source)\n\nLet’s look at the first 10 observations so we can understand the format needed to turn this data into an igraph object.\n\nhead(friendships, 10)\n\n\n\n\n\n \n  \n    source \n    source_sex \n    target \n  \n \n\n  \n    Allyssa \n    Female \n    David \n  \n  \n    Allyssa \n    Female \n    Nolan \n  \n  \n    Allyssa \n    Female \n    Jenna \n  \n  \n    Allyssa \n    Female \n    Towo \n  \n  \n    Avery \n    Male \n    John \n  \n  \n    Avery \n    Male \n    Chris \n  \n  \n    Avery \n    Male \n    Allyssa \n  \n  \n    Avery \n    Male \n    Meredith \n  \n  \n    Becca \n    Female \n    Chris \n  \n  \n    Becca \n    Female \n    Wyatt \n  \n\n\n\n\n\nWe will use the graph_from_data_frame function to create the igraph object.\n\nlibrary(igraph)\nnetwork <- graph_from_data_frame(friendships[,c('source','target','source_sex')],\n                                 directed = TRUE)\n\nnetwork\n\nIGRAPH bedd467 DN-- 28 112 -- \n+ attr: name (v/c), source_sex (e/c)\n+ edges from bedd467 (vertex names):\n [1] Allyssa->David    Allyssa->Nolan    Allyssa->Jenna    Allyssa->Towo    \n [5] Avery  ->John     Avery  ->Chris    Avery  ->Allyssa  Avery  ->Meredith\n [9] Becca  ->Chris    Becca  ->Wyatt    Becca  ->Jade     Becca  ->Jenna   \n[13] Bob    ->Dylan    Bob    ->David    Bob    ->Marlene  Bob    ->Wendy   \n[17] Bohan  ->Nolan    Bohan  ->John     Bohan  ->Katarina Bohan  ->Reigne  \n[21] Chris  ->Jacob    Chris  ->Avery    Chris  ->Bohan    Chris  ->Allyssa \n[25] Dallas ->Louis    Dallas ->Robert   Dallas ->Jenna    Dallas ->Towo    \n[29] David  ->Zach     David  ->Dallas   David  ->Bohan    David  ->Jenna   \n+ ... omitted several edges\n\n\nLet’s better understand the information contained in an igraph object:\n\nIGRAPH simply annotates network as an igraph object\nWhatever random six digit alphanumeric string follows IGRAPH is simply how igraph identifies the graph for itself, it’s not important for our purposes.\nD would tell us that it is directed graph\nN indicates that network is a named graph, in that the vertices have a name attribute\n– refers to attributes not applicable to network, but we will see them in the future:\n28 refers to the number of vertices in network\n112 refers to the number of edges in network\nattr: is a list of attributes within the graph.\n(v/c), which will appear following name, tells us that it is a vertex attribute of a character data type.\n(e/c) or (e/n) referring to edge attributes that are of character or numeric data types\n\nedges from arbitrary igraph name (vertex names): lists a sample of network’s edges using the names of the vertices which they connect.\n\n\nNow let’s create a rough plot to look at our network!\n\nlay <- layout_with_kk(network)\n\npar(bg=\"grey98\")\nplot(network, layout = lay, edge.color=\"grey80\",\n     vertex.color=\"lightblue\", vertex.label.color = \"black\")\n\n\n\n\n\n\n\n\nFor the rest of this post, we are going to talk about a few different measures of centrality, what they capture mathematically and intuitively, and we will look at plots where the size of the node corresponds to the relative centrality score."
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html#measures-of-centrality",
    "href": "R files/0 Module/2022-11-01-SNA.html#measures-of-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Measures of Centrality",
    "text": "Measures of Centrality\n\nDegree Centrality\nFor directed graphs, in-degree, or number of incoming points, is one way we can determine the importance factor for nodes. The Degree of a node is the number of edges that it has. The basic intuition is that, nodes with more connections are more influential and important in a network. In other words, the people with more friend nominations in our simulated social network are the ones with greater importance according to this metric.\n\nDegree.Directed <- degree(network)\nIndegree <- degree(network, mode=\"in\")\nOutdegree <- degree(network, mode=\"out\")\n\nCompareDegree <- cbind(Degree.Directed, Indegree, Outdegree)\n\n\nhead(CompareDegree, 10)\n\n\n\n\n\n \n  \n      \n    Degree.Directed \n    Indegree \n    Outdegree \n  \n \n\n  \n    Allyssa \n    7 \n    3 \n    4 \n  \n  \n    Avery \n    7 \n    3 \n    4 \n  \n  \n    Becca \n    7 \n    3 \n    4 \n  \n  \n    Bob \n    7 \n    3 \n    4 \n  \n  \n    Bohan \n    12 \n    8 \n    4 \n  \n  \n    Chris \n    7 \n    3 \n    4 \n  \n  \n    Dallas \n    7 \n    3 \n    4 \n  \n  \n    David \n    11 \n    7 \n    4 \n  \n  \n    Dylan \n    7 \n    3 \n    4 \n  \n  \n    Gracie \n    5 \n    1 \n    4 \n  \n\n\n\n\n\nThis is a very reasonable way to measure importance within a network. If we are trying to determine who in our classroom is the most popular, we might define that as the greatest number of friendship nominations.\n\nlay <- layout_with_kk(network)\n\npar(bg=\"grey98\")\nplot(network, layout = lay, edge.color=\"grey80\",\n     vertex.size=degree(network, mode=\"in\")*2,  # Rescaled by multiplying by 2\n     main=\"In-Degree\", vertex.label.dist=1.5,\n     vertex.color=\"lightblue\", vertex.label.color = \"black\")"
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html#betweenness-centrality",
    "href": "R files/0 Module/2022-11-01-SNA.html#betweenness-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Betweenness Centrality",
    "text": "Betweenness Centrality\nBetweenness Centrality is another centrality that is based on shortest path between nodes. It is determined as number of the shortest path passing by the given node. For starting node \\(s\\), destination node \\(t\\) and the input node \\(i\\) that holds \\(s \\ne t \\ne i\\), let \\(n_{st}^i\\) be 1 if node \\(i\\) lies on the shortest path between \\(s\\) and \\(t\\); and \\(0\\) if not. So the betweenness centrality is defined as:\n\\[x_i = \\sum_{st} n_{st}^i\\] However, there can be more than one shortest path between \\(s\\) and \\(t\\) and that will count for centrality measure more than once. Thus, we need to divide the contribution to \\(g\\_{st}\\), total number of shortest paths between \\(s\\) and \\(t\\).\n\\[x_i = \\sum_{st} \\frac{n_{st}^i}{g_{st}}\\]\nEssentially the size of the node here represents the frequency with which that node lies on the shortest path between other nodes.\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=betweenness(network)*0.25,  # Rescaled by multiplying by 0.25\n     main=\"Betweenness Centrality\", vertex.color=\"lightblue\")"
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html#eigenvector-centrality",
    "href": "R files/0 Module/2022-11-01-SNA.html#eigenvector-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Eigenvector Centrality",
    "text": "Eigenvector Centrality\nEigenvector centrality is a basic extension of degree centrality, which defines centrality of a node as proportional to its neighbors’ importance. When we sum up all connections of a node, not all neighbors are equally important. This is a very interesting way to measure popularity in our classroom where we say the popularity of your friends matters more than the number of friends. Let’s consider two nodes in a friend network with same degree, the one who is connected to more central nodes should be more central.\nFirst, we define an initial guess for the centrality of nodes in a graph as \\(x_i=1\\). Now we are going to iterate for the new centrality value \\(x_i'\\) for node \\(i\\) as following:\n\\[x_i' = \\sum_{j} A_{ij}x_j\\]\nHere \\(A_{ij}\\) is an element of the adjacency matrix, where it gives 1 or 0 for whether an edge exists between nodes \\(i\\) and \\(j\\). it can also be written in matrix notation as \\(\\mathbf{x'} = \\mathbf{Ax}\\).\nWe iterate over t steps to find the vector \\(\\mathbf{x}(t)\\) as:\n\\[\\mathbf{x}(t) = \\mathbf{A^t x}(0)\\]\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=evcent(network)$vector*15,   # Rescaled by multiplying by 15\n     main=\"Eigenvector Centrality\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\nThe plot shows, the students which have the same number of friend nominations are not necessarily in the same size. The one that is connected to more central, or “popular” nodes are larger in this visualization.\nHowever, as we can see from the definition, this can be a problematic measure for directed graphs. Let’s say that a student who received no friend nominations themselves nominates another student as a friend. Because that person has 0 friend nominations themselves, they would not contribute any importance to the person they nominated. In other words, eigenvector centrality would not take zero in-degree nodes into account in directed graphs.\nHowever there is a solution to this!"
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html#katz-centrality",
    "href": "R files/0 Module/2022-11-01-SNA.html#katz-centrality",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Katz Centrality",
    "text": "Katz Centrality\nKatz centrality introduces two positive constants \\(\\alpha\\) and \\(\\beta\\) to tackle the problem of eigenvector centrality with zero in-degree nodes:\n\\[x_i = \\alpha \\sum_{j} A_{ij} x_j + \\beta\\],\nagain \\(A_{ij}\\) is an element of the adjacency matrix, and it can also be written in matrix notation as \\(\\mathbf{x} = \\alpha \\mathbf{Ax} + \\beta \\mathbf{1}\\). This \\(\\beta\\) constant gives a free centrality contribution for all nodes even though they don’t get any contribution from other nodes. The existence of a node alone would provide it some importance. \\(\\alpha\\) constant determines the balances between the contribution from other nodes and the free constant.\nUnfortunately, igraph does not have a function to compute Katz centrality, so we will need to do it the old fashioned way.\n\nkatz.centrality = function(g, alpha, beta, t) {\n  n = vcount(g);\n  A = get.adjacency(g);\n  x0 = rep(0, n);\n  x1 = rep(1/n, n);\n  eps = 1/10^t;\n  iter = 0;\n  while (sum(abs(x0 - x1)) > eps) {\n    x0 = x1;\n    x1 = as.vector(alpha * x1 %*% A) + beta;\n    iter = iter + 1;\n  } \n  return(list(aid = x0, vector = x1, iter = iter))\n}\n\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=katz.centrality(network, 0.1, 1, 0.01)$vector*5,   # Rescaled by multiplying by 15\n     main=\"Katz Centrality\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\nAlthough this method is introduced as a solution for directed graphs, it can be useful for some applications of undirected graphs as well."
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html#hits-hubs-and-authorities",
    "href": "R files/0 Module/2022-11-01-SNA.html#hits-hubs-and-authorities",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "HITS Hubs and Authorities",
    "text": "HITS Hubs and Authorities\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5, vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=hub.score(network)$vector*15,   # Rescaled by multiplying by 15\n     main=\"HITS Hubs\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\n\npar(bg=\"grey98\")\nplot(network, layout=lay, \n     vertex.label.dist=1.5 , vertex.label.color = \"black\", edge.color=\"grey80\",\n     vertex.size=authority.score(network)$vector*15,   # Rescaled by multiplying by 15\n     main=\"HITS Authorities\", vertex.color=\"lightblue\")\n\n\n\n\n\n\n\n\nUp until this point, we have discussed the measures that captures high node centrality, however, there can be nodes in the network which are important for the network, but they are not central. In order to find out such nodes, the HITS algorithm introduces two types of central nodes: Hubs and Authorities. For Hubs, we might consider a node important if it links to many highly nominated nodes (i.e. the person nominates many popular people as their friends). For Authorities, we might consider a node to be of importance if many highly nominated nodes link to it (i.e. nominated by many popular people).\nAuthority Centrality is defined as the sum of the hub centralities which point to the node (i):\n\\[x_i = \\alpha \\sum_{j} A_{ij} y_j,\\]\nwhere \\(\\alpha\\) is constant. Likewise, Hub Centrality is the sum of the authorities which are pointed by the node \\(i\\):\n\\[y_i = \\beta \\sum_{j} A_{ji} x_j,\\]\nwith constant \\(\\beta\\). Here notice that the element of the adjacency matrix are swapped for Hub Centrality because we are concerned with outgoing edges for hubs. So in matrix notation:\n\\[\\mathbf{x} = \\alpha \\mathbf{Ay}, \\quad\\]\n\\[\\mathbf{y} = \\beta \\mathbf{A^Tx}.\\] As it can be seen from the drawing, HITS Algorithm also tackles the problem with zero in-degree nodes of Eigenvector Centrality. These zero in-degree nodes become central hubs and contribute to other nodes. Yet we can still use a free centrality contribution constant like in Katz Centrality or other variants.\nAlthough these measures are generally used in a citation network or the internet, this can be pretty interesting in the context of our classroom frienship network. Maybe we are interested in knowing who are the non-popular students that nominate many popular students as friends, but receive no reciprocity in terms of nominations."
  },
  {
    "objectID": "R files/0 Module/2022-11-01-SNA.html#other-packages-that-i-like-for-visualization",
    "href": "R files/0 Module/2022-11-01-SNA.html#other-packages-that-i-like-for-visualization",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "Other Packages that I Like for Visualization",
    "text": "Other Packages that I Like for Visualization\nI want to conclude this post with some sample code to produce a nice network plot using the ggnetwork package. igraph is only one package that exists for network visualization and centrality calculations, I encourage you to check out additional packages which may be stronger than igraph for some purposes.\n\nlibrary(GGally)\nlibrary(network)\nlibrary(ggnetwork)\n\nnet <- list(nodes=friendships[c('source', 'target', 'source_sex')], \n            edges=friendships[c('source', 'target', 'source_sex')])\n\n# create node attribute data\nnet.cet <- as.character(net$nodes$source_sex)\nnames(net.cet) = net$nodes$source\nedges <- net$edges\n\n# create network\nnet.net <- edges[, c(\"source\", \"target\") ]\nnet.net <- network::network(net.net, directed = TRUE)\n\n# create sourc sex node attribute\nnet.net %v% \"source_sex\" <- net.cet[ network.vertex.names(net.net) ]\n\n\nset.seed(1)\nggnet2(net.net, color = \"source_sex\",\n       palette = c(\"Female\" = \"purple\", \"Male\" = \"maroon\"), size = 'indegree',\n       arrow.size = 3, arrow.gap = 0.04, alpha = 1,  label = TRUE, vjust = 2.5, label.size = 3.5,\n       edge.alpha = 0.5, mode = \"kamadakawai\",edge.color = 'grey50',\n       color.legend = \"Student Sex\") + theme_bw() + theme_blank()  + \n  theme(legend.position = \"bottom\", text = element_text(size = 15),\n        plot.caption = element_text(hjust = 0)) + guides(size=F) + \n  labs(title='Social Network Mapping of Friendship Nominations for Simulated Data', \n       caption = str_wrap(\"The size of the dots represent the number of \n                          friendship nominations by others. The maroon dots represent males \n                          while the purple dots represent females. Directed edges are used \n                          to indicate the direction of friendship nominations. \n                          Edges that are bi-directional indicated friendship reciprocity. \n                          \\n\\n This particular network represents the frienship nominations \n                          of 28 students from our simulated data.\", 128))\n\n\n\n\n\n\n\n\nThank you!\nJacob"
  },
  {
    "objectID": "R files/5 Module/mod5.html",
    "href": "R files/5 Module/mod5.html",
    "title": "Module 5: Data Manipulation and Analysis II",
    "section": "",
    "text": "Download a copy of Module 5 slides\nDownload data for Module 5 lab and tutorial"
  },
  {
    "objectID": "R files/9 Module/mod9.html",
    "href": "R files/9 Module/mod9.html",
    "title": "Module 9: Writing Functions",
    "section": "",
    "text": "Download a copy of Module 9 slides"
  },
  {
    "objectID": "R files/7 Module/mod0.html",
    "href": "R files/7 Module/mod0.html",
    "title": "Module 0",
    "section": "",
    "text": "Mac Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Mac version, save the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder.\n\n\nWindows Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Windows version, and save the executable file. Run the .exe file and follow the installation instructions."
  },
  {
    "objectID": "R files/3 Module/mod3.html",
    "href": "R files/3 Module/mod3.html",
    "title": "Module 3: Vectors and Lists",
    "section": "",
    "text": "Download a copy of Module 3 slides\nDownload data for Module 3 lab and tutorial"
  },
  {
    "objectID": "R files/1 Module/mod1.html",
    "href": "R files/1 Module/mod1.html",
    "title": "Module 1: An Introduction and Motivation for R Programming",
    "section": "",
    "text": "Download a copy of Module 1 slides"
  },
  {
    "objectID": "R files/4 Module/mod4.html",
    "href": "R files/4 Module/mod4.html",
    "title": "Module 4: Data Manipulation",
    "section": "",
    "text": "Download a copy of Module 4 slides\nDownload data for Module 4 lab and tutorial"
  },
  {
    "objectID": "R files/8 Module/mod8.html",
    "href": "R files/8 Module/mod8.html",
    "title": "Module 8: Iteration",
    "section": "",
    "text": "Download a copy of Module 8 slides"
  },
  {
    "objectID": "R files/6 Module/mod7.html",
    "href": "R files/6 Module/mod7.html",
    "title": "Module 0",
    "section": "",
    "text": "Mac Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Mac version, save the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder.\n\n\nWindows Users\n\nTo Install R:\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\nTo Install RStudio:\n\nGo to https://www.rstudio.com and click on the “Download” button on the top right ( right above “Pricing”).\nClick on “Download RStudio Desktop.”\nClick on the version recommended for your system, or the latest Windows version, and save the executable file. Run the .exe file and follow the installation instructions."
  },
  {
    "objectID": "R files/2 Module/mod2.html",
    "href": "R files/2 Module/mod2.html",
    "title": "Module 2: Installing Packages and Reading Data",
    "section": "",
    "text": "Download a copy of Module 2 slides\nDownload data for Module 2 lab and tutorial"
  },
  {
    "objectID": "R files/7 Module/mod7.html",
    "href": "R files/7 Module/mod7.html",
    "title": "Module 7: Grouped Analysis",
    "section": "",
    "text": "Download a copy of Module 7 slides\nDownload data for Module 7 lab and tutorial"
  },
  {
    "objectID": "R files/6 Module/mod6.html",
    "href": "R files/6 Module/mod6.html",
    "title": "Module 6: Data Visualization as a Tool for Analysis",
    "section": "",
    "text": "Download a copy of Module 6 slides\nDownload data for Module 6 lab and tutorial"
  },
  {
    "objectID": "R files/1 Module/mod1.html#lab-1",
    "href": "R files/1 Module/mod1.html#lab-1",
    "title": "Module 1: An Introduction and Motivation for R Programming",
    "section": "Lab 1",
    "text": "Lab 1\nWe expect you to watch the Module 1 material prior to lab.\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\n\n\n\nWarm-up\n\nThe most important warm up question: do you have R and RStudio installed?\nWhich of these allow you to pull up the documentation for a command in R?\n\n\n*\n?\nhelp()\ndocumentation()\n\n\nIn the code block below, run code that will pull up documentation for the function paste0().\n\n\n?paste0()\n\nWhat does this function do?\n\nWhat are the two ways that you can assign a value to a variable?\n\n\n\nGuess the Output: Algebra\n\nGuess the output of the following code:\n\n\na &lt;- 3\nb &lt;- a^2 + 1\n\nb\n\nNow, run the code block to check your answer.\n\nGuess the output of the following code:\n\n\na &lt;- 10\nb &lt;-3 %% a\n\nb + 5\n\nHint: If you are not sure what %% does you can try running ?'%%' to better understand.\n\nGuess the output of the following code:\n\n\na &lt;- c(1,2,3)\nb &lt;- a^2 + 1\n\nb\n\n\n\nGuess the Output: Boolean\n\nGuess the output of the following code:\n\n\n25 &gt;= 14\n\n\nGuess the output of the following code:\n\n\n10 != 100\n\n\nGuess the output of the following code:\n\n\n7%%5 == 2\n\n\nGuess the output of the following code:\n\n\n(5 &gt; 7) & (7 * 7 == 49)\n\n\nOk, let’s try some logic! Try to figure out each one before running the code!\n\n\n\n\n\nTRUE & FALSE\n\n\n\n\n\nFALSE & FALSE\n\n\n\n\n\nTRUE | (FALSE & TRUE)\n\n\n\n\n\nFALSE | (TRUE | FALSE)\n\n\n\n\n\n(TRUE & (TRUE | FALSE)) | FALSE\n\n\n\nData Types\n\nRun these lines to create these variables in your environment.\n\n\nitem_1 &lt;- \"Hi, my name is item 1!\"\nitem_2 &lt;- 7\nitem_3 &lt;- FALSE\n\nWhat are the type (or mode) of each of these items?\nHint: If you are not sure, you could apply the mode() function to each item and check the output. If you are unsure about how to apply the mode() function, you can always run ?mode().\n\nGuess the output of the following code:\n\n\n(item_2 + 19 &lt;= 25) == item_3\n\n\nDo you remember earlier when you ran ?paste0()? We are now going to try to use this function. In the code block below, initialize two variables that are of mode “character”. The output when you apply paste0() to these variables should be “Hello, world!”.\n\n\n#v1 &lt;-\n#v2 &lt;- \n\nWell done! You’ve learned how to work with R to perform simple variable assignment and operations!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/1 Module/mod1.html#general-guidelines",
    "href": "R files/1 Module/mod1.html#general-guidelines",
    "title": "Module 1",
    "section": "General Guidelines:",
    "text": "General Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps"
  },
  {
    "objectID": "R files/1 Module/mod1.html#warm-up",
    "href": "R files/1 Module/mod1.html#warm-up",
    "title": "Module 1",
    "section": "Warm-up",
    "text": "Warm-up\n\nThe most important warm up question: do you have R and RStudio installed?\nWhich of these allow you to pull up the documentation for a command in R?\n\n\n*\n?\nhelp()\ndocumentation()\n\n\nIn the code block below, run code that will pull up documentation for the function paste0().\n\n\n?paste0()\n\nWhat does this function do?\n\nWhat are the two ways that you can assign a value to a variable?\n\n\nGuess the Output: Algebra\n\nGuess the output of the following code:\n\n\na <- 3\nb <- a^2 + 1\n\nb\n\nNow, run the code block to check your answer.\n\nGuess the output of the following code:\n\n\na <- 10\nb <-3 %% a\n\nb + 5\n\nHint: If you are not sure what %% does you can try running ?'%%' to better understand.\n\nGuess the output of the following code:\n\n\na <- c(1,2,3)\nb <- a^2 + 1\n\nb\n\n\n\nGuess the Output: Boolean\n\nGuess the output of the following code:\n\n\n25 >= 14\n\n\nGuess the output of the following code:\n\n\n10 != 100\n\n\nGuess the output of the following code:\n\n\n7%%5 == 2\n\n\nGuess the output of the following code:\n\n\n(5 > 7) & (7 * 7 == 49)\n\n\nOk, let’s try some logic! Try to figure out each one before running the code!\n\n\n\n\n\nTRUE & FALSE\n\n\n\n\n\nFALSE & FALSE\n\n\n\n\n\nTRUE | (FALSE & TRUE)\n\n\n\n\n\nFALSE | (TRUE | FALSE)\n\n\n\n\n\n(TRUE & (TRUE | FALSE)) | FALSE\n\n\n\nData Types\n\nRun these lines to create these variables in your environment.\n\n\nitem_1 <- \"Hi, my name is item 1!\"\nitem_2 <- 7\nitem_3 <- FALSE\n\nWhat are the type (or mode) of each of these items?\nHint: If you are not sure, you could apply the mode() function to each item and check the output. If you are unsure about how to apply the mode() function, you can always run ?mode().\n\nGuess the output of the following code:\n\n\n(item_2 + 19 <= 25) == item_3\n\n\nDo you remember earlier when you ran ?paste0()? We are now going to try to use this function. In the code block below, initialize two variables that are of mode “character”. The output when you apply paste0() to these variables should be “Hello, world!”.\n\n\n#v1 <-\n#v2 <- \n\nWell done! You’ve learned how to work with R to perform simple variable assignment and operations!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/1 Module/mod1.html#guess-the-output-algebra",
    "href": "R files/1 Module/mod1.html#guess-the-output-algebra",
    "title": "Module 1",
    "section": "Guess the Output: Algebra",
    "text": "Guess the Output: Algebra\n\nGuess the output of the following code:\n\n\na <- 3\nb <- a^2 + 1\n\nb\n\nNow, run the code block to check your answer.\n\nGuess the output of the following code:\n\n\na <- 10\nb <-3 %% a\n\nb + 5\n\nHint: If you are not sure what %% does you can try running ?'%%' to better understand.\n\nGuess the output of the following code:\n\n\na <- c(1,2,3)\nb <- a^2 + 1\n\nb"
  },
  {
    "objectID": "R files/1 Module/mod1.html#guess-the-output-boolean",
    "href": "R files/1 Module/mod1.html#guess-the-output-boolean",
    "title": "Module 1",
    "section": "Guess the Output: Boolean",
    "text": "Guess the Output: Boolean\n\nGuess the output of the following code:\n\n\n25 >= 14\n\n\nGuess the output of the following code:\n\n\n10 != 100\n\n\nGuess the output of the following code:\n\n\n7%%5 == 2\n\n\nGuess the output of the following code:\n\n\n(5 > 7) & (7 * 7 == 49)\n\n\nOk, let’s try some logic! Try to figure out each one before running the code!\n\n\n\n\n\nTRUE & FALSE\n\n\n\n\n\nFALSE & FALSE\n\n\n\n\n\nTRUE | (FALSE & TRUE)\n\n\n\n\n\nFALSE | (TRUE | FALSE)\n\n\n\n\n\n(TRUE & (TRUE | FALSE)) | FALSE"
  },
  {
    "objectID": "R files/1 Module/mod1.html#data-types",
    "href": "R files/1 Module/mod1.html#data-types",
    "title": "Module 1",
    "section": "Data Types",
    "text": "Data Types\n\nRun these lines to create these variables in your environment.\n\n\nitem_1 <- \"Hi, my name is item 1!\"\nitem_2 <- 7\nitem_3 <- FALSE\n\nWhat are the type (or mode) of each of these items?\nHint: If you are not sure, you could apply the mode() function to each item and check the output. If you are unsure about how to apply the mode() function, you can always run ?mode().\n\nGuess the output of the following code:\n\n\n(item_2 + 19 <= 25) == item_3\n\n\nDo you remember earlier when you ran ?paste0()? We are now going to try to use this function. In the code block below, initialize two variables that are of mode “character”. The output when you apply paste0() to these variables should be “Hello, world!”.\n\n\n#v1 <-\n#v2 <- \n\nWell done! You’ve learned how to work with R to perform simple variable assignment and operations!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/2 Module/mod2.html#general-guidelines",
    "href": "R files/2 Module/mod2.html#general-guidelines",
    "title": "Module 2: Installing Packages and Reading Data",
    "section": "General Guidelines:",
    "text": "General Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps"
  },
  {
    "objectID": "R files/2 Module/mod2.html#warm-up",
    "href": "R files/2 Module/mod2.html#warm-up",
    "title": "Module 2: Installing Packages and Reading Data",
    "section": "Warm-up",
    "text": "Warm-up\n\nCreate a new Rmd and add code to load the tidyverse package.\nYour classmate comes to you and says they can’t get data to load after restarting their R session. You see the code:\n\n\ninstall.packages(\"haven\")\nawesome_data <- read_dta(\"awesome_data.dta\")\n\nError in read_dta(\"awesome_data.dta\") : could not find function \"read_dta\"\n\nDiagnose the problem.\nNote: If they say the code worked before, it’s likely they had loaded haven in the console or perhaps in an earlier script. R packages will stay attached as long as the R session is live.\n\nIn general, once you have successfully used install.packages(pkg) for a “pkg”, you won’t need to do it again. Install haven and readxl using the console.\nIn your script, load haven and readxl. Notice that if you had to restart R right now. You could reproduce the entire warm-up by running the script. We strive for reproducibility by keeping the code we want organized in scripts or Rmds.\nIt’s good practice when starting a new project to clear your R environment. This helps you make sure you are not relying on data or functions you wrote in another project. After you library() statements add the following code rm(list = ls()).\nrm() is short for remove. Find the examples in ?rm and run them in the console."
  },
  {
    "objectID": "R files/2 Module/mod2.html#islr-chapter-2-q8",
    "href": "R files/2 Module/mod2.html#islr-chapter-2-q8",
    "title": "Module 2: Installing Packages and Reading Data",
    "section": "ISLR Chapter 2 Q8",
    "text": "ISLR Chapter 2 Q8\nThis exercise relates to the College data set, which can be found in the file College.csv. It contains a number of variables for 777 different universities and colleges in the US. The variables are\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nPrivate\nPublic/private indicator\n\n\nApps\nNumber of applications received\n\n\nAccept\nNumber of applicants accepted\n\n\nEnroll\nNumber of new students enrolled\n\n\nTop10perc\nNew students from top 10 % of high school class\n\n\nTop25perc\nNew students from top 25 % of high school class\n\n\nF.Undergrad\nNumber of full-time undergraduates\n\n\nP.Undergrad\nNumber of part-time undergraduates\n\n\nOutstate\nOut-of-state tuition\n\n\nRoom.Board\nRoom and board costs\n\n\nBooks\nEstimated book costs\n\n\nPersonal\nEstimated personal spending\n\n\nPhD\nPercent of faculty with Ph.D.’s\n\n\nTerminal\nPercent of faculty with terminal degree\n\n\nS.F.Ratio\nStudent/faculty ratio\n\n\nperc.alumni\nPercent of alumni who donate\n\n\nExpend\nInstructional expenditure per student\n\n\nGrad.Rate\nGraduation rate\n\n\n\nBefore reading the data into R, it can be viewed in Excel or a text editor. Make sure that you have the directory set to the correct location for the data.\n\nUse the base R read.csv() function to read the data into R with option stringsAsFactors=T (this is needed later on for plotting figures). Call the loaded data college.\nLook at the data using the View() function. You should notice that the first column is just the name of each university. Load your data and then try the following commands:\n\n\n#set your working directory ,fill in your code after this line\n\n#read in the file College.csv using read.csv() with option `stringsAsFactors=T`\ncollege <- read.csv('College.csv', stringsAsFactors = T)\n\n#Give data frame college rownames\nrownames(college) <- college[,1] \n\n# Please comment out View function after using it. Otherwise you'll see some error when knit.\n# View(college)\n\n\nYou should see that there is now a row.names column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. Next, we will remove the first column in the data where the names are stored. Try\n\n\n#Use a negative number to generate a subset with all but one column\n# college[, -c(1, 2, 3)]  will generate a subset with all but the first three columns\ncollege <- college[,-1]\n# as.factor can turn a character column to a factor column so that we can use it to plot later on\ncollege$Private <- as.factor(college$Private)\n#View(college)\n\nNow you should see that the first data column is Private. Note that another column labeled row.names now appears before the Private column. However, this is not a data column but rather the name that R is giving to each row.\n\nUse the summary() function to produce a numerical summary of the variables in the data set. Hint: summary() takes in an object such as data.frame and return the summery results\nUse the pairs() function to produce a scatterplot matrix of the first five columns or variables of the data. Recall that you can reference the first five columns of a data frame dat using dat[,1:5]\nUse the plot() function to produce side-by-side boxplots of Outstate versus Private. Hint: plot() takes two arguments one vector for x axis and one vector for y axis. Try plot(dat$col_name, dat$col_name).\n\n\n# replicate \"No\" for the same times as the number of colleges using rep()\nElite <- rep(\"No\",nrow(college))\n# change the values in Elite for colleges with proportion of students \n# coming from the top 10% of their high school classes \n# exceeds 50 % to \"Yes\"\nElite[college$Top10perc >50] <- \"Yes\"\n# as.factor change ELite, a character vector to a factor vector\n# (we will touch on factors later in class) \nElite <- as.factor(Elite)\n# add the newly created vector to the college data frame\ncollege <- data.frame(college ,Elite)\n\n\nUse the summary() function to see how many elite universities there are. Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite.\n\nContinue exploring the data, and provide a brief summary of what you discover."
  },
  {
    "objectID": "R files/2 Module/mod2.html#islr-chapter-2-q9",
    "href": "R files/2 Module/mod2.html#islr-chapter-2-q9",
    "title": "Module 2: Installing Packages and Reading Data",
    "section": "ISLR Chapter 2 Q9",
    "text": "ISLR Chapter 2 Q9\nThis exercise involves the Auto data set. na.omit() removes the missing values from the data and returns a new data frame.\n\n#load the Auto.csv into a variable called auto using read_csv()\n\n\n# remove all rows with missing values using na.omit()\nauto <- na.omit(auto)\n\nWe can use class() to check which of the columns are quantitative (numeric or integer), and which are qualitative( logical or character). And sapply() function takes in a data frame and a function (in this case class()), apply the class function to each column. Try the following commands:\n\n#apply the class() function to each column of auto data frame\nsapply(auto, class)\n\n\nWhat is the range of each quantitative columns? You can answer this using the range() function. Hint: You can call range() function individually on each column. You can also subset the quantitative columns by creating a variable quant_cols equal to all columns with a numeric mode, then use sapply the function range() with the data frame with only quantitative columns. This is not required.\nUsing the functions mean() and sd(). Find out what is the mean and standard deviation of each quantitative columns?\nNow remove the 10th through 85th observations (rows). What is the range, mean, and standard deviation of each column in the subset of the data that remains? Hint: We’ve seen removing columns in question 8. To remove the rows, we can use the negative sign - again. For example, auto[-c(1,3),] removes the first and third row\nUsing the full data set, investigate the columns graphically, using scatterplots (pairs or plot) or other tools of your choice. Create some plots highlighting the relationships among the columns Comment on your findings.\nSuppose that we wish to predict gas mileage (mpg) on the basis of the other numerical variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.\n\n\nISLR Chapter 2 Q10\nThis exercise involves the Boston housing data set.\nTo begin, load in the Boston data set. The Boston data set is part of the MASS library in R. You may need to install the package using install.packages() function if you haven’t done so.\n\n# install.packages(MASS)\nlibrary(MASS)\n\nNow the data set is contained in the object Boston.\n\nBoston\n\nRead about the data set:\n\n?Boston\n\n\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\nMake some pairwise scatterplots of the columns in this data set. Describe your findings. Hint: Use function pairs()\nHow many of the suburbs in this data set bound the Charles river? Hint: Subset the data using a logical vector to check if variable chas==1, then use nrow() to see the number of suburbs.\nUsing median(), find out what is the median pupil-teacher ratio among the towns in this data set?\n\nWell done! You’ve learned how to work with R to read in data and perform some simple analysis and exploration!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/2 Module/mod2.html#islr-chapter-2-q10",
    "href": "R files/2 Module/mod2.html#islr-chapter-2-q10",
    "title": "Module 2: Installing Packages and Reading Data",
    "section": "ISLR Chapter 2 Q10",
    "text": "ISLR Chapter 2 Q10\nThis exercise involves the Boston housing data set.\nTo begin, load in the Boston data set. The Boston data set is part of the MASS library in R. You may need to install the package using install.packages() function if you haven’t done so.\n\n# install.packages(MASS)\nlibrary(MASS)\n\nNow the data set is contained in the object Boston.\n\nBoston\n\nRead about the data set:\n\n?Boston\n\n\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\nMake some pairwise scatterplots of the columns in this data set. Describe your findings. Hint: Use function pairs()\nHow many of the suburbs in this data set bound the Charles river? Hint: Subset the data using a logical vector to check if variable chas==1, then use nrow() to see the number of suburbs.\nUsing median(), find out what is the median pupil-teacher ratio among the towns in this data set?\n\nWell done! You’ve learned how to work with R to read in data and perform some simple analysis and exploration!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/2 Module/mod2.html#lab-2",
    "href": "R files/2 Module/mod2.html#lab-2",
    "title": "Module 2: Installing Packages and Reading Data",
    "section": "Lab 2",
    "text": "Lab 2\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\n\n\n\nWarm-up\n\nCreate a new Rmd and add code to load the tidyverse package.\nYour classmate comes to you and says they can’t get data to load after restarting their R session. You see the code:\n\n\ninstall.packages(\"haven\")\nawesome_data <- read_dta(\"awesome_data.dta\")\n\nError in read_dta(\"awesome_data.dta\") : could not find function \"read_dta\"\n\nDiagnose the problem.\nNote: If they say the code worked before, it’s likely they had loaded haven in the console or perhaps in an earlier script. R packages will stay attached as long as the R session is live.\n\nIn general, once you have successfully used install.packages(pkg) for a “pkg”, you won’t need to do it again. Install haven and readxl using the console.\nIn your script, load haven and readxl. Notice that if you had to restart R right now. You could reproduce the entire warm-up by running the script. We strive for reproducibility by keeping the code we want organized in scripts or Rmds.\nIt’s good practice when starting a new project to clear your R environment. This helps you make sure you are not relying on data or functions you wrote in another project. After you library() statements add the following code rm(list = ls()).\nrm() is short for remove. Find the examples in ?rm and run them in the console.\n\n\n\nISLR Chapter 2 Q8\nThis exercise relates to the College data set, which can be found in the file College.csv. It contains a number of variables for 777 different universities and colleges in the US. The variables are\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nPrivate\nPublic/private indicator\n\n\nApps\nNumber of applications received\n\n\nAccept\nNumber of applicants accepted\n\n\nEnroll\nNumber of new students enrolled\n\n\nTop10perc\nNew students from top 10 % of high school class\n\n\nTop25perc\nNew students from top 25 % of high school class\n\n\nF.Undergrad\nNumber of full-time undergraduates\n\n\nP.Undergrad\nNumber of part-time undergraduates\n\n\nOutstate\nOut-of-state tuition\n\n\nRoom.Board\nRoom and board costs\n\n\nBooks\nEstimated book costs\n\n\nPersonal\nEstimated personal spending\n\n\nPhD\nPercent of faculty with Ph.D.’s\n\n\nTerminal\nPercent of faculty with terminal degree\n\n\nS.F.Ratio\nStudent/faculty ratio\n\n\nperc.alumni\nPercent of alumni who donate\n\n\nExpend\nInstructional expenditure per student\n\n\nGrad.Rate\nGraduation rate\n\n\n\nBefore reading the data into R, it can be viewed in Excel or a text editor. Make sure that you have the directory set to the correct location for the data.\n\nUse the base R read.csv() function to read the data into R with option stringsAsFactors=T (this is needed later on for plotting figures). Call the loaded data college.\nLook at the data using the View() function. You should notice that the first column is just the name of each university. Load your data and then try the following commands:\n\n\n#set your working directory ,fill in your code after this line\n\n#read in the file College.csv using read.csv() with option `stringsAsFactors=T`\ncollege <- read.csv('College.csv', stringsAsFactors = T)\n\n#Give data frame college rownames\nrownames(college) <- college[,1] \n\n# Please comment out View function after using it. Otherwise you'll see some error when knit.\n# View(college)\n\n\nYou should see that there is now a row.names column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. Next, we will remove the first column in the data where the names are stored. Try\n\n\n#Use a negative number to generate a subset with all but one column\n# college[, -c(1, 2, 3)]  will generate a subset with all but the first three columns\ncollege <- college[,-1]\n# as.factor can turn a character column to a factor column so that we can use it to plot later on\ncollege$Private <- as.factor(college$Private)\n#View(college)\n\nNow you should see that the first data column is Private. Note that another column labeled row.names now appears before the Private column. However, this is not a data column but rather the name that R is giving to each row.\n\nUse the summary() function to produce a numerical summary of the variables in the data set. Hint: summary() takes in an object such as data.frame and return the summery results\nUse the pairs() function to produce a scatterplot matrix of the first five columns or variables of the data. Recall that you can reference the first five columns of a data frame dat using dat[,1:5]\nUse the plot() function to produce side-by-side boxplots of Outstate versus Private. Hint: plot() takes two arguments one vector for x axis and one vector for y axis. Try plot(dat$col_name, dat$col_name).\n\n\n# replicate \"No\" for the same times as the number of colleges using rep()\nElite <- rep(\"No\",nrow(college))\n# change the values in Elite for colleges with proportion of students \n# coming from the top 10% of their high school classes \n# exceeds 50 % to \"Yes\"\nElite[college$Top10perc >50] <- \"Yes\"\n# as.factor change ELite, a character vector to a factor vector\n# (we will touch on factors later in class) \nElite <- as.factor(Elite)\n# add the newly created vector to the college data frame\ncollege <- data.frame(college ,Elite)\n\n\nUse the summary() function to see how many elite universities there are. Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite.\n\nContinue exploring the data, and provide a brief summary of what you discover.\n\n\nISLR Chapter 2 Q9\nThis exercise involves the Auto data set. na.omit() removes the missing values from the data and returns a new data frame.\n\n#load the Auto.csv into a variable called auto using read_csv()\n\n\n# remove all rows with missing values using na.omit()\nauto <- na.omit(auto)\n\nWe can use class() to check which of the columns are quantitative (numeric or integer), and which are qualitative( logical or character). And sapply() function takes in a data frame and a function (in this case class()), apply the class function to each column. Try the following commands:\n\n#apply the class() function to each column of auto data frame\nsapply(auto, class)\n\n\nWhat is the range of each quantitative columns? You can answer this using the range() function. Hint: You can call range() function individually on each column. You can also subset the quantitative columns by creating a variable quant_cols equal to all columns with a numeric mode, then use sapply the function range() with the data frame with only quantitative columns. This is not required.\nUsing the functions mean() and sd(). Find out what is the mean and standard deviation of each quantitative columns?\nNow remove the 10th through 85th observations (rows). What is the range, mean, and standard deviation of each column in the subset of the data that remains? Hint: We’ve seen removing columns in question 8. To remove the rows, we can use the negative sign - again. For example, auto[-c(1,3),] removes the first and third row\nUsing the full data set, investigate the columns graphically, using scatterplots (pairs or plot) or other tools of your choice. Create some plots highlighting the relationships among the columns Comment on your findings.\nSuppose that we wish to predict gas mileage (mpg) on the basis of the other numerical variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.\n\n\n\nISLR Chapter 2 Q10\nThis exercise involves the Boston housing data set.\nTo begin, load in the Boston data set. The Boston data set is part of the MASS library in R. You may need to install the package using install.packages() function if you haven’t done so.\n\n# install.packages(MASS)\nlibrary(MASS)\n\nNow the data set is contained in the object Boston.\n\nBoston\n\nRead about the data set:\n\n?Boston\n\n\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\nMake some pairwise scatterplots of the columns in this data set. Describe your findings. Hint: Use function pairs()\nHow many of the suburbs in this data set bound the Charles river? Hint: Subset the data using a logical vector to check if variable chas==1, then use nrow() to see the number of suburbs.\nUsing median(), find out what is the median pupil-teacher ratio among the towns in this data set?\n\nWell done! You’ve learned how to work with R to read in data and perform some simple analysis and exploration!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/3 Module/mod3.html#lab-3",
    "href": "R files/3 Module/mod3.html#lab-3",
    "title": "Module 3: Vectors and Lists",
    "section": "Lab 3",
    "text": "Lab 3\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\n\n\n\nWarm-up\n\nIn the lecture, we covered c(), :, rep(), seq() among other ways to create vectors.\n\n\ndolly = c(9, 10, 11, 12, 13, 14, 15, 16, 17) \nbees = c(\"b\", \"b\", \"b\", \"b\", \"b\")\n\n\nRecreate dolly using :.\nCreate the same vector using seq().\nRecreate bees using rep().\n\n\nWe are now going to use the functions rnorm() and runif() to initialize vectors.\n\n\nrandom_norm = rnorm(100) \nrandom_unif = runif(1000)\n\n\nHow long are the vectors random_norm and random_unif? Use length() to verify.\nWhat are the largest and smallest values in random_norm and random_unif? Use min() and max().\nUse mean() and sd() to calculate the mean and standard deviation of the two distributions.\nCreate a new vector with 10000 draws from the standard normal distribution.\nrnorm() by default sets mean = 0 (see ?rnorm). Create a vector of 10000 draws from the normal distribution with mean = 1. Use mean() to verify.\n\nNotice the functions min(), max(), mean() and sd() all take a vector with many values and summarize them as one value. These are good to use with summarize() when doing data analysis on simple dataframes.\n\nData Types\n\nUse typeof() to verify the data types of dolly, bees, random_unif\nCoerce dolly to a character vector. Recall we have functions as.<type>() for this kind of coercion.\nTry to coerce bees to type numeric. What does R do when you ask it to turn “b” into a number?\n\n\n\nVectorized Math\n\na and b are vectors of length 10. Look at them in the console.\n\n\na <- 1:10\nb <- rep(c(2, 4), 5)\n\n\nAdd a and b element by element.\nSubtract a and b element by element.\nDivide a by b element by element.\nMultiply a and b element by element.\nRaise the element of a to the power of b element by element.\nMultiply each element of a by 3 then subtract b\nRaise each element of b to the third power.\nTake the square root of each element of a.\n\n\n\nCalculating Mean and Standard Deviation\n\nCalculating the Mean\nIn this exercise, we will calculate the mean of a vector of random numbers. Wewill practice assigning new variables and using functions in R.\nWe can run the following code to create a vector of 1000 random numbers. The function set.seed() ensures that the process used to generate random numbers is the same across computers.\nNote: rf() is a R command we use to generate 1000 random numbers according to the F distribution, and 10 and 100 are parameters that specify how “peaked” the distribution is.\n\nset.seed(1)\nrandom_numbers = rf(1000, 10, 100)\n\nWrite code that gives you the sum of random_numbers and saves it to a new variable called numbers_sum:\nHint: To sum the numbers in a vector, use the sum() function.\nNote: You don’t automatically see the output of numbers_sum when you assign it to a variable. Type numbers_sum into the console and run it to see the value that you assigned it.\nWrite code that gives you the number of items in the random_numbers vector and saves it to a new variable called numbers_count:\nHint: To count the number of items in a vector, use the length() function.\nNow write code that uses the above two variables to calculate the average of random_numbers and assign it to a new variable called this_mean.\nWhat number did you get? It should have been 1.018. If it isn’t, double check your code!\nR actually has a built in function to calculate the mean for you, so you don’t have to remember how to build it from scratch each time! Check your above answer by using the mean() function on the random_numbers vector.\n\n\nCalculating the Standard Deviation\nNow that you’ve got that under your fingers, let’s move on to standard deviation.\nWe will be converting the following formula for calculating the sample standard deviation into code:\n\\(s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2} {n-1}}\\)\nFor this, we’ll review the concept of vectorization. This means that an operation like subtraction will act on all numbers in a vector at the same time.\nSubtract this_mean from the random_numbers vector. Did each number in random_numbers change?\nTry to write the formula for standard deviation in R code using the sqrt(), sum(), and length() functions, along with other operators (^, /, -). Assign it to a new variable called this_sd. Watch out for your parentheses!\nWhat number did you get for this_sd, or the standard deviation of random_numbers? If you didn’t get 0.489704, recheck your code!\nR also has a built in function for standard deviation. Check if you calculated the standard deviation correctly by using the sd() function on the random_numbers vector.\n\n\n\nMaking a Histogram of Our Numbers\nWhat do these random numbers look like, anyway? We can use base plotting in R to visualize the distribution of our random numbers.\nRun the following code to visualize the original distribution of random_numbers as a histogram.\n\nhist(random_numbers)\n\nNotice how most of the values are concentrated on the left-hand side of the graph, while there is a longer “tail” to the right? Counterintuitively, this is known as a right-skewed distribution. When we see a distribution like this, one common thing to do is to normalize it.\nThis is also known as calculating a z-score, which we will cover next.\n\n\nCalculating a Z-Score\nThe formula for calculating a z-score for a single value, or normalizing that value, is as follows:\n\\(z = \\frac{x - \\bar{x}}{s}\\)\nThis can be calculated for each value in random_numbers in context of the larger set of values.\nCan you translate this formula into code?\nUsing random_numbers, this_mean, and this_sd that are already in your environment, write a formula to transform all the values in random_numbers into z-scores, and assign it to the new variable normalized_data.\nHint: R is vectorized, so you can subtract the mean from each random number in random_numbers in a straightforward way.\nTake the mean of normalized_data and assign it to a variable called normalized_mean.\nNote: If you see something that ends in “e-16”, that means that it’s a very small decimal number (16 places to the right of the decimal point), and is essentially 0.\nTake the standard deviation of normalized_data and assign it to a variable called normalized_sd.\nWhat is the value of normalized_mean? What is the value of normalized_sd? You should get a vector that is mean zero and has a standard deviation of one, because the data has been normalized.\n\nMaking a Histogram of Z-scores\nLet’s plot the z-scores and see if our values are still skewed. How does this compare to the histogram of random_numbers? Run the following code:\n\nhist(normalized_data)\n\nIs the resulting data skewed?\n\n\n\n\nCalculating a T-Score\nT-tests are used to determine if two sample means are equal. The formula for calculating a t-score is as follows:\n\\(t = \\frac{\\overline{x}_1 - \\overline{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\)\nwhere \\(\\overline{x}_i\\) is the mean of the first or second set of data, \\(s_i\\) is the sample standard deviation of the first or second set of data, and \\(n_i\\) is the sample size of the \\(i\\)th set of data.\nWe’ll first create two data sets of random numbers following a normal distribution:\n\nset.seed(1)\ndata_1 <- rnorm(1000, 3)\ndata_2 <- rnorm(100, 2)\n\nHere’s how we’ll calculate the mean (x_1), standard deviation (s_1), and sample size (n_1) of the first data set:\n\nx_1 <- mean(data_1)\ns_1 <- sd(data_1)\nn_1 <- length(data_1)\n\nWhat numeric types do you get from doing this? Try running the typeof() function on each of x_1, s_1, and n_1. We have you started with x_1.\n\ntypeof(x_1)\n\n[1] \"double\"\n\n\nWhat object type is n_1?\nCan you calculate the same values for data_2, assigning mean, standard deviation, and length to the variables of x_2, s_2, and n_2, respectively?\nWhat values do you get for x_2 and s_2?\nNow, you should be able to translate the t-score formula (\\(\\frac{\\overline{x}_1 - \\overline{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\)) into code, based on the above calculated values.\nWhat did you get for the t-score? You should have gotten 9.243, if not, double check your code!\nThe t-score’s meaning depends on your sample size, but in general t-scores close to 0 imply that the means are not statistically distinguishable, and large t-scores (e.g. t > 3) imply the data have different means.\n\nPerforming a T-Test\nOnce again, R has a built in function that will perform a T-test for us, aptly named t.test(). Look up the arguments the function t.test() takes, and perform a T-test on data_1 and data_2.\nWhat are the sample means, and are they distinguishable from each other?\nWell done! You’ve learned how to work with R to calculate basic statistics. We’ve had you generate a few by hand, but be sure to use the built-in functions in R in the future.\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/4 Module/mod4.html#examining-wid_data",
    "href": "R files/4 Module/mod4.html#examining-wid_data",
    "title": "Module 4: Data Manipulation",
    "section": "Examining ’wid_data",
    "text": "Examining ’wid_data\n\nLook at the data. What is the main problem here?\nWe don’t have columns headers. The World Inequality Database says the “structure” of the download is as shown in the image below.\n\n\nSo we can create our own header in read_xlsx. Calling the read_xlsx function using readxl::read_xlsx() ensures that we use the read_xlsx() function from the readxl package.\n\nwid_data_raw <- readxl::read_xlsx(\"world_wealth_inequality.xlsx\",\n                                   col_names = c(\"country\", \"indicator\",\n                                                 \"percentile\", \"year\", \"value\"))\n\nNow when we look at the second column. It’s a mess. We can separate it based on where the \\n are and then deal with the data later. Don’t worry about this code right now.\n\nwid_data_raw <- readxl::read_xlsx(\"world_wealth_inequality.xlsx\",\n                                  col_names = c(\"country\", \"indicator\",\n                                                \"percentile\", \"year\", \n                                                \"value\")) %>%\n  separate(indicator, sep = \"\\\\n\", into = c(\"row_tag\", \"type\", \"notes\"))\n\nNote: We want a clean reproducible script so you should just have one block of code reading the data: that last one. The other code were building blocks. If you want to keep “extra” code temporarily in your script you can use # to comment out the code."
  },
  {
    "objectID": "R files/4 Module/mod4.html#manipulating-world-inequality-data-with-dplyr",
    "href": "R files/4 Module/mod4.html#manipulating-world-inequality-data-with-dplyr",
    "title": "Module 4: Data Manipulation",
    "section": "Manipulating World Inequality Data with dplyr",
    "text": "Manipulating World Inequality Data with dplyr\nNow we have some data and are ready to use select(), filter(), mutate(), summarize() and arrange() to explore it.\n\nThe data comes with some redundant columns that add clutter when we examine the data. What dplyr verb let’s you choose what columns to see? Remove the unwanted column row_tag and move notes to the last column position and assign the output to the name wid_data1\nLet’s start to dig into the data. We have two types of data: “Net personal wealth” and “National income”. Start by filter()ing the data so we only have “Net personal wealth” for France, name the resulting data french_data and then run the code below to visualize the data.\n\n1 Hint: You can type all the column names or use the slicker select(-notes, everything())\n# replace each ... with relevant code\nfrench_data <- wid_data %>% filter( ... , ...)\n\nNote: When refering to words in the data, make sure they are in quotes “France”, “Net personal wealth”. When referring to columns, do not use quotes.\n\nfrench_data %>% \n  ggplot(aes(y = value, x = year, color = percentile)) + \n  geom_line()\n\nNow we’re getting somewhere! The plot shows the proportion of national wealth owned by different segements of French society overtime. For example in 2000, the top 1 percent owned roughly 28 percent of the wealth, while the bottom 50 percent owned abouy 7 percent.\n\nExplain the gaps in the plot. Using filter(), look at french_data in the years between 1960 and 1970. Does what you see line up with what you guessed by looking at the graph?\nUsing mutate(), create a new column called perc_national_wealth that equals value multiplied by 100. Adjust the graph code so that the y axis shows perc_national_wealth instead of value.\nNow following the same steps, explore data from the “Russian Federation”.\nThe data for “Russian Federation” does not start in 1900, but our y-axis does. That’s because we have a bunch of NAs. Let’s filter out the NAs and remake the plot. You cannot test for NA using == (Try: NA == NA). Instead we have a function called is.na(). (Try: is.na(NA) and !is.na(NA)).\nUse two dplyr verbs to figure out what year the bottom 50 percent held the least wealth. First, choose the rows that cover the bottom 50 percent and then sort the data in descending order using arrange()2.\n\n2 Hint: Look at the examples in ?arrange\n# replace ... with relevant code\nrussian_data %>% \n  filter(...) %>% \n  arrange(...)\n\n\nFor both the Russian Federation and French data, calculate the average proportion of wealth owned by the top 10 percent over the period from 1995 to 2010. You’ll have to filter and then summarize with summarize().\n\n\n# replace ... with relevant code\nrussian_data %>% \n  filter(...) %>% \n  summarize(top10 = mean(...))"
  },
  {
    "objectID": "R files/4 Module/mod4.html#manipulating-midwest-demographic-data-with-dplyr",
    "href": "R files/4 Module/mod4.html#manipulating-midwest-demographic-data-with-dplyr",
    "title": "Module 4: Data Manipulation",
    "section": "Manipulating Midwest Demographic Data with dplyr",
    "text": "Manipulating Midwest Demographic Data with dplyr\n\nNow we’ll use midwestern demographic data which is at this link. The dataset includes county level data for a single year. We call data this type of data “cross-sectional” since it gives a point-in-time cross-section of the counties of the midwest. (The world inequality data is “timeseries” data).\nSave midwest.dta in your data folder and load it into R.\n\n\nmidwest <- read_dta('midwest.dta')\n\n\nRun the following code to get a sense of what the data looks like:\n\n\nglimpse(midwest)\n\n\nI wanted a tibble called midwest_pop that only had county identifiers and the 9 columns from midwest concerned with population counts. Replicate my work to create midwest_pop on your own3.\n\n3 Hint: notice that all the columns start with the same few letters.\nnames(midwest_pop)\n\n [1] \"county\"          \"state\"           \"poptotal\"        \"popdensity\"     \n [5] \"popwhite\"        \"popblack\"        \"popamerindian\"   \"popasian\"       \n [9] \"popother\"        \"popadults\"       \"poppovertyknown\"\n\n\nHint: I went to ?select and found a selection helper that allowed me to select those 9 columns without typing all their names\n\n# replace ... with relevant code\nmidwest_pop <- midwest %>% select(county, state, ...)\n\n\nFrom midwest_pop calculate the area of each county4. What’s the largest county in the midwest? How about in Illinois?\nFrom midwest_pop calculate percentage adults for each county. What county in the midwest has the highest proportion of adults? What’s county in the midwest has the lowest proportion of adults?\nHow many people live in Michigan?\nNote that together population density and population can give you information about the area (geographic size) of a location. What’s the total area of Illinois? You probably have no idea what the units are though. If you google, you’ll find that it doesn’t align perfectly with online sources. Given the units don’t align with other sources, can this data still be useful?\n\n4 Notice that \\(popdensity = \\frac{poptotal}{area}\\)Well done! You’ve learned how to work with R to perform simple data manipulation and analysis!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/4 Module/mod4.html#lab-4",
    "href": "R files/4 Module/mod4.html#lab-4",
    "title": "Module 4: Data Manipulation",
    "section": "Lab 4",
    "text": "Lab 4\nIn this lab, you will work with 2 data sets (world_wealth_inequality.xlsx and midwest.dta).\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\nWarm-up\n\nWhich of these are commands from dplyr?\n\n\nmutate()\nfilter()\nmean()\n\n\nIn the videos, you learned about head(). What if you wanted to get the tail end of your data instead?\nImagine you have a data set, df with 4 variables, county, year, income, and employment. You only need the year and employment status of people whose income is below $5000. Which two dplyr commands do you need to do this? Can you write the code for this?\nRemember the mean() function? What dplyr commands would we need if we want the average income in counties for the year 2003? Can you write the code for this?\nLoad tidyverse, haven, and readxl in your Rmd. If you haven’t yet, download the data from this page and put the data in your data folder and set your working directory. The data source is the World Inequality Database where you can find data about the distribution of income and wealth in several contries over time. Outside of lab time, check out wid.world for more information.\nIf you followed the set-up from above, you should be able to run the following code with no error.\n\n\nwid_data <- read_xlsx(\"world_wealth_inequality.xlsx\")\n\nExamining ’wid_data\n\nLook at the data. What is the main problem here?\nWe don’t have columns headers. The World Inequality Database says the “structure” of the download is as shown in the image below.\n\n\nSo we can create our own header in read_xlsx. Calling the read_xlsx function using readxl::read_xlsx() ensures that we use the read_xlsx() function from the readxl package.\n\nwid_data_raw <- readxl::read_xlsx(\"world_wealth_inequality.xlsx\",\n                                   col_names = c(\"country\", \"indicator\",\n                                                 \"percentile\", \"year\", \"value\"))\n\nNow when we look at the second column. It’s a mess. We can separate it based on where the \\n are and then deal with the data later. Don’t worry about this code right now.\n\nwid_data_raw <- readxl::read_xlsx(\"world_wealth_inequality.xlsx\",\n                                  col_names = c(\"country\", \"indicator\",\n                                                \"percentile\", \"year\", \n                                                \"value\")) %>%\n  separate(indicator, sep = \"\\\\n\", into = c(\"row_tag\", \"type\", \"notes\"))\n\nNote: We want a clean reproducible script so you should just have one block of code reading the data: that last one. The other code were building blocks. If you want to keep “extra” code temporarily in your script you can use # to comment out the code.\nManipulating World Inequality Data with dplyr\n\nNow we have some data and are ready to use select(), filter(), mutate(), summarize() and arrange() to explore it.\n\nThe data comes with some redundant columns that add clutter when we examine the data. What dplyr verb let’s you choose what columns to see? Remove the unwanted column row_tag and move notes to the last column position and assign the output to the name wid_data1\nLet’s start to dig into the data. We have two types of data: “Net personal wealth” and “National income”. Start by filter()ing the data so we only have “Net personal wealth” for France, name the resulting data french_data and then run the code below to visualize the data.\n\n1 Hint: You can type all the column names or use the slicker select(-notes, everything())\n# replace each ... with relevant code\nfrench_data <- wid_data %>% filter( ... , ...)\n\nNote: When refering to words in the data, make sure they are in quotes “France”, “Net personal wealth”. When referring to columns, do not use quotes.\n\nfrench_data %>% \n  ggplot(aes(y = value, x = year, color = percentile)) + \n  geom_line()\n\nNow we’re getting somewhere! The plot shows the proportion of national wealth owned by different segements of French society overtime. For example in 2000, the top 1 percent owned roughly 28 percent of the wealth, while the bottom 50 percent owned abouy 7 percent.\n\nExplain the gaps in the plot. Using filter(), look at french_data in the years between 1960 and 1970. Does what you see line up with what you guessed by looking at the graph?\nUsing mutate(), create a new column called perc_national_wealth that equals value multiplied by 100. Adjust the graph code so that the y axis shows perc_national_wealth instead of value.\nNow following the same steps, explore data from the “Russian Federation”.\nThe data for “Russian Federation” does not start in 1900, but our y-axis does. That’s because we have a bunch of NAs. Let’s filter out the NAs and remake the plot. You cannot test for NA using == (Try: NA == NA). Instead we have a function called is.na(). (Try: is.na(NA) and !is.na(NA)).\nUse two dplyr verbs to figure out what year the bottom 50 percent held the least wealth. First, choose the rows that cover the bottom 50 percent and then sort the data in descending order using arrange()2.\n\n2 Hint: Look at the examples in ?arrange\n# replace ... with relevant code\nrussian_data %>% \n  filter(...) %>% \n  arrange(...)\n\n\nFor both the Russian Federation and French data, calculate the average proportion of wealth owned by the top 10 percent over the period from 1995 to 2010. You’ll have to filter and then summarize with summarize().\n\n\n# replace ... with relevant code\nrussian_data %>% \n  filter(...) %>% \n  summarize(top10 = mean(...))\n\nManipulating Midwest Demographic Data with dplyr\n\n\nNow we’ll use midwestern demographic data which is at this link. The dataset includes county level data for a single year. We call data this type of data “cross-sectional” since it gives a point-in-time cross-section of the counties of the midwest. (The world inequality data is “timeseries” data).\nSave midwest.dta in your data folder and load it into R.\n\n\nmidwest <- read_dta('midwest.dta')\n\n\nRun the following code to get a sense of what the data looks like:\n\n\nglimpse(midwest)\n\n\nI wanted a tibble called midwest_pop that only had county identifiers and the 9 columns from midwest concerned with population counts. Replicate my work to create midwest_pop on your own3.\n\n3 Hint: notice that all the columns start with the same few letters.\nnames(midwest_pop)\n\n [1] \"county\"          \"state\"           \"poptotal\"        \"popdensity\"     \n [5] \"popwhite\"        \"popblack\"        \"popamerindian\"   \"popasian\"       \n [9] \"popother\"        \"popadults\"       \"poppovertyknown\"\n\n\nHint: I went to ?select and found a selection helper that allowed me to select those 9 columns without typing all their names\n\n# replace ... with relevant code\nmidwest_pop <- midwest %>% select(county, state, ...)\n\n\nFrom midwest_pop calculate the area of each county4. What’s the largest county in the midwest? How about in Illinois?\nFrom midwest_pop calculate percentage adults for each county. What county in the midwest has the highest proportion of adults? What’s county in the midwest has the lowest proportion of adults?\nHow many people live in Michigan?\nNote that together population density and population can give you information about the area (geographic size) of a location. What’s the total area of Illinois? You probably have no idea what the units are though. If you google, you’ll find that it doesn’t align perfectly with online sources. Given the units don’t align with other sources, can this data still be useful?\n\n4 Notice that \\(popdensity = \\frac{poptotal}{area}\\)Well done! You’ve learned how to work with R to perform simple data manipulation and analysis!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/5 Module/mod5.html#lab-5",
    "href": "R files/5 Module/mod5.html#lab-5",
    "title": "Module 5: Data Manipulation and Analysis II",
    "section": "Lab 5",
    "text": "Lab 5\nIn this lab, you will work with data sets from recent_college_grads.dta.\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\n\n\n\nWarm up\n\nData wrangling and visualization with college data\n\nWe will explore data on college majors and earnings, specifically the data behind the FiveThirtyEight story “The Economic Guide To Picking A College Major”.\nWe read it in with the read_dta function, and save the result as a new data frame called college_recent_grads. Because read_dta is a function from haven, we will need to load that package.\n\nlibrary(tidyverse)\nlibrary(haven)\ncollege_recent_grads <- read_dta('recent_college_grads.dta')\n\ncollege_recent_grads is a tidy data frame, with each row representing an observation and each column representing a variable.\nTo view the data, you can take a quick peek at your data frame and view its dimensions with the glimpse function.\n\nglimpse(college_recent_grads)\n\nThe description of the variables, i.e. the codebook, is given below.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nrank\nRank by median earnings\n\n\nmajor_code\nMajor code, FO1DP in ACS PUMS\n\n\nmajor\nMajor description\n\n\nmajor_category\nCategory of major from Carnevale et al\n\n\ntotal\nTotal number of people with major\n\n\nsample_size\nSample size (unweighted) of full-time, year-round ONLY (used for earnings)\n\n\nmen\nMale graduates\n\n\nwomen\nFemale graduates\n\n\nsharewomen\nWomen as share of total\n\n\nemployed\nNumber employed (ESR == 1 or 2)\n\n\nemployed_full_time\nEmployed 35 hours or more\n\n\nemployed_part_time\nEmployed less than 35 hours\n\n\nemployed_full_time_yearround\nEmployed at least 50 weeks (WKW == 1) and at least 35 hours (WKHP >= 35)\n\n\nunemployed\nNumber unemployed (ESR == 3)\n\n\nunemployment_rate\nUnemployed / (Unemployed + Employed)\n\n\nmedian\nMedian earnings of full-time, year-round workers\n\n\np25th\n25th percentile of earnigns\n\n\np75th\n75th percentile of earnings\n\n\ncollege_jobs\nNumber with job requiring a college degree\n\n\nnon_college_jobs\nNumber with job not requiring a college degree\n\n\nlow_wage_jobs\nNumber in low-wage service jobs\n\n\n\n\n\nWhich major has the lowest unemployment rate?\nIn order to answer this question all we need to do is sort the data. We use the arrange function to do this, and sort it by the unemployment_rate variable. By default arrange sorts in ascending order, which is what we want here – we’re interested in the major with the lowest unemployment rate.\n\ncollege_recent_grads %>%\n  arrange(unemployment_rate)\n\n# A tibble: 173 × 21\n    rank major…¹ major major…² total sampl…³ men   women share…⁴ emplo…⁵ emplo…⁶\n   <dbl>   <dbl> <chr> <chr>   <chr>   <dbl> <chr> <chr> <chr>     <dbl>   <dbl>\n 1    53    4005 Math… Comput… 609         7 500   109   0.1789…     559     584\n 2    74    3801 Mili… Indust… 124         4 124   0     0             0     111\n 3    84    3602 Bota… Biolog… 1329        9 626   703   0.5289…    1010     946\n 4   113    1106 Soil… Agricu… 685         4 476   209   0.3051…     613     488\n 5   121    2301 Educ… Educat… 804         5 280   524   0.6517…     703     733\n 6    15    2409 Engi… Engine… 4321       30 3526  795   0.1839…    3608    2999\n 7    20    3201 Cour… Law & … 1148       14 877   271   0.2360…     930     808\n 8   120    2305 Math… Educat… 14237     123 3872  10365 0.7280…   13115   11259\n 9     1    2419 Petr… Engine… 2339       36 2057  282   0.1205…    1976    1849\n10    65    1100 Gene… Agricu… 10399     158 6053  4346  0.4179…    8884    7589\n# … with 163 more rows, 10 more variables: employed_parttime <dbl>,\n#   employed_fulltime_yearround <dbl>, unemployed <dbl>,\n#   unemployment_rate <dbl>, p25th <dbl>, median <dbl>, p75th <dbl>,\n#   college_jobs <dbl>, non_college_jobs <dbl>, low_wage_jobs <dbl>, and\n#   abbreviated variable names ¹​major_code, ²​major_category, ³​sample_size,\n#   ⁴​sharewomen, ⁵​employed, ⁶​employed_fulltime\n\n\nThis gives us what we wanted, but not in an ideal form. First, the name of the major barely fits on the page. Second, some of the variables are not that useful (e.g. major_code, major_category) and some we might want front and center are not easily viewed (e.g. unemployment_rate).\nWe can use the select function to choose which variables to display, and in which order:\n\ncollege_recent_grads %>%\n  arrange(unemployment_rate) %>%\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   <dbl> <chr>                                                  <dbl>\n 1    53 Mathematics And Computer Science                     0      \n 2    74 Military Technologies                                0      \n 3    84 Botany                                               0      \n 4   113 Soil Science                                         0      \n 5   121 Educational Administration And Supervision           0      \n 6    15 Engineering Mechanics Physics And Science            0.00633\n 7    20 Court Reporting                                      0.0117 \n 8   120 Mathematics Teacher Education                        0.0162 \n 9     1 Petroleum Engineering                                0.0184 \n10    65 General Agriculture                                  0.0196 \n# … with 163 more rows\n\n\nOk, this is looking better, but do we really need all those decimal places in the unemployment variable? Not really!\n\n1a. Round unemployment_rate: We create a new variable with the mutate function. In this case, we’re overwriting the existing unemployment_rate variable, by rounding it to 1 decimal places. Incomplete code is given below to guide you in the right direction, however you will need to fill in the blanks.\n\n\ncollege_recent_grads<- college_recent_grads %>%\n  arrange(unemployment_rate) %>%\n  select(rank, major, unemployment_rate) %>%\n  mutate(unemployment_rate = ___(___, 1))\n\nWhile were making some changes, let’s change sharewomen to numeric (it appears to be a string). Make sure to save your changes by overwriting the existing data frame!\n\ncollege_recent_grads <- college_recent_grads %>%\n  mutate(sharewomen = as.numeric(___))\n\n\n\nWhich major has the highest percentage of women?\nTo answer such a question we need to arrange the data in descending order. For example, if earlier we were interested in the major with the highest unemployment rate, we would use the following:\nThe desc function specifies that we want unemployment_rate in descending order.\n\ncollege_recent_grads %>%\n  arrange(desc(unemployment_rate)) %>%\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   <dbl> <chr>                                                  <dbl>\n 1     6 Nuclear Engineering                                    0.177\n 2    90 Public Administration                                  0.159\n 3    85 Computer Networking And Telecommunications             0.152\n 4   171 Clinical Psychology                                    0.149\n 5    30 Public Policy                                          0.128\n 6   106 Communication Technologies                             0.120\n 7     2 Mining And Mineral Engineering                         0.117\n 8    54 Computer Programming And Data Processing               0.114\n 9    80 Geography                                              0.113\n10    59 Architecture                                           0.113\n# … with 163 more rows\n\n\n\n1b. Using what you’ve learned so far, arrange the data in descending order with respect to proportion of women in a major, and display only the major, the total number of people with major, and proportion of women. Show only the top 3 majors by adding head(3) at the end of the pipeline.\n\n\n#code here\n\n\n\nHow do the distributions of median income compare across major categories?\nA percentile is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value below which 20% of the observations may be found. (Source: Wikipedia\nThere are three types of incomes reported in this data frame: p25th, median, and p75th. These correspond to the 25th, 50th, and 75th percentiles of the income distribution of sampled individuals for a given major.\nThe question we want to answer “How do the distributions of median income compare across major categories?”. We need to do a few things to answer this question: First, we need to group the data by major_category. Then, we need a way to summarize the distributions of median income within these groups. This decision will depend on the shapes of these distributions. So first, we need to visualize the data.\n\n1c.Let’s start simple and take a look at the distribution of all median incomes using geom_histogram, without considering the major categories.\n\n\nggplot(data = ____,\n       mapping = aes(x = median)) +\n  geom_histogram()\n\n\n1d. Try binwidths of \\(1000\\) and \\(5000\\) and choose one. Explain your reasoning for your choice.\n\n\nggplot(data = ___,\n       mapping = aes(x = median)) +\n  geom_histogram(binwidth = ___)\n\nWe can also calculate summary statistics for this distribution using the summarise function:\n\ncollege_recent_grads %>%\n  summarise(min = min(median), max = max(median),\n            mean = mean(median), med = median(median),\n            sd = sd(median), \n            q1 = quantile(median, probs = 0.25),\n            q3 = quantile(median, probs = 0.75))\n\n# A tibble: 1 × 7\n    min    max   mean   med     sd    q1    q3\n  <dbl>  <dbl>  <dbl> <dbl>  <dbl> <dbl> <dbl>\n1 22000 110000 40151. 36000 11470. 33000 45000\n\n\n\n1e. Based on the shape of the histogram you created in the previous 1e, determine which of these summary statistics above (min, max, mean, med, sd, q1, q3) is/are useful for describing the distribution. Write up your description and include the summary statistic output as well.You can pick single/multiple statistics and briefly explain why you pick it/them.\n1f. Next, we facet the plot by major category. Plot the distribution of median income using a histogram, faceted by major_category. Use the binwidth you chose in 1e.\n\n\nggplot(data = ___,\n       mapping = aes (x=median)) +\n  geom_histogram(bindwidth = 5000) +\n  facet_wrap(.~major_category)\n\n\n1g. Use filter to find out which major has the highest median income? lowest? Which major has the median() median income? Hint: refer to the statistics in 1d.\n\n\ncollege_recent_grads %>%\n  ____(median == ____) \n\n\n1h. Which major category is the most popular in this sample? To answer this question we use a new function called count, which first groups the data , then counts the number of observations in each category and store the counts into a column named n. Add to the pipeline appropriately to arrange the results so that the major with the highest observations is on top.\n\n\ncollege_recent_grads %>%\n  count(major_category) %>% \n  ___(___(n))\n\n\n\nWhat types of majors do women tend to major in?\nFirst, let’s create a new vector called stem_categories that lists the major categories that are considered STEM fields.\n\nstem_categories <- c(\"Biology & Life Science\",\n                     \"Computers & Mathematics\",\n                     \"Engineering\",\n                     \"Physical Sciences\")\n\nThen, we can use this to create a new variable in our data frame indicating whether a major is STEM or not.\n\ncollege_recent_grads <- college_recent_grads %>%\n  mutate(major_type = ifelse(major_category %in% \n                               stem_categories, \"stem\", \"not stem\"))\n\nLet’s unpack this: with mutate we create a new variable called major_type, which is defined as \"stem\" if the major_category is in the nector called stem_categories we created earlier, and as \"not stem\" otherwise.\n\n1i. Create a scatterplot of median income vs. proportion of women in that major, colored by whether the major is in a STEM field or not. Describe the association between these three variables.\n\n\nggplot(data = ___, \n       mapping = aes(x=median, \n           y= sharewomen, \n           color=major_type)) + \n  geom_point()\n\n\n1j.. We can use the logical operators to also filter our data for STEM majors whose median earnings is less than median for all majors’s median earnings, which we found to be $36,000 earlier. Your output should only show the major name and median, 25th percentile, and 75th percentile earning for that major and should be sorted such that the major with the lowest median earning is on top.\n\n\n#code here\n\nWell done! You’ve learned how to work with R to perform basic data analysis!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/6 Module/mod6.html#lab-6",
    "href": "R files/6 Module/mod6.html#lab-6",
    "title": "Module 6: Data Visualization as a Tool for Analysis",
    "section": "Lab 6",
    "text": "Lab 6\nIn this lab, you will work with midwest.dta.\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\n\n\n\nQuestions\nRecall ggplot works by mapping data to aesthetics and then telling ggplot how to visualize the aesthetic with geoms. Like so:\n\nmidwest %&gt;%\n  ggplot(aes(x = percollege, \n             y = percbelowpoverty,\n             color = state,\n             size = poptotal,\n             alpha = percpovertyknown)) + \n  geom_point() + facet_wrap(vars(state))\n\n\n\n\n\nWhich is more highly correlated with poverty at the county level, college completion rates or high school completion rates? Is it consistent across states? Change one line of code in the above graph.\n\n\n\ngeoms\nFor the following, write code to reproduce each plot using midwest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice here inmetro is numeric, but I want it to behave like a discrete variable so I use x = as.character(inmetro). Use labs(title = \"Asian population by metro status\") to create the title.\n\n\n\n\n\n\n\nUse geom_boxplot() instead of geom_point() for “Asian population by metro status”\nUse geom_jitter() instead of geom_point() for “Asian population by metro status”\nUse geom_jitter() and geom_boxplot() at the same time for “Asian population by metro status”. Does order matter?\nHistograms are used to visualize distributions. What happens when you change the bins argument? What happens if you leave the bins argument off?\n\n\nmidwest %&gt;%\n  ggplot(aes(x = perchsd)) +\n  geom_histogram(bins = 100) +\n  labs(title = \"distribution of county-level hs completion rate\")\n\n\n\n\n\nRemake “distribution of county-level hs completion rate” with geom_density() instead of geom_histogram().\nAdd a vertical line at the median perchsd using geom_vline. You can calculate the median directly in the ggplot code.\n\n\nAesthetics\nFor the following, write code to reproduce each plot using midwest\n\nUse x, y, color and size\n\n\n\n\n\n\n\nUse x, y, color and size\n\n\n\n\n\n\n\nWhen making bar graphs, color only changes the outline of the bar. Change the aestethic name to fill to get the desired result\n\n\nmidwest %&gt;% \n  count(state) %&gt;% \n  ggplot(aes(x = state,\n             y = n, \n             color = state)) + \n  geom_col()\n\n\n\n\n\nThere’s a geom called geom_bar that takes a dataset and calculates the count. Read the following code and compare it to the geom_col code above. Describe how geom_bar() is different than geom_col\n\n\nmidwest %&gt;% \n  ggplot(aes(x = state,\n             color = state)) +\n  geom_bar()\n\n\n\n\nWell done! You’ve learned how to work with R to create some awesome looking visuals!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/7 Module/mod7.html#lab-7",
    "href": "R files/7 Module/mod7.html#lab-7",
    "title": "Module 7: Grouped Analysis",
    "section": "Lab 7",
    "text": "Lab 7\nIn this lab, you will work with data_traffic.csv.\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\n\n\n\nBackground and Data\nRead the background and data section before lab.\n\nFollow the tweet thread and you’ll see how Professor Damon Jones, of Harris, prepares and analyzes his data. In this lab, you’re going to follow his lead and dig into traffic stop data from the University of Chicago Police Department, one of the largest private police forces in the world.\n\n\nWarm-up\n\nOpen a new Rmd and save it in your coding lab folder; if you downloaded the data, move your data file to your preferred data location.\nIn your Rmd, write code to load your packages. If you load packages in the console, you will get an error when you knit because knitting starts a fresh R session.\nLoad data_traffic.csv and assign it to the name traffic_data. This data was scrapped from the UCPD website and partially cleaned by Prof. Jones.\nRecall that group_by() operates silently. Below I create a new data frame called grouped_data.\n\n\ngrouped_data <- traffic_data %>%\n  group_by(Race, Gender)\n\n\n\nHow can you tell grouped_data is different from traffic_data?\n\n\nHow many groups (Race-Gender pairs) are in the data? (This information should be available without writing additional code!)\n\n\nWithout running the code, predict the dimensions (number of rows by number of columns) of the tibbles created by traffic_data %>% summarize(n = n()) and grouped_data %>% summarize(n = n()).\n\n\nNow check you intuition by running the code.\n\n\n\nUse group_by() and summarize() to recreate the following table.\n\n\n\n# A tibble: 6 × 2\n  Race                                       n\n  <chr>                                  <int>\n1 African American                        3278\n2 American Indian/Alaskan Native            12\n3 Asian                                    226\n4 Caucasian                                741\n5 Hispanic                                 217\n6 Native Hawaiian/Other Pacific Islander     4\n\n\n\nUse count() to produce the same table.\n\n\n\nMoving beyond counts\n\nRaw counts are okay, but frequencies (or proportions) are easier to compare across data sets. Add a column with frequencies and assign the new tibble to the name traffic_stop_freq. The result should be identical to Prof. Jones’s analysis on twitter.\n\nTry on your own first. If you’re not sure how to add a frequency though, you could google “add a proportion to count with tidyverse” and find this stackoverflow post. Follow the advice of the number one answer. The green checkmark and large number of upvotes indicate the answer is likely reliable.\n\nThe frequencies out of context are not super insightful. What additional information do we need to argue the police are disproportionately stopping members of a certain group? (Hint: Prof. Jones shares the information in his tweets.)\nFor the problem above, your groupmate tried the following code. Explain why the frequencies are all 1.\n\n\ntraffic_stop_freq_bad <- traffic_data %>%\n  group_by(Race) %>% \n  summarize(n = n(),\n            freq = n / sum(n)) \n\ntraffic_stop_freq_bad\n\nNow we want to go a step further. Do outcomes differ by race? In the first code block below, I provide code so you can visualize disposition by race. “Disposition” is police jargon that means the current status or final outcome of a police interaction.\n\ncitation_strings <- c(\"citation issued\", \"citations issued\",\n                      \"citation issued\" )\n\narrest_strings <- c(\"citation issued, arrested on active warrant\",\n                    \"citation issued; arrested on warrant\",\n                    \"arrested by cpd\", \"arrested on warrant\",\n                    \"arrested\",\"arrest\")\n\ndisposition_by_race <- traffic_data %>%\n  mutate(Disposition = str_to_lower(Disposition),\n         Disposition = case_when(Disposition %in% citation_strings ~ \"citation\",\n                                 Disposition %in% arrest_strings ~ \"arrest\",\n                                 TRUE ~ Disposition)) %>%\n  count(Race, Disposition) %>% group_by(Race) %>%\n  mutate(freq = round(n / sum(n), 3))\n\ndisposition_by_race %>%\n  filter(n > 5, Disposition == \"citation\") %>%\n  ggplot(aes(y = freq, x = Race)) + geom_col() +\n  labs(y = \"Citation Rate Once Stopped\", x = \"\", title = \"Traffic Citation Rate\") +\n  theme_minimal()\n\n\n\n\nLet’s break down how we got to this code. First, I ran traffic_data %>% count(Race, Disposition) and noticed that we have a lot of variety in how officers enter information into the system. I knew I could deal with some of the issue by standardizing capitalization.\n\n\nIn the console, try out str_to_lower(...) by replacing the … with different strings. The name may be clear enough, but what does str_to_lower() do?\n\n\nAfter using mutate with str_to_lower(), I piped into count() again and looked for strings that represent the same Disposition. I stored terms in character vectors (e.g. citation_strings). The purpose is to make the case_when() easier to code and read. Once I got that right, I added frequencies to finalize disposition_by_race.\n\nTo make the graph, I first tried to get all the disposition data on the same plot.\n\n\n disposition_by_race %>%\n  ggplot(aes(y = freq, x = Race, fill = Disposition)) + \n  geom_col()\n\n\n\n\nBy default, the bar graph is stacked. Look at the resulting graph and discuss the pros and cons of this plot.\n\nI decided I would focus on citations only and added the filter(n > 5, Disposition == \"citation\") to the code. What is the impact of filtering based on n > 5? Would you make the same choice? This question doesn’t have a “right” answer. You should try different options and reflect.\nNow, you can create a similar plot based called “Search Rate” using the Search variable. Write code to re-produce this plot.\n\n\n\n\n\n\nWell done! You’ve learned how to conduct grouped analysis using real-world data!\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/8 Module/mod8.html#lab-8",
    "href": "R files/8 Module/mod8.html#lab-8",
    "title": "Module 8: Iteration",
    "section": "Lab 8",
    "text": "Lab 8\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\nWarm-up\nRecall, for-loops are an iterator that help us repeat tasks while changing inputs. The most common structure for your code will look like the following code. This can be simplified if you are not storing results.\n\n# what are you iterating over? The vector from -10:10\nitems_to_iterate_over <- c(-10:10) \n\n# pre-allocate the results\nout <- rep(0, length(items_to_iterate_over))\n# write the iteration statement --\n# we'll use indices so we can store the output easily \nfor (i in seq_along(items_to_iterate_over)) {\n# do something\n# we capture the median of three random numbers\n# from normal distributions various means\n    out[[i]] <- median(rnorm(n = 3,mean = items_to_iterate_over[[i]]))\n    }\n\nWriting for-loops\n\nWrite a for-loop that prints the numbers 5, 10, 15, 20, 250000.\nWrite a for-loop that iterates over the indices of x and prints the ith value of x.\n\n\nx <- c(5, 10, 15, 20, 250000)\n# replace the ... with the relevant code\n\nfor (i in ... ){ \n  print(x[[...]])\n  }\n\n\nWrite a for-loop that simplifies the following code so that you don’t repeat yourself! Don’t worry about storing the output yet. Use print() so that you can see the output. What happens if you don’t use print()?\n\n\nsd(rnorm(5))\nsd(rnorm(10))\nsd(rnorm(15))\nsd(rnorm(20)) \nsd(rnorm(25000))\n\n\nadjust your for-loop to see how the sd changes when you use rnorm(n, mean = 4)\n\nadjust your for-loop to see how the sd changes when you use rnorm(n, sd = 4)\n\n\n\nNow store the results of your for-loop above in a vector. Pre-allocate a vector of length 5 to capture the standard deviations.\nvectorization vs for loops\nRecall, vectorized functions operate on a vector item by item. It’s like looping over the vector! The following for-loop is better written vectorized.\nCompare the loop version\n\nnames <- c(\"Marlene\", \"Jacob\", \"Buddy\")\nout <- character(length(names))\n\nfor (i in seq_along(names)) {\n  out[[i]] <- paste0(\"Welcome \", names[[i]])\n  }\n\nto the vectorized version\n\nnames <- c(\"Marlene\", \"Jacob\", \"Buddy\") \nout <- paste0(\"Welcome \", names)\n\nThe vectorized code is preferred because it is easier to write and read, and is possibly more efficient.\n\nRewrite your first for-loop, where you printed 5, 10, 15, 20, 250000 as vectorised code\nRewrite this for-loop as vectorized code:\n\n\nradii <- c(0:10)\n\narea <- double(length(radii)) \n\nfor (i in seq_along(radii)) { \n  area[[i]] <- pi * radii[[i]] ^ 2 \n  }\n\n\nRewrite this for-loop as vectorized code:\n\n\nradii <- c(-1:10)\n\narea <- double(length(radii))\n\nfor (i in seq_along(radii)) { \n  if (radii[[i]] < 0) { \n    area[[i]] <- NaN \n  } else {\n      area[[i]] <- pi * radii[[i]] ^ 2 }\n  }\n\nExtension\nSimulating the Law of Large Numbers\nThe Law of Large Numbers says that as sample sizes increase, the mean of the sample will approach the true mean of the distribution. We are going to simulate this phenomenon!\nWe’ll start by making a vector of sample sizes from 1 to 50, to represent increasing sample sizes.\nCreate a vector called sample_sizes that is made up of the numbers 1 through 50. (Hint: You can use seq() or : notation).\nWe’ll make an empty tibble to store the results of the for loop:\n\nestimates <- tibble(n = integer(), sample_mean = double())\n\nWrite a loop over the sample_sizes you specified above. In the loop, for each sample size you will:\n\nCalculate the mean of a sample from the random normal distribution with mean = 0 and sd = 5.\nMake an intermediate tibble to store the results\nAppend the intermediate tibble to your tibble using bind_rows().\n\n\nset.seed(60637) \nfor (___ in ___) {\n  # Calculate the mean of a sample from the random normal distribution with mean = 0 and s\n  ___ <- ___\n  # Make a tibble with your estimates\n  this_estimate <- tibble(n = ___, sample_mean = ___) \n  # Append the new rows to your tibble\n  ___ <- bind_rows(estimates, ___)\n  }\n\nWe can use ggplot2 to view the results. Fill in the correct information for the data and x and y variables, so that the n column of the estimates tibble is plotted on the x-axis, while the sample_mean column of the estimates tibble is plotted on the y-axis.\n\n# your data goes in the first position\n___ %>%\n  ggplot(aes(x = ___, y = ___)) + \n  geom_line()\n\n\nAs the sample size (n) increases, does the sample mean becomes closer to 0, or farther away from 0?\n\nRewrite the loop code without looking at your previous code and use a wider range of sample sizes. Try several different sample size combinations. What happens when you increase the sample size to 100? 500? 1000? Use the seq() function to generate a sensibly spaced sequence.\n\nset.seed(60637) \nsample_sizes <- ___ \nestimates_larger_n <- ___\n\nfor (___ in ___) {\n  ___ <- ___\n  ___ <- ___\n  ___ <- ___\n}\n\n___ %>%\n  ggplot(___(___ = ___, ___ = ___)) +\n  geom_line()\n\n\nHow does this compare to before?\nExtending Our Simulation\nLooking at your results, you might think a small sample size is sufficient for estimating a mean, but your data had a relatively small standard deviation compared to the mean. Let’s run the same simulation as before with different standard deviations.\nDo the following:\n\nCreate a vector called population_sd of length 4 with values 1, 5, 10, and 20 (you’re welcome to add larger numbers if you wish).\nMake an empty tibble to store the output. Compared to before, this has an extra column for the changing population standard deviations.\nWrite a loop inside a loop over population_sd and then sample_sizes.\nThen, make a ggplot graph where the x and y axes are the same, but we facet (aka we create small multiples of individual graphs) on population_sd.\n\n\nset.seed(60637)\npopulation_sd <- ___\n# use what every you came up with in the previous part \nsample_sizes <- ___\nestimates_adjust_sd <- ___\nfor (___ in ___){ \n  for (___ in ___) {\n    ___ <- ___\n    ___ <- ___\n    ___ <- ___\n  } \n}\n\n___ %>%\n  ggplot(___) +\n  geom_line() + \n  facet_wrap(~population_sd) + \n  theme_minimal()\n\nHow do these estimates differ as you increase the standard deviation?\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "R files/9 Module/mod9.html#lab-9",
    "href": "R files/9 Module/mod9.html#lab-9",
    "title": "Module 9: Writing Functions",
    "section": "Lab 9",
    "text": "Lab 9\n\nGeneral Guidelines:\nYou will encounter a few functions we did not cover in the lecture video. This will give you some practice on how to use a new function for the first time. You can try following steps:\n\nStart by typing ?new_function in your Console to open up the help page\nRead the help page of this new_function. The description might be too technical for now. That’s OK. Pay attention to the Usage and Arguments, especially the argument x or x,y (when two arguments are required)\nAt the bottom of the help page, there are a few examples. Run the first few lines to see how it works\nApply it in your lab questions\n\nIt is highly likely that you will encounter error messages while doing this lab Here are a few steps that might help get you through it.\n\nLocate which line is causing this error first\nCheck if you may have a typo in the code. Sometimes another person can spot a typo faster than you.\nIf you enter the code without any typo, try googling the error message\nScroll through the top few links see if any of them helps\nTry working on the next few questions while waiting for answers by TAs\n\n\n\nQuestions\nRecall a function has the following form\n\nname <- function(args) { \n  # body\n  do something (probably with args) \n  }\n\n\nWrite a function called calc_quadratic that takes an input x and calculates \\(f(x) = x^2 + 2x + 1\\). For example:\n\n\ncalc_quadratic(5)\n\n[1] 36\n\n\n\nWhat are the arguments to your function? What is the body of the function?\nThis function is vectorized! (Since binary operators are vectorized). Show this is true by running calc_quadratic with an input vector that is -10 to 10.\n\n\nYou realize you want to be able to work with any quadratic. Update your functions so that it can work with any quadratic in standard form \\(f(x) = ax^2 + bx + c\\).\n\n\nYour new function will take arguments x, a, b and c.\nSet the default arguments to a=1, b=2 and c=1.\n\n\nWrite a function called solve_quadratic that takes arguments a, b and c and provides the two roots using the quadratic formula.\n\nIn our outline, we suggest you:\n\nCalculate the determinant \\(\\sqrt{b^2 − 4ac}\\) and store as an intermediate value.\nReturn two values by putting them in a vector. If you stored the roots as root_1 and root_2, then the final line of code in the function should be c(root_1, root_2) or, if you prefer, return(c(root_1, root_2)).\n\n\n# fill in the ... with appropriate code\nsolve_quadratic <- function(...){\n  determinant <- ...\n  root_1 <- ...\n  root_2 <- ...\n  c(root_1, root_2)\n}\n\nThe code should work as follows:\n\nsolve_quadratic(a = -4, b = 0, c = 1)\n\n[1] -0.5  0.5\n\n\n\nWe “normalize” a variable by subtracting the mean and dividing by the standard deviation \\(\\frac{x - \\mu}{\\sigma}\\) .\n\nWrite a function called normalize that takes a vector as input and normalizes it. You should get the following output.\n\nnormalize(1:5)\n\n[1] -1.2649111 -0.6324555  0.0000000  0.6324555  1.2649111\n\n\n\nWhat output do you get when the input vector is 0:4? How about -100:-96? Why?\nWhat happens when your input vector is c(1,2,3,4,5, NA)? Rewrite the function so the result is:\nThe txhousing data set comes with ggplot. Use your normalize function in mutate to create normalized_annual_volume to make the following graph.\n\n\n# replace the ... with the appropriate code.\ntxhousing %>%\n  group_by(year, city) %>%\n  summarize(annual_volume = sum(volume, na.rm = TRUE)) %>% \n  group_by(year) %>%\n  mutate(...) %>%\n  ggplot(aes(x = year, y = normalized_annual_volume)) + \n  geom_point() +\n  geom_line(aes(color = city))\n\n\n\n\n\n\n\n\nSimulating Data with Monte Carlo Simulations (Extension)\nYou will be asked to investigate statistical concepts using Monte Carlo simulations. We’ll try not to get too technical in the main body of the lab. There are some “technical notes” which you can ignore!\nIn a monte carlo simulation, you repeatedly:\n\nGenerate random samples of data using a known process.\nMake calculations based on the random sample.\nAggregate the results.\n\nFunctions and loops help us do these repetitious acts efficiently, without repeatedly writing similar code or copying and pasting.\nToday’s problem: Let us investigate how good the random number generator in R is.1 We hypothesize that rnorm(n, mean = true_mean) provides random sample of size n from the normal distribution with mean = true_mean and standard deviation = 1.\nThe lesson is organized as follows.\n\nWe do a single simulation.\nWe take the logic of the simulation, encapsulate it in functions and then run 1000s of simulations!\n\n\nA single simulation\nRecall our hypothesis is that rnorm() faithfully gives us random numbers from the normal distribution. If we test this with a single random draw, we might be misled. For example, let’s draw 30 numbers from a normal distribution with true mean of 0.5 and see if the observed mean appears statistically different from the true mean.\n\n# Setting a seed ensures replicability\nset.seed(4)\n# we set our parameters\ntrue_mean <- .5\nN <- 30\n# We simulate and observe outcomes\nsimulated_data <- rnorm(N, mean = true_mean) # the standard deviation is 1 by default!\nobs_mean <- mean(simulated_data)\nobs_mean\n\n[1] 0.9871873\n\n\nWow! The observed mean is twice what we expected given true_mean! Let’s calculate a z-score to put that in perspective. (Focus on the formulas, you’ll learn the intuition in stats class)\nA z-score is calculated \\(\\frac{\\bar{X}-\\mu}{\\frac{s_n}{\\sqrt{N}}}\\) where \\(\\bar{X}\\) is the sample mean, \\(\\mu\\) is the true mean, \\(s_n\\) is the sample standard deviation and \\(N\\) is the number of observations.\n\nobs_sd <- sd(simulated_data)\nzscore <- (obs_mean - true_mean) / (obs_sd / sqrt(N)) \nzscore\n\n[1] 3.303849\n\n\nWe expect the observed mean of this simulated data will be within 1.96 standard deviations of \\(\\mu\\) 95 out of 100 times.This observation is 3.3 standard deviations from Mu. The probability of that happening by chance is very small. To be more formal about this probability, we can calculate a p-value. Plug in the z-score below:\n\n(1 - pnorm(abs(zscore)))*2\n\n[1] 0.000953672\n\n\nThis says that the probability of getting this draw by chance is less than 0.1 percent or 1 in 1000.\nThat outcome seems surprising, but we could also just have made an unusual draw. In this workshop, we want to see how often we get such extreme results. We will repeat the steps above 1000 times each, but first we’ll write functions that will make this process smooth!\n\n\nWriting Helper Functions to Make Our Monte Carlo Simulation\nWe want to develop functions that automate repeated steps in our Monte Carlo. In that way, we can define a few important parameters and run the entire process without rewriting or copying and pasting code over and over again.\nAs you saw in the motivating example, we must do the following a 1000 times or B times if parameterize the number of iterations with B:\n\nSimulate data and calculate sample statistics.\nDetermine z-scores.\nTest whether the z-score is outside the threshold. Finally, we:\nMeasure to what extent our simulations match the theory.\n\nTo proceed, we’ll write the steps into their own functions, then call them in the right order in the function do_monte_carlo(). We are breaking a complicated process into smaller chunks and tackling them one by one!\nLet’s look at do_monte_carlo(). It takes a sample-size N, a true_mean, number of iterations B (1000 by default) and a significance level alpha (.05 by default). It returns the proportion of observed means that are significantly different from the true_mean with 95 percent confidence level\n\nBefore following our road map, think about how you would set up functions to automate this process. What would the inputs and outputs be of each step/function? Your processes will be different from ours, but that doesn’t mean ours is better.\n\nNow check out do_monte_carlo() below. It’s our road map.\n\ndo_monte_carlo <- function(N, true_mean, B= 1000, alpha = .05){\n# step 1: Simulate B random samples and calculate sample statistics\n  sample_statistics <- make_mc_sample(N, true_mean, B)\n# step 2: Determine z-scores\n  z_scores <- get_zscores(sample_statistics$mean, true_mean, sample_statistics$sd, N)\n# step 3: Test whether the z-scores are outside the threshold.\n  significance <- test_significance(z_scores, alpha)\n# step 4: Measure to what extent our simulations match the theory. (We expect a number close to alpha)\n  mean(significance)\n}\n\n\n\nDetermine z-scores\nWe’ll start with step 2 determine z-scores. Recall the formula for a zscore is\\(\\frac{\\bar{X}-\\mu}{\\frac{s_n}{\\sqrt{N}}}\\).\nWrite a function called get_zscores that takes the observed means and sds, the true mean and N as inputs and returns a z-score as an output. Name the arguments obs_mean, true_mean, obs_sd, and N.\n\n\n\nIf your functions works, it should return 4 for test.\n\ntest <- get_zscores(obs_mean = 4.4, true_mean = 4.3, obs_sd = 0.25, N = 100)\ntest\n\n[1] 4\n\n\nThe function you wrote should also work on vectorized functions. Run the following code which takes estimates of the mean and standard deviation from 5 random draws and returns their associated z-scores:\n\n# before running set eval = TRUE (and delete this comment)\nmade_up_means <- c(4.4, 4.1, 4.2, 4.4, 4.2)\nmade_up_sd <- c(.25, .5, .4, 1, .4)\nmade_up_zscores <- get_zscores(obs_mean = made_up_means,\n                               true_mean = 4.3,\n                               obs_sd = made_up_sd,\n                               N = 100)\nmade_up_zscores\n\n\nWhich observation from made_up_zscores is not statistically different from 4.3 with 95 percent confidence? In other words, which observed mean and standard deviation return \\(|z-score| < 1.96\\)?\n\n\n\n\nCheck for Significance\nNow we write code for step 3. Test whether the z-scores are outside the threshold.\nThe threshold depends on alpha and the formula is abs(qnorm(alpha/2)).\n\nFor example, for a two-tailed z-test at the 95% confidence level, the cutoff is set at 1.96. Verify this using the formula above.\nWrite a function test_significance() that takes zscores and a given alpha and determines if there is a significant difference at the given level.\n\nRun the following code, and check that your code matches the expected output:\n\n# before knitting set eval = TRUE (and delete this comment)\ntest_significance(zscores = 2, alpha = 0.05)\n\nShould return TRUE. And:\n\n# before knitting set eval = TRUE (and delete this comment)\ntest_significance(zscores = c(1.9, -0.3, -3), alpha = 0.05)\n\nShould return FALSE, FALSE, and TRUE.\n\nBuilding make_mc_sample()\nNow we do step 1: simulate B random samples and calculate sample statistics.\nOur goal is make_mc_sample(N, true_mean, B) a function that produces sample statistics from B random samples from the normal distribution with mean true_mean of size N. When you think of doing something B times it suggest we need a loop. Let’s start with the body of the loop. And because we’re in a lesson about functions, let’s write a function.\n\nWrite a function called calc_mean_and_sd_from_sample() that\n\n\nGenerates a random sample with rnorm() of size N centered around true_mean\nCalculate the mean() and sd() of the random sample.\nReturn the mean and sd in a tibble with column names mean and sd.\n\nIdea: To return two values from a function, we need to put those values into a data structure like a vector or tibble.\nHere’s a test! Verify your function works. Remember, what guarantees that you get the same numbers from a random number generator as we did is that we’re setting a seed to 5\n\n# before knitting set eval = TRUE (and delete this comment)\nset.seed(5)\ncalc_mean_and_sd_from_sample(N = 30, true_mean = 0.5)\n\nNow, this function only does what we need once, while we’ll need it to do it B times. This is an appropriate time for a loop!\n\nWrite the function make_mc_sample. The inputs are described above. The output is a tibble with B rows of means and standard deviations.\n\nHere’s a test.\n\nset.seed(24601)\nmake_mc_sample(N = 30, true_mean = 100, B = 3 )\n\n# A tibble: 3 × 2\n   mean    sd\n  <dbl> <dbl>\n1  99.8 0.920\n2  99.8 0.952\n3 100.  1.04 \n\n\n\n\nFunctions, Assemble\nNow you have all the helper functions that are critical for our simulation. We want to simulate 1000 sets of 30 data points drawn from a normal distribution with true mean 0.5 and then see how often our random sample mean is significantly different from the true mean at a significance level of 0.05. If everything is working as expected, we should see about 5% of the random means to be statistically different.\n\ndo_monte_carlo <- function(N, true_mean, B = 1000, alpha = .05){\n# step 1: Simulate B random samples and calculate sample statistics\nsample_statistics <- make_mc_sample(N, true_mean, B)\n# step 2: Determine z-scores\nz_scores <- get_zscores(sample_statistics$mean, true_mean, sample_statistics$sd, N)\n# step 3: Test whether the z-scores are outside the threshold.\nsignificance <- test_significance(z_scores, alpha)\n# step 5: Measure to what extent our simulations match the theory. (We expect a number close to alpha)\nmean(significance)\n}\n\n\nTest out your function with N equals 30 and true_mean equals 0.5. The resulting number should be close to .05 (alpha).\nTry again with a different alpha and verify that do_monte_carlo returns a number in the ball park of alpha.\n\nWant to improve this tutorial? Report any suggestions/bugs/improvements on here! We’re interested in learning from you how we can make this tutorial better."
  },
  {
    "objectID": "Data/viz/2023-01-03 Tidy Tuesday/TT_wk1.html",
    "href": "Data/viz/2023-01-03 Tidy Tuesday/TT_wk1.html",
    "title": "Tidy Tuesday, 2023 Week 1 🏡",
    "section": "",
    "text": "The theme of this week’s #TidyTuesday is “Bring your own data from 2022!”\nI decided to use data from Tidy Tuesday 2022-07-05. Here is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-01-03 Tidy Tuesday/TT_wk1.html#my-plot",
    "href": "Data/viz/2023-01-03 Tidy Tuesday/TT_wk1.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 1 🏡",
    "section": "My plot",
    "text": "My plot\n\n🏡 SF Housing Prices by Kate Pennington’s Craigslist scrape of Bay Area housing posts\n\nCode here"
  },
  {
    "objectID": "Data/viz/2023-01-10 Tidy Tuesday/TT_wk2.html",
    "href": "Data/viz/2023-01-10 Tidy Tuesday/TT_wk2.html",
    "title": "Tidy Tuesday, 2023 Week 2 🐦",
    "section": "",
    "text": "Data for this week’s TidyTuesday comes from FeederWatch!\nHere is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-01-10 Tidy Tuesday/TT_wk2.html#my-plot",
    "href": "Data/viz/2023-01-10 Tidy Tuesday/TT_wk2.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 2 🐦",
    "section": "My plot",
    "text": "My plot\n\n🐦 FeederWatch\n\nCode here"
  },
  {
    "objectID": "Data/viz/2023-01-17 Tidy Tuesday/TT_wk3.html",
    "href": "Data/viz/2023-01-17 Tidy Tuesday/TT_wk3.html",
    "title": "Tidy Tuesday, 2023 Week 3 🎨",
    "section": "",
    "text": "Data for this week’s TidyTuesday comes from Art History!\nHere is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-01-17 Tidy Tuesday/TT_wk3.html#my-plot",
    "href": "Data/viz/2023-01-17 Tidy Tuesday/TT_wk3.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 3 🎨",
    "section": "My plot",
    "text": "My plot\n\n🎨 Art Histpry\n\nCode here"
  },
  {
    "objectID": "Data/viz/2023-01-24 Tidy Tuesday/TT_wk4.html",
    "href": "Data/viz/2023-01-24 Tidy Tuesday/TT_wk4.html",
    "title": "Tidy Tuesday, 2023 Week 4 🐻",
    "section": "",
    "text": "Data for this week’s TidyTuesday comes from the TV show ALONE!\nHere is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-01-24 Tidy Tuesday/TT_wk4.html#my-plot",
    "href": "Data/viz/2023-01-24 Tidy Tuesday/TT_wk4.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 4 🐻",
    "section": "My plot",
    "text": "My plot\n\n🐻 ALONE\n\nCode here"
  },
  {
    "objectID": "Data/viz/2023-01-31 Tidy Tuesday/TT_wk5.html",
    "href": "Data/viz/2023-01-31 Tidy Tuesday/TT_wk5.html",
    "title": "Tidy Tuesday, 2023 Week 5 🐱",
    "section": "",
    "text": "Week\nDate\nData\nSource\nArticle\n\n\n\n\n5\n2023-01-31\nPet Cats UK\nMovebank for Animal Tracking Data\nCats on the Move\nHere is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-01-31 Tidy Tuesday/TT_wk5.html#my-plot",
    "href": "Data/viz/2023-01-31 Tidy Tuesday/TT_wk5.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 5 🐱",
    "section": "My plot",
    "text": "My plot\n\n🐱 Pet Cats UK\n\nCode here"
  },
  {
    "objectID": "Data/viz/2023-02-07 Tidy Tuesday/TT_wk6.html",
    "href": "Data/viz/2023-02-07 Tidy Tuesday/TT_wk6.html",
    "title": "Tidy Tuesday, 2023 Week 6 📈",
    "section": "",
    "text": "Week\nDate\nData\nSource\nArticle\n\n\n\n\n6\n2023-02-07\nBig Tech Stock Prices\nBig Tech Stock Prices on Kaggle\n5 Charts on Big Tech Stocks’ Collapse\nHere is a little more information about the data:"
  },
  {
    "objectID": "Data/viz/2023-02-07 Tidy Tuesday/TT_wk6.html#my-plot",
    "href": "Data/viz/2023-02-07 Tidy Tuesday/TT_wk6.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 6 📈",
    "section": "My plot",
    "text": "My plot\n\n📈 Big Tech Stock Prices\n\nCode here"
  },
  {
    "objectID": "Data/viz/2023-02-14 Tidy Tuesday/TT_wk7.html",
    "href": "Data/viz/2023-02-14 Tidy Tuesday/TT_wk7.html",
    "title": "Tidy Tuesday, 2023 Week 7 🎬",
    "section": "",
    "text": "Week\nDate\nData\nSource\nArticle\n\n\n\n\n7\n2023-02-14\nHollywood Age Gaps\nHollywood Age Gap Download Data\nHollywood Age Gap\nHere is a little more information about the data: # Hollywood Age Gaps\nThe data this week comes from Hollywood Age Gap via Data Is Plural.\nThe data follows certain rules:\nWe previously provided a dataset about the Bechdel Test. It might be interesting to see whether there is any correlation between these datasets! The Bechdel Test dataset also included additional information about the films that were used in that dataset.\nNote: The age gaps dataset includes “gender” columns, which always contain the values “man” or “woman”. These values appear to indicate how the characters in each film identify. Some of these values do not match how the actor identifies. We apologize if any characters are misgendered in the data!"
  },
  {
    "objectID": "Data/viz/2023-02-14 Tidy Tuesday/TT_wk7.html#my-plot",
    "href": "Data/viz/2023-02-14 Tidy Tuesday/TT_wk7.html#my-plot",
    "title": "Tidy Tuesday, 2023 Week 7 🎬",
    "section": "My plot",
    "text": "My plot\n\n🎬 Hollywood Age Gaps\n\nCode here"
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "",
    "text": "Odds ratios have been a long-standing mainstay in health research. Their use can be found sprinkled throughout countless academic journals and research papers. But just because something is popular doesn’t mean it’s the best choice for all contexts. The purpose of this article is to shed light on the limitations of odds ratios, particularly when it comes to interpretation, and advocate for the more intuitive marginal effects and predicted probabilities."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#the-odd-issue-with-odds-ratios",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#the-odd-issue-with-odds-ratios",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "The Odd Issue with Odds Ratios",
    "text": "The Odd Issue with Odds Ratios\nImagine a study on the effect of a new drug on reducing heart attack risk. The findings say there’s an odds ratio of 2. Sounds impressive, doesn’t it? But what does that really mean? It’s tempting to think that patients on the drug are twice as likely to avoid heart attacks. But that’s not quite right. They actually have twice the odds, which isn’t the same as probability, especially when the event is common.\nHere’s the crux of the issue: while odds ratios can be a neat statistical tool, they aren’t always intuitive. They can be easily misinterpreted, leading to overstated or misunderstood results."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#the-math-behind-odds-ratios",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#the-math-behind-odds-ratios",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "The math behind odds ratios",
    "text": "The math behind odds ratios\nTo understand why odds ratios can be confusing, let’s briefly discuss what they represent.\nIn a logistic regression, the fundamental equation is:\n\\[log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ...\\]\nWhere \\(p\\) is the probability of an event occurring.\nThe odds ratio for a given predictor (let’s say \\(X_1\\)) is simply the exponentiated coefficient for that predictor:\nOdds Ratio for \\(X_1 = E^{\\beta_1}\\)\nIt represents the multiplicative change in the odds of the outcome for a one-unit increase in \\(X_1\\) , holding other variables constant. If this sounds confusing, you’re not alone."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#when-odds-ratios-mislead",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#when-odds-ratios-mislead",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "When Odds Ratios Mislead",
    "text": "When Odds Ratios Mislead\nThe challenge with odds ratios arises particularly when the outcome of interest is common. The reason being, odds ratios tend to exaggerate relative risks for common outcomes.\nLet’s say we find an odds ratio of 2 for a drug reducing heart attack risk. Many might (mistakenly) interpret this as “patients on the drug are twice as likely to avoid heart attacks.” In reality, they have twice the odds, which doesn’t translate directly to actual probabilities, especially when the event is common."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#marginal-effects-clarity-over-confusion",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#marginal-effects-clarity-over-confusion",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Marginal Effects: Clarity over Confusion",
    "text": "Marginal Effects: Clarity over Confusion\nMarginal effects shine where odds ratios falter. They convey the change in the probability of an outcome for a one-unit change in the predictor, holding other variables constant. It’s a direct measure that offers a tangible understanding of impact.\nMathematically, for a binary predictor (like drug use: yes or no):\n\\[\\text{Marginal Effect} (dy/dx) = P(Y = 1|X=1) - P(Y=1|X=0)\\]\nFor continuous predictors, the marginal effect represents the derivative of the probability with respect to the predictor, which essentially gives us the rate of change."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#predicted-probabilities-concrete-scenarios-over-abstract-odds",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#predicted-probabilities-concrete-scenarios-over-abstract-odds",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Predicted Probabilities: Concrete Scenarios Over Abstract Odds",
    "text": "Predicted Probabilities: Concrete Scenarios Over Abstract Odds\nPredicted probabilities go a step further, providing the likelihood of an outcome under specific conditions. Instead of leaving things in the realm of odds and abstract increases, we can state tangible scenarios."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#demonstrating-with-real-world-data",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#demonstrating-with-real-world-data",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Demonstrating with “Real-World” Data",
    "text": "Demonstrating with “Real-World” Data\nTo provide concrete evidence of the limitations of odds ratios, let’s explore a simulated dataset that mirrors a health study on heart attacks, age, and drug use for high-risk individuals.\nFirst, we’ll generate a dataset with age as a continuous predictor and drug_use as a binary predictor:\nSimulate data\n\n\nShow the code\n# Setting the seed for reproducibility\nset.seed(1234)\n\nn &lt;- 1000\n\n# Age distribution: Normally distributed between 40 and 70\nage &lt;- rnorm(n, 55, 7.5)\n\n# Drug use: 50% of the participants use the drug\ndrug_use &lt;- rbinom(n, 1, 0.5)\n\n# Log odds of heart attack: Base risk, then increasing slightly with age, \n# and decreasing with drug use (this is where we're setting the odds ratio to roughly 2 for drug use)\nlog_odds_heart_attack &lt;- -2 + 0.04 * age - log(2) * drug_use\n\n# Converting log odds to probability\nprob_heart_attack &lt;- exp(log_odds_heart_attack) / (1 + exp(log_odds_heart_attack))\n\n# Simulating whether a heart attack occurs or not\nheart_attack &lt;- rbinom(n, 1, prob_heart_attack)\n\n# Create a dataframe\ndata_high_risk &lt;- data.frame(age, drug_use, heart_attack)\n\n\n\nnrow(data_high_risk)\n\n[1] 1000\n\n\n\ndata_high_risk\n\n\n\n\n\n\n\nage\ndrug_use\nheart_attack\n\n\n\n\n45.94701\n0\n1\n\n\n57.08072\n0\n1\n\n\n63.13331\n1\n0\n\n\n37.40727\n1\n0\n\n\n58.21844\n0\n1\n\n\n58.79542\n1\n0\n\n\n50.68945\n1\n0\n\n\n50.90026\n0\n0\n\n\n50.76661\n1\n0\n\n\n48.32472\n0\n1\n\n\n\n\n\n\n\n\n\nModeling and Results\nUsing a logistic regression, we’ll model the probability of a heart attack based on age and drug use:\n\nmodel_high_risk &lt;- glm(heart_attack ~ age + drug_use, \n                       data=data_high_risk, family = binomial)\n\nsummary(model_high_risk)\n\n\nCall:\nglm(formula = heart_attack ~ age + drug_use, family = binomial, \n    data = data_high_risk)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.662  -1.049  -0.854   1.188   1.685  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.24031    0.49848  -4.494 6.98e-06 ***\nage          0.04281    0.00896   4.777 1.78e-06 ***\ndrug_use    -0.66254    0.13064  -5.072 3.94e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1373.7  on 999  degrees of freedom\nResidual deviance: 1324.4  on 997  degrees of freedom\nAIC: 1330.4\n\nNumber of Fisher Scoring iterations: 4\n\n\nUpon examining the output, you’ll observe an odds ratio close to 2 for drug use (exponentiate the coeffecient). A common misinterpretation might be: “Patients on the drug have twice the likelihood to avoid heart attacks.” This is a misconception. They have twice the odds, not twice the probability. This distinction becomes especially murky when the event (heart attack) is common.\n\n\nMarginal Effects and Interpretation\nThe marginal effect of the drug can be found by computing the difference in predicted probabilities between drug users and non-users, holding other variables constant. This gives a more direct measure of the drug’s impact on the probability of heart attack.\nUsing our model, you can compute the marginal effect for any given age. For example, for someone aged 55:\n\npredicted_prob_on_drug &lt;- predict(model_high_risk, \n                                  newdata = data.frame(age = 55, drug_use = 1),\n                                  type = \"response\")\n\npredicted_prob_off_drug &lt;- predict(model_high_risk, \n                                   newdata = data.frame(age = 55, drug_use = 0),\n                                   type = \"response\")\n\nmarginal_effect_55 &lt;- predicted_prob_on_drug - predicted_prob_off_drug\n\nmarginal_effect_55\n\n         1 \n-0.1622666 \n\n\nThe result is a clear percentage point decrease in the probability of a heart attack for drug users compared to non-users.\nThis means that, for individuals aged 55, using the drug results in a 16 percentage point decrease in the probability of experiencing a heart attack compared to those not using the drug. This direct interpretation offers a tangible sense of the drug’s impact and is far more intuitive than grappling with odds ratios."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#conclusion-a-call-to-transition",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#conclusion-a-call-to-transition",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Conclusion: A Call to Transition",
    "text": "Conclusion: A Call to Transition\nThe era of odds ratios has provided us with valuable insights, but as the field of health research progresses, so should our statistical tools and reporting practices. Marginal effects and predicted probabilities provide clearer, more intuitive insights, pushing us away from the ambiguous realm of odds and into the tangible world of real impact.\nAfter all, the end goal of our research is to provide clear, actionable insights that can be readily applied to improve health outcomes. Let’s choose clarity over tradition, and marginal effects over odds ratios.\nThank you!\nJacob"
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#average-marginal-effects-a-comprehensive-view",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#average-marginal-effects-a-comprehensive-view",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Average Marginal Effects: A Comprehensive View",
    "text": "Average Marginal Effects: A Comprehensive View\nBeyond just evaluating the marginal effect at specific values, we can compute the average marginal effect (AME) across our sample. AMEs provide a holistic view of the predictor’s impact by averaging its effects over all observations in our dataset. It’s particularly insightful when dealing with continuous predictors like age.\n\nComputing the Average Marginal Effects\nTo compute the AMEs in R, you can leverage the margins package:\n\n#install.packages(\"margins\")\nlibrary(margins)\n\name &lt;- margins(model_high_risk, variables = \"drug_use\")\nsummary(ame)\n\n   factor     AME     SE       z      p   lower   upper\n drug_use -0.1556 0.0291 -5.3429 0.0000 -0.2127 -0.0985\n\n\nThis will give you the average marginal effect of drug use on the probability of a heart attack across all ages in the dataset.\n\n\nInterpreting the AMEs\nSuppose the AME for drug use is -0.156. This means that, on average, across all ages in our sample, using the drug is associated with a 15.6 percentage point decrease in the probability of experiencing a heart attack relative to not using the drug.\nThis single number provides an overarching sense of the drug’s impact, making it immensely useful for policy and clinical decisions. For instance, in public health discussions or in communications with patients, being able to state the average effect of a treatment can be more practical than specifying its impact at particular ages or conditions.\nBy incorporating AMEs into your analysis, you’re providing an extra layer of depth to your findings. This approach shifts the discussion from “what might the effect be?” to “here’s the effect, on average, across our population.” When combined with specific marginal effects and predicted probabilities, this trio offers a robust, comprehensive, and intuitive understanding of your predictors’ impacts."
  },
  {
    "objectID": "posts/2023-09-01-OR/2023-09-01-OR.html#visualizing-predicted-probabilities",
    "href": "posts/2023-09-01-OR/2023-09-01-OR.html#visualizing-predicted-probabilities",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "Visualizing Predicted Probabilities",
    "text": "Visualizing Predicted Probabilities\nTo provide a tangible representation, let’s plot the predicted probabilities of heart attacks by age from our model:\n\nlibrary(ggplot2)\n\ndata_high_risk$predicted_prob &lt;- predict(model_high_risk, type = \"response\")\npredictions &lt;- predict(model_high_risk, type = \"response\", se.fit = TRUE)\ndata_high_risk$lower_ci &lt;- predictions$fit - 1.96 * predictions$se.fit\ndata_high_risk$upper_ci &lt;- predictions$fit + 1.96 * predictions$se.fit\n\n\nggplot(data_high_risk, aes(x = age, \n                           y = predicted_prob, \n                           color = as.factor(drug_use))) +\n  geom_ribbon(aes(ymin = lower_ci, \n                  ymax = upper_ci, \n                  fill = as.factor(drug_use)), alpha = 0.3) +\n  geom_line() +\n  labs(title = \"Predicted Probability of Heart Attack by Age with 95% CI\",\n       y = \"Predicted Probability\",\n       x = \"Age\",\n       color = \"Drug Use\",\n       fill = \"Drug Use\") +\n  scale_color_discrete(labels = c(\"No Drug\", \"On Drug\")) +\n  scale_fill_discrete(labels = c(\"No Drug\", \"On Drug\")) +\n  theme_minimal()\n\n\n\n\nThe visual clearly illustrates the beneficial effect of the drug over time. It gives a tangible sense of how the risk of heart attacks evolves with age for both groups and the significant advantage of drug users. This can be especially useful for diagnostic purposes or to demonstrate the results of your model to stakeholders."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Reflecting sociocultural pressures around masculinity, measures of male gender expression (MGE) have previously been associated with health outcomes. In this study, we examine associations between of adolescent school social networksnetwork variables and school, gender norms, and with adolescent-to-young-adult MGE changes, and of MGE changes in male gender expression (GE) with young adult substance use.\n\n\n\nPublisher’s Site Code Repo\n\n\n\n\n\n\nRecruiting patients for clinical research is challenging, especially for underrepresented populations, and may be influenced by patients’ relationships with their physicians, care experiences, and engagement with care. This study sought to understand predictors of enrollment in a research study among socioeconomically diverse participants in studies of care models that promote continuity in the doctor–patient relationship.\n\n\n\nPublisher’s Site PDF"
  },
  {
    "objectID": "teaching.html#introduction-to-r-programming",
    "href": "teaching.html#introduction-to-r-programming",
    "title": "Teaching",
    "section": "Introduction to R Programming",
    "text": "Introduction to R Programming\nThis page contains the content for the “Introduction to Data Analysis in R” mini-course. The goal is to introduce R programming concepts with a focus on preparing and analyzing data, and conducting statistical simulations. We will cover how to read data into R, manipulate data with a suite of tools from the tidyverse package dplyr. We will also discuss some basic programming concepts including data types and operators. This mini-course is aimed at people with no programming background that are looking for a gentle and practical introduction to data analysis in R.\n\nHarvard T.H. Chan School of Public Health\n\nDecision Science for Public Health. Teaching Fellow (2023)\n\n\n\nHarvard Kennedy School\n\nMath Camp. Instructor (2023)\nBig Data and Machine Learning. Course Assistant (2023)\nGame Theory. Teaching Fellow (2023)\nData and Programming for Policymakers. Course Assistant (2023)\nResources, Incentives, and Choices I: Markets and Market Failures. Teaching Fellow (2022, 2023)\n\n\n\nThe University of Chicago, Center for Translational Science\n\nData, Quantitative Methods, and Applications in HSR. Teaching Assistant and Instructor (2021,2022,2023)\nIntroduction to Health Services Research. Teaching Assistant (2021)\n\n\n\nThe University of Chicago, Department of Computer Science\n\nMachine Learning for Public Policy. Teaching Assistant (2022)\nMathematics for Data Analysis and Computer Science. Teaching Assistant (2022)\n\n\n\nThe University of Chicago Booth School of Business\n\nIntroductory Finance. Teaching Assistant (2020-2022)\nData Analysis in R and Python. Teaching Assistant (2021)\n\n\n\nThe University of Chicago Harris School of Public Policy\n\nCoding Lab for Public Policy. Instructor and Curriculum Developer (2021)\n\n\n\nTeach For America, Achievement First Amistad Academy Middle School\n\n7th/8th Grade Mathematics. Mathematics Teacher (2018-2020)"
  },
  {
    "objectID": "posts/2023-09-23-docker/2023-09-23-docker.html",
    "href": "posts/2023-09-23-docker/2023-09-23-docker.html",
    "title": "Reproducibility in Health Research - The Power of Docker for Microsimulations",
    "section": "",
    "text": "In the domain of health research, particularly within the realm of medical decision-making, our ability to make accurate and impactful decisions is profoundly influenced by the robustness and reliability of our research. However, robustness is not just about good statistical practices or rigorous study design; it extends to the reproducibility of our research. Without reproducibility, even the most meticulously designed studies can lose their value and, in some cases, credibility.\n\nMedical decision-making often hinges on intricate computational models such as microsimulations. These models, while powerful, are composed of multifaceted components including various dependencies, data sources, and algorithms. A slight change or inconsistency in any of these components can lead to vastly different outcomes.\nImagine, for instance, a microsimulation that forecasts the spread of an infectious disease. Policy-makers could utilize this simulation to shape national health strategies. Now imagine if a slight variation in the software environment—say a minor version change in a data processing library—alters the projected number of cases. The implications could be enormous, ranging from resource misallocation to ineffective interventions.\nThis underlines a fundamental truth: For research to be trusted and actionable, it must be reproducible.\n\nSo, how can we ensure that our complex microsimulations run consistently, producing the same outcomes regardless of where and when they are executed? Enter Docker, a game-changer in the world of reproducible research.\n\nThink of a container as a standalone package, a box if you will, that encloses your entire software environment—your code, its dependencies, system libraries, system settings, and so forth. This container ensures that your software runs identically regardless of where the container is executed, be it your laptop, a colleague’s machine, or a cloud server on the other side of the world.\nWhy is this groundbreaking? Because it eradicates the notorious “but it works on my machine” problem. Every researcher knows the headache of trying to replicate an environment across multiple machines, battling inconsistent library versions, missing dependencies, and unexpected system behaviors. Docker containers eliminate this inconsistency.\n\nWith Docker, every element of your software environment is codified and encapsulated. You don’t just share your code; you share its entire ecosystem. Here’s what it means for reproducibility:\n\nIsolation: Docker ensures that your software’s environment is isolated from the host system, meaning external factors on a machine won’t affect your software’s execution.\nVersion Control for Environments: Just as Git allows you to version control your code, Docker lets you version control your software environment. This is crucial when using libraries and tools that are continuously updated.\nEase of Sharing: Docker containers can be easily shared with peers, ensuring that they don’t just get your code but the exact environment in which your code was designed to run.\n\nAs health researchers, our ultimate aim is to produce knowledge that can improve health outcomes. But this knowledge must be built on trust. Docker, by championing reproducibility, fosters this trust, ensuring our research remains both impactful and credible. In the next sections, we’ll delve deeper into how to leverage Docker for your microsimulations, but for now, it’s crucial to recognize its potential in safeguarding the integrity of our research."
  },
  {
    "objectID": "research.html#published-papers",
    "href": "research.html#published-papers",
    "title": "Research",
    "section": "",
    "text": "Reflecting sociocultural pressures around masculinity, measures of male gender expression (MGE) have previously been associated with health outcomes. In this study, we examine associations between of adolescent school social networksnetwork variables and school, gender norms, and with adolescent-to-young-adult MGE changes, and of MGE changes in male gender expression (GE) with young adult substance use.\n\n\n\nPublisher’s Site Code Repo\n\n\n\n\n\n\nRecruiting patients for clinical research is challenging, especially for underrepresented populations, and may be influenced by patients’ relationships with their physicians, care experiences, and engagement with care. This study sought to understand predictors of enrollment in a research study among socioeconomically diverse participants in studies of care models that promote continuity in the doctor–patient relationship.\n\n\n\nPublisher’s Site PDF"
  },
  {
    "objectID": "research.html#under-review",
    "href": "research.html#under-review",
    "title": "Research",
    "section": "Under Review",
    "text": "Under Review\n\nTo Batch or Not to Batch: Test-Ordering Variability in the Emergency Department and the Impact on Care Delivery\nEmergency Department (ED) patients may receive varying diagnostic workups and dispositions based on physician factors instead of solely based on presenting conditions. This study delves into the contrasting practices of batch-ordering multiple tests simultaneously versus the sequential ordering of tests based on previous results. Our analysis revealed stark differences in physician diagnostic approaches, even when working in similar environments. Findings suggest that physicians who predominantly make use of batching (“batchers”) tend to order more tests, which is associated with longer lengths of stay and increased costs. In contrast, other physicians (“non-batchers”) order fewer tests, which is associated with lower lengths of stay and costs, without any impact on primary ED outcome measures, such as the 72-hour rate of return.\n\n\n\nPDF Code Repo"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Jacob Jameson",
    "section": "",
    "text": "|\n\n\nI am a PhD student at Harvard University studying Health Policy and Decision Sciences. I am currently advised by Dr. Soroush Saghafian and work in The Public Impact Analytics Science Lab (PIAS-Lab) at Harvard. I also hold a research position in Boston Children’s Hospital General Pediatric Unit and I am an Educational Innovation Scholar at the Harvard Center for Health Decision Science and Global Health Education and Learning Incubator.\nI use tools from operations research, engineering, computer science, and economics to inform decisions being made under conditions of uncertainty, with a focus on hospital operations.\n\n\n\nAside from research, I am passionate about teaching. I have extensive experience teaching students in a variety of settings, including as a teaching fellow at Harvard, instructor at UChicago Medicine, and as a Teach For America corps member in New Haven Public Schools."
  },
  {
    "objectID": "Intro R.html#about-this-course",
    "href": "Intro R.html#about-this-course",
    "title": "Introduction to Programming in R",
    "section": "",
    "text": "This page contains the content for the “Introduction to Data Analysis in R” mini-course. The goal is to introduce R programming concepts with a focus on preparing and analyzing data, and conducting statistical simulations. We will cover how to read data into R, manipulate data with a suite of tools from the tidyverse package dplyr. We will also discuss some basic programming concepts including data types and operators.\nThis mini-course is aimed at people with no programming background that are looking for a gentle and practical introduction to data analysis in R.\nEach Module involves:\n\na pre-recorded lecture with guided practice exercises\na corresponding lab to practice the skills taught in the video and stretch your thinking\n\nThe labs are an essential part of the learning process as they walkthrough the use of many key functions and topics that are not explicitly covered in the videos.\n\nAdditional Resources\n\n\n\ntidyverse cheetsheets start with dplyr and ggplot\n\n\nR for Data Science: free online book with clear explanations of many tidyverse functions, the book to read on data analysis with R"
  },
  {
    "objectID": "posts/2022-11-01-SNA/2022-11-01-SNA.html#social-network-analysis",
    "href": "posts/2022-11-01-SNA/2022-11-01-SNA.html#social-network-analysis",
    "title": "Basic Network Analysis and Visualization for Directed Graphs in R",
    "section": "",
    "text": "“There is certainly no unanimity on exactly what centrality is or on its conceptual foundations, and there is little agreement on the proper procedure for its measurement.” - Linton Freeman (1977)\nSocial network analysis can be used to measure the importance of a person as a function of the social structure of a community or organization. This post uses visualization as a tool to explain how different measures of centrality may be used to analyze different questions in a network analysis. In these examples we will be specifically looking at directed graphs to compare the following centrality measures and their use-cases:\n\nDegree Centrality\nBetweenness Centrality\nEigenvector Centrality\nKatz Centrality\nHITS Hubs and Authorities\n\nAn example of a directed graph would be one in which people nominate their top 2 friends. In this graph, nodes (people) would connect to others nodes through directed edges (nominations). It is possible for Jacob to nominate Jenna without Jenna nominating him back. You can imagine why centrality in a friendship network might take into account the direction of these nominations. If I list 100 people as my friends and none of them list me back, do we think I am a popular person?"
  },
  {
    "objectID": "R files/1 Module/mod1.html#an-introduction-and-motivation-for-r-programming",
    "href": "R files/1 Module/mod1.html#an-introduction-and-motivation-for-r-programming",
    "title": "Module 1: An Introduction and Motivation for R Programming",
    "section": "",
    "text": "Download a copy of Module 1 slides"
  },
  {
    "objectID": "posts/2023-12-19-OLS/2023-12-19-OLS.html",
    "href": "posts/2023-12-19-OLS/2023-12-19-OLS.html",
    "title": "The ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research",
    "section": "",
    "text": "set.seed(0)\nn &lt;- 1000  # Number of observations\nX &lt;- matrix(rnorm(n * 2), ncol = 2)  # Two covariates\nbeta &lt;- c(0.5, -0.25)  # True effect sizes\nintercept &lt;- 0.2  # True intercept\nnoise &lt;- rnorm(n) * 0.5  # Noise\n\n# Linear outcome\nlinear_y &lt;- intercept + X %*% beta + noise\n\n# Binary outcome\nbinary_y &lt;- as.integer(linear_y &gt; 0)\n\n\nols_fit &lt;- lm(linear_y ~ X)\nsummary(ols_fit)\n\n\nCall:\nlm(formula = linear_y ~ X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.57460 -0.32400  0.00088  0.31575  1.48493 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.23440    0.01511   15.52   &lt;2e-16 ***\nX1           0.50587    0.01514   33.41   &lt;2e-16 ***\nX2          -0.24042    0.01462  -16.45   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4775 on 997 degrees of freedom\nMultiple R-squared:  0.5843,    Adjusted R-squared:  0.5834 \nF-statistic: 700.6 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\n\nlogit_fit &lt;- glm(binary_y ~ X, family = binomial)\nsummary(logit_fit)\n\n\nCall:\nglm(formula = binary_y ~ X, family = binomial)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.97739    0.09451  10.341   &lt;2e-16 ***\nX1           1.83890    0.12592  14.603   &lt;2e-16 ***\nX2          -0.86529    0.09277  -9.328   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1310.25  on 999  degrees of freedom\nResidual deviance:  859.56  on 997  degrees of freedom\nAIC: 865.56\n\nNumber of Fisher Scoring iterations: 5"
  },
  {
    "objectID": "api 222 files/section 1/section1.html",
    "href": "api 222 files/section 1/section1.html",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "",
    "text": "Notes build on previous TFs (Ibou Dieye, Laura Morris, Amy Wickett & Emily Mower)\nThe following code is meant as a first introduction to R. It is therefore helpful to run it one line at a time and see what happens. To run one line of code in RStudio, you can highlight the code you want to run and hit “Run” at the top of the script.\nOn a mac, you can highlight the code you want to run and hit Command + Enter. On a PC, you can highlight the code you want to run and hit Ctrl + Enter. If you ever forget how a function works, you can type ? followed immediately (e.g. with no space) by the function name to get the help file."
  },
  {
    "objectID": "api 222 files/section 1/section1.html#part-1-r-fundamentals",
    "href": "api 222 files/section 1/section1.html#part-1-r-fundamentals",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Part 1: R Fundamentals",
    "text": "Part 1: R Fundamentals\nValues can be assigned names and used in subsequent operations. Instead of the traditional “=” sign, the convention in R is “&lt;-”. The “=” sign also works, but I will use “&lt;-” to be consistent with convention.\nThe general form for calling R functions is:\n\n# FunctionName(arg.1 = value.1, arg.2 = value.2, ..., arg.n = value.n)"
  },
  {
    "objectID": "api 222 files/section 1/section1.html#part-2-vectors",
    "href": "api 222 files/section 1/section1.html#part-2-vectors",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Part 2: Vectors",
    "text": "Part 2: Vectors\nFirst, we will learn how to make different types of vectors. Our first vector will contain integers 1 through 4:\n\nvec1 &lt;- c(1, 2, 3, 4)\nprint(vec1)\n\n[1] 1 2 3 4\n\n\nIn R, we use square brackets for indexing. So, for example, if we want to print the 1st element of our vector:\n\nprint(vec1[1])\n\n[1] 1\n\n\nIf we want to print the 4th element of our vector:\n\nprint(vec1[4])\n\n[1] 4\n\n\nIf we want to print the 1st and the 4th elements in one go, we make a vector with the desired indices and place that index vector within square brackets:\n\nprint(vec1[c(1,4)])\n\n[1] 1 4\n\n\nAn alternative way to create vec1 would be using the seq() command, which allows us to generate a vector according to a sequence:\n\nprint(seq(4))\n\n[1] 1 2 3 4\n\nprint(seq(-4))\n\n[1]  1  0 -1 -2 -3 -4\n\nvec2 &lt;- seq(4)\nprint(vec2)\n\n[1] 1 2 3 4\n\nprint(seq(from = 100, to = 120, by = 5))\n\n[1] 100 105 110 115 120\n\n# Help file for seq function\n?seq\n\n# Breaking the sequence command into multiple lines\nprint(seq(from = 100, \n          to = 120, \n          by = 5))\n\n[1] 100 105 110 115 120\n\nprint(1:10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nprint(10:1)\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nVectors don’t have to be numeric. They could also be character/string vectors:\n\nword_vec &lt;- c(\"Hello\", \"Time To\", \"Learn\", \"R\", \"!\")\nprint(word_vec)\n\n[1] \"Hello\"   \"Time To\" \"Learn\"   \"R\"       \"!\"      \n\n# Using the which() function\nprint(which(word_vec == \"Learn\"))\n\n[1] 3\n\nprint(which(word_vec == \"Hi!\"))\n\ninteger(0)\n\n# Finding the length of a vector\nprint(length(vec1))\n\n[1] 4\n\nprint(length(seq(10)))\n\n[1] 10\n\nprint(length(seq(from = 100, to = 120, by = 2)))\n\n[1] 11\n\n# Calculating statistics about vectors\nprint(mean(vec1))\n\n[1] 2.5\n\nprint(median(vec1))\n\n[1] 2.5\n\nprint(min(vec1))\n\n[1] 1\n\nprint(max(vec1))\n\n[1] 4\n\n# Variance and standard deviation\nprint(var(vec1))\n\n[1] 1.666667\n\nprint(sd(vec1))\n\n[1] 1.290994\n\n# Comparing vectors\nvec4 &lt;- c(1, 4, 9, 16)\nprint(vec4)\n\n[1]  1  4  9 16\n\nprint(vec1 == vec4)\n\n[1]  TRUE FALSE FALSE FALSE\n\n# Checking if two vectors are exactly the same\nprint(all.equal(vec1, vec2))\n\n[1] TRUE\n\nprint(all.equal(vec1, vec4))\n\n[1] \"Mean relative difference: 2.222222\""
  },
  {
    "objectID": "api 222 files/section 1/section1.html#part-3-logical-statements",
    "href": "api 222 files/section 1/section1.html#part-3-logical-statements",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Part 3: Logical statements",
    "text": "Part 3: Logical statements\n“if statements” can be very useful. They work as follows:\nif (the logical statement in these parentheses is TRUE) {\n  do this} \nelse {\n  do that\n}\nLet’s try it.\n\n## Example 1:\nif (2 + 2 == 5) {\n  print(\"Yikes\")\n} else {\n  print(\"Good job!\")\n}\n\n[1] \"Good job!\"\n\n\n\n## Example 2:\nif (vec1[2] == 2) {\n  print(\"Hello\")\n}else {\n  print(\"Goodbye\")\n}\n\n[1] \"Hello\""
  },
  {
    "objectID": "api 222 files/section 1/section1.html#part-5-matrices",
    "href": "api 222 files/section 1/section1.html#part-5-matrices",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Part 5: Matrices",
    "text": "Part 5: Matrices\nTo make a matrix, use the matrix() command. The first element fed in is the data you want to put in matrix form. Then, you specify the number of rows and columns. By default, it fills information down the columns, but you can tell it to do by row\n\nmtx1 &lt;- matrix(vec1, nrow = 2, ncol = 2)\nprint(mtx1)\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\n\nmtx2 &lt;- matrix(vec1, nrow = 2, ncol = 2, byrow = TRUE)\nprint(mtx2)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\nNote that mtx2 is the transpose of mtx1. If you want to transpose a matrix, you can use t()\n\nprint(mtx1)\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nprint(t(mtx1))\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\nAs with vectors, you can check if two matrices are equal\n\nprint(mtx1 == mtx2)\n\n      [,1]  [,2]\n[1,]  TRUE FALSE\n[2,] FALSE  TRUE\n\nprint(all.equal(mtx1, mtx2))\n\n[1] \"Mean relative difference: 0.4\"\n\nprint(mtx1 == t(mtx2))\n\n     [,1] [,2]\n[1,] TRUE TRUE\n[2,] TRUE TRUE\n\nprint(all.equal(mtx1, t(mtx2)))\n\n[1] TRUE\n\n\nMatrices are indexed by [row,column]\n\nprint(mtx1[1,2])\n\n[1] 3\n\nprint(mtx2[1,2])\n\n[1] 2\n\n\nLet’s make a bigger matrix\n\nmtx3 &lt;- matrix(c(vec1, vec4), nrow = 4, ncol = 2)\nprint(mtx3)\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    2    4\n[3,]    3    9\n[4,]    4   16"
  },
  {
    "objectID": "api 222 files/section 1/section1.html#part-6-random-numbers",
    "href": "api 222 files/section 1/section1.html#part-6-random-numbers",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Part 6: Random numbers",
    "text": "Part 6: Random numbers\nYou can also generate random numbers in R. However, one concern with analyses. done using random numbers is that you might not be able to reproduce them. One way to avoid this is to set the “seed”. Here’s a reference for random seeds: https://en.wikipedia.org/wiki/Random_seed\n\nset.seed(222)\n\nWe can generate random numbers from all kinds of distributions. For now, we will generate a random normal variable. If I don’t specify a mean or variance, it will assume mean = 0, standard deviation = 1.\n\nnorm_var1 &lt;- rnorm(1)\nprint(norm_var1)\n\n[1] 1.487757\n\n\nAlternatively, we can specify the mean and standard deviation\n\nnorm_var2 &lt;- rnorm(1, mean = 100, sd = 10)\nprint(norm_var2)\n\n[1] 99.98108\n\n\nThe first element inside the parentheses is how many random variables I want to draw. For example, I could draw 10\n\nnorm_vec  &lt;- rnorm(10, mean = 5, sd = 1)\nprint(norm_vec)\n\n [1] 6.381021 4.619786 5.184136 4.753104 3.784439 6.561405 5.427310 3.798976\n [9] 6.052458 3.694936\n\n\nYou can also draw from other distributions, like the uniform distribution\n\nuni_var1 &lt;- runif(1)\nprint(uni_var1)\n\n[1] 0.2442779"
  },
  {
    "objectID": "api 222 files/section 1/section1.html#part-7-data-frames",
    "href": "api 222 files/section 1/section1.html#part-7-data-frames",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Part 7: Data frames",
    "text": "Part 7: Data frames\nThere are lots of great datasets available as part of R packages. Page 14 of Introduction to Statistical Learning with Applications in R. Table 1.1 lays out 15 data sets available from R packages. You can install a package in R using the install.packages() function. Once a package is installed you may use the library function to attach it so that it can be used. Then, every time you want to use the package, you use library(package_name)\n\nlibrary(ISLR)\n\nSometimes we will use outside datasets, not contained in R. In order to read data from a file, you have to know what kind of file it is. The table below lists functions that can import data from common plain-text formats.\n\n\n\nData Type\nFunction\n\n\n\n\ncomma separated\nread_csv()\n\n\ntab separated\nread_delim()\n\n\nother delimited formats\nread_table()\n\n\nfixed width\nread_fwf()\n\n\n\n\ncollege_data  &lt;- College\n?College\n\nLet’s learn about our data. To get the names of the columns in the dataframe, we can use the function colnames()\n\ncolnames(college_data)\n\n [1] \"Private\"     \"Apps\"        \"Accept\"      \"Enroll\"      \"Top10perc\"  \n [6] \"Top25perc\"   \"F.Undergrad\" \"P.Undergrad\" \"Outstate\"    \"Room.Board\" \n[11] \"Books\"       \"Personal\"    \"PhD\"         \"Terminal\"    \"S.F.Ratio\"  \n[16] \"perc.alumni\" \"Expend\"      \"Grad.Rate\"  \n\n\nTo find out how many rows and columns are in the dataset, use dim(). Recall that this gives us Rows followed by Columns\n\ndim(college_data)\n\n[1] 777  18\n\n\nTo find out what type of data is in each column, we can use typeof()\n\ntypeof(college_data[,1])\n\n[1] \"integer\"\n\ntypeof(college_data[,2])\n\n[1] \"double\"\n\n\nYou can also look in the “environment” tab, press the blue arrow next to college_data and it will drop down showing the column names with their types and first few values. For college, all columns except the first are numeric. The first column is a factor column, which means it’s categorical. To get a better sense of the data, let’s look at it\n\nView(college_data)\n\nTo grab a column from a dataframe in R, you have 3 popular options:\ndf$column_name\ndf[,column_number]\ndf[,\"column_name\"]\nThis will be useful so we can separate our outcome column from the feature columns. Let’s try! So that we aren’t overwhelmed by output, we will also use the function head(), which prints only the first few entries\n\nhead(college_data$PhD)\n\n[1] 70 29 53 92 76 67\n\nhead(college_data[,13])\n\n[1] 70 29 53 92 76 67\n\nhead(college_data[,\"PhD\"])\n\n[1] 70 29 53 92 76 67"
  },
  {
    "objectID": "API222.html#table-of-contents",
    "href": "API222.html#table-of-contents",
    "title": "API 222 Section Materials",
    "section": "Table of Contents",
    "text": "Table of Contents"
  },
  {
    "objectID": "api 222 files/section 1/section1.html#exercise-1-basic-operations-and-functions",
    "href": "api 222 files/section 1/section1.html#exercise-1-basic-operations-and-functions",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 1: Basic Operations and Functions",
    "text": "Exercise 1: Basic Operations and Functions\nCreate a new vector: Create a vector my.vector containing any five numbers. Print the vector. Basic calculations: Find the sum, product, and average of the numbers in my.vector.\n\n\nSample Solution\n# Create a vector with five numbers\nmy.vector &lt;- c(1, 3, 5, 7, 9)\nprint(my.vector)\n\n# Perform basic calculations\nsum(my.vector)\nprod(my.vector)\nmean(my.vector)\n\n\nUse a built-in function: Use the length() function to find the length of my.vector.\n\n\nSample Solution\n# Use a built-in function\nlength(my.vector)"
  },
  {
    "objectID": "api 222 files/section 1/section1.html#exercise-2-vector-manipulation",
    "href": "api 222 files/section 1/section1.html#exercise-2-vector-manipulation",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 2: Vector Manipulation",
    "text": "Exercise 2: Vector Manipulation\nCreate and modify a vector: Create a numeric vector numbers from 1 to 20. Then, extract and print the first 5 elements.\n\n\nSample Solution\n# Create and modify a vector\nnumbers &lt;- 1:20\nprint(numbers[1:5])\n\n\nLogical indexing: From numbers, create a new vector even.numbers that contains only the even numbers. Print even.numbers.\n\n\nSample Solution\n# Logical indexing for even numbers\neven.numbers &lt;- numbers[numbers %% 2 == 0]\nprint(even.numbers)\n\n\nVector arithmetic: Create a new vector that is the square of each element in numbers.\n\n\nSample Solution\nsquared.numbers &lt;- numbers^2\nprint(squared.numbers)"
  },
  {
    "objectID": "api 222 files/section 1/section1.html#exercise-3-matrices",
    "href": "api 222 files/section 1/section1.html#exercise-3-matrices",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 3: Matrices",
    "text": "Exercise 3: Matrices\nCreate a matrix: Convert numbers into a \\(4 \\times 5\\) matrix matrix.1. Print matrix.1.\n\n\nSample Solution\nmatrix.1 &lt;- matrix(numbers, nrow = 4, ncol = 5)\nprint(matrix.1)\n\n\nMatrix transposition: Print the transpose of matrix.1.\n\n\nSample Solution\n# Matrix transposition\nprint(t(matrix.1))\n\n\nMatrix indexing: Extract and print the element in the 2nd row and 3rd column of matrix.1.\n\n\nSample Solution\n# Matrix indexing\nprint(matrix.1[2, 3])"
  },
  {
    "objectID": "api 222 files/section 1/section1.html#exercise-4-logical-statements",
    "href": "api 222 files/section 1/section1.html#exercise-4-logical-statements",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 4: Logical Statements",
    "text": "Exercise 4: Logical Statements\nSimple if-else: Write an if-else statement that prints “Big” if the average of numbers is greater than 10, and “Small” otherwise.\n\n\nSample Solution\n# Simple if-else\nif (mean(numbers) &gt; 10) {\n  print(\"Big\")\n} else {\n  print(\"Small\")\n}\n\n\nNested if-else: Modify the above to include a check if the average is exactly 10, printing “Exactly 10”.\n\n\nSample Solution\n# Nested if-else\nif (mean(numbers) == 10) {\n  print(\"Exactly 10\")\n} else if (mean(numbers) &gt; 10) {\n  print(\"Big\")\n} else {\n  print(\"Small\")\n}"
  },
  {
    "objectID": "api 222 files/section 1/section1.html#exercise-5-random-numbers",
    "href": "api 222 files/section 1/section1.html#exercise-5-random-numbers",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 5: Random Numbers",
    "text": "Exercise 5: Random Numbers\nGenerate random numbers: Generate a vector of 5 random numbers drawn from a normal distribution with mean 0 and standard deviation 1. Print the vector.\n\n\nSample Solution\n# Generate random numbers\nrandom.numbers &lt;- rnorm(5)\nprint(random.numbers)\n\n\nReproducibility: Set a seed of your choice and generate the same vector of random numbers as above.\n\n\nSample Solution\n# Generate random numbers\nset.seed(222) # Set seed for reproducibility\nrandom.numbers &lt;- rnorm(5)\nprint(random.numbers)"
  },
  {
    "objectID": "api 222 files/section 1/section1.html#exercise-6-data-frames",
    "href": "api 222 files/section 1/section1.html#exercise-6-data-frames",
    "title": "Section 1 - Intro to API 222 and R",
    "section": "Exercise 6: Data Frames",
    "text": "Exercise 6: Data Frames\nExplore college.data: Print the first 6 rows of college.data.\n\n\nSample Solution\n# Explore `college.data`\nhead(college.data)\n\n\nColumn operations: Calculate the mean of the PhD column in college.data.\n\n\nSample Solution\n# Column operations\nmean(college.data$PhD)\n\n\nSubsetting: Create a new data frame small.college that only includes colleges with less than 5000 students (use college.data$Enroll for enrollment numbers).\n\n\nSample Solution\n# Subsetting data frame\nsmall.college &lt;- college.data[college.data$Enroll &lt; 5000, ]\nprint(small.college)"
  },
  {
    "objectID": "API222.html",
    "href": "API222.html",
    "title": "API 222 Section Materials",
    "section": "",
    "text": "Welcome to the Section Materials for API-222! This course is a journey through the fascinating world of machine learning, tailored to provide you with a strong foundation in both the theoretical and practical aspects of the field. With a focus on policy-making and decision-making applications, you will gain not only an understanding of the statistical theories behind the algorithms but also the skills to apply them to solve real-world problems.\nIn this supplementary section, our primary goal is to enhance your coding abilities. While the main lectures of the course delve into the concepts and theories of machine learning, here we will put those theories into practice.\nThe section materials are neatly segmented to mirror the course’s structure, which is divided into two main units:\n\nSupervised Learning: We start with a deep dive into algorithms designed to predict and analyze outcomes based on input data. You’ll learn to navigate through regression, classification, cross-validation, and more, using R to bring these concepts to life.\nUnsupervised Learning: The second half of the course explores the patterns and structures in data where outcomes aren’t provided. Clustering, dimensionality reduction, and other techniques will be covered, giving you a comprehensive toolkit for data analysis.\n\nWhether you are running regressions, classifying data, or implementing advanced machine learning models, these section materials are crafted to provide you with hands-on experience. Through a series of code examples, exercises, and assignments, you will build the confidence to tackle complex datasets and extract meaningful insights.\nRemember, while mastering the syntax and functions of a programming language is essential, the real magic lies in applying these tools to real-world scenarios. Let’s code our way to a deeper understanding of machine learning!"
  },
  {
    "objectID": "API222.html#section-materials-introduction",
    "href": "API222.html#section-materials-introduction",
    "title": "API 222 Section Materials",
    "section": "",
    "text": "Welcome to the Section Materials for API-222! This course is a journey through the fascinating world of machine learning, tailored to provide you with a strong foundation in both the theoretical and practical aspects of the field. With a focus on policy-making and decision-making applications, you will gain not only an understanding of the statistical theories behind the algorithms but also the skills to apply them to solve real-world problems.\nIn this supplementary section, our primary goal is to enhance your coding abilities. While the main lectures of the course delve into the concepts and theories of machine learning, here we will put those theories into practice.\nThe section materials are neatly segmented to mirror the course’s structure, which is divided into two main units:\n\nSupervised Learning: We start with a deep dive into algorithms designed to predict and analyze outcomes based on input data. You’ll learn to navigate through regression, classification, cross-validation, and more, using R to bring these concepts to life.\nUnsupervised Learning: The second half of the course explores the patterns and structures in data where outcomes aren’t provided. Clustering, dimensionality reduction, and other techniques will be covered, giving you a comprehensive toolkit for data analysis.\n\nWhether you are running regressions, classifying data, or implementing advanced machine learning models, these section materials are crafted to provide you with hands-on experience. Through a series of code examples, exercises, and assignments, you will build the confidence to tackle complex datasets and extract meaningful insights.\nRemember, while mastering the syntax and functions of a programming language is essential, the real magic lies in applying these tools to real-world scenarios. Let’s code our way to a deeper understanding of machine learning!"
  },
  {
    "objectID": "api 222 files/section 2/section2.html",
    "href": "api 222 files/section 2/section2.html",
    "title": "Section 2.1 - KNN and Linear Regression Notes",
    "section": "",
    "text": "Note that the material in these notes draws on past TF’s notes (Ibou Dieye, Laura Morris, Emily Mower, Amy Wickett), and the more thorough treatment of these topics in Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani."
  },
  {
    "objectID": "api 222 files/section 2/section1.html",
    "href": "api 222 files/section 2/section1.html",
    "title": "Section 2 - KNN and Linear Regression",
    "section": "",
    "text": "Notes build on previous TFs (Ibou Dieye, Laura Morris, Amy Wickett & Emily Mower)\nThe following code is meant as a first introduction to R. It is therefore helpful to run it one line at a time and see what happens. To run one line of code in RStudio, you can highlight the code you want to run and hit “Run” at the top of the script.\nOn a mac, you can highlight the code you want to run and hit Command + Enter. On a PC, you can highlight the code you want to run and hit Ctrl + Enter. If you ever forget how a function works, you can type ? followed immediately (e.g. with no space) by the function name to get the help file."
  },
  {
    "objectID": "api 222 files/section 2/section1.html#exercise-1-basic-operations-and-functions",
    "href": "api 222 files/section 2/section1.html#exercise-1-basic-operations-and-functions",
    "title": "Section 2 - KNN and Linear Regression",
    "section": "Exercise 1: Basic Operations and Functions",
    "text": "Exercise 1: Basic Operations and Functions\nCreate a new vector: Create a vector my.vector containing any five numbers. Print the vector. Basic calculations: Find the sum, product, and average of the numbers in my.vector.\n\n\nSample Solution\n# Create a vector with five numbers\nmy.vector &lt;- c(1, 3, 5, 7, 9)\nprint(my.vector)\n\n# Perform basic calculations\nsum(my.vector)\nprod(my.vector)\nmean(my.vector)\n\n\nUse a built-in function: Use the length() function to find the length of my.vector.\n\n\nSample Solution\n# Use a built-in function\nlength(my.vector)"
  },
  {
    "objectID": "api 222 files/section 2/section1.html#exercise-2-vector-manipulation",
    "href": "api 222 files/section 2/section1.html#exercise-2-vector-manipulation",
    "title": "Section 2 - KNN and Linear Regression",
    "section": "Exercise 2: Vector Manipulation",
    "text": "Exercise 2: Vector Manipulation\nCreate and modify a vector: Create a numeric vector numbers from 1 to 20. Then, extract and print the first 5 elements.\n\n\nSample Solution\n# Create and modify a vector\nnumbers &lt;- 1:20\nprint(numbers[1:5])\n\n\nLogical indexing: From numbers, create a new vector even.numbers that contains only the even numbers. Print even.numbers.\n\n\nSample Solution\n# Logical indexing for even numbers\neven.numbers &lt;- numbers[numbers %% 2 == 0]\nprint(even.numbers)\n\n\nVector arithmetic: Create a new vector that is the square of each element in numbers.\n\n\nSample Solution\nsquared.numbers &lt;- numbers^2\nprint(squared.numbers)"
  },
  {
    "objectID": "api 222 files/section 2/section1.html#exercise-3-matrices",
    "href": "api 222 files/section 2/section1.html#exercise-3-matrices",
    "title": "Section 2 - KNN and Linear Regression",
    "section": "Exercise 3: Matrices",
    "text": "Exercise 3: Matrices\nCreate a matrix: Convert numbers into a \\(4 \\times 5\\) matrix matrix.1. Print matrix.1.\n\n\nSample Solution\nmatrix.1 &lt;- matrix(numbers, nrow = 4, ncol = 5)\nprint(matrix.1)\n\n\nMatrix transposition: Print the transpose of matrix.1.\n\n\nSample Solution\n# Matrix transposition\nprint(t(matrix.1))\n\n\nMatrix indexing: Extract and print the element in the 2nd row and 3rd column of matrix.1.\n\n\nSample Solution\n# Matrix indexing\nprint(matrix.1[2, 3])"
  },
  {
    "objectID": "api 222 files/section 2/section1.html#exercise-4-logical-statements",
    "href": "api 222 files/section 2/section1.html#exercise-4-logical-statements",
    "title": "Section 2 - KNN and Linear Regression",
    "section": "Exercise 4: Logical Statements",
    "text": "Exercise 4: Logical Statements\nSimple if-else: Write an if-else statement that prints “Big” if the average of numbers is greater than 10, and “Small” otherwise.\n\n\nSample Solution\n# Simple if-else\nif (mean(numbers) &gt; 10) {\n  print(\"Big\")\n} else {\n  print(\"Small\")\n}\n\n\nNested if-else: Modify the above to include a check if the average is exactly 10, printing “Exactly 10”.\n\n\nSample Solution\n# Nested if-else\nif (mean(numbers) == 10) {\n  print(\"Exactly 10\")\n} else if (mean(numbers) &gt; 10) {\n  print(\"Big\")\n} else {\n  print(\"Small\")\n}"
  },
  {
    "objectID": "api 222 files/section 2/section1.html#exercise-5-random-numbers",
    "href": "api 222 files/section 2/section1.html#exercise-5-random-numbers",
    "title": "Section 2 - KNN and Linear Regression",
    "section": "Exercise 5: Random Numbers",
    "text": "Exercise 5: Random Numbers\nGenerate random numbers: Generate a vector of 5 random numbers drawn from a normal distribution with mean 0 and standard deviation 1. Print the vector.\n\n\nSample Solution\n# Generate random numbers\nrandom.numbers &lt;- rnorm(5)\nprint(random.numbers)\n\n\nReproducibility: Set a seed of your choice and generate the same vector of random numbers as above.\n\n\nSample Solution\n# Generate random numbers\nset.seed(222) # Set seed for reproducibility\nrandom.numbers &lt;- rnorm(5)\nprint(random.numbers)"
  },
  {
    "objectID": "api 222 files/section 2/section1.html#exercise-6-data-frames",
    "href": "api 222 files/section 2/section1.html#exercise-6-data-frames",
    "title": "Section 2 - KNN and Linear Regression",
    "section": "Exercise 6: Data Frames",
    "text": "Exercise 6: Data Frames\nExplore college.data: Print the first 6 rows of college.data.\n\n\nSample Solution\n# Explore `college.data`\nhead(college.data)\n\n\nColumn operations: Calculate the mean of the PhD column in college.data.\n\n\nSample Solution\n# Column operations\nmean(college.data$PhD)\n\n\nSubsetting: Create a new data frame small.college that only includes colleges with less than 5000 students (use college.data$Enroll for enrollment numbers).\n\n\nSample Solution\n# Subsetting data frame\nsmall.college &lt;- college.data[college.data$Enroll &lt; 5000, ]\nprint(small.college)"
  },
  {
    "objectID": "api 222 files/section 2/section2.html#k-nearest-neighbors",
    "href": "api 222 files/section 2/section2.html#k-nearest-neighbors",
    "title": "Section 2.1 - KNN and Linear Regression Notes",
    "section": "K-Nearest Neighbors",
    "text": "K-Nearest Neighbors\n\nConcept\nThe idea underlying K-Nearest Neighbors (KNN) is that we expect observations with similar features to have similar outcomes. KNN makes no other assumptions about functional form, so it is quite flexible.\n\n\nMethod\nKNN can be used for either regression or classification, though it works slightly differently depending on what setting we are in. In the classification setting, the prediction is a majority vote of the observation’s \\(K\\)-nearest neighbors. In the regression setting, the prediction is the average outcome of the observation’s \\(K\\)-nearest neighbors.\nFor KNN, bias will be lower when \\(K\\) is lower. Bias will increase quickly as k increases, with further away neighbors being included in the prediction.\nThe only choice we have to make when implementing KNN is the value of \\(K\\) (e.g. how many neighbors should we use in our prediction?). A good way to find \\(K\\) is through cross-validation, something we will cover a little later, but which broadly involves training the algorithm on one set of data and seeing how well it does on a different set.\n\n\nImplementation and Considerations\nA concern with KNN is whether you have good coverage of your feature space. Imagine that all of your training points were in one region of the feature space, but some of your test points are far away from this region. You will still use the \\(K\\) nearest neighbors to predict the outcome for these far-away test points, but it might not work as well as if the points were close together. Therefore, when implementing KNN, it’s good to think about how similar the features in your test set will be to the features in your training set. If they differ systematically, that is a concern (as it would be for other ML methods as well).\nAnother important consideration is whether there is an imbalance in the frequency of one outcome compared to another. For example, suppose we are trying to classify points as true'' orfalse’’ and most points are true.'' Even if thefalse’’ outcomes are clustered together in the feature space, if we use a large enough value of \\(K\\), we will predict true'' for these observations simply because there are many moretrue’’ observations than ``false’’ observations. Therefore, we would do better to use a small value for \\(K\\) in this setting.\nAnother consideration is whether proximity in each variable is equally important or if proximity in one variable is more important than proximity in another variable. KNN will normalize variables so that they are all on the same scale (same mean and variance) and then treat distance in all normalized variables the same. If you want to up-weight proximity for some variables and down-weight it for others, you can change the way each variable is normalized to accomplish this. Alternatively, you can include only those variables you think are important. When you have this type of uncertainty, there are more principled ways of selecting variables that will be discussed later in the course.\n\n\nExtensions\nYou might think that neighbors that are really close should be weighted more than neighbors that are a bit further away. Many people agree, so there are methods to allow you to weight different observations differently. You might also think that you shouldn’t use just the \\(K\\) nearest neighbors, but all the neighbors within a certain distance. Or maybe you think there’s information available in all observations, but there’s more information in closer neighbors. All of these adjustments fall under the umbrella of kernel regression. In fact, KNN is a special case of kernel regression. Broadly defined, kernel regression methods are a class of methods that generate predictions by taking weighted averages of observations. Because these methods (KNN included) do not specify a functional form, they are called ``non-parametric regression’’ methods."
  },
  {
    "objectID": "api 222 files/section 2/section2.html#linear-regression",
    "href": "api 222 files/section 2/section2.html#linear-regression",
    "title": "Section 2.1 - KNN and Linear Regression Notes",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nConcept\nLinear regression is a parametric model that is additive and linear in the provided features. It is a classic technique used in many fields, and its widespread popularity greatly pre-dates the popularity of machine learning. Its general form is\n\\[\\begin{equation}\n    \\hat{y} = \\hat{\\beta}X\n\\end{equation}\\]\nWhere \\(\\hat{y}\\) is a vector of predicted \\(y\\) values and \\(X\\) is a matrix whose rows correspond to observations and whose columns correspond to features.\nWhen there is only one feature on the right hand side, the model is called a simple linear regression.\" When there are multiple features on the right hand side, the model is calledmultiple linear regression.”\nWhen used for inference, we are interested in \\(\\hat{\\beta}\\). However, when used for prediction, we are only interested in \\(\\hat{y}\\), and we cannot say that the \\(\\hat{\\beta}\\)s reflect any sort of causal relationship between the features and the outcome. For more information on how to test the significance of regression coefficients, please see Chapter 3 of ISLR for a reference on \\(t\\)-tests (in the simple model) and \\(F\\)-tests (in the multivariate model).\n\n\nMethod\nTo find the coefficients \\(\\hat{\\beta}\\) in a linear regression, we find the value of \\(\\hat{\\beta}\\) that minimizes the residual sum of squares (RSS) in the training data. The classic formula for \\(\\hat{\\beta}\\) uses matrix algebra and is \\[\\begin{equation}\n    \\hat{\\beta} = (X^\\prime X)^{-1}X^\\prime y\n\\end{equation}\\] We will estimate \\(\\hat{\\beta}\\) using statistical software.\nIt is worth noting that the traditional measure of fit for linear regression is \\(R^2\\), but \\(R^2\\) mechanically increases with the inclusion of additional features. Therefore, in the prediction setting, the \\(R^2\\) on the training data is less important than the mean squared error (MSE) on the test data.\n\n\nImplementation and Considerations\nThere are a few things to watch out for as far as the features that you feed into a linear regression.\n\nThere must be fewer features than observations. Later in the semester, we will cover penalized regression methods that do variable selection to yield estimable linear models, even when the number of available features exceeds the number of observations. Common penalized regression methods are lasso and ridge regression.\nYou can use quantitative or qualitative features for the \\(X\\)s. When using qualitative features, generate indicator variables for all but one category. The omitted category will serve as the ``baseline,’’ meaning that the coefficients on the included categories can be thought of as the differential effect of being in that category compared to the baseline (omitted) one.\nThe reason you omit one category when making indicator variables is to avoid linear dependence. If all categories were represented, the indicator columns would all sum to 1, which would mean they were linearly dependent. More generally, you cannot have collinearity or multi-collinearity, which means you cannot have features that are (close to) perfectly correlated.\nYou can interact two features (e.g. create a feature that is the product of two other features), and such interactions are valid on categorical and continuous features. However, when you include an interaction, you should also include each of the features on their own as well. Interactions have intuitive appeal if you think there are synergies between two features in terms of their effect on \\(y\\).\nYou can exponentiate features and include the exponentiated features in your model. The resulting model is sometimes called polynomial regression and is appropriate when there appears to be a non-linear relationship between a feature and the outcome.\nCheck for influential points – those that are both outliers (they have an unusual or extreme \\(y\\) value) and high leverage (they have an unusual or extreme \\(x\\)), as these points can greatly influence the model fit. You may want to exclude them or at least check your model’s sensitivity to including them versus excluding them.\n\nIf you are interested in inference (e.g. looking at the \\(\\hat{\\beta}\\)s to understand a causal relationship), it is important to be aware of Omitted Variables Bias (OVB). OVB occurs when you have two correlated features that each have an effect on \\(y\\) but only one is included in the regression. In that case, the coefficient on the included feature is biased, because it is partially picking up the true effect of the feature on the outcome and is also partially picking up the effect of the omitted feature on the outcome (since the omitted feature is correlated with the included feature).\nAs an example of OVB, suppose \\(X_1\\) and \\(X_2\\) are positively (but not perfectly) correlated. If they are also both positively correlated with \\(y\\), then when \\(X_2\\) is omitted from the regression, the coefficient on \\(X_1\\) will be higher than when both \\(X_1\\) and \\(X_2\\) are included. This is because the coefficient on \\(X_1\\) will now pick up both the effect of \\(X_1\\) on \\(y\\) and part of the effect of \\(X_2\\) on \\(y\\) (since \\(X_1\\) is a proxy for \\(X_2\\) because the two features are positively correlated).\nWhen implementing linear regression, you should also take a look at your residuals and make sure there are no red flags:\n\nWhen you plot residuals, they should appear randomly scattered. Any non-linearity or patterns in the residuals suggest your model is not appropriate.\nLinear regression assumes residuals are uncorrelated. Evidence of correlated residuals indicates a problem with your model or your data that should be investigated.\nResiduals should have constant variance. If you plot your residuals and their variance seems to be a function of \\(x\\), then the errors are heteroskedastic (a fancy word for ``a function of \\(x\\)’’). In this case, traditional statistical measures of significance are invalid, but other valid methods are available.\n\n\n\nComparison to KNN\nThe main difference of note between linear regression and KNN is that linear regression is a parametric model whereas KNN is a non-parametric model. There are a few general differences between parametric and non-parametric models that are worth noting\n\nNon-parametric models are more flexible whereas parametric models impose stronger assumptions\nWhen there is a small number of observations per feature, parametric models tend to outperform non-parametric models\n\nIn addition to the general differences between parametric and non-parametric models, a key difference between KNN and linear regression is that linear regression is quite simple to fit. In fact, it only requires estimation of a few \\(\\beta\\)s, whereas KNN is much more computationally intensive. Because of this simplicity, linear regression is also more interpretable than KNN."
  },
  {
    "objectID": "api 222 files/section 2/section2.html#important-machine-learning-concepts",
    "href": "api 222 files/section 2/section2.html#important-machine-learning-concepts",
    "title": "Section 2.1 - KNN and Linear Regression Notes",
    "section": "Important Machine Learning Concepts",
    "text": "Important Machine Learning Concepts\n\nRegression vs Classification\nPrediction problems can be defined based on the characteristics of the outcome variable we want to predict.\n\nRegression problems are those where the outcome is quantitative\nClassification problems are those where the outcome is qualitative / categorical\n\nSometimes the same methods can be used for regression and classification problems, but many methods are useful for only one of the two problem types.\n\n\nBias-Variance Trade-off\nThe variance of a statistical learning method is the amount by which the prediction function would change if it was estimated on a different training set. A model that overfits has high variance, whereas a model that underfits has low variance.\nTo remember the difference between low variance and high variance models, I find it helpful to think of examples. Suppose your model was ``use the mean of the training data as the predicted value for all new data points.’’ The mean shouldn’t change much across training sets, so this has low variance. On the other hand, a model that picked up super complex patterns is likely to be picking up noise in addition to signal. The noise will vary by training set, so such a method would have high variance.\nThe bias of a statistical learning method is the error produced by representing a real world problem by a statistical learning method. Very flexible models (which are prone to overfitting) can capture complex patterns and so tend to have low bias. Very simple models (which are prone to underfitting) are limited in their ability to pick up patterns and so may have high bias.\nThe book uses the example of representing a non-linear function by a linear one to show that no matter how much data you have, a linear model will not do a great prediction job when the process generating the data is non-linear. Bias also applies to methods that might not fit your traditional concept of a statistical function. In the K-Nearest Neighbors section, we will discuss bias in that setting.\nOften, we will talk about the bias-variance trade-off. In an ideal world, we would find a model that has low variance and low bias, because that would yield a good and consistent model. In practice, you usually have to allow bias to increase in order to decrease variance and vice versa. However, there are many models that will decrease one (bias or variance) significantly while only increasing the other a little.\n\n\nSupervised v. Unsupervised Learning\nSupervised learning refers to problems where there is a known outcome. In these problems, you can train a model to take features and predict the known outcome.\nUnsupervised learning refers to problems where you are interested in uncovering patterns and do not have a target outcome in mind.\nAn example of supervised learning would be using students’ high school grades, class enrollments, and demographic variables to predict whether or not they attend college.\nAn example of unsupervised learning would be using the same grades, enrollment, and demographic features to identify ``types’’ of high school students. That is, students who look similar according to these features. Perhaps you are interested in this because you want to make classes that contain a mix of different types of students. Often, unsupervised learning is useful for creating features for supervised learning problems, but sometimes uncovering patterns is the final objective.\n\n\nMeasuring Model Performance\nThere are different functions you can use to measure model performance, and which function you choose depends on your data and your objective. These functions are called ``loss functions,’’ which is a somewhat intuitive name when you think about the fact that your machine learning algorithm is trying to minimize this function and thus minimize your loss.\nTo understand how and why loss functions depend on your data and objectives, examples can be helpful.\nConsider first that you are trying to predict the future college majors of this year’s incoming freshmen (a classification problem). In this case, your prediction will either be right (you predict the major they end up choosing) or it will be wrong. Therefore, you might use accuracy (% correct) to measure model performance.\nWhat if, though, you cared more about being wrong for some majors than others? For example, imagine that all biology majors are going to need personalized lab equipment in their junior year and that the lab equipment is really expensive if ordered last minute but a lot cheaper if ordered a year or more in advance? Then, you might want to give more weight to people who end up being biology majors so that your model does better for predicting biology majors than other majors.\nNow consider that you are trying to predict home prices (a regression problem). You might measure your performance using mean-squared error (MSE), which is found by taking the difference between the predicted sale price for each home and the true sale price (the error), squaring it for each home, and then taking the mean of these squared errors. However, home prices are skewed (e.g. some homes are extremely expensive compared to most homes on the market). This means that a 5% error on a $3 million home is a lot bigger than a 5% error on a $100,000 home. When you square the errors (as you do when calculating MSE), the difference becomes enormous.\nBut since both errors are 5%, maybe you want to penalize them the same. One option is to use Mean Percentage Error (MPE), but this has the weird effect that if you over-predict one home by 5% and under-predict the other by 5%, your MPE is zero. Therefore, a popular option is to use the Mean Absolute Percentage Error (MAPE), which is the mean of the absolute values of the percentage errors and thus would be 5% in this example.\nFor many prediction problems in the policy sphere, we may not only care about accuracy of prediction but also about fairness or other objectives. The loss function is a place where we can explicitly tell the model to optimize for these concerns in addition to predictive performance."
  },
  {
    "objectID": "api 222 files/section 2/section2.1.html",
    "href": "api 222 files/section 2/section2.1.html",
    "title": "Section 2.2 - KNN and Linear Regression Code",
    "section": "",
    "text": "There are lots of great datasets available as part of R packages. Page 14 of Introduction to Statistical Learning with Applications in R Table 1.1 lays out 15 data sets available from R packages. We will use the College dataset from the ISLR package. The first time you ever use a package, you need to install it. Then, every time you want to use the package, you use library(package_name). We will use the college data. Note that details on this data are available online: https://cran.r-project.org/web/packages/ISLR/ISLR.pdf Page 5. You can also get the same information in R by typing: help(“College”) or ?College."
  },
  {
    "objectID": "api 222 files/section 2/section2.1.html#explore-data",
    "href": "api 222 files/section 2/section2.1.html#explore-data",
    "title": "Section 2.2 - KNN and Linear Regression Code",
    "section": "Explore Data",
    "text": "Explore Data\n\nlibrary(ISLR)\n\ndata(College)\ncollege_data  &lt;- College\n\nLet’s learn about our data. To get the names of the columns in the dataframe, we can use the function colnames()\n\ncolnames(college_data)\n\n [1] \"Private\"     \"Apps\"        \"Accept\"      \"Enroll\"      \"Top10perc\"  \n [6] \"Top25perc\"   \"F.Undergrad\" \"P.Undergrad\" \"Outstate\"    \"Room.Board\" \n[11] \"Books\"       \"Personal\"    \"PhD\"         \"Terminal\"    \"S.F.Ratio\"  \n[16] \"perc.alumni\" \"Expend\"      \"Grad.Rate\"  \n\n\nTo find out how many rows and columns are in the dataset, use dim() Recall that this gives us Rows followed by Columns\n\ndim(college_data)\n\n[1] 777  18\n\n\nYou can also look in the “environment” tab, press the blue arrow next to college_data and it will drop down showing the column names with their types and first few values. For college, all columns except the first are numeric. The first column is a factor column, which means it’s categorical. To get a better sense of the data, let’s look at it:\n\nView(college_data)\n\nSuppose we are interested in predicting whether a college is private or public based on available covariates, like Number accepted, enrolled, etc. Additionally, let’s suppose you don’t want certain variables included in your dataset. You can drop these functions using -c(). For example, let’s suppose you don’t want the Apps or Student to Faculty Ratio included in your dataset.\n\ncollege_data &lt;- college_data[, -c(15, 2)]\n\nBe careful when you are dropping multiple columns. You need to put the numbers in reverse order (from highest to lowest). This is because if you drop the second column first, then the 15th column becomes the the 14th column.\n\ncollege_data &lt;- College\ncollege_data &lt;- college_data[, -c(2)]\ncollege_data &lt;- college_data[, -c(15)]\n\nA less manual way of dropping columns is to use R to first use R to find the corresponding indices in the data columns. Go back to the original college data\n\ncollege_data &lt;- College\n\nFind the indices (i.e. column positions) of the columns to drop\n\nto_drop &lt;- which(names(college_data) %in% c(\"Apps\", \"S.F.Ratio\"))\nprint(to_drop)\n\n[1]  2 15\n\n\nReverse the indices as suggested above\n\nto_drop &lt;- rev(to_drop)\nprint(to_drop)\n\n[1] 15  2\n\n\nNow use the object you have defined to drop the columns\n\ncollege_data &lt;- college_data[, -c(to_drop)]\n\nAlso sometimes we have factor variables that we want to convert to numeric variables. To check variable types, you can use the “str” function\n\nstr(college_data)\n\n'data.frame':   777 obs. of  16 variables:\n $ Private    : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n $ Accept     : num  1232 1924 1097 349 146 ...\n $ Enroll     : num  721 512 336 137 55 158 103 489 227 172 ...\n $ Top10perc  : num  23 16 22 60 16 38 17 37 30 21 ...\n $ Top25perc  : num  52 29 50 89 44 62 45 68 63 44 ...\n $ F.Undergrad: num  2885 2683 1036 510 249 ...\n $ P.Undergrad: num  537 1227 99 63 869 ...\n $ Outstate   : num  7440 12280 11250 12960 7560 ...\n $ Room.Board : num  3300 6450 3750 5450 4120 ...\n $ Books      : num  450 750 400 450 800 500 500 450 300 660 ...\n $ Personal   : num  2200 1500 1165 875 1500 ...\n $ PhD        : num  70 29 53 92 76 67 90 89 79 40 ...\n $ Terminal   : num  78 30 66 97 72 73 93 100 84 41 ...\n $ perc.alumni: num  12 16 30 37 2 11 26 37 23 15 ...\n $ Expend     : num  7041 10527 8735 19016 10922 ...\n $ Grad.Rate  : num  60 56 54 59 15 55 63 73 80 52 ...\n\n\nYou can see that the Private variable is a factor. We can convert it to a numeric variable using the “as.numeric” function. I like my binary variables in R to be 0/1. In R, most factors automatically convert to a binary 1/2 format. I usually prefer a binary 0/1 format. To transform, I subtract 1.\n\ncollege_data$Private &lt;- as.numeric(college_data$Private) - 1 \nsummary(college_data$Private)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  1.0000  0.7272  1.0000  1.0000 \n\nsummary(College$Private)\n\n No Yes \n212 565 \n\n\nLet’s get back our original sample\n\ncollege_data &lt;- College"
  },
  {
    "objectID": "api 222 files/section 2/section2.1.html#testing-and-training-sets",
    "href": "api 222 files/section 2/section2.1.html#testing-and-training-sets",
    "title": "Section 2.2 - KNN and Linear Regression Code",
    "section": "Testing and Training Sets",
    "text": "Testing and Training Sets\nIn order to make this interesting, let’s split our data into a training set and a test set. To do this, we will use set.seed(), which will allow us to draw the same pseudorandom numbers the next time we run this code, and we will use the sample() function.\n\nset.seed(222)\n\nThe sample() function takes two arguments: The first is a vector of numbers from which to draw a random sample. The second is the number of random numbers to draw. The default is to sample without replacement, but you can sample with replacement by adding “, replace = TRUE” inside the function. Now, let’s generate a list of indices from the original dataset that will be designated part of the test set using sample()\n\ntest_ids &lt;- sample(1:(nrow(college_data)), round(0.2 * nrow(college_data)))\n\nTo identify the training_ids, we want all of the numbers from 1:nrow(college_data) that aren’t test IDs. Recall that which() returns the indices for which the statement inside the parentheses is true. which(!()) returns the indices for which the statement inside the parentheses is false. The “!” means “not”. Also, if you wanted to know which values of vector A were in vector B, you can use which(A %in% B). So if you want to know which values of vector A are NOT in vector B, you use which(!(A %in B)), so that’s what we will do – vector A is the vector of all integers between 1 and the number of rows in our data. vector B is the vector of test IDs\n\ntraining_ids &lt;- which(!(1:(nrow(college_data)) %in% test_ids))\n\nWe can use these indices to define our test and training sets by putting those vectors in the row position inside square brackets.\n\ntest_data &lt;- college_data[test_ids,]\ntraining_data &lt;- college_data[training_ids,]"
  },
  {
    "objectID": "api 222 files/section 2/section2.1.html#knn-classification",
    "href": "api 222 files/section 2/section2.1.html#knn-classification",
    "title": "Section 2.2 - KNN and Linear Regression Code",
    "section": "KNN Classification",
    "text": "KNN Classification\nLet’s develop a KNN model to try to predict whether it’s a private college using all available features.\nTo use KNN for classification, we need to install and load the library “class”\n\nlibrary(class)\n\nknn() is the function we will use to run the KNN model. It takes four arguments:\n\ntrain = training data features (no outcome)\ntest = test data features (no outcome)\ncl = training data outcome (class each observation belongs to)\nk = number of nearest neighbors to use\n\nFor two-class classification problems, k should be odd (avoids tied votes). Let’s run the model with 1 NN and 9 NNs. To exclude a column, use -# in the column position insider square brackets. (e.g. df[, -2] excludes the second column of dataframe df)\n\nknn_model1 &lt;- knn(train = training_data[, -1],\n                  test = test_data[, -1],\n                  cl = training_data[, 1],\n                  k = 1)\n\nknn_model9 &lt;- knn(train = training_data[, -1],\n                  test = test_data[, -1],\n                  cl = training_data[, 1],\n                  k = 9)\n\nWe are trying to predict Private Yes/No. knn() output predicted values for our test data, so we can compare actual v. predicted values. “prediction == actual” gives a vector with the same number of elements as there are observations in the test set. Each element will either be TRUE (the prediction was correct) or FALSE (the prediction was wrong). Applying which() to this vector will yield the index numbers for all the elements equal to TRUE. Applying length() to that vector tells us how many are TRUE (e.g. for how many observations prediction == actual). We can then divide by the number of observations in the test data to obtain the accuracy rate\n\naccuracy1  &lt;- length(which(knn_model1 == test_data$Private)) / nrow(test_data)\naccuracy9 &lt;- length(which(knn_model9 == test_data$Private)) / nrow(test_data)\n\nprint(accuracy1)\n\n[1] 0.9096774\n\nprint(accuracy9)\n\n[1] 0.9225806\n\n\nLet’s visualize what is happening in a KNN classification model. We will use the ggplot2 package to create a scatterplot of the training data, and then overlay the test data on top of it. We will color the points by whether the school is private or not.\n\nlibrary(ggplot2)\nggplot(data = training_data, \n       aes(x = Outstate, y = F.Undergrad, \n           color = as.factor(Private))) +\n  geom_point() +\n  geom_point(data = test_data, aes(x = Outstate, y = F.Undergrad), \n             color = \"black\", size = 1) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  theme_minimal() +\n  guides(color = guide_legend(title = \"Private\")) \n\n\n\n\nThis seems like excellent predictive performance. However, it’s good to think about the distribution of the data. As an extreme example, if all schools in the data were private, we would expect 100% prediction accuracy regardless of our model. Let’s see how well we do if our prediction is all schools are Private. Start by calculating the proportion of private schools\n\nprint(length(which(test_data$Private == \"Yes\")) / nrow(test_data))\n\n[1] 0.716129\n\n\nWe can also check our accuracy on Private schools v. Public schools. To do this, we need to figure out which schools are private in the test data. Specifically, get the indices for the private schools\n\nprivate_schools &lt;- which(test_data$Private == \"Yes\")\npublic_schools &lt;- which(test_data$Private == \"No\")\n\nprint(private_schools)\n\n  [1]   1   2   3   8  11  12  13  14  15  16  18  19  20  21  22  23  24  27\n [19]  28  29  30  31  32  33  34  35  37  38  39  41  42  43  44  45  47  48\n [37]  49  50  51  52  53  55  56  57  58  60  61  62  63  64  65  66  68  69\n [55]  76  77  78  80  81  82  84  85  86  90  91  92  93  94  96  97  99 100\n [73] 101 102 104 106 107 108 110 112 113 116 119 120 121 122 123 125 127 128\n [91] 129 130 133 134 135 136 137 138 139 140 142 145 146 147 148 149 151 152\n[109] 153 154 155\n\nprint(public_schools)\n\n [1]   4   5   6   7   9  10  17  25  26  36  40  46  54  59  67  70  71  72  73\n[20]  74  75  79  83  87  88  89  95  98 103 105 109 111 114 115 117 118 124 126\n[39] 131 132 141 143 144 150\n\n\nTo calculate the prediction accuracy for private schools, we need to know how many (true not predicted) private schools are in the test data. Likewise, we need to know how many public schools are in the test data.\n\nnum_private_schools &lt;- length(private_schools)\nnum_public_schools &lt;- length(public_schools)\n\nNow we will calculate the prediction accuracy separately for private and public schools.\n\nprivate_accuracy1 &lt;- length(\n  which(knn_model1[private_schools] == test_data$Private[private_schools])) /\n  num_private_schools\n\nprivate_accuracy9 &lt;- length(\n  which(knn_model9[private_schools] == test_data$Private[private_schools])) /\n  num_private_schools\n\nNow we will calculate the prediction accuracy separately for private and public schools.\n\n## Private schools (% correctly predicted):\nprivate_accuracy1 &lt;- length(\n  which(knn_model1[private_schools] == test_data$Private[private_schools])) /\n  num_private_schools\n\nprivate_accuracy9 &lt;- length(\n  which(knn_model9[private_schools] == test_data$Private[private_schools])) /\n  num_private_schools\n\n\n# Public schools (% correctly predicted): \npublic_accuracy1 &lt;- length(\n  which(knn_model1[public_schools] == test_data$Private[public_schools])) /\n  num_public_schools\n\npublic_accuracy9 &lt;- length(\n  which(knn_model9[public_schools] == test_data$Private[public_schools])) /\n  num_public_schools\n\nLet’s see how it did on different school types:\n\nprint(private_accuracy1)\n\n[1] 0.9459459\n\nprint(public_accuracy1)\n\n[1] 0.8181818\n\nprint(private_accuracy9)\n\n[1] 0.972973\n\nprint(public_accuracy9)\n\n[1] 0.7954545\n\n\nTherefore, we did better on private schools than public schools because our prediction accuracy was higher on private schools. Thinking about differential performance by label is related to fairness of machine learning algorithms. For an interesting discussion on ML fairness and different ways to define fairness, see the following academic paper:\nJon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. \nInherent Trade-Offs in the Fair\nDetermination of Risk Scores, November 2016"
  },
  {
    "objectID": "api 222 files/section 2/section2.1.html#knn-for-regression",
    "href": "api 222 files/section 2/section2.1.html#knn-for-regression",
    "title": "Section 2.2 - KNN and Linear Regression Code",
    "section": "KNN for Regression",
    "text": "KNN for Regression\nSuppose we wanted to predict how many students would enroll given the other features available in the data. In that case, the classification function we used above will not work. We will need a KNN function designed for regression problems. This function is knn.reg() in the FNN package, so we should install then read in the FNN package.\n\n#install.packages(\"FNN\")\nlibrary(FNN)\n\nknn.reg() takes four arguments: - training data with only features (no outcome) - test data with only features (no outcome) - training outcomes - k = number of neighbors\nEnrollment is the fourth column, so we will exclude that from the features. Because public / private is a factor, we either need to convert it to a numeric variable or exclude it. We will exclude it for now. Note that you can scale your features using scale(). Deciding to scale your features or not is problem dependent. We will not scale here. If you’re not sure whether or not to scale, you can always try it both ways and see how the performance changes.\n\nknn_reg1 &lt;- knn.reg(training_data[, -c(1, 4)],\n                    test_data[, -c(1, 4)],\n                    training_data$Enroll,\n                    k = 1)\n\nknn_reg5 &lt;- knn.reg(training_data[, -c(1, 4)],\n                    test_data[,-c(1, 4)],\n                    training_data$Enroll,\n                    k = 5)\n\nMSE is an appropriate loss function for regression whereas accuracy is only relevant for classification\n\nmse_knn1 &lt;- mean((knn_reg1$pred - test_data$Enroll)^2)\nmse_knn5 &lt;- mean((knn_reg5$pred - test_data$Enroll)^2)\n\nprint(mse_knn1)\n\n[1] 124500.9\n\nprint(mse_knn5)\n\n[1] 73296.56"
  },
  {
    "objectID": "api 222 files/section 2/section2.1.html#standard-linear-regression",
    "href": "api 222 files/section 2/section2.1.html#standard-linear-regression",
    "title": "Section 2.2 - KNN and Linear Regression Code",
    "section": "Standard Linear Regression",
    "text": "Standard Linear Regression\nWe will now do linear regression. To run a linear regression in R, we use the function lm(), which stands for linear model. lm() takes two main arguments. The first is the formula, which should be of the form Dependent Variable ~ Feature1 + Feature2 + … The second is the training data – including both features and the outcome. Note that “~.” means regress this variable on all other variables\n\nenroll_reg &lt;- lm(Enroll ~ ., training_data)\n\nlm() returns a list, which includes among other things coefficients, residuals, and fitted values for the training data. You can look at the elements in RStudio by using the blue arrow next to enroll_reg in the environment tab. In order to call one element of a list, you can use $\n\nenroll_reg$coefficients\n\n  (Intercept)    PrivateYes          Apps        Accept     Top10perc \n187.938169332   7.806939217  -0.027759639   0.146504112   4.016271367 \n    Top25perc   F.Undergrad   P.Undergrad      Outstate    Room.Board \n -2.268522370   0.144249348  -0.011850547  -0.003105257  -0.024152785 \n        Books      Personal           PhD      Terminal     S.F.Ratio \n -0.027184975   0.008447046  -0.431202139  -0.539993156  -0.253400149 \n  perc.alumni        Expend     Grad.Rate \n  2.319201329   0.003018132   0.135713594 \n\n\nIn order to see a more traditional regression output, use summary()\n\nsummary(enroll_reg)\n\n\nCall:\nlm(formula = Enroll ~ ., data = training_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1284.27   -60.18    -8.62    51.46  1544.82 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 187.938169  89.614941   2.097 0.036393 *  \nPrivateYes    7.806939  30.036405   0.260 0.795017    \nApps         -0.027760   0.008149  -3.406 0.000702 ***\nAccept        0.146504   0.014710   9.959  &lt; 2e-16 ***\nTop10perc     4.016271   1.283269   3.130 0.001834 ** \nTop25perc    -2.268522   0.996912  -2.276 0.023222 *  \nF.Undergrad   0.144249   0.004298  33.560  &lt; 2e-16 ***\nP.Undergrad  -0.011851   0.006753  -1.755 0.079809 .  \nOutstate     -0.003105   0.004185  -0.742 0.458327    \nRoom.Board   -0.024153   0.010712  -2.255 0.024500 *  \nBooks        -0.027185   0.049391  -0.550 0.582244    \nPersonal      0.008447   0.013655   0.619 0.536410    \nPhD          -0.431202   1.002234  -0.430 0.667174    \nTerminal     -0.539993   1.094362  -0.493 0.621887    \nS.F.Ratio    -0.253400   2.843330  -0.089 0.929015    \nperc.alumni   2.319201   0.879334   2.637 0.008568 ** \nExpend        0.003018   0.002639   1.144 0.253219    \nGrad.Rate     0.135714   0.647956   0.209 0.834168    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 202.3 on 604 degrees of freedom\nMultiple R-squared:  0.9558,    Adjusted R-squared:  0.9546 \nF-statistic: 768.8 on 17 and 604 DF,  p-value: &lt; 2.2e-16\n\n\nIf you want to use the coefficients from enroll_reg to predict enrollment values in the test data, you can use the function predict(). The first argument is the lm object (the whole thing – not just the coefficients) and the second argument is the test data frame without the outcome column\n\npredicted_enroll &lt;- predict(enroll_reg, test_data[, -4])\n\nLet’s see how well we did in terms of MSE\n\nMSE_lm_enroll &lt;- mean((predicted_enroll - test_data$Enroll)^2)\nprint(MSE_lm_enroll)\n\n[1] 39312.19\n\n\nWe can see how this compared to our training MSE\n\nprint(mean((enroll_reg$residuals)^2))\n\n[1] 39760.39\n\n\nTraining MSE as % of Test MSE:\n\nprint(mean((enroll_reg$residuals)^2) / MSE_lm_enroll)\n\n[1] 1.011401\n\n\nWe know that the coefficients might change if we exclude some variables. Let’s pretend we only had Apps and Accept (columns 2 and 3) as features\n\nsmall_enroll_reg  &lt;- lm(Enroll ~ Apps + Accept, training_data)\n\nWe can compare coefficients from the small regression and the full regression. If the coefficients in the small regression are different from the coefficients in the full regression, then the small regression suffers from Omitted Variables Bias (OVB).\n\nsmall_enroll_reg$coefficients\n\n(Intercept)        Apps      Accept \n86.88115150 -0.05243254  0.42420181 \n\nenroll_reg$coefficients\n\n  (Intercept)    PrivateYes          Apps        Accept     Top10perc \n187.938169332   7.806939217  -0.027759639   0.146504112   4.016271367 \n    Top25perc   F.Undergrad   P.Undergrad      Outstate    Room.Board \n -2.268522370   0.144249348  -0.011850547  -0.003105257  -0.024152785 \n        Books      Personal           PhD      Terminal     S.F.Ratio \n -0.027184975   0.008447046  -0.431202139  -0.539993156  -0.253400149 \n  perc.alumni        Expend     Grad.Rate \n  2.319201329   0.003018132   0.135713594"
  },
  {
    "objectID": "api 222 files/section 2/section2.1.html#stargazer-for-regression-output",
    "href": "api 222 files/section 2/section2.1.html#stargazer-for-regression-output",
    "title": "Section 2.2 - KNN and Linear Regression Code",
    "section": "Stargazer for Regression Output",
    "text": "Stargazer for Regression Output\nIf you want to compare the coefficients from different regressions, you can use the stargazer package. This package is not installed by default, so you will need to install it.\n\n#install.packages(\"stargazer\")\nlibrary(stargazer)\n\nstargazer(small_enroll_reg, enroll_reg, \n          type = \"text\", column.labels = c(\"Small Model\", \"Full Model\"))\n\n\n========================================================================\n                                    Dependent variable:                 \n                    ----------------------------------------------------\n                                           Enroll                       \n                           Small Model                Full Model        \n                               (1)                        (2)           \n------------------------------------------------------------------------\nPrivateYes                                               7.807          \n                                                       (30.036)         \n                                                                        \nApps                        -0.052***                  -0.028***        \n                             (0.013)                    (0.008)         \n                                                                        \nAccept                       0.424***                  0.147***         \n                             (0.021)                    (0.015)         \n                                                                        \nTop10perc                                              4.016***         \n                                                        (1.283)         \n                                                                        \nTop25perc                                              -2.269**         \n                                                        (0.997)         \n                                                                        \nF.Undergrad                                            0.144***         \n                                                        (0.004)         \n                                                                        \nP.Undergrad                                             -0.012*         \n                                                        (0.007)         \n                                                                        \nOutstate                                                -0.003          \n                                                        (0.004)         \n                                                                        \nRoom.Board                                             -0.024**         \n                                                        (0.011)         \n                                                                        \nBooks                                                   -0.027          \n                                                        (0.049)         \n                                                                        \nPersonal                                                 0.008          \n                                                        (0.014)         \n                                                                        \nPhD                                                     -0.431          \n                                                        (1.002)         \n                                                                        \nTerminal                                                -0.540          \n                                                        (1.094)         \n                                                                        \nS.F.Ratio                                               -0.253          \n                                                        (2.843)         \n                                                                        \nperc.alumni                                            2.319***         \n                                                        (0.879)         \n                                                                        \nExpend                                                   0.003          \n                                                        (0.003)         \n                                                                        \nGrad.Rate                                                0.136          \n                                                        (0.648)         \n                                                                        \nConstant                    86.881***                  187.938**        \n                             (20.984)                  (89.615)         \n                                                                        \n------------------------------------------------------------------------\nObservations                   622                        622           \nR2                            0.820                      0.956          \nAdjusted R2                   0.819                      0.955          \nResidual Std. Error     403.989 (df = 619)        202.349 (df = 604)    \nF Statistic         1,405.761*** (df = 2; 619) 768.822*** (df = 17; 604)\n========================================================================\nNote:                                        *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nYou can also also use stargazer to get the latex code for a table. This is useful if you want to include the table in a paper or a presentation.\n\nstargazer(small_enroll_reg, enroll_reg, \n          type = \"latex\", \n          column.labels = c(\"Small Model\", \"Full Model\"))\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nEnroll\n\n\n\n\n\n\nSmall Model\n\n\nFull Model\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nPrivateYes\n\n\n\n\n7.807\n\n\n\n\n\n\n\n\n(30.036)\n\n\n\n\n\n\n\n\n\n\n\n\nApps\n\n\n-0.052***\n\n\n-0.028***\n\n\n\n\n\n\n(0.013)\n\n\n(0.008)\n\n\n\n\n\n\n\n\n\n\n\n\nAccept\n\n\n0.424***\n\n\n0.147***\n\n\n\n\n\n\n(0.021)\n\n\n(0.015)\n\n\n\n\n\n\n\n\n\n\n\n\nTop10perc\n\n\n\n\n4.016***\n\n\n\n\n\n\n\n\n(1.283)\n\n\n\n\n\n\n\n\n\n\n\n\nTop25perc\n\n\n\n\n-2.269**\n\n\n\n\n\n\n\n\n(0.997)\n\n\n\n\n\n\n\n\n\n\n\n\nF.Undergrad\n\n\n\n\n0.144***\n\n\n\n\n\n\n\n\n(0.004)\n\n\n\n\n\n\n\n\n\n\n\n\nP.Undergrad\n\n\n\n\n-0.012*\n\n\n\n\n\n\n\n\n(0.007)\n\n\n\n\n\n\n\n\n\n\n\n\nOutstate\n\n\n\n\n-0.003\n\n\n\n\n\n\n\n\n(0.004)\n\n\n\n\n\n\n\n\n\n\n\n\nRoom.Board\n\n\n\n\n-0.024**\n\n\n\n\n\n\n\n\n(0.011)\n\n\n\n\n\n\n\n\n\n\n\n\nBooks\n\n\n\n\n-0.027\n\n\n\n\n\n\n\n\n(0.049)\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal\n\n\n\n\n0.008\n\n\n\n\n\n\n\n\n(0.014)\n\n\n\n\n\n\n\n\n\n\n\n\nPhD\n\n\n\n\n-0.431\n\n\n\n\n\n\n\n\n(1.002)\n\n\n\n\n\n\n\n\n\n\n\n\nTerminal\n\n\n\n\n-0.540\n\n\n\n\n\n\n\n\n(1.094)\n\n\n\n\n\n\n\n\n\n\n\n\nS.F.Ratio\n\n\n\n\n-0.253\n\n\n\n\n\n\n\n\n(2.843)\n\n\n\n\n\n\n\n\n\n\n\n\nperc.alumni\n\n\n\n\n2.319***\n\n\n\n\n\n\n\n\n(0.879)\n\n\n\n\n\n\n\n\n\n\n\n\nExpend\n\n\n\n\n0.003\n\n\n\n\n\n\n\n\n(0.003)\n\n\n\n\n\n\n\n\n\n\n\n\nGrad.Rate\n\n\n\n\n0.136\n\n\n\n\n\n\n\n\n(0.648)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n86.881***\n\n\n187.938**\n\n\n\n\n\n\n(20.984)\n\n\n(89.615)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n622\n\n\n622\n\n\n\n\nR2\n\n\n0.820\n\n\n0.956\n\n\n\n\nAdjusted R2\n\n\n0.819\n\n\n0.955\n\n\n\n\nResidual Std. Error\n\n\n403.989 (df = 619)\n\n\n202.349 (df = 604)\n\n\n\n\nF Statistic\n\n\n1,405.761*** (df = 2; 619)\n\n\n768.822*** (df = 17; 604)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01"
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html",
    "title": "Demystifying KNN",
    "section": "",
    "text": "In the realm of machine learning, few algorithms are as intuitively appealing as the K-Nearest Neighbors (KNN). It’s a method that echoes the human instinct to classify based on similarity and proximity, offering a gateway into the world of pattern recognition and predictive analytics."
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html#the-mathematical-compass-guiding-through-knn-and-linear-regression",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html#the-mathematical-compass-guiding-through-knn-and-linear-regression",
    "title": "Understanding KNN",
    "section": "The Mathematical Compass: Guiding Through KNN and Linear Regression",
    "text": "The Mathematical Compass: Guiding Through KNN and Linear Regression\nBefore we dive into the practical applications, let’s lay the mathematical groundwork that underpins these techniques—a foundation as crucial to understanding their functionality as a compass is to navigation.\n\nK-Nearest Neighbors (KNN): A Non-Parametric Approach\nKNN’s beauty lies in its simplicity and the intuitive concept of “neighborliness.” It posits that data points with similar characteristics (features) are likely to share the same outcome. Whether we’re classifying flowers based on petal sizes or predicting housing prices from neighborhood characteristics, KNN asks, “Who are your nearest neighbors?”\n\nMathematical Intuition:\nFor a given data point, KNN looks at the ‘K’ closest points (neighbors) and makes a prediction based on their majority class (classification) or average value (regression). The distance between points—Euclidean, Manhattan, or any other metric—serves as the basis for determining “closeness.”\nTo elaborate on this with mathematical expressions and to visualize it within a coordinate plane, let’s dive deeper:\n\nDistance Metrics\nThe most common distance metric used in KNN is the Euclidean distance, which in a two-dimensional space can be expressed as:\n\\[d(p, q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}\\]\nwhere \\(p = (p_1, p_2)\\) and \\(q = (q_1, q_2)\\) are two points in the Euclidean plane.\nFor higher dimensions, the formula generalizes to:\n\\[d(p, q) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}\\]\nwhere \\(n\\) is the number of dimensions (features) and \\(p_i\\), \\(q_i\\) are the coordinates of \\(p\\) and \\(q\\) in each dimension.\n\n\nKNN Algorithm\n\nCompute Distance: Calculate the distance between the query instance and all the training samples.\nSort Distances: Order the training samples by their distance from the query instance.\nSelect K Nearest Neighbors: Identify the top \\(K\\) closest training samples.\nMajority Vote or Average: For classification, the predicted class is the most common class among the \\(K\\) nearest neighbors. For regression, it is the average of the values.\n\n\n\nDecision Boundary Visualization\nIn a 2D coordinate plane, imagine plotting various data points, each belonging to one of two classes. The decision boundary that KNN creates is not linear but forms curves that encircle clusters of points belonging to the same class. This can be visualized as follows:\n\nCreate a dense grid of points covering the entire plane.\nUse KNN to classify each point on the grid.\nColor the points differently based on the predicted class, revealing the decision boundary.\n\nThis boundary demarcates the regions of the plane where a query point would be classified as one class or the other. It’s worth noting that the shape of the boundary depends on \\(K\\) and the distance metric used.\n\n\nK Selection\nChoosing the right \\(K\\) is critical for the model’s performance. Too small a \\(K\\) leads to a highly complex model that may overfit, capturing noise in the training data. Conversely, too large a \\(K\\) simplifies the model excessively, potentially underfitting and missing key patterns.\n\\[K_{optimal} = \\text{argmin}_K (\\text{Error}(K))\\]\nThe optimal \\(K\\) minimizes the prediction error, which can be determined through cross-validation.\nBy understanding the mathematical foundations of KNN and visualizing its application in a coordinate plane, we can better appreciate its flexibility and the importance of distance metrics and \\(K\\) selection in shaping the decision boundaries."
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html#exploring-the-iris-dataset",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html#exploring-the-iris-dataset",
    "title": "Demystifying KNN",
    "section": "Exploring the Iris Dataset",
    "text": "Exploring the Iris Dataset\nThe Iris dataset is a classic in machine learning and statistics, known for its simplicity and utility in demonstrating basic principles of classification. It consists of 150 observations of iris flowers, divided into three species: Setosa, Versicolor, and Virginica. Each observation includes four features: sepal length, sepal width, petal length, and petal width, all measured in centimeters.\n\nSummary Statistics and Visualizations\nLet’s start by exploring the dataset with some summary statistics and visualizations:\n\nlibrary(ggplot2)\n\niris.data &lt;- iris \n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# Scatter plot visualizing petal width and length grouped by species\nscatter &lt;- ggplot(iris.data, aes(x = Petal.Width, y = Petal.Length, color = Species)) +\n  geom_point(size = 3, alpha = 0.6) +\n  theme_classic() +\n  theme(legend.position = \"right\") +\n  ggtitle(\"Scatter Plot of Petal Dimensions by Species\")\n\nprint(scatter)\n\n\n\n# Boxplot visualizing variation in petal width between species\nboxplot &lt;- ggplot(iris.data, aes(x = Species, y = Petal.Width, fill = Species)) +\n  geom_boxplot() +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  ggtitle(\"Boxplot of Petal Width by Species\")\n\nprint(boxplot)"
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html#a-quick-note-on-training-and-testing-data",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html#a-quick-note-on-training-and-testing-data",
    "title": "Demystifying KNN",
    "section": "A Quick Note on Training and Testing Data",
    "text": "A Quick Note on Training and Testing Data\nOne of the pivotal steps in the machine learning workflow is the division of your dataset into training and testing sets. This practice is not just routine but foundational, ensuring that we evaluate our models accurately and fairly. But why do we take this step, and what does it achieve?\n\nCrafting and Validating Predictive Models\nThe essence of machine learning lies in learning from data and making predictions. When we train a model, we are essentially ‘teaching’ it to recognize patterns and make decisions based on historical data. However, the true test of a model’s mettle is not how well it memorizes the training data, but how effectively it can apply its learned knowledge to new, unseen data. This is where the concept of generalization comes into play.\n\n\nWhy Not Learn from All the Data?\nA natural question arises: if our goal is to make the best possible predictions, why not train our model on the entire dataset? The answer lies in the risk of overfitting. An overfitted model is akin to a student who memorizes facts for an exam rather than understanding the underlying concepts. Just as the student might struggle to apply their knowledge in real-world situations, an overfitted model performs well on its training data but poorly on any new data.\n\n\nTraining Set: The Learning Phase\nThe training set serves as the educational cornerstone for our model. It’s the data on which the model trains, learns patterns, and adjusts its parameters. For KNN, this involves storing the features and labels of the training examples to later find the nearest neighbors of unseen instances.\n\n\nTesting Set: The Examination Phase\nAfter training, we introduce the model to the testing set, a separate portion of the data withheld from the training phase. This step is the model’s exam—it’s where we assess its ability to generalize the patterns it learned during training to new examples. The performance on the testing set gives us a realistic estimate of how the model is expected to perform in real-world scenarios.\n\n\nThe Significance of the Split\nSplitting data into training and testing sets is a critical step that balances the need for a model to learn effectively and the necessity of evaluating its predictive power honestly. By adhering to this practice, we ensure that our models are tested in a manner that mimics their eventual use on new, unseen data, providing a reliable measure of their performance and generalization capability.\nEnd of note :)"
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html#splitting-the-data-and-visualizing-decision-boundaries-for-k3",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html#splitting-the-data-and-visualizing-decision-boundaries-for-k3",
    "title": "Demystifying KNN",
    "section": "Splitting the Data and Visualizing Decision Boundaries for k=3",
    "text": "Splitting the Data and Visualizing Decision Boundaries for k=3\nNow, let’s split the dataset into training and testing sets, apply KNN classification with k=3, and visualize the decision boundary and compute the error rate. Why are we choosing k=3? It’s a common starting point, and we’ll explore the impact of different k values in the next section. We are going to work with a simplified version of the iris dataset using only three features: Sepal.Length, Sepal.Width, and Species.\n\n# Load necessary libraries\nlibrary(class)\nlibrary(dplyr)\n\nset.seed(513) # For reproducibility\n\n# Select features and species for simplicity\niris_simplified &lt;- iris %&gt;%\n  select(Sepal.Length, Sepal.Width, Species)\n\n# Split data into training and test sets\nsample_size &lt;- nrow(iris_simplified) * 0.7 # 70% for training\ntraining_indices &lt;- sample(1:nrow(iris_simplified), sample_size)\n\ntraining_data &lt;- iris_simplified[training_indices, ]\ntest_data &lt;- iris_simplified[-training_indices, ]\n\n# Perform KNN classification with k = 3\nknn_result &lt;- knn(train = training_data[, 1:2],\n                  test = test_data[, 1:2],\n                  cl = training_data[, 3],\n                  k = 3)\n\n# Add the predictions to the test_data dataframe for plotting\ntest_data$PredictedSpecies &lt;- as.factor(knn_result)\n\n# Visualize the results\nggplot(data = test_data, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(aes(color = Species, shape = PredictedSpecies), size = 3) +\n  scale_shape_manual(values = c(15, 17, 18)) + # Different shapes for actual vs. predicted\n  scale_color_manual(values = c('red', 'blue', 'green')) +\n  labs(title = \"KNN Classification of Iris Species\",\n       x = \"Sepal Length\",\n       y = \"Sepal Width\",\n       color = \"Actual Species\",\n       shape = \"Predicted Species\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html#fine-tuning-our-approach",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html#fine-tuning-our-approach",
    "title": "Demystifying KNN",
    "section": "Fine-tuning Our Approach",
    "text": "Fine-tuning Our Approach\nHaving visualized how K-Nearest Neighbors (KNN) operates with (k = 3), we’ve seen firsthand the impact of the choice of (k) on our model’s decision boundaries and, consequently, its predictions. But this naturally leads us to a pivotal question: Is (k = 3) truly the best choice for our Iris classification task? Or, more broadly, how do we pinpoint the most suitable number of neighbors for any given problem?"
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html#the-significance-of-k",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html#the-significance-of-k",
    "title": "Demystifying KNN",
    "section": "The Significance of (k)",
    "text": "The Significance of (k)\nThe parameter (k) in KNN serves as a tuning knob, adjusting the balance between the simplicity and complexity of the model. A smaller (k) makes the model more sensitive to noise in the data, potentially leading to overfitting. Conversely, a larger (k) smoothens the decision boundaries, which might simplify the model to the point of underfitting. Thus, finding the optimal (k) is crucial for achieving the best model performance."
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html#search-for-optimal-k",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html#search-for-optimal-k",
    "title": "Demystifying KNN",
    "section": "Search for Optimal (k)",
    "text": "Search for Optimal (k)\nTo embark on this quest, we employ a systematic approach: evaluating the model’s performance across a range of (k) values and selecting the one that minimizes error. Specifically, we compute the error rates for (k = 1) through (k = 10) on our Iris dataset. The error rate here is defined as the proportion of incorrect predictions out of all predictions made by the model on the test set.\n\nThe Process Unfolded\n\nIterate Over (k): We loop through each (k) value from 1 to 10, applying the KNN model to our training data and making predictions on our test data at each iteration.\nCompute Error Rates: For each (k), we calculate the error rate by comparing the predicted species against the actual species in our test set.\nTabulate Results: We record the (k) values and their corresponding error rates in a table, allowing us to clearly visualize how the error rate varies with (k).\nSelect the Optimal (k): The optimal (k) is the one with the lowest error rate, striking the perfect balance between overfitting and underfitting for our dataset.\n\n\n\nWhat This Means for Our Iris Classification Task\nBy undertaking this analysis, we ensure that our choice of (k) is not arbitrary but is instead data-driven and optimized for performance. This methodical approach not only enhances the accuracy of our KNN model on the Iris dataset but also exemplifies a best practice in machine learning that can be applied to various classification tasks.\n\n# Initialize an empty data frame to store k values and theirerror rates\nerror_rates &lt;- data.frame(k = integer(), error_rate = numeric())\n\n# Loop through k values from 1 to 10\nfor (k in 1:10) {\n  # Apply KNN model\n  predicted_species &lt;- knn(train = training_data[, 1:2],\n                           test = test_data[, 1:2],\n                           cl = training_data[, 3],\n                           k = k)\n\n  # Compute the error rate\n  error_rate &lt;- sum(predicted_species != test_data[, 3]) / nrow(test_data)\n  \n  # Add the results to the error_rates data frame\n  error_rates &lt;- rbind(error_rates, data.frame(k = k, error_rate = error_rate))\n}\n\n\nknitr::kable(error_rates, caption = \"Error Rates for K=1 to 10\")\n\n\nError Rates for K=1 to 10\n\n\nk\nerror_rate\n\n\n\n\n1\n0.2888889\n\n\n2\n0.3111111\n\n\n3\n0.2444444\n\n\n4\n0.2444444\n\n\n5\n0.1777778\n\n\n6\n0.2000000\n\n\n7\n0.1777778\n\n\n8\n0.2222222\n\n\n9\n0.2000000\n\n\n10\n0.1777778\n\n\n\n\n\nIt looks like the error rate is lowest for (k = 5), which means that the KNN model with (k = 5) yields the most accurate predictions on the Iris dataset.\nLet’s visualize our ned results and see how the decision boundaries look for (k = 5).\n\n# Plot the decision boundaries by coloring the grid\nx_range &lt;- seq(from = min(iris$Sepal.Length) - 0.5, \n               to = max(iris$Sepal.Length) + 0.5, by = 0.01)\n\ny_range &lt;- seq(from = min(iris$Sepal.Width) - 0.5, \n               to = max(iris$Sepal.Width) + 0.5, by = 0.01)\n\ngrid &lt;- expand.grid(Sepal.Length = x_range, Sepal.Width = y_range)\n\n# Predict species for each point in the grid\ngrid$Species &lt;- knn(train = training_data[, 1:2],\n                    test = grid,\n                    cl = training_data[, 3],\n                    k = 5)\n\n# Convert grid predictions into a factor for coloring\ngrid$Species &lt;- as.factor(grid$Species)\n\nggplot() +\n  geom_tile(data = grid, aes(x = Sepal.Length, y = Sepal.Width, fill = Species), alpha = 0.5) +\n  geom_point(data = test_data, aes(x = Sepal.Length, y = Sepal.Width, color = Species), size = 2) +\n  scale_fill_manual(values = c('setosa' = 'red', 'versicolor' = 'blue', 'virginica' = 'green')) +\n  scale_color_manual(values = c('setosa' = 'red', 'versicolor' = 'blue', 'virginica' = 'green')) +\n  labs(title = \"KNN Decision Boundaries with Test Data\",\n       x = \"Sepal Length\",\n       y = \"Sepal Width\",\n       fill = \"Predicted Species\",\n       color = \"Actual Species\") +\n  theme_bw()"
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html#the-mathematical-compass",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html#the-mathematical-compass",
    "title": "Demystifying KNN",
    "section": "The Mathematical Compass",
    "text": "The Mathematical Compass\nBefore we dive into the practical applications, let’s lay the mathematical groundwork that underpins these techniques—a foundation as crucial to understanding their functionality as a compass is to navigation.\n\nK-Nearest Neighbors (KNN): A Non-Parametric Approach\nKNN’s beauty lies in its simplicity and the intuitive concept of “neighborliness.” It posits that data points with similar characteristics (features) are likely to share the same outcome. Whether we’re classifying flowers based on petal sizes or predicting housing prices from neighborhood characteristics, KNN asks, “Who are your nearest neighbors?”\n\nMathematical Intuition:\nFor a given data point, KNN looks at the ‘K’ closest points (neighbors) and makes a prediction based on their majority class (classification) or average value (regression). The distance between points—Euclidean, Manhattan, or any other metric—serves as the basis for determining “closeness.”\nTo elaborate on this with mathematical expressions and to visualize it within a coordinate plane, let’s dive deeper:\n\nDistance Metrics\nThe most common distance metric used in KNN is the Euclidean distance, which in a two-dimensional space can be expressed as:\n\\[d(p, q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}\\]\nwhere \\(p = (p_1, p_2)\\) and \\(q = (q_1, q_2)\\) are two points in the Euclidean plane.\nFor higher dimensions, the formula generalizes to:\n\\[d(p, q) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}\\]\nwhere \\(n\\) is the number of dimensions (features) and \\(p_i\\), \\(q_i\\) are the coordinates of \\(p\\) and \\(q\\) in each dimension.\n\n\nKNN Algorithm\n\nCompute Distance: Calculate the distance between the query instance and all the training samples.\nSort Distances: Order the training samples by their distance from the query instance.\nSelect K Nearest Neighbors: Identify the top \\(K\\) closest training samples.\nMajority Vote or Average: For classification, the predicted class is the most common class among the \\(K\\) nearest neighbors. For regression, it is the average of the values.\n\n\n\nDecision Boundary Visualization\nIn a 2D coordinate plane, imagine plotting various data points, each belonging to one of two classes. The decision boundary that KNN creates is not linear but forms curves that encircle clusters of points belonging to the same class. This can be visualized as follows:\n\nCreate a dense grid of points covering the entire plane.\nUse KNN to classify each point on the grid.\nColor the points differently based on the predicted class, revealing the decision boundary.\n\nThis boundary demarcates the regions of the plane where a query point would be classified as one class or the other. It’s worth noting that the shape of the boundary depends on \\(K\\) and the distance metric used.\n\n\nK Selection\nChoosing the right \\(K\\) is critical for the model’s performance. Too small a \\(K\\) leads to a highly complex model that may overfit, capturing noise in the training data. Conversely, too large a \\(K\\) simplifies the model excessively, potentially underfitting and missing key patterns.\n\\[K_{optimal} = \\text{argmin}_K (\\text{Error}(K))\\]\nThe optimal \\(K\\) minimizes the prediction error, which can be determined through cross-validation.\nBy understanding the mathematical foundations of KNN and visualizing its application in a coordinate plane, we can better appreciate its flexibility and the importance of distance metrics and \\(K\\) selection in shaping the decision boundaries."
  },
  {
    "objectID": "posts/2024-02-01-KNN/2024-02-01-KNN.html#conclusion",
    "href": "posts/2024-02-01-KNN/2024-02-01-KNN.html#conclusion",
    "title": "Demystifying KNN",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we have explored the K-Nearest Neighbors (KNN) algorithm and its application to the Iris dataset. We have seen how KNN operates by classifying new data points based on their similarity to existing data points. We have also visualized the decision boundaries of the KNN model and observed how the choice of (k) impacts the model’s predictions.\nFurthermore, we have demonstrated the significance of selecting the optimal (k) value for our KNN model. By systematically evaluating the model’s performance across a range of (k) values, we have identified the most suitable (k) for our Iris classification task. This approach ensures that our choice of (k) is not arbitrary but is instead data-driven and optimized for performance."
  },
  {
    "objectID": "api 222 files/section 3/section3.html",
    "href": "api 222 files/section 3/section3.html",
    "title": "Section 3 - More on Linear Regression",
    "section": "",
    "text": "As Professor Saghafian noted on Slide 14 of lecture 6, there are certain skills you are expected to have about inference in general, particularly when it comes to linear regression models. The goal of today’s section is to practice (some of) these skills. The session involves executing coding exercises and answering conceptual questions along the way. We will work with the data set, which is a part of the package. I will give you time to work on each subsection and then I will share my proposed code and answers to the questions. You are encouraged to work in pairs.\nNote that in some cases there are several ways to write the code to yield the same result.\n\n1 Exploratory data analysis\n\nLoad the Credit data set from ISLR package. Check the codebook to understand the structure of the data set and the definition and unit of each variable.\n\n\n\nSample Solution\n# Store the data in a clean object and cast the data into a \"data.table\" object\n# As noted earlier this package simplifies some of the data cleaning...\ncredit_data  &lt;- as.data.table(Credit) \n\n\n\nHow many observations and variables does the data set include?\n\n\n\nSample Solution\ndim(credit_data)\n\n# [1] 400  12\n\n\n\nWhat are the categorical variables in the data set?\n\n\n\nSample Solution\nstr(credit_data)\n\n\n#Classes ‘data.table’ and 'data.frame': 400 obs. of  12 variables:\n# $ ID       : int  1 2 3 4 5 6 7 8 9 10 ...\n# $ Income   : num  14.9 106 104.6 148.9 55.9 ...\n# $ Limit    : int  3606 6645 7075 9504 4897 8047 3388 7114 3300 6819 ...\n# $ Rating   : int  283 483 514 681 357 569 259 512 266 491 ...\n# $ Cards    : int  2 3 4 3 2 4 2 2 5 3 ...\n# $ Age      : int  34 82 71 36 68 77 37 87 66 41 ...\n# $ Education: int  11 15 11 11 16 10 12 9 13 19 ...\n# $ Gender   : Factor w/ 2 levels \" Male\",\"Female\": 1 2 1 2 1 1 2 1 2 2 ...\n# $ Student  : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 1 1 1 1 2 ...\n# $ Married  : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 1 2 1 1 1 1 2 ...\n# $ Ethnicity: Factor w/ 3 levels \"African American\",..: 3 2 2 2 3 3 1 2 3 1 ...\n# $ Balance  : int  333 903 580 964 331 1151 203 872 279 1350 ...\n# - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\n# The categorical variables are: Gender, Student, Married, and Ethnicity.\n\n\n\nAre they rows with missing values? If so, how many? Hint: checkout the complete.cases function.\n\n\n\nSample Solution\nnrow(credit_data[!complete.cases(credit_data),])\n\n# [1] 0\n\n\n\nGenerate summary statistics of all the variables. What is the mean and standard deviation of income?\n\n\n\nSample Solution\nsummary(credit_data)\n\n\n#       ID            Income           Limit           Rating          Cards      \n# Min.   :  1.0   Min.   : 10.35   Min.   :  855   Min.   : 93.0   Min.   :1.000  \n# 1st Qu.:100.8   1st Qu.: 21.01   1st Qu.: 3088   1st Qu.:247.2   1st Qu.:2.000  \n# Median :200.5   Median : 33.12   Median : 4622   Median :344.0   Median :3.000  \n# Mean   :200.5   Mean   : 45.22   Mean   : 4736   Mean   :354.9   Mean   :2.958  \n# 3rd Qu.:300.2   3rd Qu.: 57.47   3rd Qu.: 5873   3rd Qu.:437.2   3rd Qu.:4.000  \n# Max.   :400.0   Max.   :186.63   Max.   :13913   Max.   :982.0   Max.   :9.000  \n#      Age          Education        Gender    Student   Married  \n# Min.   :23.00   Min.   : 5.00    Male :193   No :360   No :155  \n# 1st Qu.:41.75   1st Qu.:11.00   Female:207   Yes: 40   Yes:245  \n# Median :56.00   Median :14.00                                   \n# Mean   :55.67   Mean   :13.45                                   \n# 3rd Qu.:70.00   3rd Qu.:16.00                                   \n# Max.   :98.00   Max.   :20.00                                   \n#            Ethnicity      Balance       \n# African American: 99   Min.   :   0.00  \n# Asian           :102   1st Qu.:  68.75  \n# Caucasian       :199   Median : 459.50  \n#                        Mean   : 520.01  \n#                        3rd Qu.: 863.00  \n#                        Max.   :1999.00  \n\n\nThe mean income is:\n\n\nSample Solution\nround(mean(credit_data$Income, na.rm = TRUE), 2)\n\n# [1] 45.22\n\n\nThe standard deviation of income is:\n\n\nSample Solution\nround(sd(credit_data$Income, na.rm = TRUE), 2)\n\n# [1] 35.24\n\n\n\nPlot the relationship between balance (y-axis) and income (x-axis). What do you notice about the relationship?\n\n\n\nSample Solution\nplot(x = credit_data$Income, y = credit_data$Balance,\n     main = \"Average credit card balance vs. Income\",\n     xlab = \"Income\",\n     ylab = \"Average credit card balance\")\n\n\n# There appears to be a positive relationship between these two variables.\n\n\n\n\n2 Inference\n\nRegress balance (y-variable) on income (x-variable). Interpret the income coefficient.\n\n\n\nSample Solution\nmod1 &lt;- lm(Balance ~ Income, credit_data)\nsummary(mod1)\n\n\n#Call:\n#lm(formula = Balance ~ Income, data = credit_data)\n\n#Residuals:\n#    Min      1Q  Median      3Q     Max \n#-803.64 -348.99  -54.42  331.75 1100.25 \n\n#Coefficients:\n#            Estimate Std. Error t value Pr(&gt;|t|)    \n#(Intercept) 246.5148    33.1993   7.425  6.9e-13 ***\n#Income        6.0484     0.5794  10.440  &lt; 2e-16 ***\n#---\n#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n#Residual standard error: 407.9 on 398 degrees of freedom\n#Multiple R-squared:  0.215,    Adjusted R-squared:  0.213 \n#F-statistic:   109 on 1 and 398 DF,  p-value: &lt; 2.2e-16\n\n\n# A $1,000 increase in income is associated with an \n# average balance increase of $6.05. The coefficient is statistically significant. \n\n\n\nNow add gender as an explanatory variable.\n\n\n\nSample Solution\nmod2 &lt;- lm(Balance ~ Income + Gender, credit_data)\nsummary(mod2)\n\n\n#Call:\n#lm(formula = Balance ~ Income + Gender, data = credit_data)\n\n#Residuals:\n#    Min      1Q  Median      3Q     Max \n#-791.23 -351.34  -51.57  328.18 1112.87 \n\n#Coefficients:\n#             Estimate Std. Error t value Pr(&gt;|t|)    \n#(Intercept)  233.7663    39.5322   5.913 7.24e-09 ***\n#Income         6.0521     0.5799  10.437  &lt; 2e-16 ***\n#GenderFemale  24.3108    40.8470   0.595    0.552    \n#---\n#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n#Residual standard error: 408.2 on 397 degrees of freedom\n#Multiple R-squared:  0.2157,   Adjusted R-squared:  0.2117 \n#F-statistic: 54.58 on 2 and 397 DF,  p-value: &lt; 2.2e-16\n\n\n\nInterpret all three coefficients (intercept, income coefficient, gender coefficient).\n\n\n\nSample Solution\n# - The average balance for males is $233.77. \n\n# - The average balance for females is $24.31 higher than the average \n#   balance of males, when controlling for income. Note however, that this \n#   difference is not statistically significant. \n\n# - A $1,000 increase in income is associated with an average balance\n#   increase of \\$6.05. The coefficient is statistically significant.  \n\n\n\nTest the null hypothesis that there is no relationship between balance and gender (i.e. \\(\\beta_{gender} = 0\\)). What do you conclude about the test?\n\n\n\nSample Solution\n# - H0: the difference in balance between females and males \n#   (after controlling for income) is 0, that is $\\beta_{gender} = 0$. \n\n# - Ha: the difference in balance between females and males\n#   (after controlling for income) is different from 0, that is $\\beta_{gender} \\neq 0$.\n\n# P-value suggests we cannot reject the NULL at any reasonable level of \n# significance (1\\%, 5\\%, 10\\%)\n\n\n\nWhat is the confidence interval of the gender coefficient? Interpret this coefficient. Hint: checkout the function.\n\n\n\nSample Solution\nconfint(mod2)\n\n\n# We can be 95\\% confident that the true difference in balance is between \n# females and males is between -55.99 and 104.61. Notice this interval \n# includes 0, which is consistent with our conclusion on the hypothesis test above. \n\n\n\nFind and interpret the \\(R^2\\) of this regression.\n\n\n\nSample Solution\n# The $R^2$ is 0.2157. This means that income and gender together\n# explain ~22\\% of the variation in average card balance.\n\n\n\nNow add an interaction term between income and gender to the regression in part 2.\n\n\nInterpret the coefficient on the interaction term.\n\n\n\nSample Solution\nmod3 &lt;- lm(Balance ~ Income*Gender, credit_data)\nsummary(mod3)\n\n#Call:\n#lm(formula = Balance ~ Income * Gender, data = credit_data)\n\n#Residuals:\n#    Min      1Q  Median      3Q     Max \n#-797.35 -352.35  -53.42  328.98 1114.47 \n\n#Coefficients:\n#                    Estimate Std. Error t value Pr(&gt;|t|)    \n#(Intercept)         227.7682    47.8567   4.759 2.73e-06 ***\n#Income                6.1836     0.8276   7.472 5.12e-13 ***\n#GenderFemale         36.0236    66.5744   0.541    0.589    \n#Income:GenderFemale  -0.2589     1.1612  -0.223    0.824    \n#---\n#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n#Residual standard error: 408.7 on 396 degrees of freedom\n#Multiple R-squared:  0.2158,   Adjusted R-squared:  0.2098 \n#F-statistic: 36.32 on 3 and 396 DF,  p-value: &lt; 2.2e-16\n\n\n\nWhat is \\(R^2\\) and the adjusted \\(R^2\\) of this regression. What do these two values tell you about the usefulness of the interaction term?\n\n\n\nSample Solution\n# The $R^2$ is 0.2158 and the adjusted $R^2$ is 0.2098. In the model without the\n# interaction term the $R^2$ was 0.2157, and the adjusted $R^2$ was 0.2117. \n# The $R^2$ has increased as expected given we have a added a term. \n# However, the adjusted adjusted $R^2$ has decreased suggesting the \n# interaction term does not add value \n# (when considering the complexity it adds to the model).\n\n\n\nPlot the residuals. What does the plot tell you about your model fit?\n\n\n\nSample Solution\nplot(mod3$residuals)\n\n# There's no discernible pattern in the residuals, the model fit appears reasonable.\n\n\n\nRerun the model in part 3 using the log-transformed version of the balance and income variables. Interpret the coefficient on the income term.\n\n\n\nSample Solution\nmod4 &lt;- lm(log(Balance + 0.0001) ~ log(Income)*Gender, credit_data)\nsummary(mod4)\n\n\n#Call:\n#lm(formula = log(Balance + 1e-04) ~ log(Income) * Gender, data = credit_data)\n\n#Residuals:\n#     Min       1Q   Median       3Q      Max \n#-14.5434  -0.1351   2.4202   4.1069   7.2967 \n\n#Coefficients:\n#                         Estimate Std. Error t value Pr(&gt;|t|)    \n#(Intercept)               -6.2982     2.3019  -2.736 0.006498 ** \n#log(Income)                2.4470     0.6340   3.859 0.000133 ***\n#GenderFemale              -0.4506     3.2929  -0.137 0.891238    \n#log(Income):GenderFemale   0.3034     0.9073   0.334 0.738248    \n#---\n#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n#Residual standard error: 6.264 on 396 degrees of freedom\n#Multiple R-squared:  0.0789,   Adjusted R-squared:  0.07192 \n#F-statistic: 11.31 on 3 and 396 DF,  p-value: 3.939e-07\n\n\n\n# A 1\\% increase in income is associated with 2.45\\% decreases \n# average balance, when controlling for gender. \n\n\n\n3 BONUS: Prediction\nNow let’s revisit prediction models using linear regression and KNN.\n\nPrepare the input datasets\n\n\nDrop the ID column, the categorical columns, and any rows with missing values.\n\n\n\nSample Solution\n# Remove rows with non-missing values\ncredit_data_complete &lt;- credit_data[complete.cases(credit_data), ]\n\n# Drop the district and municipality variables\ncredit_data_complete[, c(\"ID\", \"Gender\", \"Student\", \n                         \"Married\", \"Ethnicity\") := NULL] # \"data.table\" syntax\n\n\n\nRandomly split the data into a training set (75% of the observations) and a test set (the remaining 25% of the observations).\n\n\n\nSample Solution\n# Set a seed\nset.seed(222)\n# Extract the random test and training IDs\ntest_ids &lt;- sample(seq(nrow(credit_data_complete)), \n                   round(0.25 * nrow(credit_data_complete)))\ntraining_ids &lt;- which(!(seq(nrow(credit_data_complete)) %in% test_ids))\n\n# Now use the IDs to get the two sets\ntest_data &lt;- credit_data_complete[test_ids,]\ntraining_data &lt;- credit_data_complete[training_ids,]\n\n\n\nWhen you use your training data to build a linear model that regresses account balance on all other features available in the data (plus an intercept), what is your test Mean Squared Error?\n\n\n\nSample Solution\n# The model\nmod5 &lt;- lm(Balance ~ ., training_data)\n\n# Generate test predictions\npredicted_bal &lt;- predict(mod5, test_data[, -7])\n\n## Let's see how well we did in terms of MSE\nMSE_lm_bal &lt;- mean((predicted_bal - test_data$Balance)^2)\nprint(MSE_lm_bal)\n\n# [1] 33096.38\n\n\n\nWhen you use your training data to build a KNN model that regresses account balance on all other features in the data, what is your test Mean Squared Error with \\(K = 1\\)?\n\n\n\nSample Solution\n# Library for KNN regression\nlibrary(FNN)\n\n# The model\nknn_reg1 &lt;- knn.reg(training_data[, -c(7)],\n                    test_data[, -c(7)],\n                    training_data$Balance,\n                    k = 1)\n\n# The MSE\nmse_knn1 &lt;- mean((knn_reg1$pred - test_data$Balance)^2)\nprint(mse_knn1)\n\n# [1] 72397.03\n\n\n\nIn last Friday’s review session, one of your classmates asked: “Instead of testing a few individual values of K, could we use a more systematic approach that computes the Mean Squared Error for many values of \\(K\\) and then plot model performance as a function of \\(K\\).”\n\n\nIn the first review session, we went through the basics of looping. Use a “for” loop to implement the approach your classmate suggested. Test \\(K\\) values going from 1 to 100.\n\n\n\nSample Solution\n# Define the range of K values to test\nk_guesses &lt;- 1:100\n\n# Initialize a tracker for the MSE values for each K\nmse_res &lt;- NULL\n\n# Now loop through all the values \nfor(i in 1:length(k_guesses)){\n  # For each value, run the model using the current K guess\n  knn_reg &lt;- knn.reg(training_data[, -c(7)],\n                     test_data[, -c(7)],\n                     training_data$Balance,\n                     k = k_guesses[i]) # key line here\n  \n  # The MSE\n  mse_knn &lt;- mean((knn_reg$pred - test_data$Balance)^2)\n  \n  # Now update the tracker\n  mse_res[i] &lt;- mse_knn\n}\n\n# Now plot the results\nplot(x = k_guesses, y = mse_res, main = \"MSE vs. K\", xlab = \"K\", ylab = \"MSE\")\n\n\n\nWhat can you conclude about the optimal \\(K\\) value for this model.\n\n\n\nSample Solution\n# Find the K that gives the minimum MSE\nwhich.min(mse_res)\n\n# It looks like $K = 8$ would give you the lowest MSE in this case. \n# Note: this result may be different from yours depending on how your sampling played out."
  },
  {
    "objectID": "git files/section 1/section1.html",
    "href": "git files/section 1/section1.html",
    "title": "Part 1: Git Installation and SSH Instructions",
    "section": "",
    "text": "Install homebrew following the instructions at https://brew.sh/.\nOpen Terminal and check if git is already installed.\ngit --version\nIf not, install git using brew install git. Then verify, its installed by running git --version.\n\n\n\n\n\nGo to gitforwindows.org. Make sure you include Git Bash in your installation!"
  },
  {
    "objectID": "git files/section 1/section1.html#part-ii-creating-a-github-account",
    "href": "git files/section 1/section1.html#part-ii-creating-a-github-account",
    "title": "Part 1: Git Installation and SSH Instructions",
    "section": "Part II: Creating a GitHub Account",
    "text": "Part II: Creating a GitHub Account\n\nGo to GitHub.\nClick the “Sign Up” button.\nFollow the on-screen instructions to create your account."
  },
  {
    "objectID": "git files/section 1/section1.html#part-iii-git-setup",
    "href": "git files/section 1/section1.html#part-iii-git-setup",
    "title": "Part 1: Git Installation and SSH Instructions",
    "section": "Part III: Git Setup",
    "text": "Part III: Git Setup\n\nConfigure git with your name and email address. Be sure to use the same email associated with your Github account.\ngit config --global user.name \"YOUR NAME\"\ngit config --global user.email \"YOUR EMAIL ADDRESS\""
  },
  {
    "objectID": "git files/section 1/section1.html#part-iv-ssh",
    "href": "git files/section 1/section1.html#part-iv-ssh",
    "title": "Part 1: Git Installation and SSH Instructions",
    "section": "Part IV: SSH",
    "text": "Part IV: SSH\nIn order to write code locally on our computer and be able to push to GitHub (or pull from GitHub) daily without constantly having to enter a username and password each time, we’re going to set up SSH keys.\n\nSSH keys come in pairs, a public key that gets shared with services like GitHub, and a private key that is stored only on your computer. If the keys match, you’re granted access.\nThe cryptography behind SSH keys ensures that no one can reverse engineer your private key from the public one.\nsource: https://jdblischak.github.io/2014-09-18-chicago/novice/git/05-sshkeys.html\n\nThe following steps are a simplification of the steps found in GitHub’s documentation. If you prefer, feel free to follow the steps at that link. Otherwise, for a simplified experience continue on below!\nSimplified Setup Steps\n\nStep 1: Check to see if you already have keys.\nRun the following command.\nls -al ~/.ssh/\nIf you see any output, that probably means you already have a public and private SSH key. If you have keys, you will most likely you will have two files, one named id_rsa (that contains your private key) and id_rsa.pub (that contains your public key).\nsidenote: Those files may also be named something like: id_ecdsa.pub or id_ed25519.pub. That just means you’re using a different encryption algorithm to generate your keys. You can learn more about that here if you chose to. Or, don’t worry about it and power on!\nIf you already have keys, continue to step 3. Otherwise, read on!\nStep 2: Create new SSH keys.\nRun the following comamnd, but makes sure to replace your_email@example.com with your own email address. Use the same email address you used to sign up to GitHub with.\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\nYou may then see a prompt like the one below. Just hit enter to save the key in the default location.\nEnter file in which to save the key (/Users/jacob/.ssh/id_rsa):\nAfter that, the system will prompt you to enter a passphrase. We’re not going to use a passphrase here, so just go ahead and leave that blank and hit enter twice.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nFinally you should see some randomart that looks like this\nYour identification has been saved in /Users/jacob/.ssh/id_rsa.\nYour public key has been saved in /Users/jacob/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:2AazdvCBP8d1li9tF8cszM2KbtjPe7iwfCK8gUgzIGY your_email@example.com\nThe key's randomart image is:\n+---[RSA 4096]----+\n|                 |\n|       .     o * |\n|  E . = .   . B.*|\n| o . . X o . + =o|\n|      B S o . o =|\n|     o * + +   o.|\n|      . ..o =  . |\n|          o+.=o .|\n|          .ooo=+ |\n+----[SHA256]-----+\nStep 3: Add your key to GitHub\nRun the following command to view your public key\ncat ~/.ssh/id_rsa.pub\nNavigate to https://github.com/settings/keys and hit “New SSH key”. Paste the SSH key from the last command into the text box as shown below and then hit “Add SSH key”. Make sure you copy paste exactly. The key will likely start with ssh_rsa and end with your email address. You can give the key a title like “My Macbook Pro” so you know which computer this key comes from.\n\nStep 4: Verify that it worked!\nRun the following command to test your computer’s SSH connection to GitHub\nssh -T git@github.com\nIf the connection is successful, you will see a message like this\n&gt; Hi username! You've successfully authenticated, but GitHub does not\n&gt; provide shell access.\n\n\nRecap: What did we just do?\nWe just created a public/private SSH Key pair. There is now a folder on your computer called .ssh (it is a hidden folder, hidden folders have names that start with .). You can run this command to see the files in that folder.\nls -al ~/.ssh/\nid_rsa.pub contains your public key, you can see what that looks like by running:\ncat ~/.ssh/id_rsa.pub\nid_rsa contains your private key, you can see what that looks like by running:\ncat ~/.ssh/id_rsa\nThis public and private key pair are mathematically linked. As the name suggests, you can share your public key far and wide, but must keep your private key safe and secure. Since you have shared your public key with GitHub, your computer can encrypt files with your private key and send them to GitHub. Since GitHub has your public key, it can match that file and verify that it is coming from you. Your computer can now securely communicate with GitHub without needing a username and password every time.\nTo get started with Git, you need to install it on your computer. You can download the installer from the Git website. Once you have Git installed, you can use the git command in your terminal to interact with Git."
  },
  {
    "objectID": "Intro Git.html",
    "href": "Intro Git.html",
    "title": "Introduction to Git and GitHub",
    "section": "",
    "text": "Welcome to the Introduction to Git and GitHub. This is a collection of notes and references for using Git and GitHub. The goal is to provide a quick introduction and reference for common commands and workflows.\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nPart 1: Git Installation and SSH Instructions\n\n\n\n\n\nHelp on installing git and setting up SSH keys for GitHub\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nPart 2: Introduction to Git and GitHub\n\n\n\n\n\nIn this guide, we’ll cover the basics of Git and GitHub\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nPart 3: Branching\n\n\n\n\n\n\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nPart 4: Reference\n\n\n\n\n\nA reference guide for git commands\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nPart 5: Workflow\n\n\n\n\n\nA reference guide for git commands\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nPart 6: Additional Resources\n\n\n\n\n\nLinks to additional git resource\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "git files/section 1/section1.html#part-i-installing-git",
    "href": "git files/section 1/section1.html#part-i-installing-git",
    "title": "Part 1: Git Installation and SSH Instructions",
    "section": "",
    "text": "Install homebrew following the instructions at https://brew.sh/.\nOpen Terminal and check if git is already installed.\ngit --version\nIf not, install git using brew install git. Then verify, its installed by running git --version.\n\n\n\n\n\nGo to gitforwindows.org. Make sure you include Git Bash in your installation!"
  },
  {
    "objectID": "git files/section 1/section1.html#table-of-contents",
    "href": "git files/section 1/section1.html#table-of-contents",
    "title": "Git Installation and SSH Instructions",
    "section": "Table of Contents",
    "text": "Table of Contents"
  },
  {
    "objectID": "git files/section 2/section 2.html",
    "href": "git files/section 2/section 2.html",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "",
    "text": "Git (/ɡɪt/) is a version control system (VCS) for tracking changes in computer files and coordinating work on those files among multiple people. It is primarily used for source code management in software development, but it can be used to keep track of changes in any set of files. As a distributed revision control system it is aimed at speed, data integrity, and support for distributed, non-linear workflows. -Wikipedia\n\n\nGit is an open source program for tracking changes in text files. -GitHub (https://help.github.com/articles/github-glossary/)\n\n\n\nKeeping track of file versions is hard.\n\n\n\nAbove all else, Git is a fast and distributed version control system, that allows you to efficiently handle projects large and small.\nHere are some problems we face as developers, and how git solves them:\n\n\nGit allows us to make save points at any time. These save points are called ‘commits’. Once a save point is made, it’s permanent, and allows us to go back to that save point at any time. From there, we can see what the code looked like at that point, or even start building off that version.\n\n\n\n\nEvery commit has a description (commit message), which allows us to describe what changes were made between the current and previous commit. This is usually a description of what features were added or what bugs were fixed.\nAdditionally, git supports tagging, which allows us to mark a specific commit as a specific version of our code (e.g. ‘2.4.5’).\n\n\n\n\nIt’s often important to see content of the actual changes that were made. This can be useful when:\n\ntracking down when and how a bug was introduced\nunderstanding the changes a team member made so you can stay up-to-date with progress\nreviewing code as a team for correctness or quality/style\n\nGit allows us to easily see these changes (called a diff) for any given commit.\n\n\n\nGit enables you to work using a non-linear workflow. This means that you can have multiple versions of a project or “branches” with different save points, or “commits”, simultaneously within the same folder and easily toggle bgttween them. You can split new branches off a project when you’re looking to experiment or implement a new feature, and you can merge those branches back into the main (formerly known as “master”) branch when you’re ready to incorporate them into a project.\n\n\n\n\nIn developing software, we often want to experiment in adding a feature or refactoring (rewriting) existing code. Because git makes it easy to go back to a known good state, we can experiment without worrying that we’ll be unable to undo the experimental work.\n\n\n\n\n\n\nGit is a distributed version control system. It is a technology.\n\nGitHub is a social coding platform where git repositories are stored and where people can collaborate on projects. GitHub is great both for collaboration within your organization, but also provides an excellent model for open source collaboration across organizations or with the public.\n\nOn GitHub you can find Git repositories.\n\n\nLearn More: https://jahya.net/blog/git-vs-github/\n\n\n\nThe basics:\n\nGit - version control software\nRepository - a folder containing your files and also containing a structure that helps keep track of changes in those files. When you intialize a repository, git creates a hidden folder (.git folder) that stores the changes to those files.\nCommit - the basic unit of a git repository is a commit. It is a set of changes to a file. A commit usually comes with an id as well as a commit message that describes the change.\n\nWorking with others:\n\nGitHub - a place to host git repositories and collaborate\nLocal Repository - the version of a git repository on your local computer\nRemote Repository - the version of a git repository stored somewhere else that your local repository is connected to (frequently on GitHub)"
  },
  {
    "objectID": "git files/section 2/section 2.html#what-is-git",
    "href": "git files/section 2/section 2.html#what-is-git",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "",
    "text": "Keeping track of file versions is hard.\n\n\n\nAbove all else, Git is a fast and distributed version control system, that allows you to efficiently handle projects large and small.\nHere are some problems we face as developers, and how git solves them:\n\n\nGit allows us to make save points at any time. These save points are called ‘commits’. Once a save point is made, it’s permanent, and allows us to go back to that save point at any time. From there, we can see what the code looked like at that point, or even start building off that version.\n\n\n\n\nEvery commit has a description (commit message), which allows us to describe what changes were made between the current and previous commit. This is usually a description of what features were added or what bugs were fixed.\nAdditionally, git supports tagging, which allows us to mark a specific commit as a specific version of our code (e.g. ‘2.4.5’).\n\n\n\n\nIt’s often important to see content of the actual changes that were made. This can be useful when:\n\ntracking down when and how a bug was introduced\nunderstanding the changes a team member made so you can stay up-to-date with progress\nreviewing code as a team for correctness or quality/style\n\nGit allows us to easily see these changes (called a diff) for any given commit.\n\n\n\nGit enables you to work using a non-linear workflow. This means that you can have multiple versions of a project or “branches” with different save points, or “commits”, simultaneously within the same folder and easily toggle bgttween them. You can split new branches off a project when you’re looking to experiment or implement a new feature, and you can merge those branches back into the main (formerly known as “master”) branch when you’re ready to incorporate them into a project.\n\n\n\n\nIn developing software, we often want to experiment in adding a feature or refactoring (rewriting) existing code. Because git makes it easy to go back to a known good state, we can experiment without worrying that we’ll be unable to undo the experimental work."
  },
  {
    "objectID": "git files/section 2/section 2.html#git-versus-github",
    "href": "git files/section 2/section 2.html#git-versus-github",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "",
    "text": "Git is a distributed version control system. It is a technology.\n\nGitHub is a social coding platform where git repositories are stored and where people can collaborate on projects. GitHub is great both for collaboration within your organization, but also provides an excellent model for open source collaboration across organizations or with the public.\n\nOn GitHub you can find Git repositories.\n\n\nLearn More: https://jahya.net/blog/git-vs-github/"
  },
  {
    "objectID": "git files/section 2/section 2.html#some-vocabulary",
    "href": "git files/section 2/section 2.html#some-vocabulary",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "",
    "text": "The basics:\n\nGit - version control software\nRepository - a folder containing your files and also containing a structure that helps keep track of changes in those files. When you intialize a repository, git creates a hidden folder (.git folder) that stores the changes to those files.\nCommit - the basic unit of a git repository is a commit. It is a set of changes to a file. A commit usually comes with an id as well as a commit message that describes the change.\n\nWorking with others:\n\nGitHub - a place to host git repositories and collaborate\nLocal Repository - the version of a git repository on your local computer\nRemote Repository - the version of a git repository stored somewhere else that your local repository is connected to (frequently on GitHub)"
  },
  {
    "objectID": "git files/section 2/section 2.html#lets-dive-in",
    "href": "git files/section 2/section 2.html#lets-dive-in",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "Lets Dive In!",
    "text": "Lets Dive In!\nVocab\n\nlocal repository a folder stored on your computer\nremote repository a folder stored on on GitHub\n\nLets take a look at a repository that is on GitHub.\n\nhttps://github.com/code4MDM/simple-r-script"
  },
  {
    "objectID": "git files/section 2/section 2.html#some-of-githubs-features",
    "href": "git files/section 2/section 2.html#some-of-githubs-features",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "(Some of) GitHub’s Features",
    "text": "(Some of) GitHub’s Features\n\nThe README.md file\nGithub looks for a “readme” file and renders it as you’re navigating through the file structure. This is a great way to guide people through your code.\n\nReadme files are often given the .md extension, meaning they’re written in a language called markdown that allows for nicer formatting. You can check out this markdown cheet sheet (https://www.markdownguide.org/cheat-sheet/) if you want to see how formatting works, but you can also save a readme files as plain text. Github will also detect .txt files, or you can just write plain text inside your .md file.\n\n\nCommit Log\n\nVocab\n\ncommit - the basic unit of a git repository is a commit. It is a set of changes to a file. A commit usually comes with an commit id as well as a commit message that describes the change.\ncommit log (aka Git History) - all of the commits (previous changes) to all of the files in your repository\n\n https://github.com/dmil/my-simple-website/commits/master\n\n\nHistory, Raw, and Blame for any file\n\n\nRaw - actual contents of the file without any formatting applied.\n\nexample: https://github.com/fivethirtyeight/chartbuilder-2/blob/master/src/styles/core.scss\n\nHistory - every change ever made to that file within this branch.\n\nexample: https://github.com/fivethirtyeight/chartbuilder-2/commits/master/src/styles/core.scss\n\nBlame - provenance of each line currently in the file you’re looking at in the branch you’re looking at\n\nexample: https://github.com/fivethirtyeight/chartbuilder-2/blame/master/src/styles/core.scss\n\n\n\n\nBranches\n\n\nPull Requests\n\n\nProposing Edits\n\nYou can edit a file in GitHub in a new branch, thus proposing a change without actually making the change in the master branch. Just make sure to leave a meaningful description of the change you made in the commit message.\n\n\n\nDrag and Drop\n\n\n\nCollaboration\nCollaborators can push to the repository without asking your permission, they have full read and write access.\n\nIf I wasn’t a collaborator, I could still work with you on an open source project through a process called forking where I can make a copy of your repository in my GitHub account, make changes, and request that you merge them back into your project. We will discuss forking more in depth later.\n\n\nServing up Websites!\nGitHub is also great for serving up static websites. GitHub is only storing the code. Luckily, if your code happens to be a website, GitHub can also host it for you through a feature called “GitHub Pages”.\nSimply go to the “settings” menu, scroll down to “GitHub Pages”, and select “master branch”\n\nWhatever is in your master branch on GitHub should now appear at\nhttp://your-username.github.io/repository-name\nin my case it is http://madisoncoots.github.io/"
  },
  {
    "objectID": "git files/section 2/section 2.html#github-for-things-other-than-code",
    "href": "git files/section 2/section 2.html#github-for-things-other-than-code",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "GitHub for things other than code",
    "text": "GitHub for things other than code\n\nAuditing system for changes on a file\nFor collaboratively editing a text document\nFor drafting government web design standards!\nOpen comment period for policy\nDrafting and collaborating on legal documents\nDesign (image diff)\n\nhttps://help.github.com/articles/rendering-and-diffing-images/\n\nOpen journalsim showcase\n\nhttps://github.com/showcases/open-journalism\n\nGithub for Government\n\nhttps://government.github.com/\nhttps://government.github.com/community/"
  },
  {
    "objectID": "git files/section 2/section 2.html#some-vocabulary-1",
    "href": "git files/section 2/section 2.html#some-vocabulary-1",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "Some Vocabulary",
    "text": "Some Vocabulary\n\nGit - version control software\nRepository - a folder containing your files and also containing a structure that helps keep track of changes in those files. When you intialize a repository, git creates a hidden folder (.git folder) that stores the changes to those files.\nGitHub - a place to host git repositories and collaborate\nLocal Repository - the version of a git repository on your local computer\nRemote Repository - the version of a git repository stored somewhere else that your local repository is connected to (frequently on GitHub)\nCommit - the basic unit of a git repository is a commit. It is a set of changes to a file. A commit usually comes with an id as well as a commit message that describes the change.\n\nWithin a Repository you have\n\nUntracked Changes - files that are in your folder but that git doesn’t pay attention to.\nStaging Area - a place where you can put files before you commit them. Once files are in the staging area, git is paying attention to them.\nCommit Log (aka Git History) - all of the commits (previous changes) to all of the files in your repository."
  },
  {
    "objectID": "git files/section 2/section 2.html#components-of-a-git-repository",
    "href": "git files/section 2/section 2.html#components-of-a-git-repository",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "Components of a Git Repository",
    "text": "Components of a Git Repository\n\n\nThe working directory\n\n\ngit init creates a git repo inside current working directory. This means that this command can turn a regular folder into a git repository by generating a hidden .git folder that starts to keep track of changes.\ngit clone takes a git repo from somewhere else and makes a copy of that repo into your current working directory. We will frequently be cloning repos from GitHub.\n\n\nThe staging area\n\n\ngit add . adds changes from the working directory to the staging area\ngit add &lt;filename&gt; adds changes to filenames specified from the working directory to the staging area\n\n\nThe commit\n\n\ngit commit -m \"commit message\" adds changes in staging area to the repository\ngit log shows\n\nProtip: Run git status after each command in the beginning because it allows you to visualize what just happaned."
  },
  {
    "objectID": "git files/section 2/section 2.html#pushing-to-github",
    "href": "git files/section 2/section 2.html#pushing-to-github",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "Pushing to GitHub",
    "text": "Pushing to GitHub\n\nKey Terms\n\ngithub - a service that hosts git remote repositories, and provides a web app to interact / collaborate on them\nremote - another repository that can be syncronized with a remote\nupstream - the name for a remote read-only repository\norigin - the name for a remote read-and-write repository\nclone - download an entire remote repository, to be used as a local repository\nfetch - downloading the set of changes (commits) from a remote repository\npull - fetching changes and merging them into the current branch\n\n\nIn order to show your remotes, you can run git remote -v show. The default remote is named “origin”\nIn order to push, you run git push. By default this will push from the branch you are on to a remote branch with the same name. (If you’d like to specify a branch, you can do that. The full formulation of this command is git push &lt;remote&gt; &lt;branch&gt;. So, for example you might say git push origin main to push to the “main” branch of the “origin” remote.)"
  },
  {
    "objectID": "git files/section 2/section 2.html#example-simple-rscript",
    "href": "git files/section 2/section 2.html#example-simple-rscript",
    "title": "Part 2: Introduction to Git and GitHub",
    "section": "❇️ Example: simple-rscript",
    "text": "❇️ Example: simple-rscript\nLet’s give it a try! We’re going to clone a repository for a simple R script from GitHub down to our computer where we can work with it locally. We will make some edits to the code, commit those changes and then push the changes back up to the remote repository in GitHub.\nhttps://github.com/code4MDM/simple-r-script"
  },
  {
    "objectID": "git files/section 3/section 3.html",
    "href": "git files/section 3/section 3.html",
    "title": "Part 3: Branching",
    "section": "",
    "text": "When using Git, branches are a part of your everyday development process. Git branches are effectively a pointer to a snapshot of your changes. When you want to add a new feature or fix a bug—no matter how big or how small—you spawn a new branch to encapsulate your changes. This makes it harder for unstable code to get merged into the main code base, and it gives you the chance to clean up your future’s history before merging it into the main branch.\nThere are many different use cases for branches ranging from the aforementioned “feature branches”, “release branches”, dev/test/qa branches, and more.\n\n\n\nWhen creating a new repository, git by default starts with a main branch which is what we have been using until now. When we create branches, we are branching off of the most recent commit on the main branch.\n\nThe diagram above visualizes a repository with two isolated lines of development, one for a little feature, and one for a longer-running feature. By developing them in branches, it’s not only possible to work on both of them in parallel, but it also keeps the main main branch free from questionable code.\nAfter developing a feature, it needs to be merged back into the main branch. We will do this using a Pull Request.\n\n\n\n\nWhen creating a branch, the head of the branch splits off of a common base as seen in the picture above. This is why we refer to Git as a non-linear workflow. There are three new commits on the feature branch and 2 new commits directly on the main branch.\n\nWhen come time to merge, the difference between the 2 new commits on the main branch and the new code in the feature branch is resolved in what is referred to as a merge commit. Git tries to be as smart as possible when merging code but occasionally there are changes that cannot automatically be merged. This is when we run into a merge conflict. We will talk more about this in detail.\nSources:\n\nhttps://www.atlassian.com/git/tutorials/using-branches\nhttps://www.atlassian.com/git/tutorials/using-branches/git-merge\n\n\n\n\nWhile there are other ways to merge branches, we will be using pull requests. When using the shared repository model (one repository, multiple collaborators),\n\nbase: almost always the main branch. this is where you are merging on to\ncompare: this is your feature branch. this is where you are merging from\n\n\n\n\n\n\ngit branch\n\n\n\nThe -b flag creates a new branch.\ngit checkout -b &lt;branchname&gt;\n\n\n\ngit checkout &lt;branchname&gt;\n\n\n\n\n\nGet into groups of 2 or 3.\nThe product owner should:\n\ncreate a repository under their account named demo-website (or use your project website!)\nadd the remaining team members (and me) as collaborators to the repo. This is done under the repo settings\nenable github pages on this repo with the main branch as the source\n\nThe product owner should create an index.html and a page called our-team/index.html. Commit and push this change directly to the main branch.\n\nHere is a sample index.html:\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Demo Website&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Our Demonstration Website&lt;/h1&gt;\n            &lt;a href='our-team/index.html'&gt;Learn more about our team&lt;/a&gt;\n        &lt;/body&gt;\n    &lt;/html&gt;\nHere is a sample `our-team/index.html`:\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Demo Website&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;About Us!&lt;/h1&gt;\n        &lt;/body&gt;\n    &lt;/html&gt;\n\nEveryone should clone the repository.\ngit clone git@github.com:XXXXX/demo-website.git\nAll team members, should create branches titled add-member-&lt;name&gt;. For example, I would do:\n\ngit checkout -b add-member-jacobjameson\n\nIn this branch, each team member should edit our-team/index.html and add some basic information about yourself to this page. Be sure to only create this one file - there should be no other changes to the repository. It’s important to keep your code changes isolated when working with git to avoid unecessary merge conflicts.\n\nEach member should:\n\ncommit this change to the branch\npush it\ncreate a pull request\n\nProduct owner should review and merge all of the PRs. There should be no conflicts.\n\nAll team members should checkout the main branch and pull the latest code.\n\n\n\ngit branch -d &lt;branchname&gt;"
  },
  {
    "objectID": "git files/section 3/section 3.html#why-branches",
    "href": "git files/section 3/section 3.html#why-branches",
    "title": "Part 3: Branching",
    "section": "",
    "text": "When using Git, branches are a part of your everyday development process. Git branches are effectively a pointer to a snapshot of your changes. When you want to add a new feature or fix a bug—no matter how big or how small—you spawn a new branch to encapsulate your changes. This makes it harder for unstable code to get merged into the main code base, and it gives you the chance to clean up your future’s history before merging it into the main branch.\nThere are many different use cases for branches ranging from the aforementioned “feature branches”, “release branches”, dev/test/qa branches, and more."
  },
  {
    "objectID": "git files/section 3/section 3.html#how-do-branches-work",
    "href": "git files/section 3/section 3.html#how-do-branches-work",
    "title": "Part 3: Branching",
    "section": "",
    "text": "When creating a new repository, git by default starts with a main branch which is what we have been using until now. When we create branches, we are branching off of the most recent commit on the main branch.\n\nThe diagram above visualizes a repository with two isolated lines of development, one for a little feature, and one for a longer-running feature. By developing them in branches, it’s not only possible to work on both of them in parallel, but it also keeps the main main branch free from questionable code.\nAfter developing a feature, it needs to be merged back into the main branch. We will do this using a Pull Request."
  },
  {
    "objectID": "git files/section 3/section 3.html#how-does-merging-work",
    "href": "git files/section 3/section 3.html#how-does-merging-work",
    "title": "Part 3: Branching",
    "section": "",
    "text": "When creating a branch, the head of the branch splits off of a common base as seen in the picture above. This is why we refer to Git as a non-linear workflow. There are three new commits on the feature branch and 2 new commits directly on the main branch.\n\nWhen come time to merge, the difference between the 2 new commits on the main branch and the new code in the feature branch is resolved in what is referred to as a merge commit. Git tries to be as smart as possible when merging code but occasionally there are changes that cannot automatically be merged. This is when we run into a merge conflict. We will talk more about this in detail.\nSources:\n\nhttps://www.atlassian.com/git/tutorials/using-branches\nhttps://www.atlassian.com/git/tutorials/using-branches/git-merge"
  },
  {
    "objectID": "git files/section 3/section 3.html#pull-requests",
    "href": "git files/section 3/section 3.html#pull-requests",
    "title": "Part 3: Branching",
    "section": "",
    "text": "While there are other ways to merge branches, we will be using pull requests. When using the shared repository model (one repository, multiple collaborators),\n\nbase: almost always the main branch. this is where you are merging on to\ncompare: this is your feature branch. this is where you are merging from"
  },
  {
    "objectID": "git files/section 3/section 3.html#commands",
    "href": "git files/section 3/section 3.html#commands",
    "title": "Part 3: Branching",
    "section": "",
    "text": "git branch\n\n\n\nThe -b flag creates a new branch.\ngit checkout -b &lt;branchname&gt;\n\n\n\ngit checkout &lt;branchname&gt;"
  },
  {
    "objectID": "git files/section 3/section 3.html#exercise",
    "href": "git files/section 3/section 3.html#exercise",
    "title": "Part 3: Branching",
    "section": "",
    "text": "Get into groups of 2 or 3.\nThe product owner should:\n\ncreate a repository under their account named demo-website (or use your project website!)\nadd the remaining team members (and me) as collaborators to the repo. This is done under the repo settings\nenable github pages on this repo with the main branch as the source\n\nThe product owner should create an index.html and a page called our-team/index.html. Commit and push this change directly to the main branch.\n\nHere is a sample index.html:\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Demo Website&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;Our Demonstration Website&lt;/h1&gt;\n            &lt;a href='our-team/index.html'&gt;Learn more about our team&lt;/a&gt;\n        &lt;/body&gt;\n    &lt;/html&gt;\nHere is a sample `our-team/index.html`:\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n        &lt;head&gt;\n            &lt;title&gt;Demo Website&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;h1&gt;About Us!&lt;/h1&gt;\n        &lt;/body&gt;\n    &lt;/html&gt;\n\nEveryone should clone the repository.\ngit clone git@github.com:XXXXX/demo-website.git\nAll team members, should create branches titled add-member-&lt;name&gt;. For example, I would do:\n\ngit checkout -b add-member-jacobjameson\n\nIn this branch, each team member should edit our-team/index.html and add some basic information about yourself to this page. Be sure to only create this one file - there should be no other changes to the repository. It’s important to keep your code changes isolated when working with git to avoid unecessary merge conflicts.\n\nEach member should:\n\ncommit this change to the branch\npush it\ncreate a pull request\n\nProduct owner should review and merge all of the PRs. There should be no conflicts.\n\nAll team members should checkout the main branch and pull the latest code.\n\n\n\ngit branch -d &lt;branchname&gt;"
  },
  {
    "objectID": "R files/6 Module/mod6.html#data-visualization-as-a-tool-for-analysis",
    "href": "R files/6 Module/mod6.html#data-visualization-as-a-tool-for-analysis",
    "title": "Module 6: Data Visualization as a Tool for Analysis",
    "section": "",
    "text": "Download a copy of Module 6 slides\nDownload data for Module 6 lab and tutorial"
  },
  {
    "objectID": "git files/section 4/section 4.html",
    "href": "git files/section 4/section 4.html",
    "title": "Part 4: Reference",
    "section": "",
    "text": "Git: Reference\n\n\nGetting Started\n\nCreate a new repository on GitHub\nClone that repository locally onto your computer\ngit clone git@github.com:jacobjameson/simples_script.git\n\n\n\nLinear Workflow\n\ncd into the folder containing your project.\ncd ~/path/to/project\nCheck the status of your local repository to make sure you didn’t forget to commit any work.\ngit status\nThen pull the latest changes from the remote repository on GitHub.\ngit pull\nDo a discrete chunk of work on your project (lets say you added a new analysis)\nCheck the status again, then add the files you’d like to commit to the staging area.\ngit status\ngit add analysis.R\ngit status\nCommit with a descriptive summary of exactly what you did\ngit commit -m \"add new analysis\"\nPush that change back to GitHub\ngit push\n\n\n\nNon-linear branching workflow\n\ncd into the folder containing your project.\ncd ~/path/to/project\nCheck the status of your local repository to make sure you didn’t forget to commit any work. Run git branch to see which branch you’re on. You should ideally be on the master branch.\ngit status\ngit branch\nThen pull the latest changes from the master branch of the remote repository on GitHub.\ngit pull\nCreate a new branch with a descriptive name (remember the -b option will create a new branch, you can check out an existing branch by not using that option)\ngit checkout -b analysis-new\nDo your work in discrete chunks. at the end of each chunk, add the file to the staging area, then commit it. Its usually a good idea to also push the latest to GitHub, although some people prefer to do that at the end.\nDo some work\ngit status\ngit add analysis.R\ngit commit -m \"add blank script\"\ngit push\ngit status\nDo more work\ngit status\ngit add analysis.R\ngit commit -m \"add code to analysis script\"\ngit push\ngit status\nDo more work\ngit status\ngit add analysis.R\ngit commit -m \"fixed bug\"\ngit push\ngit status\nDo more work - lets imagine this work took place across two files, an html file and a stylesheet file.\ngit status\ngit add analysis.R\ngit add graph.png\ngit commit -m \"Added the plot from the analysis\"\ngit push\ngit status\nOnce everything has been pushed to GitHub, issue a pull request from your branch back to the master branch.\n\nYou can have a discussion on this pull request using GitHub’s social features, and then merge it into the master branch when everyone agrees its a good idea to do so.\nFinally, once the pull request has been merged into the master branch in the remote repository on GitHub, you’ll want to get the latest version of the master branch on your local machine. Checkout the master branch locally and then pull.\ngit checkout master\ngit pull"
  },
  {
    "objectID": "git files/section 5/section 5.html",
    "href": "git files/section 5/section 5.html",
    "title": "Part 5: Workflow",
    "section": "",
    "text": "When working with branches, here is the general workflow to adhere to.\n\nBefore starting work:\n# always start your branching from the master branch\ngit checkout master\n\n# pull the latest\ngit pull\n\n# create a new branch, branch-ed off of the master branch\ngit checkout -b my-awesome-feature\nCommit changes and push to your branch in GitHub regularly.\n\n# add files to the staging area\ngit add filename1\ngit add filename2\ngit add filename3\n\n# commit with a descritive message\ngit commit -m \"descriptive message of the change i just made\"\n\n# push to your branch on GitHub\ngit push\nAlso make sure to periodically pull from master:\ngit pull origin master\nPulling from master periodically is very important! This will keep your code relatively in-sync and prevent deferring massive merge conflicts down the line.\nWhen you’re done with your work\n# makes sure you've commited and pushed \n# all the changes to your branch in GitHub\n\ngit status\nthen open up GitHub and issue a pull request back to master.\n\n\n\n\n\n\n\nThe feature branch workflow is where every small or large feature gets its own branch. These branches are shortlived and are quickly merged back into master. Each feature branch corresponds to a pull request and the branch is deleted after the PR is merged.\n\n\n\n\nThe team member workflow is where each team member gets their own branch. These branches stay alive throughout the duration of the project and can be merged back in as frequently as you like. In this approach you can only have one PR open at a given time per team member."
  },
  {
    "objectID": "git files/section 5/section 5.html#overall-workflow",
    "href": "git files/section 5/section 5.html#overall-workflow",
    "title": "Part 5: Workflow",
    "section": "",
    "text": "When working with branches, here is the general workflow to adhere to.\n\nBefore starting work:\n# always start your branching from the master branch\ngit checkout master\n\n# pull the latest\ngit pull\n\n# create a new branch, branch-ed off of the master branch\ngit checkout -b my-awesome-feature\nCommit changes and push to your branch in GitHub regularly.\n\n# add files to the staging area\ngit add filename1\ngit add filename2\ngit add filename3\n\n# commit with a descritive message\ngit commit -m \"descriptive message of the change i just made\"\n\n# push to your branch on GitHub\ngit push\nAlso make sure to periodically pull from master:\ngit pull origin master\nPulling from master periodically is very important! This will keep your code relatively in-sync and prevent deferring massive merge conflicts down the line.\nWhen you’re done with your work\n# makes sure you've commited and pushed \n# all the changes to your branch in GitHub\n\ngit status\nthen open up GitHub and issue a pull request back to master."
  },
  {
    "objectID": "git files/section 5/section 5.html#workflow-types",
    "href": "git files/section 5/section 5.html#workflow-types",
    "title": "Part 5: Workflow",
    "section": "",
    "text": "The feature branch workflow is where every small or large feature gets its own branch. These branches are shortlived and are quickly merged back into master. Each feature branch corresponds to a pull request and the branch is deleted after the PR is merged.\n\n\n\n\nThe team member workflow is where each team member gets their own branch. These branches stay alive throughout the duration of the project and can be merged back in as frequently as you like. In this approach you can only have one PR open at a given time per team member."
  },
  {
    "objectID": "Intro Git.html#further-reading",
    "href": "Intro Git.html#further-reading",
    "title": "Introduction to Git and GitHub",
    "section": "Further Reading",
    "text": "Further Reading\n\nUnderstanding Git Conceptually\n\n​https://www.sbf5.com/~cduan/technical/git/\n\nReference Guides to Git Commands\n\nhttps://git-scm.com/docs\n\nGit Screwup Guide\n\nhttp://ohshitgit.com/\n\nGit/Github Cheat Sheets\n\nhttps://training.github.com/kit/downloads/github-git-cheat-sheet.pdf\nhttp://ndpsoftware.com/git-cheatsheet.html\n\nLicensing Open Source Code\n\n​http://choosealicense.com/\nhttps://help.github.com/articles/open-source-licensing/\n\nhttps://cs61.seas.harvard.edu/wiki/2012/Git\nhttp://www.eecs.harvard.edu/~cs161/resources/git.html\nhttps://git-scm.com/video/what-is-version-control\nhttps://git-scm.com/video/quick-wins\nhttp://slides.com/dhrumilmehta/how-to-tell-a-story-with-data-tools-of-the-trade-2#/4/21"
  },
  {
    "objectID": "Intro Git.html#learn-to-use-git-from-the-command-line",
    "href": "Intro Git.html#learn-to-use-git-from-the-command-line",
    "title": "Introduction to Git and GitHub",
    "section": "Learn to use Git from the Command Line",
    "text": "Learn to use Git from the Command Line\n\nInteractive Tutorials\n\n​https://try.github.io\nhttps://github.com/jlord/git-it-electron\n\nBasic Git Commands\n\n​http://www.teaching-materials.org/git/slides.html\nhttp://rogerdudler.github.io/git-guide/"
  },
  {
    "objectID": "Intro Git.html#credits",
    "href": "Intro Git.html#credits",
    "title": "Introduction to Git and GitHub",
    "section": "Credits",
    "text": "Credits\nInformation sourced from: https://github.com/AlJohri/DAT-DC-12/blob/master/notebooks/intro-git.ipynb and Dhrumil Mehta"
  },
  {
    "objectID": "git files/section 6/section 6.html",
    "href": "git files/section 6/section 6.html",
    "title": "Part 6: Additional Resources",
    "section": "",
    "text": "Understanding Git Conceptually\n\n​https://www.sbf5.com/~cduan/technical/git/\n\nReference Guides to Git Commands\n\nhttps://git-scm.com/docs\n\nGit Screwup Guide\n\nhttp://ohshitgit.com/\n\nGit/Github Cheat Sheets\n\nhttps://training.github.com/kit/downloads/github-git-cheat-sheet.pdf\nhttp://ndpsoftware.com/git-cheatsheet.html\n\nLicensing Open Source Code\n\n​http://choosealicense.com/\nhttps://help.github.com/articles/open-source-licensing/\n\nhttps://cs61.seas.harvard.edu/wiki/2012/Git\nhttp://www.eecs.harvard.edu/~cs161/resources/git.html\nhttps://git-scm.com/video/what-is-version-control\nhttps://git-scm.com/video/quick-wins\nhttp://slides.com/dhrumilmehta/how-to-tell-a-story-with-data-tools-of-the-trade-2#/4/21\n\n\n\n\n\nInteractive Tutorials\n\n​https://try.github.io\nhttps://github.com/jlord/git-it-electron\n\nBasic Git Commands\n\n​http://www.teaching-materials.org/git/slides.html\nhttp://rogerdudler.github.io/git-guide/\n\n\n\n\n\nInformation sourced from: https://github.com/AlJohri/DAT-DC-12/blob/master/notebooks/intro-git.ipynb and Dhrumil Mehta"
  },
  {
    "objectID": "git files/section 6/section 6.html#further-reading",
    "href": "git files/section 6/section 6.html#further-reading",
    "title": "Part 6: Additional Resources",
    "section": "",
    "text": "Understanding Git Conceptually\n\n​https://www.sbf5.com/~cduan/technical/git/\n\nReference Guides to Git Commands\n\nhttps://git-scm.com/docs\n\nGit Screwup Guide\n\nhttp://ohshitgit.com/\n\nGit/Github Cheat Sheets\n\nhttps://training.github.com/kit/downloads/github-git-cheat-sheet.pdf\nhttp://ndpsoftware.com/git-cheatsheet.html\n\nLicensing Open Source Code\n\n​http://choosealicense.com/\nhttps://help.github.com/articles/open-source-licensing/\n\nhttps://cs61.seas.harvard.edu/wiki/2012/Git\nhttp://www.eecs.harvard.edu/~cs161/resources/git.html\nhttps://git-scm.com/video/what-is-version-control\nhttps://git-scm.com/video/quick-wins\nhttp://slides.com/dhrumilmehta/how-to-tell-a-story-with-data-tools-of-the-trade-2#/4/21"
  },
  {
    "objectID": "git files/section 6/section 6.html#learn-to-use-git-from-the-command-line",
    "href": "git files/section 6/section 6.html#learn-to-use-git-from-the-command-line",
    "title": "Part 6: Additional Resources",
    "section": "",
    "text": "Interactive Tutorials\n\n​https://try.github.io\nhttps://github.com/jlord/git-it-electron\n\nBasic Git Commands\n\n​http://www.teaching-materials.org/git/slides.html\nhttp://rogerdudler.github.io/git-guide/"
  },
  {
    "objectID": "git files/section 6/section 6.html#credits",
    "href": "git files/section 6/section 6.html#credits",
    "title": "Part 6: Additional Resources",
    "section": "",
    "text": "Information sourced from: https://github.com/AlJohri/DAT-DC-12/blob/master/notebooks/intro-git.ipynb and Dhrumil Mehta"
  },
  {
    "objectID": "api 222 files/API222.html",
    "href": "api 222 files/API222.html",
    "title": "API 222 Section Materials",
    "section": "",
    "text": "Welcome to the Section Materials for API-222! This course is a journey through the fascinating world of machine learning, tailored to provide you with a strong foundation in both the theoretical and practical aspects of the field. With a focus on policy-making and decision-making applications, you will gain not only an understanding of the statistical theories behind the algorithms but also the skills to apply them to solve real-world problems.\nIn this supplementary section, our primary goal is to enhance your coding abilities. While the main lectures of the course delve into the concepts and theories of machine learning, here we will put those theories into practice.\nThe section materials are neatly segmented to mirror the course’s structure, which is divided into two main units:\n\nSupervised Learning: We start with a deep dive into algorithms designed to predict and analyze outcomes based on input data. You’ll learn to navigate through regression, classification, cross-validation, and more, using R to bring these concepts to life.\nUnsupervised Learning: The second half of the course explores the patterns and structures in data where outcomes aren’t provided. Clustering, dimensionality reduction, and other techniques will be covered, giving you a comprehensive toolkit for data analysis.\n\nWhether you are running regressions, classifying data, or implementing advanced machine learning models, these section materials are crafted to provide you with hands-on experience. Through a series of code examples, exercises, and assignments, you will build the confidence to tackle complex datasets and extract meaningful insights.\nRemember, while mastering the syntax and functions of a programming language is essential, the real magic lies in applying these tools to real-world scenarios. Let’s code our way to a deeper understanding of machine learning!"
  },
  {
    "objectID": "api 222 files/API222.html#section-materials-introduction",
    "href": "api 222 files/API222.html#section-materials-introduction",
    "title": "API 222 Section Materials",
    "section": "",
    "text": "Welcome to the Section Materials for API-222! This course is a journey through the fascinating world of machine learning, tailored to provide you with a strong foundation in both the theoretical and practical aspects of the field. With a focus on policy-making and decision-making applications, you will gain not only an understanding of the statistical theories behind the algorithms but also the skills to apply them to solve real-world problems.\nIn this supplementary section, our primary goal is to enhance your coding abilities. While the main lectures of the course delve into the concepts and theories of machine learning, here we will put those theories into practice.\nThe section materials are neatly segmented to mirror the course’s structure, which is divided into two main units:\n\nSupervised Learning: We start with a deep dive into algorithms designed to predict and analyze outcomes based on input data. You’ll learn to navigate through regression, classification, cross-validation, and more, using R to bring these concepts to life.\nUnsupervised Learning: The second half of the course explores the patterns and structures in data where outcomes aren’t provided. Clustering, dimensionality reduction, and other techniques will be covered, giving you a comprehensive toolkit for data analysis.\n\nWhether you are running regressions, classifying data, or implementing advanced machine learning models, these section materials are crafted to provide you with hands-on experience. Through a series of code examples, exercises, and assignments, you will build the confidence to tackle complex datasets and extract meaningful insights.\nRemember, while mastering the syntax and functions of a programming language is essential, the real magic lies in applying these tools to real-world scenarios. Let’s code our way to a deeper understanding of machine learning!"
  },
  {
    "objectID": "git files/Intro Git.html",
    "href": "git files/Intro Git.html",
    "title": "Introduction to Git and GitHub",
    "section": "",
    "text": "Welcome to the Introduction to Git and GitHub. This is a collection of notes and references for using Git and GitHub. The goal is to provide a quick introduction and reference for common commands and workflows.\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nPart 1: Git Installation and SSH Instructions\n\n\n\n\n\nHelp on installing git and setting up SSH keys for GitHub\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nPart 2: Introduction to Git and GitHub\n\n\n\n\n\nIn this guide, we’ll cover the basics of Git and GitHub\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nPart 3: Branching\n\n\n\n\n\n\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nPart 4: Reference\n\n\n\n\n\nA reference guide for git commands\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nPart 5: Workflow\n\n\n\n\n\nA reference guide for git commands\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nPart 6: Additional Resources\n\n\n\n\n\nLinks to additional git resource\n\n\n\n\n\n\nJacob Jameson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blog.html",
    "href": "posts/blog.html",
    "title": "Jacob Jameson",
    "section": "",
    "text": "Welcome to my blog!\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWhat is Linear Discriminant Analysis (LDA)?\n\n\n\n\n\n\n\nLDA\n\n\n\n\nA brief overview of Linear Discriminant Analysis (LDA) and how it can be used to classify data.\n\n\n\n\n\n\nFeb 15, 2024\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nDemystifying KNN\n\n\n\n\n\n\n\nknn\n\n\n\n\nA deep dive into the K-Nearest Neighbors (KNN) algorithm, exploring its mathematical foundations and practical applications.\n\n\n\n\n\n\nFeb 1, 2024\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nThe ‘Odd’ Obsession with Odds Ratios: Making a Case for Marginal Effects in Health Research\n\n\n\n\n\n\n\nmarginal effects\n\n\n\n\nMarginal Effects &gt;&gt;&gt; Odds Ratios\n\n\n\n\n\n\nSep 1, 2023\n\n\nJacob Jameson\n\n\n\n\n\n\n  \n\n\n\n\nBasic Network Analysis and Visualization for Directed Graphs in R\n\n\n\n\n\n\n\nnetworks\n\n\ncentrality\n\n\n\n\nChoosing the Right Centrality Measure.\n\n\n\n\n\n\nNov 1, 2022\n\n\nJacob Jameson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "R files/Intro R.html",
    "href": "R files/Intro R.html",
    "title": "Introduction to Programming in R",
    "section": "",
    "text": "This page contains the content for the “Introduction to Data Analysis in R” mini-course. The goal is to introduce R programming concepts with a focus on preparing and analyzing data, and conducting statistical simulations. We will cover how to read data into R, manipulate data with a suite of tools from the tidyverse package dplyr. We will also discuss some basic programming concepts including data types and operators.\nThis mini-course is aimed at people with no programming background that are looking for a gentle and practical introduction to data analysis in R.\nEach Module involves:\n\na pre-recorded lecture with guided practice exercises\na corresponding lab to practice the skills taught in the video and stretch your thinking\n\nThe labs are an essential part of the learning process as they walkthrough the use of many key functions and topics that are not explicitly covered in the videos.\n\nAdditional Resources\n\n\n\ntidyverse cheetsheets start with dplyr and ggplot\n\n\nR for Data Science: free online book with clear explanations of many tidyverse functions, the book to read on data analysis with R"
  },
  {
    "objectID": "R files/Intro R.html#about-this-course",
    "href": "R files/Intro R.html#about-this-course",
    "title": "Introduction to Programming in R",
    "section": "",
    "text": "This page contains the content for the “Introduction to Data Analysis in R” mini-course. The goal is to introduce R programming concepts with a focus on preparing and analyzing data, and conducting statistical simulations. We will cover how to read data into R, manipulate data with a suite of tools from the tidyverse package dplyr. We will also discuss some basic programming concepts including data types and operators.\nThis mini-course is aimed at people with no programming background that are looking for a gentle and practical introduction to data analysis in R.\nEach Module involves:\n\na pre-recorded lecture with guided practice exercises\na corresponding lab to practice the skills taught in the video and stretch your thinking\n\nThe labs are an essential part of the learning process as they walkthrough the use of many key functions and topics that are not explicitly covered in the videos.\n\nAdditional Resources\n\n\n\ntidyverse cheetsheets start with dplyr and ggplot\n\n\nR for Data Science: free online book with clear explanations of many tidyverse functions, the book to read on data analysis with R"
  },
  {
    "objectID": "R files/Intro R.html#table-of-contents",
    "href": "R files/Intro R.html#table-of-contents",
    "title": "Introduction to Programming in R",
    "section": "Table of Contents",
    "text": "Table of Contents"
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#what-is-git",
    "href": "git files/section 2/section 2 slides.html#what-is-git",
    "title": "Section 2",
    "section": "What is Git?",
    "text": "What is Git?\nKeeping track of file versions is hard."
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#git-versus-github",
    "href": "git files/section 2/section 2 slides.html#git-versus-github",
    "title": "Section 2",
    "section": "Git versus GitHub",
    "text": "Git versus GitHub\n\nGit is a distributed version control system. It is a technology.\n\nGitHub is a social coding platform where git repositories are stored and where people can collaborate on projects. GitHub is great both for collaboration within your organization, but also provides an excellent model for open source collaboration across organizations or with the public.\n\nOn GitHub you can find Git repositories.\n\n\nLearn More: https://jahya.net/blog/git-vs-github/"
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#some-vocabulary",
    "href": "git files/section 2/section 2 slides.html#some-vocabulary",
    "title": "Section 2",
    "section": "Some Vocabulary",
    "text": "Some Vocabulary\nThe basics:\n\nGit - version control software\nRepository - a folder containing your files and also containing a structure that helps keep track of changes in those files. When you intialize a repository, git creates a hidden folder (.git folder) that stores the changes to those files.\nCommit - the basic unit of a git repository is a commit. It is a set of changes to a file. A commit usually comes with an id as well as a commit message that describes the change.\n\nWorking with others:\n\nGitHub - a place to host git repositories and collaborate\nLocal Repository - the version of a git repository on your local computer\nRemote Repository - the version of a git repository stored somewhere else that your local repository is connected to (frequently on GitHub)"
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#lets-dive-in",
    "href": "git files/section 2/section 2 slides.html#lets-dive-in",
    "title": "Section 2",
    "section": "Lets Dive In!",
    "text": "Lets Dive In!\nVocab\n\nlocal repository a folder stored on your computer\nremote repository a folder stored on on GitHub\n\nLets take a look at a repository that is on GitHub.\n\nhttps://github.com/code4MDM/simple-r-script"
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#some-of-githubs-features",
    "href": "git files/section 2/section 2 slides.html#some-of-githubs-features",
    "title": "Section 2",
    "section": "(Some of) GitHub’s Features",
    "text": "(Some of) GitHub’s Features\nThe README.md file\nGithub looks for a “readme” file and renders it as you’re navigating through the file structure. This is a great way to guide people through your code.\n\nReadme files are often given the .md extension, meaning they’re written in a language called markdown that allows for nicer formatting. You can check out this markdown cheet sheet (https://www.markdownguide.org/cheat-sheet/) if you want to see how formatting works, but you can also save a readme files as plain text. Github will also detect .txt files, or you can just write plain text inside your .md file."
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#github-for-things-other-than-code",
    "href": "git files/section 2/section 2 slides.html#github-for-things-other-than-code",
    "title": "Section 2",
    "section": "GitHub for things other than code",
    "text": "GitHub for things other than code\n\nAuditing system for changes on a file\nFor collaboratively editing a text document\nFor drafting government web design standards!\nOpen comment period for policy\nDrafting and collaborating on legal documents\nDesign (image diff)\n\nhttps://help.github.com/articles/rendering-and-diffing-images/\n\nOpen journalsim showcase\n\nhttps://github.com/showcases/open-journalism\n\nGithub for Government\n\nhttps://government.github.com/\nhttps://government.github.com/community/"
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#some-vocabulary-1",
    "href": "git files/section 2/section 2 slides.html#some-vocabulary-1",
    "title": "Section 2",
    "section": "Some Vocabulary",
    "text": "Some Vocabulary\n\nGit - version control software\nRepository - a folder containing your files and also containing a structure that helps keep track of changes in those files. When you intialize a repository, git creates a hidden folder (.git folder) that stores the changes to those files.\nGitHub - a place to host git repositories and collaborate\nLocal Repository - the version of a git repository on your local computer\nRemote Repository - the version of a git repository stored somewhere else that your local repository is connected to (frequently on GitHub)\nCommit - the basic unit of a git repository is a commit. It is a set of changes to a file. A commit usually comes with an id as well as a commit message that describes the change.\n\nWithin a Repository you have\n\nUntracked Changes - files that are in your folder but that git doesn’t pay attention to.\nStaging Area - a place where you can put files before you commit them. Once files are in the staging area, git is paying attention to them.\nCommit Log (aka Git History) - all of the commits (previous changes) to all of the files in your repository."
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#components-of-a-git-repository",
    "href": "git files/section 2/section 2 slides.html#components-of-a-git-repository",
    "title": "Section 2",
    "section": "Components of a Git Repository",
    "text": "Components of a Git Repository\n\n\nThe working directory\n\n\ngit init creates a git repo inside current working directory. This means that this command can turn a regular folder into a git repository by generating a hidden .git folder that starts to keep track of changes.\ngit clone takes a git repo from somewhere else and makes a copy of that repo into your current working directory. We will frequently be cloning repos from GitHub.\n\n\nThe staging area\n\n\ngit add . adds changes from the working directory to the staging area\ngit add &lt;filename&gt; adds changes to filenames specified from the working directory to the staging area\n\n\nThe commit\n\n\ngit commit -m \"commit message\" adds changes in staging area to the repository\ngit log shows\n\nProtip: Run git status after each command in the beginning because it allows you to visualize what just happaned."
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#pushing-to-github",
    "href": "git files/section 2/section 2 slides.html#pushing-to-github",
    "title": "Section 2",
    "section": "Pushing to GitHub",
    "text": "Pushing to GitHub\nKey Terms\n\ngithub - a service that hosts git remote repositories, and provides a web app to interact / collaborate on them\nremote - another repository that can be syncronized with a remote\nupstream - the name for a remote read-only repository\norigin - the name for a remote read-and-write repository\nclone - download an entire remote repository, to be used as a local repository\nfetch - downloading the set of changes (commits) from a remote repository\npull - fetching changes and merging them into the current branch\n\n\nIn order to show your remotes, you can run git remote -v show. The default remote is named “origin”\nIn order to push, you run git push. By default this will push from the branch you are on to a remote branch with the same name. (If you’d like to specify a branch, you can do that. The full formulation of this command is git push &lt;remote&gt; &lt;branch&gt;. So, for example you might say git push origin main to push to the “main” branch of the “origin” remote.)"
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#example-simple-rscript",
    "href": "git files/section 2/section 2 slides.html#example-simple-rscript",
    "title": "Section 2",
    "section": "❇️ Example: simple-rscript",
    "text": "❇️ Example: simple-rscript\nLet’s give it a try! We’re going to clone a repository for a simple R script from GitHub down to our computer where we can work with it locally. We will make some edits to the code, commit those changes and then push the changes back up to the remote repository in GitHub.\nhttps://github.com/code4MDM/simple-r-script"
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#git-introduction",
    "href": "git files/section 2/section 2 slides.html#git-introduction",
    "title": "Section 2",
    "section": "Git Introduction",
    "text": "Git Introduction\n\nGit (/ɡɪt/) is a version control system (VCS) for tracking changes in computer files and coordinating work on those files among multiple people. It is primarily used for source code management in software development, but it can be used to keep track of changes in any set of files. As a distributed revision control system it is aimed at speed, data integrity, and support for distributed, non-linear workflows. -Wikipedia\n\n\nGit is an open source program for tracking changes in text files. -GitHub (https://help.github.com/articles/github-glossary/)"
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#github-introduction",
    "href": "git files/section 2/section 2 slides.html#github-introduction",
    "title": "Section 2",
    "section": "GitHub Introduction",
    "text": "GitHub Introduction\nGitHub is a place for:\n\nStoring and viewing your Git repositories\nCollaborating on coding projects within an organization\nOpen Source Collboration: People in other organizations or complete strangers can see your code and participate with you on your project. They can suggest fixes, point out errors, and start discussions. You can find other people’s work, and make sure you’re not duplicating something that already exists. Or you can interact with someone who has already worked on something similar to what you’re trying to tackle.\nWeb Hosting: The “Github Pages” feature allows you to host a static website for free, right on GitHub."
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#using-git-locally",
    "href": "git files/section 2/section 2 slides.html#using-git-locally",
    "title": "Section 2",
    "section": "Using Git Locally",
    "text": "Using Git Locally\nNow let’s learn how to use Git locally from our computer. While there are GUIs (graphical user interfaces) for Git on your computer like GitHub Desktop, we’re going to use the command line to learn Git. That’s because this is a key transferrable skill that applies to any kind of coding.\n\nLearning objective: Build a “mental model” of what your computer is doing."
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#so-what-is-git-and-why-does-it-help-us",
    "href": "git files/section 2/section 2 slides.html#so-what-is-git-and-why-does-it-help-us",
    "title": "Section 2",
    "section": "So what is Git, and why does it help us?",
    "text": "So what is Git, and why does it help us?\nAbove all else, Git is a fast and distributed version control system, that allows you to efficiently handle projects large and small.\nHere are some problems we face as developers, and how git solves them:"
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#reverting-to-past-save-points-commits",
    "href": "git files/section 2/section 2 slides.html#reverting-to-past-save-points-commits",
    "title": "Section 2",
    "section": "Reverting to past save points (commits)",
    "text": "Reverting to past save points (commits)\nGit allows us to make save points at any time. These save points are called ‘commits’. Once a save point is made, it’s permanent, and allows us to go back to that save point at any time. From there, we can see what the code looked like at that point, or even start building off that version."
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#keeping-track-of-what-each-save-point-meant-commit-messages",
    "href": "git files/section 2/section 2 slides.html#keeping-track-of-what-each-save-point-meant-commit-messages",
    "title": "Section 2",
    "section": "Keeping track of what each save point ‘meant’ (commit messages)",
    "text": "Keeping track of what each save point ‘meant’ (commit messages)\nEvery commit has a description (commit message), which allows us to describe what changes were made between the current and previous commit. This is usually a description of what features were added or what bugs were fixed.\nAdditionally, git supports tagging, which allows us to mark a specific commit as a specific version of our code (e.g. ‘2.4.5’)."
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#comparing-changes-to-past-save-points-diff",
    "href": "git files/section 2/section 2 slides.html#comparing-changes-to-past-save-points-diff",
    "title": "Section 2",
    "section": "Comparing changes to past save points (diff)",
    "text": "Comparing changes to past save points (diff)\nIt’s often important to see content of the actual changes that were made. This can be useful when:\n\ntracking down when and how a bug was introduced\nunderstanding the changes a team member made so you can stay up-to-date with progress\nreviewing code as a team for correctness or quality/style\n\nGit allows us to easily see these changes (called a diff) for any given commit."
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#non-linear-workflow-branches",
    "href": "git files/section 2/section 2 slides.html#non-linear-workflow-branches",
    "title": "Section 2",
    "section": "Non-linear workflow (branches)",
    "text": "Non-linear workflow (branches)\nGit enables you to work using a non-linear workflow. This means that you can have multiple versions of a project or “branches” with different save points, or “commits”, simultaneously within the same folder and easily toggle bgttween them. You can split new branches off a project when you’re looking to experiment or implement a new feature, and you can merge those branches back into the main (formerly known as “master”) branch when you’re ready to incorporate them into a project."
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#fearlessness-in-making-changes",
    "href": "git files/section 2/section 2 slides.html#fearlessness-in-making-changes",
    "title": "Section 2",
    "section": "Fearlessness in making changes",
    "text": "Fearlessness in making changes\nIn developing software, we often want to experiment in adding a feature or refactoring (rewriting) existing code. Because git makes it easy to go back to a known good state, we can experiment without worrying that we’ll be unable to undo the experimental work."
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#other-uses",
    "href": "git files/section 2/section 2 slides.html#other-uses",
    "title": "Section 2",
    "section": "Other Uses",
    "text": "Other Uses\n\nA place to store code\nA place to store data\nA place for discussion about data and code\nA place for open source collaboration on projects\nA place for collaboration within an organization\nA place to find people working on the same things you are\nA place to find sources / people with expertise in a particular dataset\nA place to find other digital tools (scrapers, file converters, etc) \nA place to find open source work and analysis\nA portfolio of your technical work (or technical learning)"
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#commit-log",
    "href": "git files/section 2/section 2 slides.html#commit-log",
    "title": "Section 2",
    "section": "Commit Log",
    "text": "Commit Log\n\nVocab\n\ncommit - the basic unit of a git repository is a commit. It is a set of changes to a file. A commit usually comes with an commit id as well as a commit message that describes the change.\ncommit log (aka Git History) - all of the commits (previous changes) to all of the files in your repository\n\n https://github.com/dmil/my-simple-website/commits/master"
  },
  {
    "objectID": "git files/section 2/section 2 slides.html#history-raw-and-blame-for-any-file",
    "href": "git files/section 2/section 2 slides.html#history-raw-and-blame-for-any-file",
    "title": "Section 2",
    "section": "History, Raw, and Blame for any file",
    "text": "History, Raw, and Blame for any file\n\n\nRaw - actual contents of the file without any formatting applied.\n\nexample: https://github.com/fivethirtyeight/chartbuilder-2/blob/master/src/styles/core.scss\n\nHistory - every change ever made to that file within this branch.\n\nexample: https://github.com/fivethirtyeight/chartbuilder-2/commits/master/src/styles/core.scss\n\nBlame - provenance of each line currently in the file you’re looking at in the branch you’re looking at\n\nexample: https://github.com/fivethirtyeight/chartbuilder-2/blame/master/src/styles/core.scss\n\n\nBranches\nPull Requests\nProposing Edits\n\nYou can edit a file in GitHub in a new branch, thus proposing a change without actually making the change in the master branch. Just make sure to leave a meaningful description of the change you made in the commit message.\n\nDrag and Drop\n\nCollaboration\nCollaborators can push to the repository without asking your permission, they have full read and write access.\n\nIf I wasn’t a collaborator, I could still work with you on an open source project through a process called forking where I can make a copy of your repository in my GitHub account, make changes, and request that you merge them back into your project. We will discuss forking more in depth later.\nServing up Websites!\nGitHub is also great for serving up static websites. GitHub is only storing the code. Luckily, if your code happens to be a website, GitHub can also host it for you through a feature called “GitHub Pages”.\nSimply go to the “settings” menu, scroll down to “GitHub Pages”, and select “master branch”\n\nWhatever is in your master branch on GitHub should now appear at\nhttp://your-username.github.io/repository-name\nin my case it is http://madisoncoots.github.io/\nGitHub for things other than code\n\nAuditing system for changes on a file\nFor collaboratively editing a text document\nFor drafting government web design standards!\nOpen comment period for policy\nDrafting and collaborating on legal documents\nDesign (image diff)\n\nhttps://help.github.com/articles/rendering-and-diffing-images/\n\nOpen journalsim showcase\n\nhttps://github.com/showcases/open-journalism\n\nGithub for Government\n\nhttps://government.github.com/\nhttps://government.github.com/community/"
  },
  {
    "objectID": "api 222 files/section 4/section 4.html",
    "href": "api 222 files/section 4/section 4.html",
    "title": "Section 4.1 - Classification Notes",
    "section": "",
    "text": "Note that the material in these notes draws on past TF’s notes (Ibou Dieye, Laura Morris, Emily Mower, Amy Wickett), and the more thorough treatment of these topics in Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#concept",
    "href": "api 222 files/section 4/section 4.html#concept",
    "title": "Section 4.1 - Classification Notes",
    "section": "Concept",
    "text": "Concept\nLogistic regression is a parametric model that models the probability that \\(Y\\) belongs to a particular category. It is somewhat similar to linear regression, but the linear regression form of \\(\\beta_0+\\beta_1 X_1 + \\beta_2 X_2 + \\dots\\) undergoes a transformation that ensures the output will be bounded between 0 and 1 and can thus be interpreted as a probability.\n\\[\\begin{equation}\n     p(X) = \\frac{e^{\\beta_0+\\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\end{equation}\\]\nTherefore, while linear regression is best suited for regression problems, logistic regression is best suited for classification problems. Note that logistic regression produces a probability of class membership that then needs to be transformed to 0 or 1 using a decision rule, such as if \\(p(X)\\geq 0.5\\) then predict 1 otherwise predict 0."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#method",
    "href": "api 222 files/section 4/section 4.html#method",
    "title": "Section 4.1 - Classification Notes",
    "section": "Method",
    "text": "Method\nLogistic regression is estimated using Maximum Likelihood Estimation (MLE). MLE finds the values of \\(\\beta_0\\), \\(\\beta_1\\), etc. that maximize the probability of observing the observations in your training data given the values of the parameters \\(\\beta_0\\), \\(\\beta_1\\), etc. and the assumed functional form (e.g. see above)."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#implementation-and-considerations",
    "href": "api 222 files/section 4/section 4.html#implementation-and-considerations",
    "title": "Section 4.1 - Classification Notes",
    "section": "Implementation and Considerations",
    "text": "Implementation and Considerations\nWhen implementing logistic regression, the restrictions on the features are the same as for linear regression (e.g. no collinearity, number of features must be less than number of observations, etc.). The outcome should be a binary class membership. You can extend the logistic regression to cover a scenario with more than two classes, and this is called multinomial logistic regression, but we will not cover that in class.\nWhen you run logistic regression, the prediction output is a continuous value that reflects the predicted probability that the observation belongs to class 1. Therefore, a decision rule is required to convert the predicted probability to a predicted class (0 or 1). If you care equally about wrongly predicting positive for a True Negative (e.g. predicting class 1 for someone who is actually in class 0) and predicting negative for a True Positive, then a good decision rule is if \\(p(X)\\geq 0.5\\), predict 1 and otherwise predict 0. However, sometimes you care more about an error in one direction than the other. An example of this would be not wanting to offer a loan to someone who will default even if that means you deny more people who wouldn’t default. In that case, you might lower the threshold to 0.2 or some other value. We explore this more in the code."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#methods",
    "href": "api 222 files/section 4/section 4.html#methods",
    "title": "Section 4.1 - Classification Notes",
    "section": "Methods",
    "text": "Methods\nAt this point in the course, you have been introduced to three methods. The methods and their properties are summarized in the table below.\nWhen thinking about if a model is parametric or non-parametric, it can be helpful to think: Do I have a set of parameters that I can use to find the predicted value of any new observation? If yes, it’s parametric. When thinking about if a problem is a classification problem or a regression problem, it is helpful to think about the outcome in the training data. If the outcome is continuous, then it’s a regression problem. If the outcome is categorical, it’s a classification problem. The emphasis on the outcome in the training data is to avoid the confusion that arises when you look at prediction output. As we saw with logistic regression, even though it’s a classification problem, the output will be a probability (which is continuous and needs to be converted to 0 or 1 in order to measure performance)."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#classification",
    "href": "api 222 files/section 4/section 4.html#classification",
    "title": "Section 4.1 - Classification Notes",
    "section": "Classification",
    "text": "Classification\nClassification is really a two-step process. Usually, the model will predict something that looks like a probability that your observation belongs to each class. You then need to convert the probability to a class membership using a decision rule. A good general rule is: ``whichever class is assigned the highest probability is the predicted class.’’ However, when you have reason to prefer an error in one direction (e.g. predicting more people will default than actually will), you should change this threshold. Exactly which threshold is optimal will depend on domain knowledge and other factors (such as how costly defaults are or how profitable repaid loans are)."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#concept-1",
    "href": "api 222 files/section 4/section 4.html#concept-1",
    "title": "Section 4.1 - Classification Notes",
    "section": "Concept",
    "text": "Concept\nRecall that the Bayes’ Classifier is the unattainable gold standard classifier. It assumes knowledge of the true underlying distribution of the data, which you will not have in practice. Given features \\(X\\), it knows the true probability that \\(Y\\) belongs to each possible class. It predicts the most likely class, which is the best decision rule given the available features.\nLinear Discriminant Analysis (LDA) approximates the Bayes’ Classifier, given the information available and the assumption that features are Normally (Gaussian) distributed within each class. The result is decision boundaries that are linear in the included features.\nWhen there is one feature (predictor), LDA estimates class-specific means \\(\\hat{\\mu}_k\\) and a single variance \\(\\hat{\\sigma}^2\\) that is common to all classes. When there are multiple features, LDA estimates class-specific mean vectors \\(\\hat{\\mu}_k\\) and a single variance-covariance matrix \\(\\hat{\\Sigma}\\) that is assumed to be relevant to all classes. In both cases (one feature or many features), LDA also calculates the unconditional probability of belonging to each class \\(\\hat{\\pi}_k\\). LDA then takes these components (means, variance, and unconditional class probability) and calculates a discriminant function for each observation and each class. For each observation, the predicted class is determined by the largest discriminant.\nQuadratic Discriminant Analysis (QDA) is conceptually similar, though instead of requiring all classes to share the same variance or variance-covariance matrix, it allows for class-specific variances. This has the effect of allowing non-linear decision boundaries. The drawback, though, is that allowing for class-specific variances (and especially class-specific variance-covariance matrices) increases the number of parameters to estimate, increasing the likelihood of overfitting."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#method-1",
    "href": "api 222 files/section 4/section 4.html#method-1",
    "title": "Section 4.1 - Classification Notes",
    "section": "Method",
    "text": "Method\nTo estimate LDA or QDA, you estimate the feature means, feature variance(s), and unconditional (empirical) class probabilities for each class. Let \\(k\\) index the classes, then if there is only one feature, LDA calculates the following discriminant function for each observation for each class: \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x \\cdot \\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2}-\\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2}+\\log(\\hat{\\pi}_k)\n\\end{equation}\\] Where \\(\\hat{\\pi}_k\\) is the unconditional class probability, \\(\\hat{\\mu}_k\\) is the mean feature value for class \\(k\\), and \\(\\hat{\\sigma}^2\\) is the common feature variance. When \\(p&gt;1\\) (e.g. there are multiple predictors), then we use \\(\\hat{\\Sigma}\\) to represent the common feature variance-covariance matrix, \\(\\hat{\\mu}_k\\) becomes a vector, and thus the LDA discriminant function becomes: \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_k-\\frac{1}{2}\\hat{\\mu}_k^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_k+\\log(\\hat{\\pi}_k)\n\\end{equation}\\] For QDA, the one feature discriminant function is: \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x \\cdot \\frac{\\hat{\\mu}_k}{\\hat{\\sigma}_k^2}-\\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}_k^2}+\\log(\\hat{\\pi}_k)\n\\end{equation}\\] Note that \\(\\hat{\\sigma}\\) now has a subscript to indicate that the variance is class-specific. For multiple predictors, the QDA discriminant function is again just like the LDA one but with a subscripted \\(\\Sigma\\): \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x^T\\hat{\\Sigma}_k^{-1}\\hat{\\mu}_k-\\frac{1}{2}\\hat{\\mu}_k^T\\hat{\\Sigma}_k^{-1}\\hat{\\mu}_k+\\log(\\hat{\\pi}_k)\n\\end{equation}\\] In practice, we will use the lda() and qda() functions that are part of the MASS package in R."
  },
  {
    "objectID": "api 222 files/section 4/section 4.html#implementation-and-considerations-1",
    "href": "api 222 files/section 4/section 4.html#implementation-and-considerations-1",
    "title": "Section 4.1 - Classification Notes",
    "section": "Implementation and Considerations",
    "text": "Implementation and Considerations\nLDA and QDA both generalize easily to settings where there are more than two classes. They are also parametric, which means they are computationally efficient with large data sets compared to non-parametric KNN. However, they both make the assumption that the features are normally distributed, so you should pay attention to your data. For example, binary variables will never be normally distributed nor well approximated by a normal distribution, and so the methods are not appropriate to use in the presence of binary features.\nQDA differs from LDA by assuming the variance or variance-covariance matrix of the feature(s) varies from class to class. This allows for more flexible and non-linear decision boundaries, but requires estimation of more parameters. As with all other models we’ve seen, estimating more parameters increases the likelihood of overfitting and so should only be used when the number of observations is large relative to the number of features and classes."
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html",
    "href": "api 222 files/section 4/section 4.1.html",
    "title": "Section 4.1 - Classification Notes",
    "section": "",
    "text": "Note that the material in these notes draws on past TF’s notes (Ibou Dieye, Laura Morris, Emily Mower, Amy Wickett), and the more thorough treatment of these topics in Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani."
  },
  {
    "objectID": "api 222 files/section 4/section 4.2.html",
    "href": "api 222 files/section 4/section 4.2.html",
    "title": "Section 4.2 - Classification and Helpful Commands in R",
    "section": "",
    "text": "The goal of this session is to learn how to implement three classification models: 1. A logistic model 2. LDA 3. QDA"
  },
  {
    "objectID": "api 222 files/section 4/section 4.2.html#reminders",
    "href": "api 222 files/section 4/section 4.2.html#reminders",
    "title": "Section 4.2 - Classification and Helpful Commands in R",
    "section": "Reminders",
    "text": "Reminders\nTo run one line of code in RStudio, you can highlight the code you want to run and hit “Run” at the top of the script. Alternatively, on a mac, you can highlight the code to run and hit Command + Enter. Or on a PC, you can highlight the code to run and hit Ctrl + Enter. If you ever forget how a function works, you can type ? followed immediately (e.g. with no space) by the function name to get the help file\nLet’s start by loading the necessary packages and data. We will use the “Default” dataset available from the ISLR package. This dataset contains information on credit card defaults, including a binary response variable “default” and three predictors: “student”, “balance”, and “income”. We will use this dataset to build and evaluate classification models.\n\n## Load the packages\nlibrary(ISLR)\nlibrary(FNN)\n\nData preparation\nNow extract the data and name it “default_data”\n\ndefault_data &lt;- Default\n\nLet’s get to know our data\n\nsummary(default_data)\n\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n\n\nIt looks like we have two categorical variables. We can convert them both to numeric\n\ndefault_data$default &lt;- as.numeric(default_data$default == \"Yes\")\ndefault_data$student &lt;- as.numeric(default_data$student == \"Yes\")\n\nLet’s again split our data into test and training data sets with a 20/80 split. We use set.seed() to ensure replicability.\n\nset.seed(222)\n\nThen we can use the sample function to split the data (as before)\n\n## First pick the test observations (20% of the data)\ntest_obs &lt;- sample(seq(nrow(default_data)), \n                   round(0.2 * nrow(default_data)))\n\n## The training observations are the remaining observations\ntrain_obs &lt;- setdiff(seq(nrow(default_data)), test_obs)\n\n## Use the indices now to extract the corresponding subsets of the data\ntest_data &lt;- default_data[test_obs,]\ntrain_data &lt;- default_data[train_obs,]"
  },
  {
    "objectID": "api 222 files/section 4/section 4.2.html#logistic-regression",
    "href": "api 222 files/section 4/section 4.2.html#logistic-regression",
    "title": "Section 4.2 - Classification and Helpful Commands in R",
    "section": "Logistic regression",
    "text": "Logistic regression\nNow, let’s say we are interested in using a logistic regression. For a base case, let’s try to predict default from the other available variables using logistic regression. To run logistic regression in R, use glm(), which requires three arguments: - 1st: Your formula (y ~ x1 + x2) - 2nd: Family = binomial tells it to use logistic regression - 3rd: You data, including both x and y columns.\nWe will train the model on the training data, make predictions for the test data using predict(), and measure performance with Accuracy.\n\nlogistic_default &lt;- glm(default ~ student + balance + income,\n                        family = binomial,\n                        data = train_data)\n\nTo view information about the logistic regression, including coefficients, use summary()\n\nsummary(logistic_default)\n\n\nCall:\nglm(formula = default ~ student + balance + income, family = binomial, \n    data = train_data)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.108e+01  5.654e-01 -19.588   &lt;2e-16 ***\nstudent     -5.036e-01  2.638e-01  -1.909   0.0563 .  \nbalance      5.739e-03  2.619e-04  21.916   &lt;2e-16 ***\nincome       6.727e-06  9.312e-06   0.722   0.4701    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2320.3  on 7999  degrees of freedom\nResidual deviance: 1252.6  on 7996  degrees of freedom\nAIC: 1260.6\n\nNumber of Fisher Scoring iterations: 8\n\n\nTo predict outcomes on the test_data using logistic regression, use:\n\nlogistic_predict &lt;-predict(logistic_default,\n                           test_data,\n                           type = \"response\")\n\nLet’s look at what we got from this prediction model. We will use the head() function, which prints the first few values of the object inside the parentheses. If you want to change the number of observations that are printed to 100, you can use head(object, n = 100)\n\nhead(logistic_predict)\n\n        4350           18         6678         4788         9481         9994 \n5.133197e-04 2.176129e-04 2.515555e-03 3.317590e-02 2.508148e-05 4.935658e-03 \n\n\nWe see that the prediction outputs are probabilities, so in order to make predictions we have to decide on a decision rule. A common one is if the predicted probability is &gt; 0.5, predict 1 otherwise 0. Let’s see how we would do using this rule. Because it’s a classification problem, accuracy is a good measure (% correct)\n\nclass_predictions &lt;- as.numeric(logistic_predict &gt; 0.5)\nlogistic_accuracy &lt;- mean(class_predictions == test_data$default)\nprint(logistic_accuracy)\n\n[1] 0.9745\n\n\nThe accuracy looks great! But, we might care more about different types of errors than overall error rate. For example, we may not want to give loans to people who will default even if this means denying loans to some people who wouldn’t default. We can measure error rate by true default status. Note that for defaulters, default = 1. Here, I pull out all the predictions for the true defaulters and see what fraction of those equal 1.\n\ntrue_pos_accuracy &lt;- mean(class_predictions[which(test_data$default == 1)] == 1)\nprint(true_pos_accuracy)\n\n[1] 0.3333333\n\n\nLike-wise for the non-defaulters, I see what fraction of those equal 0. This gives class-specific accuracy rates.\n\ntrue_neg_accuracy &lt;- mean(class_predictions[which(test_data$default == 0)] == 0)\nprint(true_neg_accuracy)\n\n[1] 0.9974107\n\n\nThese values summarise what can also be seen in the following table. Where the columns correspond to the true values and the rows correspond to the predicted values.\n\ntable(class_predictions, test_data$default)\n\n                 \nclass_predictions    0    1\n                0 1926   46\n                1    5   23\n\n\nSuppose instead of the accuracy, you wanted to directly calculate the error rate. How would you do it? Hint, errors are ones where the prediction does not equal the true value. In R, we use != for “does not equal”\nWe can also calculate the error rate for the true defaulters and non-defaulters.\n\ntrue_pos_error &lt;- mean(class_predictions\n                       [which(test_data$default == 1)] != 1)\nprint(true_pos_error)\n\n[1] 0.6666667\n\ntrue_neg_error &lt;- mean(class_predictions\n                       [which(test_data$default == 0)] != 0)\nprint(true_neg_error)\n\n[1] 0.002589332\n\n\nWe see that we did a lot better on the true negatives than the true positives. Among all the people who will default, we only predicted about 1/3% of them would default. If we want to do a better job identifying these people, we can do this by lowering the default threshold from a predicted probability of 0.5 to something lower, say 0.2. Note, though, that lowering this threshold means increasing the number of default predictions for people who don’t default as well. Since we aren’t really sure how low we want to make this threshold, we can try for a bunch of threshold values and then see how the performance changes to pick the one that is best for our setting. This involves domain knowledge, such as the cost of default and the earnings on loans extended to people who repay, so there’s not one right answer, but we can more clearly see the tradeoffs by trying many values and plotting the error rates in each group as a function of the threshold.\nTo do this, we can use a loop to try a bunch of threshold values and then calculate the error rates for each threshold. We can then plot the error rates as a function of the threshold to see how the error rates change as we change the threshold.\n\n## First, we need to specify the list of threshold values to assess\nthreshold_values &lt;- seq(from = 0.00, to = 0.50, by = 0.01)\n\nThen we initialize a matrix of error rates. This matrix will have a number of rows corresponding to the length of the list of threshold values and 2 columns corresponding to the true positive and true negative accuracy for each value that we test\n\nerror_rates &lt;- matrix(0, nrow = length(threshold_values), ncol = 2)\n\nNow we can start the loop. We initialize a tracker for the row index, then for each threshold value in our specified list of values, we update the tracker to reflect the row, generate the predicted classes using the specific threshold, calculate the true positive accuracy, calculate the true negative accuracy, and add the results to our matrix.\n\nindx &lt;- 0\n\nfor(threshold in threshold_values) {\n  \n  ## Update the tracker to reflect the row\n  indx &lt;- indx + 1\n  \n  ## Then generate the predicted classes using the specific threshold\n  class_predictions &lt;- as.numeric(logistic_predict &gt; threshold)\n  \n  ## Then calculate the true positive accuracy\n  true_pos_accuracy &lt;- mean(class_predictions[which(test_data$default == 1)] == 1)\n  \n  ## Then calculate the true negative accuracy\n  true_neg_accuracy &lt;- mean(class_predictions[which(test_data$default == 0)] == 0)\n  \n  ## Now we can add the results to our matrix \n  error_rates[indx,] &lt;- c(true_pos_accuracy, true_neg_accuracy)\n}\n\nLet’s plot each of these as a function of the threshold\n\nmatplot(x = threshold_values,\n        y = error_rates, \n        type = \"l\",\n        col = 3:4,\n        xlab = \"Threshold\",\n        ylab = \"Accuracy\",\n        main = \"Accuracy as a Function of Threshold\")\n        legend(\"topright\", legend = c(\"Defaulters\", \"Non-defaulters\"), \n               col = 3:4, pch = 1)"
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html#concept",
    "href": "api 222 files/section 4/section 4.1.html#concept",
    "title": "Section 4.1 - Classification Notes",
    "section": "Concept",
    "text": "Concept\nLogistic regression is a parametric model that models the probability that \\(Y\\) belongs to a particular category. It is somewhat similar to linear regression, but the linear regression form of \\(\\beta_0+\\beta_1 X_1 + \\beta_2 X_2 + \\dots\\) undergoes a transformation that ensures the output will be bounded between 0 and 1 and can thus be interpreted as a probability.\n\\[\\begin{equation}\n     p(X) = \\frac{e^{\\beta_0+\\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\end{equation}\\]\nTherefore, while linear regression is best suited for regression problems, logistic regression is best suited for classification problems. Note that logistic regression produces a probability of class membership that then needs to be transformed to 0 or 1 using a decision rule, such as if \\(p(X)\\geq 0.5\\) then predict 1 otherwise predict 0."
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html#method",
    "href": "api 222 files/section 4/section 4.1.html#method",
    "title": "Section 4.1 - Classification Notes",
    "section": "Method",
    "text": "Method\nLogistic regression is estimated using Maximum Likelihood Estimation (MLE). MLE finds the values of \\(\\beta_0\\), \\(\\beta_1\\), etc. that maximize the probability of observing the observations in your training data given the values of the parameters \\(\\beta_0\\), \\(\\beta_1\\), etc. and the assumed functional form (e.g. see above)."
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html#implementation-and-considerations",
    "href": "api 222 files/section 4/section 4.1.html#implementation-and-considerations",
    "title": "Section 4.1 - Classification Notes",
    "section": "Implementation and Considerations",
    "text": "Implementation and Considerations\nWhen implementing logistic regression, the restrictions on the features are the same as for linear regression (e.g. no collinearity, number of features must be less than number of observations, etc.). The outcome should be a binary class membership. You can extend the logistic regression to cover a scenario with more than two classes, and this is called multinomial logistic regression, but we will not cover that in class.\nWhen you run logistic regression, the prediction output is a continuous value that reflects the predicted probability that the observation belongs to class 1. Therefore, a decision rule is required to convert the predicted probability to a predicted class (0 or 1). If you care equally about wrongly predicting positive for a True Negative (e.g. predicting class 1 for someone who is actually in class 0) and predicting negative for a True Positive, then a good decision rule is if \\(p(X)\\geq 0.5\\), predict 1 and otherwise predict 0. However, sometimes you care more about an error in one direction than the other. An example of this would be not wanting to offer a loan to someone who will default even if that means you deny more people who wouldn’t default. In that case, you might lower the threshold to 0.2 or some other value. We explore this more in the code."
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html#methods",
    "href": "api 222 files/section 4/section 4.1.html#methods",
    "title": "Section 4.1 - Classification Notes",
    "section": "Methods",
    "text": "Methods\nAt this point in the course, you have been introduced to three methods. The methods and their properties are summarized in the table below.\nWhen thinking about if a model is parametric or non-parametric, it can be helpful to think: Do I have a set of parameters that I can use to find the predicted value of any new observation? If yes, it’s parametric. When thinking about if a problem is a classification problem or a regression problem, it is helpful to think about the outcome in the training data. If the outcome is continuous, then it’s a regression problem. If the outcome is categorical, it’s a classification problem. The emphasis on the outcome in the training data is to avoid the confusion that arises when you look at prediction output. As we saw with logistic regression, even though it’s a classification problem, the output will be a probability (which is continuous and needs to be converted to 0 or 1 in order to measure performance)."
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html#classification",
    "href": "api 222 files/section 4/section 4.1.html#classification",
    "title": "Section 4.1 - Classification Notes",
    "section": "Classification",
    "text": "Classification\nClassification is really a two-step process. Usually, the model will predict something that looks like a probability that your observation belongs to each class. You then need to convert the probability to a class membership using a decision rule. A good general rule is: ``whichever class is assigned the highest probability is the predicted class.’’ However, when you have reason to prefer an error in one direction (e.g. predicting more people will default than actually will), you should change this threshold. Exactly which threshold is optimal will depend on domain knowledge and other factors (such as how costly defaults are or how profitable repaid loans are)."
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html#concept-1",
    "href": "api 222 files/section 4/section 4.1.html#concept-1",
    "title": "Section 4.1 - Classification Notes",
    "section": "Concept",
    "text": "Concept\nRecall that the Bayes’ Classifier is the unattainable gold standard classifier. It assumes knowledge of the true underlying distribution of the data, which you will not have in practice. Given features \\(X\\), it knows the true probability that \\(Y\\) belongs to each possible class. It predicts the most likely class, which is the best decision rule given the available features.\nLinear Discriminant Analysis (LDA) approximates the Bayes’ Classifier, given the information available and the assumption that features are Normally (Gaussian) distributed within each class. The result is decision boundaries that are linear in the included features.\nWhen there is one feature (predictor), LDA estimates class-specific means \\(\\hat{\\mu}_k\\) and a single variance \\(\\hat{\\sigma}^2\\) that is common to all classes. When there are multiple features, LDA estimates class-specific mean vectors \\(\\hat{\\mu}_k\\) and a single variance-covariance matrix \\(\\hat{\\Sigma}\\) that is assumed to be relevant to all classes. In both cases (one feature or many features), LDA also calculates the unconditional probability of belonging to each class \\(\\hat{\\pi}_k\\). LDA then takes these components (means, variance, and unconditional class probability) and calculates a discriminant function for each observation and each class. For each observation, the predicted class is determined by the largest discriminant.\nQuadratic Discriminant Analysis (QDA) is conceptually similar, though instead of requiring all classes to share the same variance or variance-covariance matrix, it allows for class-specific variances. This has the effect of allowing non-linear decision boundaries. The drawback, though, is that allowing for class-specific variances (and especially class-specific variance-covariance matrices) increases the number of parameters to estimate, increasing the likelihood of overfitting."
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html#method-1",
    "href": "api 222 files/section 4/section 4.1.html#method-1",
    "title": "Section 4.1 - Classification Notes",
    "section": "Method",
    "text": "Method\nTo estimate LDA or QDA, you estimate the feature means, feature variance(s), and unconditional (empirical) class probabilities for each class. Let \\(k\\) index the classes, then if there is only one feature, LDA calculates the following discriminant function for each observation for each class: \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x \\cdot \\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2}-\\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2}+\\log(\\hat{\\pi}_k)\n\\end{equation}\\] Where \\(\\hat{\\pi}_k\\) is the unconditional class probability, \\(\\hat{\\mu}_k\\) is the mean feature value for class \\(k\\), and \\(\\hat{\\sigma}^2\\) is the common feature variance. When \\(p&gt;1\\) (e.g. there are multiple predictors), then we use \\(\\hat{\\Sigma}\\) to represent the common feature variance-covariance matrix, \\(\\hat{\\mu}_k\\) becomes a vector, and thus the LDA discriminant function becomes: \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_k-\\frac{1}{2}\\hat{\\mu}_k^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_k+\\log(\\hat{\\pi}_k)\n\\end{equation}\\] For QDA, the one feature discriminant function is: \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x \\cdot \\frac{\\hat{\\mu}_k}{\\hat{\\sigma}_k^2}-\\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}_k^2}+\\log(\\hat{\\pi}_k)\n\\end{equation}\\] Note that \\(\\hat{\\sigma}\\) now has a subscript to indicate that the variance is class-specific. For multiple predictors, the QDA discriminant function is again just like the LDA one but with a subscripted \\(\\Sigma\\): \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x^T\\hat{\\Sigma}_k^{-1}\\hat{\\mu}_k-\\frac{1}{2}\\hat{\\mu}_k^T\\hat{\\Sigma}_k^{-1}\\hat{\\mu}_k+\\log(\\hat{\\pi}_k)\n\\end{equation}\\] In practice, we will use the lda() and qda() functions that are part of the MASS package in R."
  },
  {
    "objectID": "api 222 files/section 4/section 4.1.html#implementation-and-considerations-1",
    "href": "api 222 files/section 4/section 4.1.html#implementation-and-considerations-1",
    "title": "Section 4.1 - Classification Notes",
    "section": "Implementation and Considerations",
    "text": "Implementation and Considerations\nLDA and QDA both generalize easily to settings where there are more than two classes. They are also parametric, which means they are computationally efficient with large data sets compared to non-parametric KNN. However, they both make the assumption that the features are normally distributed, so you should pay attention to your data. For example, binary variables will never be normally distributed nor well approximated by a normal distribution, and so the methods are not appropriate to use in the presence of binary features.\nQDA differs from LDA by assuming the variance or variance-covariance matrix of the feature(s) varies from class to class. This allows for more flexible and non-linear decision boundaries, but requires estimation of more parameters. As with all other models we’ve seen, estimating more parameters increases the likelihood of overfitting and so should only be used when the number of observations is large relative to the number of features and classes."
  },
  {
    "objectID": "posts/2024-02-15-LDA/2024-02-15-LDA.html",
    "href": "posts/2024-02-15-LDA/2024-02-15-LDA.html",
    "title": "What is Linear Discriminant Analysis (LDA)?",
    "section": "",
    "text": "Linear Discriminant Analysis (LDA) is a classic method in statistics and machine learning for classification and dimensionality reduction. LDA is particularly known for its simplicity, efficiency, and interpretability. This post aims to demystify LDA by exploring its mathematical foundations and demonstrating its application through a simulated example in R."
  },
  {
    "objectID": "posts/2024-02-15-LDA/2024-02-15-LDA.html#the-essence-of-lda",
    "href": "posts/2024-02-15-LDA/2024-02-15-LDA.html#the-essence-of-lda",
    "title": "What is Linear Discriminant Analysis (LDA)?",
    "section": "The Essence of LDA",
    "text": "The Essence of LDA\nImagine you’re at a party and there are two types of fruit on the table: apples and oranges. You’re blindfolded and asked to classify the fruits using only a scale (to weigh them) and a ruler (to measure their diameter). Intuitively, you might notice that, generally, oranges are slightly heavier and larger in diameter than apples. LDA does something similar with data; it tries to find the “scale” and “ruler” (metaphorically speaking) that best separate different classes, like apples and oranges, based on their features."
  },
  {
    "objectID": "posts/2024-02-15-LDA/2024-02-15-LDA.html#the-math-behind-the-magic",
    "href": "posts/2024-02-15-LDA/2024-02-15-LDA.html#the-math-behind-the-magic",
    "title": "What is Linear Discriminant Analysis (LDA)?",
    "section": "The Math Behind the Magic",
    "text": "The Math Behind the Magic\n\nBayes’ Theorem at a Glance\nLDA is based on Bayes’ theorem, a fundamental concept in probability theory. Bayes’ theorem tells us how to update our beliefs (probabilities) about an event happening (like identifying an orange) given some evidence (the fruit’s weight and diameter). It’s about revising our assumptions with new data.\n\n\nAssumptions of LDA\nLDA makes a couple of key assumptions:\n\nNormal Distribution: It assumes that the data points for each class (like our apples and oranges) are normally distributed. This means if you plot the features (weight and diameter), they’ll form a bell curve, with most apples (or oranges) near the average, and fewer as you move away from the center.\nEqual Variance: LDA assumes that these bell curves for each class have the same shape, though they might be centered at different points. This is like saying, while apples and oranges might differ in average size and weight, the variation around their averages is similar.\n\n\n\nThe LDA Decision Rule\nLDA looks for a line (or in more complex cases, a plane or hyperplane) that best separates our classes (apples from oranges) based on their features. It calculates the means (averages) and variances (spread) for each class and then finds the line where the distance between the means is maximized relative to the variance.\nMathematically, this involves calculating a score (the discriminant score) for each data point that measures how far it is from each class’s mean, adjusted for the overall variance. Data points are then classified based on which score is higher, indicating which class they’re closer to."
  },
  {
    "objectID": "posts/2024-02-15-LDA/2024-02-15-LDA.html#putting-lda-into-practice-simulating-data-in-r",
    "href": "posts/2024-02-15-LDA/2024-02-15-LDA.html#putting-lda-into-practice-simulating-data-in-r",
    "title": "What is Linear Discriminant Analysis (LDA)?",
    "section": "Putting LDA into Practice: Simulating Data in R",
    "text": "Putting LDA into Practice: Simulating Data in R\nNow, let’s see how this works in practice with our R example. We simulate two classes with distinct means but shared variances, plotting them to visualize the data. We will then fit an LDA model to the data and plot the decision boundary, which shows how LDA separates the two classes based on their features.\n\n# Simulate the data\n\nlibrary(MASS) # For lda() and mvrnorm()\nset.seed(100) # Ensure reproducibility\n\nmu1 &lt;- c(-1, 0) # Mean for class 1\nmu2 &lt;- c(1, 0)  # Mean for class 2\nSigma &lt;- matrix(c(2, 1, 1, 2), ncol = 2) # Same covariance matrix for both classes\n\n# Generate data\nclass1 &lt;- mvrnorm(n = 50, mu = mu1, Sigma = Sigma)\nclass2 &lt;- mvrnorm(n = 50, mu = mu2, Sigma = Sigma)\n\ndata &lt;- rbind(class1, class2)\nlabels &lt;- factor(c(rep(0, 50), rep(1, 50)))\ndata &lt;- data.frame(data, class = labels)\n\nWith the data simulated, we can plot it to visualize the distribution of the two classes:\n\nggplot(data, aes(x = X1, y = X2, color = class)) +\n  geom_point() + theme_minimal() +\n  labs(title = \"Plot of the Data\",\n       x = \"Feature 1\", y = \"Feature 2\") +\n  scale_color_manual(values = c('#0e4a6b', '#ed7b2e')) +\n  guides(color = guide_legend(title = \"Class\", override.aes = list(size = 3))) \n\n\n\n\nHaving visualized our data, we proceed to apply LDA to classify these points and find the decision boundary that best separates them:\n\n# Fit LDA model\nlda_fit &lt;- lda(class ~ ., data = data)\n\nWe can now visualize the decision boundary that LDA has learned from the data:\n\n# plot the decision boundary\nx &lt;- seq(-5, 5, length.out = 1000)\ny &lt;- seq(-5, 5, length.out = 1000)\ngrid &lt;- expand.grid(X1 = x, X2 = y)\ngrid$class &lt;- predict(lda_fit, newdata = grid)$class\n\nmeans &lt;- as.data.frame(lda_fit$means)\nmeans$class &lt;- factor(0:1)\nmeans$size &lt;- 5\n\nggplot(data, aes(x = X1, y = X2, color = class)) +\n  geom_point() + theme_minimal() +\n  geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = \"black\") +\n  geom_point(data = means, aes(x = X1, y = X2), color = 'black', size = 8, shape=18) +\n  geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +\n  labs(title = \"LDA Decision Boundary\",\n       subtitle = \"Diamonds represent Class Means\",\n       x = \"Feature 1\", y = \"Feature 2\") +\n  scale_color_manual(values = c('#0e4a6b', '#ed7b2e')) +\n  guides(color = guide_legend(title = \"Class\", override.aes = list(size = 3))) \n\n\n\n\n\nWhat Does This Mean?\nWhen we plot the decision boundary found by LDA, we’re seeing the line that best distinguishes between our classes based on the data. Points on one side of the line are more likely to be Class 0, and on the other, Class 1. The class means (marked as diamonds on our plot) help us visualize the centers around which our data clusters, and the decision boundary shows how LDA uses these centers to classify the data."
  },
  {
    "objectID": "posts/2024-02-15-LDA/2024-02-15-LDA.html#conclusion",
    "href": "posts/2024-02-15-LDA/2024-02-15-LDA.html#conclusion",
    "title": "What is Linear Discriminant Analysis (LDA)?",
    "section": "Conclusion",
    "text": "Conclusion\nLDA is a powerful and intuitive method for classification and dimensionality reduction. It’s a great starting point for understanding more complex methods. By demystifying the math behind LDA and demonstrating its application in R, I hope this post has made LDA more accessible and understandable.\nThanks for reading!"
  }
]