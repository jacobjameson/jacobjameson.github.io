{
  "hash": "bf5638f2cd7314a2d6e8d2c854b0f79c",
  "result": {
    "markdown": "---\ntitle: \"Beyond DiD: Autoregressive Models for Policy Evaluation When Parallel Trends Fail\"\ndescription: \"This post demonstrates how autoregressive models can overcome the limitations of difference-in-differences analysis when evaluating health policies with non-parallel pre-treatment trends, providing researchers with practical tools to improve causal inference in observational studies.\"\nauthor: \"Jacob Jameson\"\ndate: \"2025-04-29\"\ncategories: [causal inference, policy evaluation, methodology]\noutput:\n  tufte::tufte_html: default\n  tufte::tufte_handout:\n    citation_package: natbib\n    latex_engine: xelatex\n  tufte::tufte_book:\n    citation_package: natbib\n    latex_engine: xelatex\nlink-citations: true\nreference-location: margin\ncomments:\n  utterances:\n    repo: jacobjameson/jacobjameson.github.io\nformat:\n  html:\n    toc: true\n    toc-location: left\npage-layout: full\n---\n\n\n\n## Introduction\n\nThe difference-in-differences (DiD) framework has become a cornerstone of policy evaluation in health services research and beyond. When a policy change affects some regions or groups but not others, DiD offers a compelling approach to estimate causal effects. However, the validity of DiD hinges critically on the parallel trends assumption: that treatment and control groups would have followed parallel trajectories in the absence of intervention.\n\nWhat happens when this assumption fails? In this post, we explore autoregressive models as a robust alternative for policy evaluation when parallel trends are violated. We'll walk through:\n\n1. Why parallel trends matter for DiD\n2. When and how they fail in real-world applications\n3. How autoregressive models can help\n4. A simulation comparing both approaches\n5. Practical implementation in R\n\n## The Problem with Parallel Trends\n\nThe parallel trends assumption is the lynchpin of DiD methodology. It states that in the absence of treatment, the difference between treatment and control groups would have remained constant over time. This allows us to attribute any deviation from this pattern to the causal effect of the intervention.\n\nHowever, in health policy research, this assumption often fails for several reasons:\n\n- Health outcomes frequently have complex, non-linear trajectories\n- Treatment assignment may be related to pre-existing trends (e.g., policies targeting areas with worsening health metrics)\n- Anticipation effects may cause behavior changes before policy implementation\n- Differential shocks to treatment and control groups before the policy\n\nWhen parallel trends are violated, traditional DiD estimates become biased, potentially leading to incorrect policy conclusions.\n\n## Autoregressive Models as an Alternative\n\nAutoregressive models offer a compelling alternative by explicitly modeling the relationship between past and current outcomes. Rather than assuming parallel trends, they directly account for the dynamic evolution of the outcome variable over time.\n\nThe basic form of an autoregressive model for policy evaluation can be expressed as:\n\n$$Y_{it} = \\alpha + \\rho Y_{i,t-1} + \\beta D_{it} + \\gamma X_{it} + \\delta_i + \\lambda_t + \\epsilon_{it}$$\n\nWhere:\n- $Y_{it}$ is the outcome for unit $i$ at time $t$\n- $Y_{i,t-1}$ is the lagged outcome (the autoregressive component)\n- $D_{it}$ is the treatment indicator\n- $X_{it}$ represents time-varying controls\n- $\\delta_i$ are unit fixed effects\n- $\\lambda_t$ are time fixed effects\n- $\\epsilon_{it}$ is the error term\n\nThe coefficient $\\beta$ represents the treatment effect.\n\nBy including the lagged dependent variable, these models can accommodate different pre-treatment trajectories, making them particularly valuable when parallel trends are suspect.\n\n## Simulation: DiD vs. Autoregressive Models\n\nLet's compare these approaches through simulation. We'll generate data where the parallel trends assumption is violated and compare the performance of DiD and autoregressive models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(lfe)       # For fixed effects models\nlibrary(ggplot2)\nlibrary(kableExtra)\nlibrary(fixest)  \n\n# Set seed for reproducibility\nset.seed(123)\n\n# Simulation parameters\nn_states <- 50        # 25 treatment, 25 control\nn_periods <- 20       # 10 pre-treatment, 10 post-treatment\ntreatment_period <- 11  # Treatment starts at period 11\ntrue_effect <- 2      # True policy effect\n\n# Generate panel data\nsimulate_data <- function() {\n  # Create empty dataframe\n  df <- expand.grid(\n    state = 1:n_states,\n    time = 1:n_periods\n  )\n  \n  # Assign treatment (first half of states)\n  df$treated <- ifelse(df$state <= n_states/2, 1, 0)\n  \n  # Generate treatment indicator (post-treatment for treated states)\n  df$treatment <- ifelse(df$treated == 1 & df$time >= treatment_period, 1, 0)\n  \n  # Different trends for treatment and control groups\n  df$state_trend <- ifelse(df$treated == 1, 0.5, 0.2)  # Non-parallel trends\n  \n  # Generate outcomes\n  # First, create a starting value for each state\n  state_initial <- tibble(\n    state = 1:n_states,\n    initial_value = 10 + rnorm(n_states, 0, 2)\n  )\n  \n  df <- left_join(df, state_initial, by = \"state\")\n  \n  # Generate outcomes with AR(1) process and non-parallel trends\n  # This requires multiple passes\n  df$outcome <- df$initial_value  # Initialize\n  \n  for (t in 2:n_periods) {\n    # Get previous period data\n    prev_data <- df %>% \n      filter(time == t-1) %>% \n      select(state, prev_outcome = outcome)\n    \n    # Update current period\n    df <- df %>%\n      left_join(prev_data, by = \"state\") %>%\n      mutate(\n        outcome = ifelse(\n          time == t,\n          0.7 * prev_outcome + state_trend * time + treatment * true_effect + rnorm(n(), 0, 1),\n          outcome\n        )\n      ) %>%\n      select(-prev_outcome)\n  }\n  \n  return(df)\n}\n\n# Generate data\npolicy_data <- simulate_data()\n```\n:::\n\n\n\nNow let's visualize the data to confirm we have non-parallel trends:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate group means by time\ngroup_means <- policy_data %>%\n  group_by(time, treated) %>%\n  summarize(mean_outcome = mean(outcome), .groups = \"drop\") %>%\n  mutate(group = ifelse(treated == 1, \"Treatment Group\", \"Control Group\"))\n\n# Plot trends with intervention line\nggplot(group_means, aes(x = time, y = mean_outcome, color = group, group = group)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = treatment_period, linetype = \"dashed\", color = \"red\") +\n  annotate(\"text\", x = treatment_period + 0.5, y = max(group_means$mean_outcome), \n           label = \"Policy Implementation\", hjust = 0, color = \"red\") +\n  labs(\n    title = \"Outcome Trends by Group\",\n    subtitle = \"Note the non-parallel pre-treatment trends\",\n    x = \"Time Period\",\n    y = \"Outcome\",\n    color = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    text = element_text(size = 12),\n    plot.title = element_text(face = \"bold\")\n  )\n```\n\n::: {.cell-output-display}\n![](2025-04-29-AR_files/figure-html/unnamed-chunk-2-1.png){width=768}\n:::\n:::\n\n\n\n\nAs we can see, the treatment and control groups follow different trajectories even before the policy implementation (vertical dashed line), violating the parallel trends assumption.\n\nNow let's estimate the treatment effect using both methods:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Standard DiD model\ndid_model <- felm(outcome ~ treated + time + treatment | 0 | 0 | state, data = policy_data)\n\n# 2. Autoregressive model\n# First, create lagged outcome\npolicy_data_lagged <- policy_data %>%\n  arrange(state, time) %>%\n  group_by(state) %>%\n  mutate(lag_outcome = lag(outcome)) %>%\n  ungroup() %>%\n  filter(!is.na(lag_outcome))  # Drop first period for each state\n\n# Run autoregressive model\nar_model <- felm(outcome ~ lag_outcome + treated + time + treatment | 0 | 0 | state, \n                data = policy_data_lagged)\n\n# Compare results\nresults_df <- tibble(\n  Model = c(\"True Effect\", \"DiD Estimate\", \"Autoregressive\"),\n  Estimate = c(true_effect, coef(did_model)[\"treatment\"], coef(ar_model)[\"treatment\"]),\n  Bias = c(0, coef(did_model)[\"treatment\"] - true_effect, \n           coef(ar_model)[\"treatment\"] - true_effect)\n)\n\n# Format table\nresults_df %>%\n  mutate(\n    Estimate = round(Estimate, 3),\n    Bias = round(Bias, 3),\n    `Percent Bias` = ifelse(Model == \"True Effect\", \"-\", \n                           paste0(round(Bias / true_effect * 100, 1), \"%\"))\n  ) %>%\n  kable(align = \"lrrr\", caption = \"Comparison of Model Estimates\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>Comparison of Model Estimates</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> Estimate </th>\n   <th style=\"text-align:right;\"> Bias </th>\n   <th style=\"text-align:right;\"> Percent Bias </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> True Effect </td>\n   <td style=\"text-align:right;\"> 2.000 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n   <td style=\"text-align:right;\"> - </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> DiD Estimate </td>\n   <td style=\"text-align:right;\"> 13.371 </td>\n   <td style=\"text-align:right;\"> 11.371 </td>\n   <td style=\"text-align:right;\"> 568.6% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Autoregressive </td>\n   <td style=\"text-align:right;\"> 3.070 </td>\n   <td style=\"text-align:right;\"> 1.070 </td>\n   <td style=\"text-align:right;\"> 53.5% </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n\n## Why Autoregressive Models Work\n\nThe results demonstrate why autoregressive models often outperform DiD when parallel trends are violated. Here are the key advantages:\n\n1. **Dynamic relationships**: By including the lagged dependent variable, autoregressive models directly account for the relationship between past and current outcomes, capturing pre-existing dynamics.\n\n2. **Different pre-treatment trajectories**: Instead of assuming parallel trends, autoregressive models can accommodate different trajectories between treatment and control groups.\n\n3. **Controlling for omitted variables**: The lagged outcome can serve as a proxy for unobserved time-varying confounders that affect the trajectory of the outcome.\n\n4. **Anticipation effects**: These models can better handle situations where units change behavior in anticipation of policy changes.\n\n## Model Assumptions and Limitations\n\nWhile autoregressive models address the parallel trends issue, they come with their own assumptions:\n\n1. **No contemporaneous reverse causality**: The policy implementation cannot be simultaneously determined by current outcomes.\n\n2. **Dynamic completeness**: The model must include sufficient lags to capture the full dynamics of the process.\n\n3. **Correct functional form**: The relationship between past and current values must be correctly specified.\n\n4. **Exogeneity of treatment**: The policy implementation should be exogenous after controlling for lagged outcomes.\n\n5. **Nickell bias**: In short panels with unit fixed effects, coefficients on the lagged dependent variable can be biased. This may require specialized estimators like Arellano-Bond for correction.\n\n## Application to Health Policy Research\n\nThe flexibility of autoregressive models makes them particularly valuable for health policy research, where outcomes often have complex dynamics:\n\n- **Hospital readmission policies**: When evaluating policies to reduce readmissions, hospitals often have different pre-existing trends\n- **Medicaid expansion**: States that expanded Medicaid may have had different health trajectories before expansion\n- **Prescription drug monitoring programs**: States implementing these programs often do so in response to worsening opioid trends\n\n## Practical Implementation\n\nTo implement autoregressive models for policy evaluation in R:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Basic autoregressive model\nar_basic <- lm(outcome ~ lag_outcome + treatment, data = policy_data_lagged)\n\n# With fixed effects and time trends\nar_fe <- felm(outcome ~ lag_outcome + treatment | state + time | 0 | state, \n             data = policy_data_lagged)\n\n# With clustered standard errors\nar_cluster <- felm(outcome ~ lag_outcome + treatment | state + time | 0 | state, \n                  data = policy_data_lagged)\n\n# Custom function to create leads and lags\ncreate_leads_lags <- function(data, id_var = \"state\", time_var = \"time\", \n                             treatment_var = \"treatment\", leads = 3, lags = 5) {\n  # Ensure data is arranged correctly\n  data <- data %>% arrange(.data[[id_var]], .data[[time_var]])\n  \n  # Create lag variables\n  for (i in 1:lags) {\n    lag_name <- paste0(\"lag\", i, \"_\", treatment_var)\n    data <- data %>%\n      group_by(.data[[id_var]]) %>%\n      mutate(!!lag_name := lag(.data[[treatment_var]], n = i)) %>%\n      ungroup()\n  }\n  \n  # Create lead variables\n  for (i in 1:leads) {\n    lead_name <- paste0(\"lead\", i, \"_\", treatment_var)\n    data <- data %>%\n      group_by(.data[[id_var]]) %>%\n      mutate(!!lead_name := lead(.data[[treatment_var]], n = i)) %>%\n      ungroup()\n  }\n  \n  # Create lagged outcome variable for autoregressive component\n  data <- data %>%\n    group_by(.data[[id_var]]) %>%\n    mutate(lag_outcome = lag(.data[[\"outcome\"]], n = 1)) %>%\n    ungroup()\n  \n  return(data)\n}\n```\n:::\n\n\n## Testing Model Suitability\n\nBefore choosing an autoregressive approach, it's important to assess whether parallel trends are indeed violated:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# When evaluating policy interventions with dynamic effects, we need to look at both\n# pre-treatment effects (to check for anticipation) and post-treatment effects\n# (to assess how impacts evolve over time).\n\n# This creates both the leads/lags AND the required lag_outcome variable\npolicy_data_dynamic <- create_leads_lags(policy_data_lagged, \n                                      id_var = \"state\", \n                                      time_var = \"time\", \n                                      treatment_var = \"treatment\", \n                                      leads = 3, \n                                      lags = 3)\n\n# Keep only complete cases for this analysis\npolicy_data_dynamic <- policy_data_dynamic %>%\n  filter(!is.na(lag_outcome))\n\n# For dynamic effects (leads and lags of treatment)\nar_dynamic <- tryCatch({\n  felm(outcome ~ lag_outcome + lag1_treatment + lag2_treatment + lag3_treatment + \n      treatment + lead1_treatment + lead2_treatment + lead3_treatment | \n      state + time | 0 | state, data = policy_data_dynamic)\n}, error = function(e) {\n  message(\"Error in dynamic model: \", e$message)\n  # Simpler fallback model\n  felm(outcome ~ lag_outcome + treatment | state + time | 0 | state, \n      data = policy_data_dynamic)\n})\n\n# Print summary\nsummary(ar_dynamic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\n   felm(formula = outcome ~ lag_outcome + lag1_treatment + lag2_treatment +      lag3_treatment + treatment + lead1_treatment + lead2_treatment +      lead3_treatment | state + time | 0 | state, data = policy_data_dynamic) \n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.93016 -0.65451  0.01728  0.62855  3.12924 \n\nCoefficients:\n                Estimate Cluster s.e. t value Pr(>|t|)    \nlag_outcome      0.58767      0.03071  19.136  < 2e-16 ***\nlag1_treatment   0.62796      0.34698   1.810 0.076466 .  \nlag2_treatment   0.34763      0.44517   0.781 0.438612    \nlag3_treatment   1.37402      0.32756   4.195 0.000114 ***\ntreatment        1.94893      0.37674   5.173 4.26e-06 ***\nlead1_treatment  1.18728      0.41469   2.863 0.006158 ** \nlead2_treatment  0.39214      0.35710   1.098 0.277510    \nlead3_treatment  0.56319      0.37265   1.511 0.137134    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.015 on 580 degrees of freedom\n  (250 observations deleted due to missingness)\nMultiple R-squared(full model): 0.9867   Adjusted R-squared: 0.9851 \nMultiple R-squared(proj model): 0.9152   Adjusted R-squared: 0.9051 \nF-statistic(full model, *iid*):624.1 on 69 and 580 DF, p-value: < 2.2e-16 \nF-statistic(proj model): 625.9 on 8 and 49 DF, p-value: < 2.2e-16 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary packages\nlibrary(fixest)  # For event study plots\n\n# 1. Visual inspection with leads and lags plot\n# Create relative time variable (time to treatment)\npolicy_data <- policy_data %>%\n  mutate(rel_time = ifelse(treated == 1, time - treatment_period, NA))\n\n# Use fixest for event study\nevent_study <- feols(outcome ~ i(rel_time, ref = -1) + i(time) | state, \n                    data = policy_data %>% filter(treated == 1, !is.na(rel_time)))\n\n# Plot the event study coefficients\niplot(event_study, main = \"Event Study: Effect Relative to Treatment Time\",\n     xlab = \"Time Relative to Treatment\", ylab = \"Estimated Effect\")\n```\n\n::: {.cell-output-display}\n![](2025-04-29-AR_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot coefficients for visual inspection of pre-trends\n\n# 2. Statistical test for pre-trends\npre_data <- policy_data %>% filter(time < treatment_period)\nsummary(felm(outcome ~ treated * time | state | 0 | state, data = pre_data))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\n   felm(formula = outcome ~ treated * time | state | 0 | state,      data = pre_data) \n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7575 -1.3608 -0.2419  1.1661  6.3767 \n\nCoefficients:\n             Estimate Cluster s.e. t value Pr(>|t|)    \ntreated           NaN      0.00000     NaN      NaN    \ntime         -0.43327      0.05084  -8.523 3.06e-11 ***\ntreated:time  0.95150      0.06717  14.167  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.91 on 448 degrees of freedom\nMultiple R-squared(full model): 0.6632   Adjusted R-squared: 0.6249 \nMultiple R-squared(proj model): 0.3654   Adjusted R-squared: 0.2932 \nF-statistic(full model, *iid*): 17.3 on 51 and 448 DF, p-value: < 2.2e-16 \nF-statistic(proj model): 70.67 on 3 and 49 DF, p-value: < 2.2e-16 \n```\n:::\n\n```{.r .cell-code}\n# If interaction is significant, parallel trends is likely violated\n```\n:::\n\n\n\n## Conclusion\n\nWhen evaluating policy interventions, the reliability of our estimates hinges on the validity of model assumptions. While DiD remains a powerful tool when its assumptions hold, autoregressive models offer a valuable alternative when parallel trends are violated.\n\nBy explicitly modeling the dynamic relationship between past and current outcomes, these models can deliver more accurate causal estimates in complex policy environments. Rather than viewing them as competitors, researchers should understand the strengths and limitations of both approaches, selecting the appropriate method based on the specific context and data characteristics.\n\nIn health policy research, where outcomes often follow complex trajectories and policy implementations are frequently endogenous to pre-existing trends, the autoregressive approach offers a flexible framework that can accommodate these realities while still enabling credible causal inference.\n",
    "supporting": [
      "2025-04-29-AR_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}