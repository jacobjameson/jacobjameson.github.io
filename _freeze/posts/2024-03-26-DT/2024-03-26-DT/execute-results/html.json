{
  "hash": "0fc77dc16685e265142c60135adb3856",
  "result": {
    "markdown": "---\ntitle: \"DIY Decision Trees\"\ndescription: \"Embarking on a DIY Journey with Machine Learning Algorithms.\"\nauthor: \"Jacob Jameson\"\ndate: \"2024-03-26\"\ncategories: [decison trees, machine learning]\noutput:\n  tufte::tufte_html: default\n  tufte::tufte_handout:\n    citation_package: natbib\n    latex_engine: xelatex\n  tufte::tufte_book:\n    citation_package: natbib\n    latex_engine: xelatex\nlink-citations: true\nreference-location: margin\ncomments:\n  utterances:\n    repo: jacobjameson/jacobjameson.github.io\nformat:\n  html:\n    toc: true\n    toc-location: left\npage-layout: full\n---\n\n\n\n\n# Decision Trees\n\nDecision trees are a fundamental machine learning algorithm that's easy to understand and interpret. They're a popular choice for classification and regression tasks, offering a visual representation of the decision-making process. In this series, we'll explore the inner workings of decision trees, starting with the basics and gradually building up to more advanced concepts.\n\n## Embarking on a DIY Journey with Machine Learning Algorithms\n\nWhy go through the trouble of building a machine learning algorithm by hand, especially when libraries like `sklearn` in Python or `rpart` in R are just a function call away? The answer lies in the deeper understanding and insights that come from peeling back the layers of these algorithms ourselves.\n\n\n## Practical Benefits of a Hands-On Approach\n\nBeyond the theoretical, there's immense practical value in understanding the inner workings of these models. It allows for more informed decisions about feature selection—knowing which to discard because they don't fit the model's assumptions and which to transform for better representation. This knowledge often translates to improved model performance on new, unseen data.\n\nAnd let's not overlook the sheer joy of the process. Diving deep into algorithms offers a unique way to tackle problem-solving, enhancing our execution and grasp of machine learning concepts.\n\n## Metrics: The Compass for Improvement\n\nHow do we navigate through the countless ways to split our data? We turn to optimization metrics, the compass guiding each iteration towards better performance. For decision trees, two key metrics stand out: Gini Impurity and Entropy.\n\n#### Gini Impurity\n\nGini Impurity is an insightful metric focused on misclassification. It hinges on the probability of classifying a data point correctly within our dataset. For any given class $c$ with frequency $f_c$, we predict that class for a data point $f_c$ proportion of the time. The error for each class is the chance of selecting the class multiplied by the likelihood of being incorrect:\n\n$$Error_c = f_c \\times (1 - f_c)$$\n\nThe Gini Impurity is the aggregate of these individual errors across all classes:\n\n$$Gini\\ Impurity = \\sum_c f_c \\times (1 - f_c) = \\sum_c f_c - \\sum_c f_c^2 = 1 - \\sum_c f_c^2$$\n\nImproving the Gini Impurity, specifically by minimizing it across data regions, directly leads to a reduction in misclassification.\n\n#### Gini Impurity Illustrated with a Coin Toss\n\nConsider the Gini Impurity of a coin flip, where the probability of heads is $p_H$:\n\n$$Gini\\ Impurity = 1 - p_H^2 - (1 - p_H)^2 = 2 \\times p_H \\times (1 - p_H)$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- seq(0.01, 0.99, 0.01)\nplot(p, 1-p^2-(1-p)^2, type='l', ylim=c(0,1), \n     ylab = \"Gini Impurity\", xlab=\"Probability of Heads\", \n     col=\"blue\")\n```\n\n::: {.cell-output-display}\n![](2024-03-26-DT_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\nWhen $p_H$ is at 0 or 1, there's no chance of error—our predictions are certain. Yet at $p_H = 0.5$, Gini Impurity peaks, reflecting the highest error rate and aligning with our intuitive understanding.\n\n#### Entropy\n\nEntropy, on the other hand, takes a different tack. It measures the expected information content—or surprise—of a class in a dataset. If $f_c$ is the proportion of a class, and $I(c)$ represents the information content of the class, then entropy is defined as:\n\n$$Entropy = \\sum_c f_c \\times I(c)$$\n\nWithout delving too deep into the underlying theory, the formula for entropy, given that $I(c) = -\\log_2(f_c)$, simplifies to:\n\n$$Entropy = -\\sum_c f_c \\times \\log_2(f_c)$$\n\nThough the logarithm base could be arbitrary, we align it with the binary nature of the decision trees we're examining in this series.\n\n#### Entropy in the Context of a Coin Toss\n\nExtending our analogy to the coin toss, let's examine how entropy changes with varying probabilities of heads, $p_H$:\n\n$$Entropy = -p_H \\log_2(p_H) - (1 - p_H) \\log_2(1 - p_H)$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- seq(0.01, 0.99, 0.01)\nplot(p, -p*log2(p)-(1-p)*log2(1-p), type='l', ylim=c(0,1), \n     ylab = \"Entropy\", xlab=\"Probability of Heads\", col=\"blue\")\n```\n\n::: {.cell-output-display}\n![](2024-03-26-DT_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nEntropy achieves its maximum at $p_H = 0.5$, where uncertainty—and the potential for error—is greatest. This resonates with the principle that the greatest entropy occurs where prediction is most challenging.\n\n## Refining Our Approach with Information Gain\n\nWith our metrics in place to judge how well our dataset's predictions are doing, it's time to focus on improvement—how can we make our predictions even better? We're going to talk about something called 'Information Gain', a concept which is the difference we want to maximize when deciding how to split our data.\n\n### Fine-tuning a Decision Tree for Classification\n\nWhen we build a classifier—a kind of decision tree—our goal is to organize our data into groups that make our predictions as accurate as possible. The concept of 'Information Gain' is a bit like a treasure map; it helps us find these valuable groupings by measuring how much more ordered, or less chaotic, our data becomes after a split.\n\n$$Information\\ Gain = Gini\\ Impurity_{before\\ split} - \\sum_{each\\ split} \\frac{|split\\ group|}{|total\\ data|} Gini\\ Impurity_{split\\ group}$$\n\n$$Information\\ Gain = Entropy_{before\\ split} - \\sum_{each\\ split} \\frac{|split\\ group|}{|total\\ data|} Entropy_{split\\ group}$$\n\nIn these equations, $Gini\\ Impurity_{before\\ split}$ and $Entropy_{before\\ split}$ represent the disorder in our data before any splits. The sum of the Gini Impurity or Entropy of each split group is weighted by the proportion of data in that group. The Information Gain is the difference between the disorder before the split and the weighted sum of disorder after the split.\n\n\nIn essence, it tells us how much clearer our predictions become after we've sorted the data into separate buckets based on certain features.\n\n### Balancing Decision Trees to Avoid Overfitting\n\nA common pitfall with decision trees is that they can get a little too enthusiastic, creating very specific rules that work perfectly for the data they were trained on but not so well for new data. This is known as overfitting.\n\nTo avoid this, we can trim or 'prune' the tree after it's grown, or we can set some ground rules that stop the tree from getting overly complicated in the first place. These rules can include limits like:\n\n- Maximum depth of the tree\n- Minimum number of data points to justify creating a new branch\n- Minimum improvement needed to make a new split\n- A threshold below which we won't consider any new splits\n\n\n## The Iris Dataset: Our Classification Playground\n\nWe'll be using the well-loved Iris dataset for classification tasks. The Species attribute will be our target for prediction. Here's a peek at the dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(datasets)\ndata(iris)\nhead(iris, 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n:::\n:::\n\n\nBy using the Iris dataset, we can focus on classifying flower species, a perfect starting point for practicing with decision trees.\n\nBuilding the Classifier: The Search for the First Split\nIn our script, we're not just taking wild guesses. We're methodically examining each feature to find the value that offers the best division of our data, aiming to increase Information Gain.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nentropy <- function(y) {\n  if(length(y) == 0) return(0)\n  p <- table(y) / length(y)\n  sum(-p * log2(p + 1e-9))\n}\n\ngini_impurity <- function(y) {\n  if(length(y) == 0) return(0)\n  p <- table(y) / length(y)\n  1 - sum(p^2)\n}\n```\n:::\n\n\nNow we can calculate the Information Gain, considering the improvements brought by different partitions in our data. We use a mask—a logical vector indicating if a row belongs to the first or second partition—to guide this process:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninformation_gain <- function(y, mask, metric_func = entropy) {\n  # We don't want partitions with no data points\n  if (sum(mask) == 0 || sum(!mask) == 0) return(0)\n  metric_func(y) - (sum(mask) / length(mask)) * metric_func(y[mask]) - \n    (sum(!mask) / length(!mask)) * metric_func(y[!mask])\n}\n```\n:::\n\n\n## Seeking the Best Split Across All Features\n\nWith our metric functions in hand, we'll comb through all the features to find the ultimate split:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmax_information_gain_split <- function(y, x, metric_func = gini_impurity) {\n  # Initialize the best change and split value\n  best_change <- NA\n  split_value <- NA\n  # Check if the feature is numeric or categorical\n  is_numeric <- !(is.factor(x) || is.logical(x) || is.character(x))\n\n  for(val in sort(unique(x))) {\n    mask <- if (is_numeric) { x < val } else { x == val }\n    change <- information_gain(y, mask, metric_func)\n    if(is.na(best_change) || change > best_change) {\n      best_change <- change\n      split_value <- val\n    }\n  }\n\n  return(list(\"best_change\" = best_change, \"split_value\" = split_value, \"is_numeric\" = is_numeric))\n}\n```\n:::\n\n\nNow, let's use this to find the best split for a single feature:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(max_information_gain_split(iris$Species, iris$Petal.Width))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$best_change\n[1] 0.3333333\n\n$split_value\n[1] 1\n\n$is_numeric\n[1] TRUE\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsapply(iris[, 1:4], function(x) max_information_gain_split(iris$Species, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Sepal.Length Sepal.Width Petal.Length Petal.Width\nbest_change 0.2277603    0.1269234   0.3333333    0.3333333  \nsplit_value 5.5          3.4         3            1          \nis_numeric  TRUE         TRUE        TRUE         TRUE       \n```\n:::\n:::\n\n\nAfter determining the feature and value that provide the most significant information gain, we can then carve our dataset into two distinct sets. Think of it like sorting a deck of cards where one feature is the deciding factor.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_feature_split <- function(X, y) {\n  results <- sapply(X, function(x) max_information_gain_split(y, x))\n  best_name <- names(which.max(results['best_change',]))\n  best_result <- results[, best_name]\n  best_result[[\"name\"]] <- best_name\n  return(best_result)\n}\n```\n:::\n\n\nLet's run it for the iris dataset and see which feature provides the best split.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.data.frame(best_feature_split(iris[, 1:4], iris[, 5]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  best_change split_value is_numeric         name\n1   0.3333333           3       TRUE Petal.Length\n```\n:::\n:::\n\n\nThis neat little function sifts through our data and finds the golden split—the point where dividing our dataset yields the most clarity according to our chosen metric.\n\nNow, imagine we've found our winning feature. The next step is to partition the iris dataset into two new datasets based on this feature. We'll refer to them as 'left' and 'right' datasets, aligning with the branching structure of a tree.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_best_mask <- function(X, best_feature_list) {\n  best_mask <- X[, best_feature_list$name] < best_feature_list$split_value\n  return(best_mask)\n}\n```\n:::\n\n\n\nWe'll get the best mask and split the iris dataset accordingly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_iris_split <- best_feature_split(iris[, 1:4], iris[, 5])\nbest_iris_mask <- get_best_mask(iris[, 1:4], best_iris_split)\n```\n:::\n\n\n\nPartition the iris dataset into 'left' and 'right'\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleft_iris <- iris[best_iris_mask, ]\nright_iris <- iris[!best_iris_mask, ]\n```\n:::\n\n\n\nAnd just like that, with our data now split into 'left' and 'right', we've taken the first step in constructing our decision tree. This process will serve as the foundation for building a data structure that captures the essence of our tree, complete with branches that stem from each decision point.\n\n# Building our Decision Tree\n\nNow that we've laid the groundwork, get ready because we're about to build our decision tree from scratch. Wish me luck!\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncalculate_entropy <- function(y) {\n  if (length(y) == 0) return(0)\n  p <- table(y) / length(y)\n  -sum(p * log2(p + 1e-9))\n}\n\ncalculate_gini_impurity <- function(y) {\n  if (length(y) == 0) return(0)\n  p <- table(y) / length(y)\n  1 - sum(p^2)\n}\n\ncalculate_information_gain <- function(y, mask, metric_func) {\n  s1 <- sum(mask)\n  s2 <- length(mask) - s1\n  if (s1 == 0 || s2 == 0) return(0)\n  metric_func(y) - s1 / (s1 + s2) * metric_func(y[mask]) - s2 / (s1 + s2) * metric_func(y[!mask])\n}\n\nfind_best_split <- function(X, y, metric_func) {\n  features <- names(X)\n  best_gain <- 0\n  best_split <- NULL\n  best_feature <- NULL\n  for (feature in features) {\n    unique_values <- unique(X[[feature]])\n    for (value in unique_values) {\n      mask <- X[[feature]] < value\n      gain <- calculate_information_gain(y, mask, metric_func)\n      if (gain > best_gain) {\n        best_gain <- gain\n        best_split <- value\n        best_feature <- feature\n      }\n    }\n  }\n  list(gain = best_gain, feature = best_feature, split = best_split)\n}\n\nsplit_data <- function(X, y, best_split) {\n  mask <- X[[best_split$feature]] < best_split$split\n  list(\n    left_X = X[mask, ],\n    left_y = y[mask],\n    right_X = X[!mask, ],\n    right_y = y[!mask]\n  )\n}\n\ncreate_decision_tree <- function(X, y, max_depth = 3, depth = 0, metric_func = calculate_gini_impurity) {\n  if (depth == max_depth || length(unique(y)) == 1) {\n    return(list(prediction = ifelse(is.factor(y), names(sort(-table(y)))[1], mean(y))))\n  }\n  \n  best_split <- find_best_split(X, y, metric_func)\n  if (best_split$gain == 0) {\n    return(list(prediction = ifelse(is.factor(y), names(sort(-table(y)))[1], mean(y))))\n  }\n  \n  split_sets <- split_data(X, y, best_split)\n  return(list(\n    feature = best_split$feature,\n    split = best_split$split,\n    left = create_decision_tree(split_sets$left_X, split_sets$left_y, max_depth, depth + 1, metric_func),\n    right = create_decision_tree(split_sets$right_X, split_sets$right_y, max_depth, depth + 1, metric_func)\n  ))\n}\n\n\nprint_decision_tree <- function(node, prefix = \"\") {\n  if (!is.null(node$prediction)) {\n    cat(prefix, \"Predict:\", node$prediction, \"\\n\")\n  } else {\n    cat(prefix, \"If\", node$feature, \"<\", node$split, \":\\n\")\n    print_decision_tree(node$left, paste0(prefix, \"  \"))\n    \n    cat(prefix, \"Else (\", node$feature, \">=\", node$split, \"):\\n\")\n    print_decision_tree(node$right, paste0(prefix, \"  \"))\n  }\n}\n\niris_tree <- create_decision_tree(iris[, 1:4], iris[, 5], max_depth = 3)\n\nprint_decision_tree(iris_tree)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n If Petal.Length < 3 :\n   Predict: setosa \n Else ( Petal.Length >= 3 ):\n   If Petal.Width < 1.8 :\n     If Petal.Length < 5 :\n       Predict: versicolor \n     Else ( Petal.Length >= 5 ):\n       Predict: virginica \n   Else ( Petal.Width >= 1.8 ):\n     If Petal.Length < 4.9 :\n       Predict: virginica \n     Else ( Petal.Length >= 4.9 ):\n       Predict: virginica \n```\n:::\n:::\n\n\n\n\n## Comparing Our Tree to `rpart`\n\nAfter meticulously constructing our decision tree from scratch and gaining insights into its inner workings, it's illuminating to compare our results with those from the well-established `rpart` package. This comparison not only validates our approach but also offers perspective on how different methodologies might influence the model's structure and performance.\n\n## Building the Tree with `rpart`\n\nTo make this comparison, we first need to construct a decision tree using rpart on the same dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart)\n# Building the decision tree model with rpart\nrpart_tree <- rpart(Species ~ ., data = iris, method = \"class\")\n```\n:::\n\n\nSummary of the rpart tree\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(rpart_tree)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nrpart(formula = Species ~ ., data = iris, method = \"class\")\n  n= 150 \n\n    CP nsplit rel error xerror       xstd\n1 0.50      0      1.00   1.24 0.04636090\n2 0.44      1      0.50   0.85 0.06069047\n3 0.01      2      0.06   0.09 0.02908608\n\nVariable importance\n Petal.Width Petal.Length Sepal.Length  Sepal.Width \n          34           31           21           14 \n\nNode number 1: 150 observations,    complexity param=0.5\n  predicted class=setosa      expected loss=0.6666667  P(node) =1\n    class counts:    50    50    50\n   probabilities: 0.333 0.333 0.333 \n  left son=2 (50 obs) right son=3 (100 obs)\n  Primary splits:\n      Petal.Length < 2.45 to the left,  improve=50.00000, (0 missing)\n      Petal.Width  < 0.8  to the left,  improve=50.00000, (0 missing)\n      Sepal.Length < 5.45 to the left,  improve=34.16405, (0 missing)\n      Sepal.Width  < 3.35 to the right, improve=19.03851, (0 missing)\n  Surrogate splits:\n      Petal.Width  < 0.8  to the left,  agree=1.000, adj=1.00, (0 split)\n      Sepal.Length < 5.45 to the left,  agree=0.920, adj=0.76, (0 split)\n      Sepal.Width  < 3.35 to the right, agree=0.833, adj=0.50, (0 split)\n\nNode number 2: 50 observations\n  predicted class=setosa      expected loss=0  P(node) =0.3333333\n    class counts:    50     0     0\n   probabilities: 1.000 0.000 0.000 \n\nNode number 3: 100 observations,    complexity param=0.44\n  predicted class=versicolor  expected loss=0.5  P(node) =0.6666667\n    class counts:     0    50    50\n   probabilities: 0.000 0.500 0.500 \n  left son=6 (54 obs) right son=7 (46 obs)\n  Primary splits:\n      Petal.Width  < 1.75 to the left,  improve=38.969400, (0 missing)\n      Petal.Length < 4.75 to the left,  improve=37.353540, (0 missing)\n      Sepal.Length < 6.15 to the left,  improve=10.686870, (0 missing)\n      Sepal.Width  < 2.45 to the left,  improve= 3.555556, (0 missing)\n  Surrogate splits:\n      Petal.Length < 4.75 to the left,  agree=0.91, adj=0.804, (0 split)\n      Sepal.Length < 6.15 to the left,  agree=0.73, adj=0.413, (0 split)\n      Sepal.Width  < 2.95 to the left,  agree=0.67, adj=0.283, (0 split)\n\nNode number 6: 54 observations\n  predicted class=versicolor  expected loss=0.09259259  P(node) =0.36\n    class counts:     0    49     5\n   probabilities: 0.000 0.907 0.093 \n\nNode number 7: 46 observations\n  predicted class=virginica   expected loss=0.02173913  P(node) =0.3066667\n    class counts:     0     1    45\n   probabilities: 0.000 0.022 0.978 \n```\n:::\n:::\n\n\nThis code snippet fits a decision tree to predict the Species from the iris dataset, mimicking the setup of our manually built tree. The summary provides detailed information about the tree's splits, size, and predictive performance.\n\n### Visualizing the rpart Tree\n\nVisualizing the tree helps in understanding the decisions it makes. The `rpart.plot` package offers tools for this purpose:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart.plot)\nrpart.plot(rpart_tree, type = 3, box.palette = \"RdBu\", extra = 104)\n```\n\n::: {.cell-output-display}\n![](2024-03-26-DT_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nThis visualization shows the splits made by the tree, the criteria for each decision, and the distribution of classes at each node. The type, color palette, and extra parameters are adjustable to enhance readability and insight.\n\n## Evaluating the Models\n\nWith both trees constructed, we can now evaluate their performance on the dataset. This comparison will shed light on the predictive power of each model and how they differ in their decision-making processes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict_manual_tree <- function(tree, newdata) {\n  if (!is.null(tree$prediction)) {\n    # If it's a leaf node, return the prediction\n    return(tree$prediction)\n  } else {\n    # Determine whether to follow the left or right branch\n    if (!is.null(tree$split) && newdata[[tree$feature]] < tree$split) {\n      return(predict_manual_tree(tree$left, newdata))\n    } else {\n      return(predict_manual_tree(tree$right, newdata))\n    }\n  }\n}\n\npredicted_manual <- sapply(1:nrow(iris), function(i) predict_manual_tree(iris_tree, iris[i, ]))\nmanual_accuracy <- mean(predicted_manual == iris$Species)\n\n# For the rpart tree\npredicted_rpart <- predict(rpart_tree, iris, type = \"class\")\nrpart_accuracy <- mean(predicted_rpart == iris$Species)\n\ncat(\"Manual Tree Accuracy:\", manual_accuracy, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nManual Tree Accuracy: 0.9733333 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Rpart Tree Accuracy:\", rpart_accuracy, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRpart Tree Accuracy: 0.96 \n```\n:::\n:::\n\n\nInteresting! Our manually constructed tree had higher accuracy than the rpart tree on the iris dataset. How did that happen? The difference in accuracy could be due to the hyperparameters used, the splitting criterion, or the stopping criteria. It's a reminder that the choice of algorithm and its parameters can significantly impact the model's performance.\n\n## Conclusion\n\nIn this project, we delved into the inner workings of decision trees, building one from scratch and comparing it with the `rpart` package. We explored the concepts of entropy, information gain, and Gini impurity, which are fundamental to decision tree algorithms. By constructing a decision tree manually, we gained a deeper understanding of how these models make decisions and split the data.\n\nThank you! I hope you learned as much as I did while working on this post! If you have any questions or feedback, please feel free to reach out. Happy learning!\n",
    "supporting": [
      "2024-03-26-DT_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}