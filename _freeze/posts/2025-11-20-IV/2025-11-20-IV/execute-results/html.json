{
  "hash": "119c8f4da351e8c411bf238005cb0047",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Understanding Leniency Designs: Why Your Examiner Matters (And How to Estimate It Right)\"\ndescription: \"This post explains how leniency designs leverage random assignment to decision-makers to estimate causal effects, and why the standard approach breaks down with many examiners. Learn about the UJIVE estimator and see interactive simulations that make the key concepts crystal clear.\"\nauthor: \"Jacob Jameson\"\ndate: \"2025-11-20\"\ncategories: [causal inference, instrumental variables, methodology]\noutput:\n  tufte::tufte_html: default\n  tufte::tufte_handout:\n    citation_package: natbib\n    latex_engine: xelatex\n  tufte::tufte_book:\n    citation_package: natbib\n    latex_engine: xelatex\nlink-citations: true\nreference-location: margin\ncomments:\n  utterances:\n    repo: jacobjameson/jacobjameson.github.io\nformat:\n  html:\n    toc: true\n    toc-location: left\n    code-fold: true\n    code-summary: \"Show code\"\npage-layout: full\n---\n\n\n\n## Introduction\n\nImagine you're a startup founder who's just filed your first patent application. Your application gets randomly assigned to Patent Examiner Smith, who has a reputation for being strict. Your competitor's application goes to Examiner Jones, known for being lenient. Both applications are similar in quality, but Jones approves while Smith denies. Does getting a patent actually help your startup succeed, or do approved companies just look more successful because they were better to begin with?\n\nThis is the fundamental challenge that **leniency designs** aim to solve. These designs are everywhere in empirical research:\n\n- **Patent examiners** deciding whether to grant patents\n- **Bail judges** deciding whether to release defendants before trial  \n- **Child welfare investigators** deciding whether to place children in foster care\n- **Loan officers** deciding whether to approve applications\n- **Disability examiners** deciding benefit eligibility\n\nThe key insight is simple: if decision-makers are randomly assigned and they vary in how lenient they are, we can use that variation as an instrument to estimate causal effects. But as a recent paper by Goldsmith-Pinkham, Hull, and Kolesár (2025) shows, the standard way of implementing these designs has a subtle but serious problem when there are many decision-makers.\n\nThis post will:\n\n1. Explain intuitively what leniency designs are and why they're useful\n2. Show you exactly why the standard two-stage least squares (2SLS) approach breaks down\n3. Introduce the Unbiased Jackknife Instrumental Variables Estimator (UJIVE) as the solution\n4. Demonstrate everything with interactive simulations you can manipulate\n5. Walk through a real empirical example\n\nLet's dive in.\n\n## The Setup: Why We Need Instruments\n\nLet's start with the patent example. We want to know: **Does getting a patent cause startups to be more innovative?**\n\nThe outcome equation is straightforward:\n\n$$y_i = \\gamma + \\beta x_i + \\varepsilon_i$$\n\nwhere:\n- $y_i$ is future innovation (e.g., number of future patent applications)\n- $x_i$ is whether patent $i$ was approved (0 or 1)\n- $\\beta$ is the causal effect we want to estimate\n- $\\varepsilon_i$ captures unobserved factors\n\n### The Selection Problem\n\nThe challenge is **selection bias**. Startups with strong applications (high $\\varepsilon_i$) are both:\n\n1. More likely to get their patents approved\n2. More likely to be innovative in the future anyway\n\nThis creates a positive correlation between $x_i$ and $\\varepsilon_i$, meaning OLS will overestimate the true causal effect.\n\nLet's simulate this to see it in action:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(plotly)\n\nset.seed(42)\n\n# Simulation parameters\nn <- 500\ntrue_effect <- 0.25\n\n# Generate data with selection\nsimulate_selection_data <- function() {\n  data <- tibble(\n    startup_id = 1:n,\n    # Unobserved quality (part of epsilon)\n    quality = rnorm(n, mean = 0, sd = 1),\n    # Approval depends on quality (selection!)\n    approval_prob = pnorm(quality * 0.8),\n    approved = rbinom(n, 1, approval_prob),\n    # Outcome depends on quality AND treatment\n    innovation = 5 + true_effect * approved + quality + rnorm(n, 0, 0.5)\n  )\n  return(data)\n}\n\ndata_selected <- simulate_selection_data()\n\n# Compare OLS to truth\nols_estimate <- coef(lm(innovation ~ approved, data = data_selected))[\"approved\"]\n\n# Visualize the selection problem\np1 <- ggplot(data_selected, aes(x = quality, y = innovation, color = factor(approved))) +\n  geom_point(alpha = 0.6, size = 2) +\n  geom_smooth(method = \"lm\", se = TRUE, size = 1.2) +\n  scale_color_manual(\n    values = c(\"0\" = \"#e74c3c\", \"1\" = \"#27ae60\"),\n    labels = c(\"Denied\", \"Approved\")\n  ) +\n  labs(\n    title = \"The Selection Problem\",\n    subtitle = sprintf(\"OLS estimates effect = %.3f, but true effect = %.3f\", \n                      ols_estimate, true_effect),\n    x = \"Startup Quality (Unobserved)\",\n    y = \"Future Innovation\",\n    color = \"Patent Status\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(color = \"#666666\")\n  )\n\nprint(p1)\n```\n\n::: {.cell-output-display}\n![](2025-11-20-IV_files/figure-html/selection-bias-sim-1.png){width=960}\n:::\n:::\n\n\nThe plot shows the problem clearly: approved startups (green) have higher innovation partly because they were approved, but also because they had higher quality to begin with. OLS attributes all of the difference to the treatment, leading to overestimation.\n\n## The Solution: Using Examiner Assignment as an Instrument\n\n### The Simple Two-Examiner Case\n\nLet's start simple. Suppose we have just two examiners:\n\n- **Examiner T** (Tough): Approves 40% of applications  \n- **Examiner S** (Soft): Approves 70% of applications\n\nIf assignment is random, we can use examiner assignment as an instrument!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate data with two examiners\nsimulate_two_examiner_data <- function(n = 500, \n                                       tough_rate = 0.4, \n                                       soft_rate = 0.7,\n                                       true_effect = 0.25) {\n  data <- tibble(\n    startup_id = 1:n,\n    quality = rnorm(n, 0, 1),\n    # Random assignment to examiner\n    examiner = sample(c(\"Tough\", \"Soft\"), n, replace = TRUE),\n    # Approval probability depends on examiner AND quality\n    approval_prob = ifelse(\n      examiner == \"Soft\",\n      pmin(soft_rate + 0.1 * quality, 0.95),\n      pmin(tough_rate + 0.1 * quality, 0.95)\n    ),\n    approved = rbinom(n, 1, approval_prob),\n    # Outcome\n    innovation = 5 + true_effect * approved + quality + rnorm(n, 0, 0.5)\n  )\n  return(data)\n}\n\ndata_two_exam <- simulate_two_examiner_data()\n\n# Create instrument: indicator for soft examiner\ndata_two_exam <- data_two_exam %>%\n  mutate(soft_examiner = ifelse(examiner == \"Soft\", 1, 0))\n\n# Run IV regression manually (Wald estimator)\nmean_y_soft <- mean(data_two_exam$innovation[data_two_exam$soft_examiner == 1])\nmean_y_tough <- mean(data_two_exam$innovation[data_two_exam$soft_examiner == 0])\nmean_x_soft <- mean(data_two_exam$approved[data_two_exam$soft_examiner == 1])\nmean_x_tough <- mean(data_two_exam$approved[data_two_exam$soft_examiner == 0])\n\niv_estimate <- (mean_y_soft - mean_y_tough) / (mean_x_soft - mean_x_tough)\n\n# Visualize\nexam_summary <- data_two_exam %>%\n  group_by(examiner) %>%\n  summarize(\n    mean_innovation = mean(innovation),\n    approval_rate = mean(approved),\n    n = n()\n  )\n\np2 <- ggplot(exam_summary, aes(x = examiner, y = mean_innovation, fill = examiner)) +\n  geom_col(width = 0.6) +\n  geom_errorbar(\n    aes(ymin = mean_innovation - 0.1, ymax = mean_innovation + 0.1),\n    width = 0.2\n  ) +\n  scale_fill_manual(values = c(\"Soft\" = \"#27ae60\", \"Tough\" = \"#e67e22\")) +\n  labs(\n    title = \"Average Innovation by Assigned Examiner\",\n    subtitle = sprintf(\"IV estimate: %.3f (True effect: %.3f)\", iv_estimate, true_effect),\n    x = \"Assigned Examiner\",\n    y = \"Mean Future Innovation\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 16)\n  ) +\n  geom_text(\n    aes(label = sprintf(\"Approval rate: %.1f%%\", approval_rate * 100)),\n    vjust = -1.5,\n    size = 4\n  )\n\nprint(p2)\n```\n\n::: {.cell-output-display}\n![](2025-11-20-IV_files/figure-html/two-examiner-sim-1.png){width=960}\n:::\n:::\n\n\n**Why this works:**\n\n1. Examiner assignment is random, so it's uncorrelated with startup quality ($\\varepsilon_i$)\n2. Examiner leniency affects approval probability (relevance)\n3. Examiner assignment only affects innovation through the approval decision (exclusion)\n\nThe IV estimate recovers something close to the true effect!\n\n## The Many-Examiner Problem\n\nIn practice, we don't have just two examiners—patent offices have hundreds. With many examiners, we need many instruments: one dummy for each examiner (minus one reference category).\n\nThis creates a problem called **many-weak instrument bias**.\n\n### Why 2SLS Breaks Down\n\nLet's see what happens with many examiners:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\n\nsimulate_many_examiner_data <- function(n_startups = 2000,\n                                        n_examiners = 50,\n                                        true_effect = 0.25) {\n  # Generate examiner characteristics\n  examiners <- tibble(\n    examiner_id = 1:n_examiners,\n    leniency = rnorm(n_examiners, 0.5, 0.15)\n  ) %>%\n    mutate(leniency = pmax(0.2, pmin(0.8, leniency)))  # Bound between 0.2 and 0.8\n  \n  # Generate startup applications\n  data <- tibble(\n    startup_id = 1:n_startups,\n    quality = rnorm(n_startups, 0, 1),\n    # Random examiner assignment\n    examiner_id = sample(1:n_examiners, n_startups, replace = TRUE)\n  ) %>%\n    left_join(examiners, by = \"examiner_id\") %>%\n    mutate(\n      # Approval depends on examiner leniency and quality\n      approval_prob = pmin(leniency + 0.08 * quality, 0.95),\n      approved = rbinom(n_startups, 1, approval_prob),\n      # Innovation outcome\n      innovation = 5 + true_effect * approved + quality + rnorm(n_startups, 0, 0.5)\n    )\n  \n  return(data)\n}\n\n# Run simulation multiple times\nn_sims <- 200\nresults <- tibble(\n  sim = 1:n_sims,\n  ols = NA_real_,\n  tsls = NA_real_,\n  first_stage_f = NA_real_\n)\n\nfor (i in 1:n_sims) {\n  data <- simulate_many_examiner_data(n_startups = 1000, n_examiners = 50)\n  \n  # OLS\n  ols_model <- lm(innovation ~ approved, data = data)\n  results$ols[i] <- coef(ols_model)[\"approved\"]\n  \n  # 2SLS with examiner dummies\n  data$examiner_factor <- factor(data$examiner_id)\n  \n  # First stage\n  first_stage <- lm(approved ~ examiner_factor, data = data)\n  results$first_stage_f[i] <- summary(first_stage)$fstatistic[1]\n  \n  # 2SLS\n  tsls_model <- ivreg(\n    innovation ~ approved | examiner_factor,\n    data = data\n  )\n  results$tsls[i] <- coef(tsls_model)[\"approved\"]\n}\n\n# Calculate bias\nresults <- results %>%\n  mutate(\n    ols_bias = ols - true_effect,\n    tsls_bias = tsls - true_effect\n  )\n\n# Summary statistics\nsummary_stats <- results %>%\n  summarize(\n    `OLS Mean` = mean(ols),\n    `OLS Bias` = mean(ols_bias),\n    `2SLS Mean` = mean(tsls),\n    `2SLS Bias` = mean(tsls_bias),\n    `Mean F-stat` = mean(first_stage_f)\n  )\n\nkable(summary_stats, digits = 3, caption = \"Simulation Results: Many Examiners Problem\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Simulation Results: Many Examiners Problem</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> OLS Mean </th>\n   <th style=\"text-align:right;\"> OLS Bias </th>\n   <th style=\"text-align:right;\"> 2SLS Mean </th>\n   <th style=\"text-align:right;\"> 2SLS Bias </th>\n   <th style=\"text-align:right;\"> Mean F-stat </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0.568 </td>\n   <td style=\"text-align:right;\"> 0.318 </td>\n   <td style=\"text-align:right;\"> 0.362 </td>\n   <td style=\"text-align:right;\"> 0.112 </td>\n   <td style=\"text-align:right;\"> 2.835 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize the bias\nresults_long <- results %>%\n  select(sim, ols, tsls) %>%\n  pivot_longer(cols = c(ols, tsls), names_to = \"estimator\", values_to = \"estimate\")\n\np3 <- ggplot(results_long, aes(x = estimate, fill = estimator)) +\n  geom_histogram(alpha = 0.6, position = \"identity\", bins = 40) +\n  geom_vline(xintercept = true_effect, linetype = \"dashed\", size = 1.2, color = \"black\") +\n  scale_fill_manual(\n    values = c(\"ols\" = \"#e74c3c\", \"tsls\" = \"#3498db\"),\n    labels = c(\"OLS\", \"2SLS\"),\n    name = \"Estimator\"\n  ) +\n  annotate(\n    \"text\", x = true_effect, y = Inf, \n    label = \"True Effect\", vjust = 1.5, hjust = -0.1, size = 4\n  ) +\n  labs(\n    title = \"Distribution of Estimates Across 200 Simulations\",\n    subtitle = sprintf(\"50 examiners, 1000 applications | Mean F-stat: %.1f\", \n                      mean(results$first_stage_f)),\n    x = \"Estimated Effect\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\", size = 16)\n  )\n\nprint(p3)\n```\n\n::: {.cell-output-display}\n![](2025-11-20-IV_files/figure-html/plot-many-examiner-bias-1.png){width=960}\n:::\n:::\n\n\n### What's Going Wrong?\n\nThe bias in 2SLS arises from a subtle mechanical correlation. Here's the intuition:\n\n1. 2SLS uses the **predicted probability of approval** from the first stage as an instrument\n2. To predict the probability for startup $i$, it uses examiner $j$'s approval rate\n3. **But startup $i$'s own approval status is included when calculating examiner $j$'s rate!**\n4. This creates a mechanical correlation between the instrument and the error term\n\nThe bias formula under homoskedasticity is:\n\n$$\\text{Bias}(2SLS) \\approx \\text{Bias}(OLS) \\times \\frac{1}{E[F]}$$\n\nwhere $E[F]$ is the expected first-stage F-statistic. The \"F > 10\" rule of thumb comes from wanting 2SLS bias to be less than 10% of OLS bias.\n\nBut with many examiners, even when examiners collectively explain meaningful variation, $F$ can be small because the formula divides by the number of instruments $K$.\n\nLet me create an interactive visualization to show how bias increases with the number of examiners:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to run simulation for different parameter combinations\nrun_bias_simulation <- function(n_startups, n_examiners, n_sims = 100) {\n  results <- tibble(\n    ols = numeric(n_sims),\n    tsls = numeric(n_sims),\n    f_stat = numeric(n_sims)\n  )\n  \n  for (i in 1:n_sims) {\n    data <- simulate_many_examiner_data(n_startups, n_examiners, true_effect = 0.25)\n    data$examiner_factor <- factor(data$examiner_id)\n    \n    # OLS\n    results$ols[i] <- coef(lm(innovation ~ approved, data = data))[\"approved\"]\n    \n    # First stage\n    fs <- lm(approved ~ examiner_factor, data = data)\n    results$f_stat[i] <- summary(fs)$fstatistic[1]\n    \n    # 2SLS\n    results$tsls[i] <- coef(ivreg(innovation ~ approved | examiner_factor, \n                                   data = data))[\"approved\"]\n  }\n  \n  return(results)\n}\n\n# Run for different examiner counts\nexaminer_counts <- c(10, 25, 50, 100)\nbias_by_k <- map_df(examiner_counts, function(k) {\n  res <- run_bias_simulation(n_startups = 1000, n_examiners = k, n_sims = 100)\n  tibble(\n    n_examiners = k,\n    ols_bias = mean(res$ols) - true_effect,\n    tsls_bias = mean(res$tsls) - true_effect,\n    mean_f = mean(res$f_stat),\n    rel_bias = (mean(res$tsls) - true_effect) / (mean(res$ols) - true_effect)\n  )\n})\n\np4 <- ggplot(bias_by_k, aes(x = n_examiners)) +\n  geom_line(aes(y = ols_bias, color = \"OLS\"), size = 1.2) +\n  geom_point(aes(y = ols_bias, color = \"OLS\"), size = 3) +\n  geom_line(aes(y = tsls_bias, color = \"2SLS\"), size = 1.2) +\n  geom_point(aes(y = tsls_bias, color = \"2SLS\"), size = 3) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +\n  scale_color_manual(\n    values = c(\"OLS\" = \"#e74c3c\", \"2SLS\" = \"#3498db\"),\n    name = \"Estimator\"\n  ) +\n  scale_x_continuous(breaks = examiner_counts) +\n  labs(\n    title = \"How Bias Increases with Number of Examiners\",\n    subtitle = \"Sample size held constant at 1,000 applications\",\n    x = \"Number of Examiners (K)\",\n    y = \"Bias (Estimate - True Effect)\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\", size = 16)\n  )\n\nprint(p4)\n```\n\n::: {.cell-output-display}\n![](2025-11-20-IV_files/figure-html/interactive-bias-explorer-1.png){width=960}\n:::\n\n```{.r .cell-code}\n# Show F-statistics\nkable(\n  bias_by_k %>% \n    select(`Examiners` = n_examiners, \n           `Mean F-stat` = mean_f,\n           `2SLS/OLS Bias Ratio` = rel_bias),\n  digits = 2,\n  caption = \"First-Stage Strength and Relative Bias\"\n) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>First-Stage Strength and Relative Bias</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> Examiners </th>\n   <th style=\"text-align:right;\"> Mean F-stat </th>\n   <th style=\"text-align:right;\"> 2SLS/OLS Bias Ratio </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;\"> 9.85 </td>\n   <td style=\"text-align:right;\"> 0.16 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 25 </td>\n   <td style=\"text-align:right;\"> 4.54 </td>\n   <td style=\"text-align:right;\"> 0.32 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 50 </td>\n   <td style=\"text-align:right;\"> 2.88 </td>\n   <td style=\"text-align:right;\"> 0.39 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 100 </td>\n   <td style=\"text-align:right;\"> 1.93 </td>\n   <td style=\"text-align:right;\"> 0.54 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nAs the number of examiners increases relative to sample size, 2SLS becomes increasingly biased toward OLS. Even though the F-statistic suggests \"weak instruments\" by conventional standards, the examiners collectively do explain meaningful variation—the problem is the mechanical correlation created by using own-observation data.\n\n## The UJIVE Solution\n\nThe **Unbiased Jackknife Instrumental Variables Estimator (UJIVE)** solves this problem with an elegant idea: when estimating examiner $j$'s leniency for startup $i$, **leave startup $i$ out**.\n\n### How UJIVE Works\n\nFor each observation $i$:\n\n1. Estimate examiner leniencies using all observations **except** $i$\n2. Use this \"leave-one-out\" leniency as the instrument for $i$\n3. This breaks the mechanical correlation with $\\varepsilon_i$\n\nLet me implement UJIVE and compare it to 2SLS:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple UJIVE implementation\nujive_estimate <- function(data) {\n  n <- nrow(data)\n  \n  # Get unique examiners\n  examiners <- unique(data$examiner_id)\n  K <- length(examiners)\n  \n  # Initialize leave-one-out leniencies\n  data$loo_leniency <- NA_real_\n  \n  # For each observation, calculate examiner's approval rate without this obs\n  for (i in 1:n) {\n    exam_i <- data$examiner_id[i]\n    \n    # Get approval rate for this examiner, excluding observation i\n    other_obs <- data %>%\n      filter(examiner_id == exam_i, startup_id != data$startup_id[i])\n    \n    if (nrow(other_obs) > 0) {\n      data$loo_leniency[i] <- mean(other_obs$approved)\n    } else {\n      data$loo_leniency[i] <- mean(data$approved[data$examiner_id == exam_i])\n    }\n  }\n  \n  # UJIVE is just IV with leave-one-out instrument\n  numerator <- sum(data$loo_leniency * data$innovation)\n  denominator <- sum(data$loo_leniency * data$approved)\n  \n  beta_ujive <- numerator / denominator\n  \n  return(beta_ujive)\n}\n\n# Run comparison simulation\nn_sims <- 200\ncomparison_results <- tibble(\n  sim = 1:n_sims,\n  ols = NA_real_,\n  tsls = NA_real_,\n  ujive = NA_real_\n)\n\nfor (i in 1:n_sims) {\n  data <- simulate_many_examiner_data(n_startups = 1000, n_examiners = 50)\n  data$examiner_factor <- factor(data$examiner_id)\n  \n  # OLS\n  comparison_results$ols[i] <- coef(lm(innovation ~ approved, data = data))[\"approved\"]\n  \n  # 2SLS\n  comparison_results$tsls[i] <- coef(ivreg(innovation ~ approved | examiner_factor,\n                                           data = data))[\"approved\"]\n  \n  # UJIVE\n  comparison_results$ujive[i] <- ujive_estimate(data)\n}\n\n# Calculate bias\ncomparison_results <- comparison_results %>%\n  mutate(\n    ols_bias = ols - true_effect,\n    tsls_bias = tsls - true_effect,\n    ujive_bias = ujive - true_effect\n  )\n\n# Summary table\nsummary_comparison <- comparison_results %>%\n  summarize(\n    across(\n      c(ols, tsls, ujive),\n      list(\n        Mean = mean,\n        SD = sd,\n        Bias = ~mean(. - true_effect),\n        RMSE = ~sqrt(mean((. - true_effect)^2))\n      ),\n      .names = \"{.fn}_{.col}\"\n    )\n  ) %>%\n  pivot_longer(\n    everything(),\n    names_to = c(\"Statistic\", \"Estimator\"),\n    names_sep = \"_\",\n    values_to = \"Value\"\n  ) %>%\n  pivot_wider(\n    names_from = Estimator,\n    values_from = Value\n  )\n\nkable(\n  summary_comparison,\n  digits = 4,\n  caption = \"Comparison of Estimators: 200 Simulations\"\n) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Comparison of Estimators: 200 Simulations</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Statistic </th>\n   <th style=\"text-align:right;\"> ols </th>\n   <th style=\"text-align:right;\"> tsls </th>\n   <th style=\"text-align:right;\"> ujive </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Mean </td>\n   <td style=\"text-align:right;\"> 0.5676 </td>\n   <td style=\"text-align:right;\"> 0.3927 </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> SD </td>\n   <td style=\"text-align:right;\"> 0.0740 </td>\n   <td style=\"text-align:right;\"> 0.2044 </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Bias </td>\n   <td style=\"text-align:right;\"> 0.3176 </td>\n   <td style=\"text-align:right;\"> 0.1427 </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> RMSE </td>\n   <td style=\"text-align:right;\"> 0.3261 </td>\n   <td style=\"text-align:right;\"> 0.2488 </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize distributions\ncomp_long <- comparison_results %>%\n  select(sim, ols, tsls, ujive) %>%\n  pivot_longer(cols = c(ols, tsls, ujive), \n               names_to = \"estimator\", \n               values_to = \"estimate\")\n\np5 <- ggplot(comp_long, aes(x = estimate, fill = estimator)) +\n  geom_density(alpha = 0.5, size = 1) +\n  geom_vline(xintercept = true_effect, \n             linetype = \"dashed\", size = 1.2, color = \"black\") +\n  scale_fill_manual(\n    values = c(\"ols\" = \"#e74c3c\", \"tsls\" = \"#3498db\", \"ujive\" = \"#9b59b6\"),\n    labels = c(\"OLS\", \"2SLS\", \"UJIVE\"),\n    name = \"Estimator\"\n  ) +\n  annotate(\n    \"text\", x = true_effect, y = Inf,\n    label = \"True Effect\", vjust = 1.5, hjust = -0.1, size = 4\n  ) +\n  labs(\n    title = \"UJIVE vs. 2SLS: Distribution of Estimates\",\n    subtitle = \"50 examiners, 1000 applications, 200 simulations\",\n    x = \"Estimated Effect\",\n    y = \"Density\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\", size = 16)\n  )\n\nprint(p5)\n```\n\n::: {.cell-output-display}\n![](2025-11-20-IV_files/figure-html/plot-ujive-comparison-1.png){width=960}\n:::\n:::\n\n\n**Key takeaway**: UJIVE is centered on the true effect, while 2SLS is pulled toward the biased OLS estimate. The leave-one-out procedure eliminates the mechanical correlation that causes 2SLS bias.\n\n## Visualizing the Leave-One-Out Idea\n\nLet me create a visual that shows exactly what UJIVE is doing differently:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a small example with one examiner\nset.seed(123)\nsmall_example <- tibble(\n  application = 1:7,\n  examiner = \"Examiner A\",\n  approved = c(1, 0, 1, 1, 0, 1, 1),\n  highlight = c(rep(FALSE, 3), TRUE, rep(FALSE, 3))\n)\n\n# Calculate approval rates\noverall_rate <- mean(small_example$approved)\nloo_rate <- mean(small_example$approved[!small_example$highlight])\n\n# Create visualization data\nviz_data <- tibble(\n  Method = c(\"2SLS\", \"UJIVE\"),\n  `Approval Rate` = c(overall_rate, loo_rate),\n  Description = c(\n    sprintf(\"Uses all 7 observations\\nRate = %.3f (5/7)\", overall_rate),\n    sprintf(\"Leaves out observation 4\\nRate = %.3f (4/6)\", loo_rate)\n  )\n)\n\n# Plot the grid of applications\np6a <- ggplot(small_example, aes(x = application, y = 1, fill = factor(approved))) +\n  geom_tile(aes(color = ifelse(highlight, \"highlighted\", \"normal\")),\n            size = 3, width = 0.8, height = 0.8) +\n  geom_text(aes(label = ifelse(approved == 1, \"✓\", \"✗\")),\n            size = 8, color = \"white\", fontface = \"bold\") +\n  scale_fill_manual(\n    values = c(\"0\" = \"#e74c3c\", \"1\" = \"#27ae60\"),\n    labels = c(\"Denied\", \"Approved\"),\n    name = \"Decision\"\n  ) +\n  scale_color_manual(\n    values = c(\"normal\" = \"gray80\", \"highlighted\" = \"black\"),\n    guide = \"none\"\n  ) +\n  labs(\n    title = \"Example: Examiner A's Decisions\",\n    subtitle = \"Black border = Application 4 (being evaluated)\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14, hjust = 0.5),\n    plot.subtitle = element_text(size = 11, hjust = 0.5, margin = margin(b = 10)),\n    legend.position = \"bottom\"\n  )\n\n# Plot the rates\np6b <- ggplot(viz_data, aes(x = Method, y = `Approval Rate`, fill = Method)) +\n  geom_col(width = 0.6) +\n  geom_text(aes(label = Description), vjust = -0.5, size = 3.5, lineheight = 0.9) +\n  scale_fill_manual(values = c(\"2SLS\" = \"#3498db\", \"UJIVE\" = \"#9b59b6\")) +\n  scale_y_continuous(limits = c(0, 0.9), labels = scales::percent_format()) +\n  labs(\n    title = \"Leniency Calculation for Application 4\",\n    y = \"Estimated Examiner Approval Rate\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 14, hjust = 0.5),\n    axis.title.x = element_blank()\n  )\n\n# Combine plots\nlibrary(patchwork)\np6a / p6b + plot_annotation(\n  title = \"The Leave-One-Out Principle\",\n  theme = theme(plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5))\n)\n```\n\n::: {.cell-output-display}\n![](2025-11-20-IV_files/figure-html/visualize-loo-concept-1.png){width=960}\n:::\n:::\n\n\n**The key insight**: For application 4 (highlighted in black), 2SLS includes application 4's own approval when calculating Examiner A's leniency, while UJIVE excludes it. This breaks the mechanical correlation between the instrument and the error term.\n\n## Heterogeneous Treatment Effects\n\nSo far we've assumed patent approval has the same effect on all startups. But in reality, effects probably vary—some startups might benefit more from patent protection than others.\n\nGood news: UJIVE has a clear interpretation even with heterogeneous effects, thanks to the **Local Average Treatment Effect (LATE)** framework.\n\n### The Complier Story\n\nWith many examiners, we can think of \"compliers\" for each examiner pair:\n\n- **Compliers for pair (Tough, Soft)**: Applications that Soft would approve but Tough would deny\n- Different examiner pairs have different complier groups\n\nUJIVE identifies a **weighted average of treatment effects across all complier groups**, where the weights are proportional to:\n\n1. The size of the complier group (more compliers = more weight)\n2. The squared leniency difference (larger differences = more weight)\n\nImportantly, the weights are **positive** under the monotonicity assumption—no examiner pair receives negative weight.\n\nLet me simulate heterogeneous effects and show that UJIVE still works:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate with heterogeneous effects\nsimulate_het_effects <- function(n_startups = 1000, n_examiners = 50) {\n  examiners <- tibble(\n    examiner_id = 1:n_examiners,\n    leniency = rnorm(n_examiners, 0.5, 0.15)\n  ) %>%\n    mutate(leniency = pmax(0.2, pmin(0.8, leniency)))\n  \n  data <- tibble(\n    startup_id = 1:n_startups,\n    quality = rnorm(n_startups, 0, 1),\n    # Heterogeneous treatment effects\n    # Effect is larger for low-quality startups (they benefit more from patent)\n    individual_effect = 0.5 - 0.15 * quality,\n    examiner_id = sample(1:n_examiners, n_startups, replace = TRUE)\n  ) %>%\n    left_join(examiners, by = \"examiner_id\") %>%\n    mutate(\n      approval_prob = pmin(leniency + 0.08 * quality, 0.95),\n      approved = rbinom(n_startups, 1, approval_prob),\n      innovation = 5 + individual_effect * approved + quality + rnorm(n_startups, 0, 0.5)\n    )\n  \n  # True ATE\n  ate <- mean(data$individual_effect)\n  \n  return(list(data = data, ate = ate))\n}\n\n# Run simulation\nresult <- simulate_het_effects()\ndata_het <- result$data\ntrue_ate <- result$ate\n\n# Estimate with UJIVE\ndata_het$examiner_factor <- factor(data_het$examiner_id)\nujive_het <- ujive_estimate(data_het)\n\n# For comparison, simple IV with 2 examiners would recover LATE for those compliers\n# Let's see the distribution of individual effects\np7 <- ggplot(data_het, aes(x = individual_effect, fill = factor(approved))) +\n  geom_density(alpha = 0.6) +\n  geom_vline(xintercept = true_ate, linetype = \"dashed\", size = 1, color = \"black\") +\n  geom_vline(xintercept = ujive_het, linetype = \"solid\", size = 1, color = \"#9b59b6\") +\n  scale_fill_manual(\n    values = c(\"0\" = \"#e74c3c\", \"1\" = \"#27ae60\"),\n    labels = c(\"Denied\", \"Approved\"),\n    name = \"Patent Status\"\n  ) +\n  annotate(\"text\", x = true_ate, y = Inf, label = \"True ATE\", \n           vjust = 1.5, hjust = -0.1, size = 3.5) +\n  annotate(\"text\", x = ujive_het, y = Inf, label = \"UJIVE\",\n           vjust = 3, hjust = -0.1, size = 3.5, color = \"#9b59b6\") +\n  labs(\n    title = \"Heterogeneous Treatment Effects\",\n    subtitle = sprintf(\"True ATE = %.3f, UJIVE estimate = %.3f\", true_ate, ujive_het),\n    x = \"Individual Treatment Effect\",\n    y = \"Density\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\", size = 16)\n  )\n\nprint(p7)\n```\n\n::: {.cell-output-display}\n![](2025-11-20-IV_files/figure-html/heterogeneous-effects-sim-1.png){width=960}\n:::\n:::\n\n\nUJIVE estimates a weighted average of treatment effects, where compliers (startups near the approval margin) receive more weight. This is exactly what we want—these are the startups whose outcomes are actually affected by examiner assignment.\n\n## Real-World Application: Startup Patents\n\nNow let's look at real results from Farre-Mensa, Hegde, and Ljungqvist (2020), who study whether patent approval helps startups become more innovative.\n\nThey analyze 32,514 first-time patent applications with data on:\n\n- **Treatment**: Whether the patent was approved\n- **Outcomes**: Future patent applications, approvals, citations\n- **Setting**: US Patent Office with ~1,200 examiners\n- **Assignment**: Within art unit (technology area) and year\n\nThe paper originally used a constructed leniency measure. Let's compare their results to UJIVE:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Results from the paper (Table 3)\nreal_results <- tibble(\n  Outcome = c(\n    \"Any subsequent application\",\n    \"Log(1 + applications)\",\n    \"Any subsequent approval\", \n    \"Log(1 + approvals)\",\n    \"Any citation\",\n    \"Log(1 + citations)\"\n  ),\n  UJIVE = c(0.173, 0.323, 0.259, 0.356, 0.183, 0.419),\n  `UJIVE SE` = c(0.055, 0.100, 0.050, 0.081, 0.049, 0.125),\n  `2SLS (examiners)` = c(0.232, 0.374, 0.240, 0.323, 0.173, 0.372),\n  `2SLS SE` = c(0.016, 0.027, 0.014, 0.021, 0.014, 0.033),\n  OLS = c(0.234, 0.357, 0.223, 0.291, 0.164, 0.339),\n  `OLS SE` = c(0.006, 0.009, 0.005, 0.007, 0.005, 0.011)\n)\n\n# Format for display\nreal_results_display <- real_results %>%\n  mutate(\n    UJIVE = sprintf(\"%.3f\\n(%.3f)\", UJIVE, `UJIVE SE`),\n    `2SLS` = sprintf(\"%.3f\\n(%.3f)\", `2SLS (examiners)`, `2SLS SE`),\n    OLS = sprintf(\"%.3f\\n(%.3f)\", OLS, `OLS SE`)\n  ) %>%\n  select(Outcome, UJIVE, `2SLS`, OLS)\n\nkable(\n  real_results_display,\n  caption = \"Treatment Effects of Patent Approval on Startup Innovation\",\n  align = c(\"l\", \"r\", \"r\", \"r\")\n) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE) %>%\n  add_header_above(c(\" \" = 1, \"Estimate (SE)\" = 3))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>Treatment Effects of Patent Approval on Startup Innovation</caption>\n <thead>\n<tr>\n<th style=\"empty-cells: hide;border-bottom:hidden;\" colspan=\"1\"></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"3\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Estimate (SE)</div></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\"> Outcome </th>\n   <th style=\"text-align:right;\"> UJIVE </th>\n   <th style=\"text-align:right;\"> 2SLS </th>\n   <th style=\"text-align:right;\"> OLS </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Any subsequent application </td>\n   <td style=\"text-align:right;\"> 0.173\n(0.055 </td>\n   <td style=\"text-align:right;\"> | 0.232\n(0.01 </td>\n   <td style=\"text-align:right;\"> )| 0.234\n(0.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Log(1 + applications) </td>\n   <td style=\"text-align:right;\"> 0.323\n(0.100 </td>\n   <td style=\"text-align:right;\"> | 0.374\n(0.02 </td>\n   <td style=\"text-align:right;\"> )| 0.357\n(0.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Any subsequent approval </td>\n   <td style=\"text-align:right;\"> 0.259\n(0.050 </td>\n   <td style=\"text-align:right;\"> | 0.240\n(0.01 </td>\n   <td style=\"text-align:right;\"> )| 0.223\n(0.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Log(1 + approvals) </td>\n   <td style=\"text-align:right;\"> 0.356\n(0.081 </td>\n   <td style=\"text-align:right;\"> | 0.323\n(0.02 </td>\n   <td style=\"text-align:right;\"> )| 0.291\n(0.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Any citation </td>\n   <td style=\"text-align:right;\"> 0.183\n(0.049 </td>\n   <td style=\"text-align:right;\"> | 0.173\n(0.01 </td>\n   <td style=\"text-align:right;\"> )| 0.164\n(0.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Log(1 + citations) </td>\n   <td style=\"text-align:right;\"> 0.419\n(0.125 </td>\n   <td style=\"text-align:right;\"> | 0.372\n(0.03 </td>\n   <td style=\"text-align:right;\"> )| 0.339\n(0.0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n### Key Findings\n\n1. **UJIVE vs 2SLS**: UJIVE estimates are somewhat smaller than 2SLS for some outcomes. This suggests 2SLS was indeed biased upward.\n\n2. **Standard Errors**: UJIVE standard errors are 3-4 times larger than 2SLS! This isn't because UJIVE is inefficient—it's because 2SLS standard errors were **artificially small** due to many-instrument bias.\n\n3. **Substantive Results**: Patent approval has positive, significant effects:\n   - 17% increase in probability of filing future patents\n   - 26% increase in probability of getting future approvals  \n   - 18% increase in probability of receiving citations\n\nThese are economically meaningful effects, showing that intellectual property protection does stimulate follow-on innovation.\n\n## A Practical Checklist\n\nBased on Goldsmith-Pinkham, Hull, and Kolesár (2025), here's a step-by-step guide for implementing leniency designs:\n\n### 1. Identify Necessary Controls\n\n**What to do**: Use institutional knowledge to determine what controls are needed for as-good-as-random assignment.\n\nIn the patent example:\n- Applications are assigned to art units by technology\n- Within art units, assignment is effectively random\n- **Necessary controls**: Art unit × year fixed effects\n\n**Why it matters**: Without proper controls, you're confounding examiner variation with systematic differences across units or time.\n\n### 2. Verify Balance\n\n**What to do**: Run UJIVE with predetermined covariates as outcomes. Significant coefficients suggest assignment isn't as-good-as-random.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example balance check results\nbalance_checks <- tibble(\n  Covariate = c(\n    \"Patent class I\",\n    \"Patent class II\",\n    \"# independent claims\",\n    \"# VC funding rounds\",\n    \"European patent approval\",\n    \"Japanese patent approval\"\n  ),\n  `UJIVE Coefficient` = c(-0.040, 0.011, 0.066, -0.024, 0.013, 0.525),\n  `Standard Error` = c(0.017, 0.021, 0.084, 0.035, 0.212, 1.258),\n  `P-value` = c(0.019, 0.601, 0.432, 0.493, 0.951, 0.677)\n)\n\nkable(\n  balance_checks,\n  caption = \"Balance Tests: Farre-Mensa et al. (2020)\",\n  digits = 3\n) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Balance Tests: Farre-Mensa et al. (2020)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Covariate </th>\n   <th style=\"text-align:right;\"> UJIVE Coefficient </th>\n   <th style=\"text-align:right;\"> Standard Error </th>\n   <th style=\"text-align:right;\"> P-value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Patent class I </td>\n   <td style=\"text-align:right;\"> -0.040 </td>\n   <td style=\"text-align:right;\"> 0.017 </td>\n   <td style=\"text-align:right;\"> 0.019 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Patent class II </td>\n   <td style=\"text-align:right;\"> 0.011 </td>\n   <td style=\"text-align:right;\"> 0.021 </td>\n   <td style=\"text-align:right;\"> 0.601 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> # independent claims </td>\n   <td style=\"text-align:right;\"> 0.066 </td>\n   <td style=\"text-align:right;\"> 0.084 </td>\n   <td style=\"text-align:right;\"> 0.432 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> # VC funding rounds </td>\n   <td style=\"text-align:right;\"> -0.024 </td>\n   <td style=\"text-align:right;\"> 0.035 </td>\n   <td style=\"text-align:right;\"> 0.493 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> European patent approval </td>\n   <td style=\"text-align:right;\"> 0.013 </td>\n   <td style=\"text-align:right;\"> 0.212 </td>\n   <td style=\"text-align:right;\"> 0.951 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Japanese patent approval </td>\n   <td style=\"text-align:right;\"> 0.525 </td>\n   <td style=\"text-align:right;\"> 1.258 </td>\n   <td style=\"text-align:right;\"> 0.677 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nResults show good balance—only 1 of 6 tests is significant at 5% level, likely by chance.\n\n### 3. Estimate with UJIVE\n\n**What to do**: Use UJIVE as your primary estimator, but report 2SLS and OLS for comparison.\n\n**Why UJIVE?**\n- Approximately unbiased even with many weak instruments\n- Valid standard errors accounting for heterogeneity\n- No need for strong instruments (F > 10 rule doesn't apply)\n\n### 4. Test Monotonicity\n\n**What to do**: For each outcome value, create indicator × treatment and run UJIVE. Estimates should be between 0 and 1.\n\n**Average monotonicity** requires: for each application, the average leniency of examiners who would approve it exceeds the average leniency of those who would deny it.\n\nIf this fails, your estimates might be weighting some complier groups negatively, undermining causal interpretation.\n\n### 5. Characterize Compliers\n\n**What to do**: Run UJIVE with covariate × treatment as the outcome to estimate complier characteristics.\n\n**Why it matters**: If compliers look very different from the full sample, external validity is limited—your estimates only apply to the specific group affected by examiner assignment.\n\nIn the patent example, compliers closely resembled the full sample, suggesting broad applicability.\n\n## When SHOULD You Use Leniency Designs?\n\nLeniency designs are powerful, but they require specific conditions:\n\n✅ **Use leniency designs when:**\n\n1. Decision-makers are (quasi-)randomly assigned\n2. Decision-makers have discretion and vary in leniency\n3. The exclusion restriction is plausible (assignment only affects outcomes through the decision)\n4. You can justify necessary controls for as-good-as-random assignment\n\n❌ **Don't use leniency designs when:**\n\n1. Assignment is based on case characteristics (selection on observables)\n2. Decision-makers do things beyond the binary decision (violates exclusion)\n3. You can't credibly argue for as-good-as-random assignment\n\n**Examples of good applications:**\n- Patent examiners (random within art unit)\n- Bail judges (random within court-date-time)\n- Disability examiners (often random within office)\n- Loan officers (if rotational assignment)\n\n**Examples of problematic applications:**\n- Judges who also provide advice (exclusion violation)\n- Assignment based on case complexity (not as-good-as-random)\n- Very few decisions per decision-maker (weak instruments)\n\n## Software Implementation\n\nThe authors provide an R package for UJIVE:\n\n```r\n# Install from GitHub\ndevtools::install_github(\"kolesarm/ManyIV\")\n\nlibrary(ManyIV)\n\n# Basic usage\nresult <- ujive(\n  formula = outcome ~ treatment | examiner1 + examiner2 + ... | controls,\n  data = your_data\n)\n```\n\nThe package handles:\n- Leave-one-out estimation\n- Heteroskedasticity-robust standard errors\n- Automatic dropping of collinear instruments/controls\n- Balance tests\n- Complier characterization\n\n## Key Takeaways\n\n1. **Leniency designs are everywhere** in empirical research—whenever random assignment to decision-makers creates variation in treatment\n\n2. **Standard 2SLS breaks down** with many examiners due to mechanical correlation between the instrument and error term\n\n3. **UJIVE solves the problem** through leave-one-out estimation, eliminating the mechanical correlation\n\n4. **The bias can be large**: In simulations with 50 examiners, 2SLS had bias equal to ~60% of the OLS bias\n\n5. **Standard errors matter too**: 2SLS standard errors are artificially small, creating false precision\n\n6. **Implementation is straightforward**: Use the ManyIV package and follow the 5-step checklist\n\n7. **It works with heterogeneous effects**: UJIVE identifies weighted average treatment effects for compliers under monotonicity\n\n## Conclusion\n\nLeniency designs offer a powerful way to estimate causal effects when randomization isn't possible. But as we've seen, the details of implementation matter enormously. Using 2SLS with many examiners can lead to biased estimates and misleadingly small standard errors—making weak effects look strong and uncertain effects look precise.\n\nUJIVE provides a robust solution that works even when you have hundreds of decision-makers and complex control structures. The leave-one-out principle is simple but profound: by excluding each observation from its own instrument calculation, we break the mechanical correlation that plagues standard approaches.\n\nFor researchers using leniency designs, the message is clear: **use UJIVE, not 2SLS**. Your estimates will be more credible, your standard errors will be honest, and your conclusions will be more reliable.\n\n## References\n\nFarre-Mensa, J., Hegde, D., & Ljungqvist, A. (2020). What is a patent worth? Evidence from the U.S. patent \"lottery\". *The Journal of Finance*, 75(2), 639-682.\n\nGoldsmith-Pinkham, P., Hull, P., & Kolesár, M. (2025). Leniency Designs: An Operator's Manual. Working paper.\n\nImbens, G. W., & Angrist, J. D. (1994). Identification and estimation of local average treatment effects. *Econometrica*, 62(2), 467-475.\n\nKolesár, M. (2013). Estimation in an instrumental variables model with treatment effect heterogeneity. Working paper, Princeton University.\n\n---\n\n*Want to explore more? Check out my other posts on [instrumental variables](link), [causal inference](link), and [applied econometrics](link).*\n",
    "supporting": [
      "2025-11-20-IV_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}