{
  "hash": "c3dc191536cbea8c894afa4a93be4d64",
  "result": {
    "markdown": "---\ntitle: \"What is Linear Discriminant Analysis (LDA)?\"\ndescription: \"A brief overview of Linear Discriminant Analysis (LDA) and how it can be used to classify data.\"\nauthor: \"Jacob Jameson\"\ndate: \"2024-02-15\"\ncategories: [LDA]\noutput:\n  tufte::tufte_html: default\n  tufte::tufte_handout:\n    citation_package: natbib\n    latex_engine: xelatex\n  tufte::tufte_book:\n    citation_package: natbib\n    latex_engine: xelatex\nlink-citations: true\nreference-location: margin\ncomments:\n  utterances:\n    repo: jacobjameson/jacobjameson.github.io\nformat:\n  html:\n    toc: true\n    toc-location: left\npage-layout: full\n---\n\n\n\n\n\nLinear Discriminant Analysis (LDA) is a classic method in statistics and machine learning for classification and dimensionality reduction. LDA is particularly known for its simplicity, efficiency, and interpretability. This post aims to demystify LDA by exploring its mathematical foundations and demonstrating its application through a simulated example in R.\n\n\n## The Essence of LDA\n\nImagine you're at a party and there are two types of fruit on the table: apples and oranges. You're blindfolded and asked to classify the fruits using only a scale (to weigh them) and a ruler (to measure their diameter). Intuitively, you might notice that, generally, oranges are slightly heavier and larger in diameter than apples. LDA does something similar with data; it tries to find the \"scale\" and \"ruler\" (metaphorically speaking) that best separate different classes, like apples and oranges, based on their features.\n\n## The Math Behind the Magic\n\n### Bayes' Theorem at a Glance\n\nLDA is based on Bayes' theorem, a fundamental concept in probability theory. Bayes' theorem tells us how to update our beliefs (probabilities) about an event happening (like identifying an orange) given some evidence (the fruit's weight and diameter). It's about revising our assumptions with new data.\n\n### Assumptions of LDA\nLDA makes a couple of key assumptions:\n\n- Normal Distribution: It assumes that the data points for each class (like our apples and oranges) are normally distributed. This means if you plot the features (weight and diameter), they'll form a bell curve, with most apples (or oranges) near the average, and fewer as you move away from the center.\n\n- Equal Variance: LDA assumes that these bell curves for each class have the same shape, though they might be centered at different points. This is like saying, while apples and oranges might differ in average size and weight, the variation around their averages is similar.\n\n### The LDA Decision Rule\n\nLDA looks for a line (or in more complex cases, a plane or hyperplane) that best separates our classes (apples from oranges) based on their features. It calculates the means (averages) and variances (spread) for each class and then finds the line where the distance between the means is maximized relative to the variance.\n\nMathematically, this involves calculating a score (the discriminant score) for each data point that measures how far it is from each class's mean, adjusted for the overall variance. Data points are then classified based on which score is higher, indicating which class they're closer to.\n\n## Putting LDA into Practice: Simulating Data in R\n\nNow, let's see how this works in practice with our R example. We simulate two classes with distinct means but shared variances, plotting them to visualize the data. We will then fit an LDA model to the data and plot the decision boundary, which shows how LDA separates the two classes based on their features.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate the data\n\nlibrary(MASS) # For lda() and mvrnorm()\nset.seed(100) # Ensure reproducibility\n\nmu1 <- c(-1, 0) # Mean for class 1\nmu2 <- c(1, 0)  # Mean for class 2\nSigma <- matrix(c(2, 1, 1, 2), ncol = 2) # Same covariance matrix for both classes\n\n# Generate data\nclass1 <- mvrnorm(n = 50, mu = mu1, Sigma = Sigma)\nclass2 <- mvrnorm(n = 50, mu = mu2, Sigma = Sigma)\n\ndata <- rbind(class1, class2)\nlabels <- factor(c(rep(0, 50), rep(1, 50)))\ndata <- data.frame(data, class = labels)\n```\n:::\n\n\nWith the data simulated, we can plot it to visualize the distribution of the two classes:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data, aes(x = X1, y = X2, color = class)) +\n  geom_point() + theme_minimal() +\n  labs(title = \"Plot of the Data\",\n       x = \"Feature 1\", y = \"Feature 2\") +\n  scale_color_manual(values = c('#0e4a6b', '#ed7b2e')) +\n  guides(color = guide_legend(title = \"Class\", override.aes = list(size = 3))) \n```\n\n::: {.cell-output-display}\n![](2024-02-15-LDA_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nHaving visualized our data, we proceed to apply LDA to classify these points and find the decision boundary that best separates them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit LDA model\nlda_fit <- lda(class ~ ., data = data)\n```\n:::\n\n\nWe can now visualize the decision boundary that LDA has learned from the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the decision boundary\nx <- seq(-5, 5, length.out = 1000)\ny <- seq(-5, 5, length.out = 1000)\ngrid <- expand.grid(X1 = x, X2 = y)\ngrid$class <- predict(lda_fit, newdata = grid)$class\n\nmeans <- as.data.frame(lda_fit$means)\nmeans$class <- factor(0:1)\nmeans$size <- 5\n\nggplot(data, aes(x = X1, y = X2, color = class)) +\n  geom_point() + theme_minimal() +\n  geom_contour(data = grid, aes(x = X1, y = X2, z = as.numeric(class)), color = \"black\") +\n  geom_point(data = means, aes(x = X1, y = X2), color = 'black', size = 8, shape=18) +\n  geom_point(data = means, aes(x = X1, y = X2, color = class), size = 6, shape=18) +\n  labs(title = \"LDA Decision Boundary\",\n       subtitle = \"Diamonds represent Class Means\",\n       x = \"Feature 1\", y = \"Feature 2\") +\n  scale_color_manual(values = c('#0e4a6b', '#ed7b2e')) +\n  guides(color = guide_legend(title = \"Class\", override.aes = list(size = 3))) \n```\n\n::: {.cell-output-display}\n![](2024-02-15-LDA_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n### What Does This Mean?\n\nWhen we plot the decision boundary found by LDA, we're seeing the line that best distinguishes between our classes based on the data. Points on one side of the line are more likely to be Class 0, and on the other, Class 1. The class means (marked as diamonds on our plot) help us visualize the centers around which our data clusters, and the decision boundary shows how LDA uses these centers to classify the data.\n\n\n## Conclusion\n\nLDA is a powerful and intuitive method for classification and dimensionality reduction. It's a great starting point for understanding more complex methods. By demystifying the math behind LDA and demonstrating its application in R, I hope this post has made LDA more accessible and understandable.\n\nThanks for reading!\n\n\n\n\n\n\n",
    "supporting": [
      "2024-02-15-LDA_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}